{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3d8e92",
   "metadata": {},
   "source": [
    "# 1. Construction for Physics-informed Neural Networks (PINN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f00ea",
   "metadata": {},
   "source": [
    "- Neural Networks play the role as modeling to approximate the PDE solution with given condition.\n",
    "- The Given conditions(Data-driven) solutions in PDE / ODE are set as follows:\n",
    "\n",
    "$$u_t + N[u; \\lambda ] = 0, x \\in \\Omega, t \\in [0,T] $$\n",
    "\n",
    "where $u(t,x)$ denotes the solution, $N[.; \\lambda]$ is a nonlinear operator with parameter $\\lambda$, and the domain $\\Omega$ is in $\\mathbb{R}^D$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161f188",
   "metadata": {},
   "source": [
    "# 2. Solving PDE/ ODE from Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c65111",
   "metadata": {},
   "source": [
    "### 1. PDE / ODE  given boundary condition and initial condition setting\n",
    "\n",
    "The concept involves employing neural networks to solve differential equations. This is achieved by utilizing a neural network to represent the solution and training it to meet the conditions specified by initial condition and boundary condition of the differential equation.\n",
    "\n",
    "Consider a system of partial differential equations:\n",
    "\n",
    "- $ \\frac{du}{dt} = f(u, t) $ where $ t$ is in $[0, 1]$ and\n",
    "\n",
    "- $ u(x, 0) = u_0 $ as given boundary condition.\n",
    "\n",
    "- $ u(0, t) = g_1(x)$ and $ u(L, t) = g_2(x)$ as given initial condition where $\\Omega = [0, L]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6ea96",
   "metadata": {},
   "source": [
    "### 2. Solving the PDE / ODE\n",
    "To solve this, we approximate the solution as a result of a neural network:\n",
    "\n",
    "$$ NN(t) \\approx u(t) $$\n",
    "\n",
    "For the assumption as $NN(t)$ were the true solution, then we know $ NN'(t) = f(NN(t), t) $. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7948e",
   "metadata": {},
   "source": [
    "### 3.  Error estimaties by Loss(Cost) function \n",
    "Based on the above $NN(t)$, our loss function is:\n",
    "\n",
    "$$L(\\omega) = \\sum_i \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i), t_i)\\right)^2 $$\n",
    "\n",
    "The $ t_i $ can be chosen in various ways, such as randomly or on a grid. When this loss function is minimized, we have $ \\frac{dNN(t_i)}{dt} \\approx f(NN(t_i), t_i) $, and thus $ NN(t)$ approximately solves the differential equation.\n",
    "\n",
    "On the PDE domain $ \\Omega \\times [0, t_0]$ of $(x,t)$, to reflect the boundary condition $u_0 = u (x, 0)$ and boundary condition $u(0, t), u(L, t)$ of our solution, these terms is added to the loss function:\n",
    "\n",
    "$$ L(\\omega) = \\sum_i [ \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i), t_i)\\right)^2 + (NN(0) - u_0)^2 + (NN(0,t) - u(0,t))^2+ (NN(L,t) - u(L,t))^2] $$\n",
    "\n",
    "Here, $ \\omega $  represents the parameters defining the neural network $ NN $ that approximates $ u $. This process result in finding weights that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02e50e",
   "metadata": {},
   "source": [
    "# 3. Example - One Dimensional Heat Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4abf5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "efe9ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76cc77",
   "metadata": {},
   "source": [
    "### 3.1 Define the class for Neural Network\n",
    "We designate Net as our solution $u_\\omega (x, t)$.\n",
    "\n",
    "When constructing the network, it's crucial to consider the number of inputs and outputs. For the input variables, there are x and t. For the output variables, there is one variable u.\n",
    "\n",
    "We can incorporate as many hidden layers as needed, each with as many neurons as desired. A more intricate network is better equipped to discover complex solutions, but it also demands more data.\n",
    "\n",
    "Now, create this network as the following codes.\n",
    "Minimum 8 hidden layers, each containing 5 neurons. The first def code indicates linear transformation of Neural Network, and the second def code means transfromation through Sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0d2ec62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(2,5) # number of input variables = 2, output variables = 5\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.hidden_layer5 = nn.Linear(5,5)\n",
    "        self.hidden_layer6 = nn.Linear(5,5)\n",
    "        self.hidden_layer7 = nn.Linear(5,5)\n",
    "        self.hidden_layer8 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self, x,t):\n",
    "        inputs = torch.cat([x,t],axis=1) # Merged two single-column arrays into a single array with two columns.\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs)) #Sigmoid function -> σ(x)= 1/(1+exp⁡(-x))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        layer6_out = torch.sigmoid(self.hidden_layer6(layer5_out))\n",
    "        layer7_out = torch.sigmoid(self.hidden_layer7(layer6_out))\n",
    "        layer8_out = torch.sigmoid(self.hidden_layer8(layer7_out))\n",
    "        output = self.output_layer(layer8_out)\n",
    "        ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "# Result of the above class: \n",
    "# transformation x,t -> u -> sigmoid(u) through each hiddeon layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96703273",
   "metadata": {},
   "source": [
    "### 3.2 Weight update setting - Optimizer\n",
    "To iterate the nueral network, we update the weight of the linear model we set in chapter 2 by using Adam Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8611185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "\n",
    "# Set cost function as Mean squared error\n",
    "mse_cost_function = torch.nn.MSELoss()\n",
    "\n",
    "#Run Adam algorithm with output of the neural network processing.\n",
    "optimizer = torch.optim.Adam(net.parameters()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714359a",
   "metadata": {},
   "source": [
    "### 3.3 Setting PDE with drawn u(x,t)\n",
    "Employing the u(x,t) in the previous step, we set the 1-dimensional equation function f(x,t) for $ x \\in [-2, 2]$ and $ t \\in [0,1]$. Also we set its loss(cost) function to optimize the error bound.\n",
    "The Heat equation setting is as follows with the code below:\n",
    "\n",
    "The function $u(x, t)$, representing heat flow, needs to adhere to the partial differential equation below.\n",
    "$$u_t = u_{xx}$$\n",
    "Moreover, we impose two additional conditions. Initially, we set the temperature at both ends of the domain of $x$, specifying u(0, t) and u(L, t). Therefore, we assume that both ends are maintained at a temperature of 0 degrees Celsius:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6503f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Differential Equation (PDE) and its associated loss function.\n",
    "def pde_loss(x, t, net):\n",
    "    # The dependent variable 'u' is obtained from the network based on independent variables x and t.\n",
    "    u = net(x, t)     \n",
    "    # Before defining the PDE = du/dt - d^2(u)/(dx)^2, we need to compute du/dx and du/dt.\n",
    "    u_x = torch.autograd.grad(u,x,torch.ones(u.shape),retain_graph=True,create_graph=True)[0]\n",
    "    #torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x,x,torch.ones(u_x.shape),create_graph=True)[0] \n",
    "    #torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "    # torch.autograd.grad <- Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_t - u_xx\n",
    "    return pde\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b14038",
   "metadata": {},
   "source": [
    "### 3.4 Boundary condition setting\n",
    "Boundary condition for the heat equation is given as $u(x,0) = sin(\\pi x)$ as follows. For the training data on the domain, is it used as the initial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e1107c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Boundary Conditions\n",
    "# Initial condition: u(x,0) = sin(pi * x)\n",
    "# Boundary conditions provide initial data points for training.\n",
    "\n",
    "# Boundary conditions state that for any x in the range [-2, 2] and time in [0, 1], the value of u is given by sin(pi * x).\n",
    "# Generate 500 random numbers for x\n",
    "x_bc = (2 - (-2)) *torch.rand(500,1) - 2\n",
    "#np.random.uniform(low=0.0, high=6.0, size=(500,1))\n",
    "t_bc = torch.zeros(500,1)\n",
    "\n",
    "t_inner = torch.rand(500,1)\n",
    "#np.zeros((500,1))\n",
    "# Compute u based on the boundary condition: u(x,0) = sin(pi * x)\n",
    "u_initial1 = t_bc\n",
    "u_initial2 = t_bc\n",
    "u_bc = np.sin(np.pi *x_bc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632fac4",
   "metadata": {},
   "source": [
    "### 3.5 Fitting model with training with iteration 20000\n",
    "We train the data while optimizing the weight in the linear model of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dfb3be72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Loss: tensor(0.6745)\n",
      "1 Training Loss: tensor(0.6706)\n",
      "2 Training Loss: tensor(0.6667)\n",
      "3 Training Loss: tensor(0.6629)\n",
      "4 Training Loss: tensor(0.6591)\n",
      "5 Training Loss: tensor(0.6554)\n",
      "6 Training Loss: tensor(0.6517)\n",
      "7 Training Loss: tensor(0.6481)\n",
      "8 Training Loss: tensor(0.6445)\n",
      "9 Training Loss: tensor(0.6409)\n",
      "10 Training Loss: tensor(0.6374)\n",
      "11 Training Loss: tensor(0.6340)\n",
      "12 Training Loss: tensor(0.6306)\n",
      "13 Training Loss: tensor(0.6272)\n",
      "14 Training Loss: tensor(0.6239)\n",
      "15 Training Loss: tensor(0.6207)\n",
      "16 Training Loss: tensor(0.6175)\n",
      "17 Training Loss: tensor(0.6143)\n",
      "18 Training Loss: tensor(0.6112)\n",
      "19 Training Loss: tensor(0.6082)\n",
      "20 Training Loss: tensor(0.6052)\n",
      "21 Training Loss: tensor(0.6022)\n",
      "22 Training Loss: tensor(0.5993)\n",
      "23 Training Loss: tensor(0.5965)\n",
      "24 Training Loss: tensor(0.5937)\n",
      "25 Training Loss: tensor(0.5909)\n",
      "26 Training Loss: tensor(0.5883)\n",
      "27 Training Loss: tensor(0.5856)\n",
      "28 Training Loss: tensor(0.5830)\n",
      "29 Training Loss: tensor(0.5805)\n",
      "30 Training Loss: tensor(0.5780)\n",
      "31 Training Loss: tensor(0.5756)\n",
      "32 Training Loss: tensor(0.5732)\n",
      "33 Training Loss: tensor(0.5709)\n",
      "34 Training Loss: tensor(0.5686)\n",
      "35 Training Loss: tensor(0.5664)\n",
      "36 Training Loss: tensor(0.5642)\n",
      "37 Training Loss: tensor(0.5621)\n",
      "38 Training Loss: tensor(0.5600)\n",
      "39 Training Loss: tensor(0.5579)\n",
      "40 Training Loss: tensor(0.5559)\n",
      "41 Training Loss: tensor(0.5540)\n",
      "42 Training Loss: tensor(0.5521)\n",
      "43 Training Loss: tensor(0.5502)\n",
      "44 Training Loss: tensor(0.5484)\n",
      "45 Training Loss: tensor(0.5467)\n",
      "46 Training Loss: tensor(0.5450)\n",
      "47 Training Loss: tensor(0.5433)\n",
      "48 Training Loss: tensor(0.5417)\n",
      "49 Training Loss: tensor(0.5401)\n",
      "50 Training Loss: tensor(0.5385)\n",
      "51 Training Loss: tensor(0.5370)\n",
      "52 Training Loss: tensor(0.5355)\n",
      "53 Training Loss: tensor(0.5341)\n",
      "54 Training Loss: tensor(0.5327)\n",
      "55 Training Loss: tensor(0.5314)\n",
      "56 Training Loss: tensor(0.5301)\n",
      "57 Training Loss: tensor(0.5288)\n",
      "58 Training Loss: tensor(0.5276)\n",
      "59 Training Loss: tensor(0.5264)\n",
      "60 Training Loss: tensor(0.5252)\n",
      "61 Training Loss: tensor(0.5241)\n",
      "62 Training Loss: tensor(0.5230)\n",
      "63 Training Loss: tensor(0.5219)\n",
      "64 Training Loss: tensor(0.5209)\n",
      "65 Training Loss: tensor(0.5199)\n",
      "66 Training Loss: tensor(0.5189)\n",
      "67 Training Loss: tensor(0.5180)\n",
      "68 Training Loss: tensor(0.5171)\n",
      "69 Training Loss: tensor(0.5162)\n",
      "70 Training Loss: tensor(0.5153)\n",
      "71 Training Loss: tensor(0.5145)\n",
      "72 Training Loss: tensor(0.5137)\n",
      "73 Training Loss: tensor(0.5129)\n",
      "74 Training Loss: tensor(0.5122)\n",
      "75 Training Loss: tensor(0.5115)\n",
      "76 Training Loss: tensor(0.5108)\n",
      "77 Training Loss: tensor(0.5101)\n",
      "78 Training Loss: tensor(0.5095)\n",
      "79 Training Loss: tensor(0.5089)\n",
      "80 Training Loss: tensor(0.5083)\n",
      "81 Training Loss: tensor(0.5077)\n",
      "82 Training Loss: tensor(0.5072)\n",
      "83 Training Loss: tensor(0.5066)\n",
      "84 Training Loss: tensor(0.5061)\n",
      "85 Training Loss: tensor(0.5056)\n",
      "86 Training Loss: tensor(0.5051)\n",
      "87 Training Loss: tensor(0.5047)\n",
      "88 Training Loss: tensor(0.5043)\n",
      "89 Training Loss: tensor(0.5038)\n",
      "90 Training Loss: tensor(0.5034)\n",
      "91 Training Loss: tensor(0.5031)\n",
      "92 Training Loss: tensor(0.5027)\n",
      "93 Training Loss: tensor(0.5023)\n",
      "94 Training Loss: tensor(0.5020)\n",
      "95 Training Loss: tensor(0.5017)\n",
      "96 Training Loss: tensor(0.5014)\n",
      "97 Training Loss: tensor(0.5011)\n",
      "98 Training Loss: tensor(0.5008)\n",
      "99 Training Loss: tensor(0.5005)\n",
      "100 Training Loss: tensor(0.5002)\n",
      "101 Training Loss: tensor(0.5000)\n",
      "102 Training Loss: tensor(0.4998)\n",
      "103 Training Loss: tensor(0.4995)\n",
      "104 Training Loss: tensor(0.4993)\n",
      "105 Training Loss: tensor(0.4991)\n",
      "106 Training Loss: tensor(0.4989)\n",
      "107 Training Loss: tensor(0.4987)\n",
      "108 Training Loss: tensor(0.4986)\n",
      "109 Training Loss: tensor(0.4984)\n",
      "110 Training Loss: tensor(0.4982)\n",
      "111 Training Loss: tensor(0.4981)\n",
      "112 Training Loss: tensor(0.4979)\n",
      "113 Training Loss: tensor(0.4978)\n",
      "114 Training Loss: tensor(0.4977)\n",
      "115 Training Loss: tensor(0.4975)\n",
      "116 Training Loss: tensor(0.4974)\n",
      "117 Training Loss: tensor(0.4973)\n",
      "118 Training Loss: tensor(0.4972)\n",
      "119 Training Loss: tensor(0.4971)\n",
      "120 Training Loss: tensor(0.4970)\n",
      "121 Training Loss: tensor(0.4969)\n",
      "122 Training Loss: tensor(0.4968)\n",
      "123 Training Loss: tensor(0.4968)\n",
      "124 Training Loss: tensor(0.4967)\n",
      "125 Training Loss: tensor(0.4966)\n",
      "126 Training Loss: tensor(0.4965)\n",
      "127 Training Loss: tensor(0.4965)\n",
      "128 Training Loss: tensor(0.4964)\n",
      "129 Training Loss: tensor(0.4964)\n",
      "130 Training Loss: tensor(0.4963)\n",
      "131 Training Loss: tensor(0.4963)\n",
      "132 Training Loss: tensor(0.4962)\n",
      "133 Training Loss: tensor(0.4962)\n",
      "134 Training Loss: tensor(0.4961)\n",
      "135 Training Loss: tensor(0.4961)\n",
      "136 Training Loss: tensor(0.4961)\n",
      "137 Training Loss: tensor(0.4960)\n",
      "138 Training Loss: tensor(0.4960)\n",
      "139 Training Loss: tensor(0.4960)\n",
      "140 Training Loss: tensor(0.4959)\n",
      "141 Training Loss: tensor(0.4959)\n",
      "142 Training Loss: tensor(0.4959)\n",
      "143 Training Loss: tensor(0.4959)\n",
      "144 Training Loss: tensor(0.4958)\n",
      "145 Training Loss: tensor(0.4958)\n",
      "146 Training Loss: tensor(0.4958)\n",
      "147 Training Loss: tensor(0.4958)\n",
      "148 Training Loss: tensor(0.4958)\n",
      "149 Training Loss: tensor(0.4958)\n",
      "150 Training Loss: tensor(0.4957)\n",
      "151 Training Loss: tensor(0.4957)\n",
      "152 Training Loss: tensor(0.4957)\n",
      "153 Training Loss: tensor(0.4957)\n",
      "154 Training Loss: tensor(0.4957)\n",
      "155 Training Loss: tensor(0.4957)\n",
      "156 Training Loss: tensor(0.4957)\n",
      "157 Training Loss: tensor(0.4957)\n",
      "158 Training Loss: tensor(0.4957)\n",
      "159 Training Loss: tensor(0.4957)\n",
      "160 Training Loss: tensor(0.4957)\n",
      "161 Training Loss: tensor(0.4956)\n",
      "162 Training Loss: tensor(0.4956)\n",
      "163 Training Loss: tensor(0.4956)\n",
      "164 Training Loss: tensor(0.4956)\n",
      "165 Training Loss: tensor(0.4956)\n",
      "166 Training Loss: tensor(0.4956)\n",
      "167 Training Loss: tensor(0.4956)\n",
      "168 Training Loss: tensor(0.4956)\n",
      "169 Training Loss: tensor(0.4956)\n",
      "170 Training Loss: tensor(0.4956)\n",
      "171 Training Loss: tensor(0.4956)\n",
      "172 Training Loss: tensor(0.4956)\n",
      "173 Training Loss: tensor(0.4956)\n",
      "174 Training Loss: tensor(0.4956)\n",
      "175 Training Loss: tensor(0.4956)\n",
      "176 Training Loss: tensor(0.4956)\n",
      "177 Training Loss: tensor(0.4956)\n",
      "178 Training Loss: tensor(0.4956)\n",
      "179 Training Loss: tensor(0.4956)\n",
      "180 Training Loss: tensor(0.4956)\n",
      "181 Training Loss: tensor(0.4956)\n",
      "182 Training Loss: tensor(0.4956)\n",
      "183 Training Loss: tensor(0.4956)\n",
      "184 Training Loss: tensor(0.4956)\n",
      "185 Training Loss: tensor(0.4956)\n",
      "186 Training Loss: tensor(0.4956)\n",
      "187 Training Loss: tensor(0.4956)\n",
      "188 Training Loss: tensor(0.4956)\n",
      "189 Training Loss: tensor(0.4956)\n",
      "190 Training Loss: tensor(0.4956)\n",
      "191 Training Loss: tensor(0.4956)\n",
      "192 Training Loss: tensor(0.4956)\n",
      "193 Training Loss: tensor(0.4956)\n",
      "194 Training Loss: tensor(0.4956)\n",
      "195 Training Loss: tensor(0.4956)\n",
      "196 Training Loss: tensor(0.4956)\n",
      "197 Training Loss: tensor(0.4956)\n",
      "198 Training Loss: tensor(0.4956)\n",
      "199 Training Loss: tensor(0.4956)\n",
      "200 Training Loss: tensor(0.4956)\n",
      "201 Training Loss: tensor(0.4956)\n",
      "202 Training Loss: tensor(0.4956)\n",
      "203 Training Loss: tensor(0.4956)\n",
      "204 Training Loss: tensor(0.4956)\n",
      "205 Training Loss: tensor(0.4956)\n",
      "206 Training Loss: tensor(0.4956)\n",
      "207 Training Loss: tensor(0.4956)\n",
      "208 Training Loss: tensor(0.4956)\n",
      "209 Training Loss: tensor(0.4956)\n",
      "210 Training Loss: tensor(0.4956)\n",
      "211 Training Loss: tensor(0.4956)\n",
      "212 Training Loss: tensor(0.4956)\n",
      "213 Training Loss: tensor(0.4956)\n",
      "214 Training Loss: tensor(0.4956)\n",
      "215 Training Loss: tensor(0.4956)\n",
      "216 Training Loss: tensor(0.4956)\n",
      "217 Training Loss: tensor(0.4956)\n",
      "218 Training Loss: tensor(0.4956)\n",
      "219 Training Loss: tensor(0.4956)\n",
      "220 Training Loss: tensor(0.4956)\n",
      "221 Training Loss: tensor(0.4956)\n",
      "222 Training Loss: tensor(0.4956)\n",
      "223 Training Loss: tensor(0.4956)\n",
      "224 Training Loss: tensor(0.4956)\n",
      "225 Training Loss: tensor(0.4956)\n",
      "226 Training Loss: tensor(0.4956)\n",
      "227 Training Loss: tensor(0.4956)\n",
      "228 Training Loss: tensor(0.4956)\n",
      "229 Training Loss: tensor(0.4956)\n",
      "230 Training Loss: tensor(0.4956)\n",
      "231 Training Loss: tensor(0.4956)\n",
      "232 Training Loss: tensor(0.4956)\n",
      "233 Training Loss: tensor(0.4956)\n",
      "234 Training Loss: tensor(0.4956)\n",
      "235 Training Loss: tensor(0.4956)\n",
      "236 Training Loss: tensor(0.4956)\n",
      "237 Training Loss: tensor(0.4956)\n",
      "238 Training Loss: tensor(0.4956)\n",
      "239 Training Loss: tensor(0.4956)\n",
      "240 Training Loss: tensor(0.4956)\n",
      "241 Training Loss: tensor(0.4956)\n",
      "242 Training Loss: tensor(0.4956)\n",
      "243 Training Loss: tensor(0.4956)\n",
      "244 Training Loss: tensor(0.4956)\n",
      "245 Training Loss: tensor(0.4956)\n",
      "246 Training Loss: tensor(0.4956)\n",
      "247 Training Loss: tensor(0.4956)\n",
      "248 Training Loss: tensor(0.4956)\n",
      "249 Training Loss: tensor(0.4956)\n",
      "250 Training Loss: tensor(0.4956)\n",
      "251 Training Loss: tensor(0.4956)\n",
      "252 Training Loss: tensor(0.4956)\n",
      "253 Training Loss: tensor(0.4956)\n",
      "254 Training Loss: tensor(0.4956)\n",
      "255 Training Loss: tensor(0.4956)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 Training Loss: tensor(0.4956)\n",
      "257 Training Loss: tensor(0.4956)\n",
      "258 Training Loss: tensor(0.4956)\n",
      "259 Training Loss: tensor(0.4956)\n",
      "260 Training Loss: tensor(0.4956)\n",
      "261 Training Loss: tensor(0.4956)\n",
      "262 Training Loss: tensor(0.4956)\n",
      "263 Training Loss: tensor(0.4956)\n",
      "264 Training Loss: tensor(0.4956)\n",
      "265 Training Loss: tensor(0.4956)\n",
      "266 Training Loss: tensor(0.4956)\n",
      "267 Training Loss: tensor(0.4956)\n",
      "268 Training Loss: tensor(0.4956)\n",
      "269 Training Loss: tensor(0.4956)\n",
      "270 Training Loss: tensor(0.4956)\n",
      "271 Training Loss: tensor(0.4956)\n",
      "272 Training Loss: tensor(0.4956)\n",
      "273 Training Loss: tensor(0.4956)\n",
      "274 Training Loss: tensor(0.4956)\n",
      "275 Training Loss: tensor(0.4956)\n",
      "276 Training Loss: tensor(0.4956)\n",
      "277 Training Loss: tensor(0.4956)\n",
      "278 Training Loss: tensor(0.4956)\n",
      "279 Training Loss: tensor(0.4956)\n",
      "280 Training Loss: tensor(0.4956)\n",
      "281 Training Loss: tensor(0.4956)\n",
      "282 Training Loss: tensor(0.4956)\n",
      "283 Training Loss: tensor(0.4956)\n",
      "284 Training Loss: tensor(0.4956)\n",
      "285 Training Loss: tensor(0.4956)\n",
      "286 Training Loss: tensor(0.4956)\n",
      "287 Training Loss: tensor(0.4956)\n",
      "288 Training Loss: tensor(0.4956)\n",
      "289 Training Loss: tensor(0.4956)\n",
      "290 Training Loss: tensor(0.4956)\n",
      "291 Training Loss: tensor(0.4956)\n",
      "292 Training Loss: tensor(0.4956)\n",
      "293 Training Loss: tensor(0.4956)\n",
      "294 Training Loss: tensor(0.4956)\n",
      "295 Training Loss: tensor(0.4956)\n",
      "296 Training Loss: tensor(0.4956)\n",
      "297 Training Loss: tensor(0.4956)\n",
      "298 Training Loss: tensor(0.4956)\n",
      "299 Training Loss: tensor(0.4956)\n",
      "300 Training Loss: tensor(0.4956)\n",
      "301 Training Loss: tensor(0.4956)\n",
      "302 Training Loss: tensor(0.4956)\n",
      "303 Training Loss: tensor(0.4956)\n",
      "304 Training Loss: tensor(0.4956)\n",
      "305 Training Loss: tensor(0.4956)\n",
      "306 Training Loss: tensor(0.4956)\n",
      "307 Training Loss: tensor(0.4956)\n",
      "308 Training Loss: tensor(0.4956)\n",
      "309 Training Loss: tensor(0.4956)\n",
      "310 Training Loss: tensor(0.4956)\n",
      "311 Training Loss: tensor(0.4956)\n",
      "312 Training Loss: tensor(0.4956)\n",
      "313 Training Loss: tensor(0.4956)\n",
      "314 Training Loss: tensor(0.4956)\n",
      "315 Training Loss: tensor(0.4956)\n",
      "316 Training Loss: tensor(0.4956)\n",
      "317 Training Loss: tensor(0.4956)\n",
      "318 Training Loss: tensor(0.4956)\n",
      "319 Training Loss: tensor(0.4956)\n",
      "320 Training Loss: tensor(0.4956)\n",
      "321 Training Loss: tensor(0.4956)\n",
      "322 Training Loss: tensor(0.4956)\n",
      "323 Training Loss: tensor(0.4956)\n",
      "324 Training Loss: tensor(0.4956)\n",
      "325 Training Loss: tensor(0.4956)\n",
      "326 Training Loss: tensor(0.4956)\n",
      "327 Training Loss: tensor(0.4956)\n",
      "328 Training Loss: tensor(0.4956)\n",
      "329 Training Loss: tensor(0.4956)\n",
      "330 Training Loss: tensor(0.4956)\n",
      "331 Training Loss: tensor(0.4956)\n",
      "332 Training Loss: tensor(0.4956)\n",
      "333 Training Loss: tensor(0.4956)\n",
      "334 Training Loss: tensor(0.4956)\n",
      "335 Training Loss: tensor(0.4956)\n",
      "336 Training Loss: tensor(0.4956)\n",
      "337 Training Loss: tensor(0.4956)\n",
      "338 Training Loss: tensor(0.4956)\n",
      "339 Training Loss: tensor(0.4956)\n",
      "340 Training Loss: tensor(0.4956)\n",
      "341 Training Loss: tensor(0.4956)\n",
      "342 Training Loss: tensor(0.4956)\n",
      "343 Training Loss: tensor(0.4956)\n",
      "344 Training Loss: tensor(0.4956)\n",
      "345 Training Loss: tensor(0.4956)\n",
      "346 Training Loss: tensor(0.4956)\n",
      "347 Training Loss: tensor(0.4956)\n",
      "348 Training Loss: tensor(0.4956)\n",
      "349 Training Loss: tensor(0.4956)\n",
      "350 Training Loss: tensor(0.4956)\n",
      "351 Training Loss: tensor(0.4956)\n",
      "352 Training Loss: tensor(0.4956)\n",
      "353 Training Loss: tensor(0.4956)\n",
      "354 Training Loss: tensor(0.4956)\n",
      "355 Training Loss: tensor(0.4956)\n",
      "356 Training Loss: tensor(0.4956)\n",
      "357 Training Loss: tensor(0.4956)\n",
      "358 Training Loss: tensor(0.4956)\n",
      "359 Training Loss: tensor(0.4956)\n",
      "360 Training Loss: tensor(0.4956)\n",
      "361 Training Loss: tensor(0.4956)\n",
      "362 Training Loss: tensor(0.4956)\n",
      "363 Training Loss: tensor(0.4956)\n",
      "364 Training Loss: tensor(0.4956)\n",
      "365 Training Loss: tensor(0.4956)\n",
      "366 Training Loss: tensor(0.4956)\n",
      "367 Training Loss: tensor(0.4956)\n",
      "368 Training Loss: tensor(0.4956)\n",
      "369 Training Loss: tensor(0.4956)\n",
      "370 Training Loss: tensor(0.4956)\n",
      "371 Training Loss: tensor(0.4956)\n",
      "372 Training Loss: tensor(0.4956)\n",
      "373 Training Loss: tensor(0.4956)\n",
      "374 Training Loss: tensor(0.4956)\n",
      "375 Training Loss: tensor(0.4956)\n",
      "376 Training Loss: tensor(0.4956)\n",
      "377 Training Loss: tensor(0.4956)\n",
      "378 Training Loss: tensor(0.4956)\n",
      "379 Training Loss: tensor(0.4956)\n",
      "380 Training Loss: tensor(0.4956)\n",
      "381 Training Loss: tensor(0.4956)\n",
      "382 Training Loss: tensor(0.4956)\n",
      "383 Training Loss: tensor(0.4956)\n",
      "384 Training Loss: tensor(0.4956)\n",
      "385 Training Loss: tensor(0.4956)\n",
      "386 Training Loss: tensor(0.4956)\n",
      "387 Training Loss: tensor(0.4956)\n",
      "388 Training Loss: tensor(0.4956)\n",
      "389 Training Loss: tensor(0.4956)\n",
      "390 Training Loss: tensor(0.4956)\n",
      "391 Training Loss: tensor(0.4956)\n",
      "392 Training Loss: tensor(0.4956)\n",
      "393 Training Loss: tensor(0.4956)\n",
      "394 Training Loss: tensor(0.4956)\n",
      "395 Training Loss: tensor(0.4956)\n",
      "396 Training Loss: tensor(0.4956)\n",
      "397 Training Loss: tensor(0.4956)\n",
      "398 Training Loss: tensor(0.4956)\n",
      "399 Training Loss: tensor(0.4956)\n",
      "400 Training Loss: tensor(0.4956)\n",
      "401 Training Loss: tensor(0.4956)\n",
      "402 Training Loss: tensor(0.4956)\n",
      "403 Training Loss: tensor(0.4956)\n",
      "404 Training Loss: tensor(0.4956)\n",
      "405 Training Loss: tensor(0.4956)\n",
      "406 Training Loss: tensor(0.4956)\n",
      "407 Training Loss: tensor(0.4956)\n",
      "408 Training Loss: tensor(0.4956)\n",
      "409 Training Loss: tensor(0.4956)\n",
      "410 Training Loss: tensor(0.4956)\n",
      "411 Training Loss: tensor(0.4956)\n",
      "412 Training Loss: tensor(0.4956)\n",
      "413 Training Loss: tensor(0.4956)\n",
      "414 Training Loss: tensor(0.4956)\n",
      "415 Training Loss: tensor(0.4956)\n",
      "416 Training Loss: tensor(0.4956)\n",
      "417 Training Loss: tensor(0.4956)\n",
      "418 Training Loss: tensor(0.4956)\n",
      "419 Training Loss: tensor(0.4956)\n",
      "420 Training Loss: tensor(0.4956)\n",
      "421 Training Loss: tensor(0.4956)\n",
      "422 Training Loss: tensor(0.4956)\n",
      "423 Training Loss: tensor(0.4956)\n",
      "424 Training Loss: tensor(0.4956)\n",
      "425 Training Loss: tensor(0.4956)\n",
      "426 Training Loss: tensor(0.4956)\n",
      "427 Training Loss: tensor(0.4956)\n",
      "428 Training Loss: tensor(0.4956)\n",
      "429 Training Loss: tensor(0.4956)\n",
      "430 Training Loss: tensor(0.4956)\n",
      "431 Training Loss: tensor(0.4956)\n",
      "432 Training Loss: tensor(0.4956)\n",
      "433 Training Loss: tensor(0.4956)\n",
      "434 Training Loss: tensor(0.4956)\n",
      "435 Training Loss: tensor(0.4956)\n",
      "436 Training Loss: tensor(0.4956)\n",
      "437 Training Loss: tensor(0.4956)\n",
      "438 Training Loss: tensor(0.4956)\n",
      "439 Training Loss: tensor(0.4956)\n",
      "440 Training Loss: tensor(0.4956)\n",
      "441 Training Loss: tensor(0.4956)\n",
      "442 Training Loss: tensor(0.4956)\n",
      "443 Training Loss: tensor(0.4956)\n",
      "444 Training Loss: tensor(0.4956)\n",
      "445 Training Loss: tensor(0.4956)\n",
      "446 Training Loss: tensor(0.4956)\n",
      "447 Training Loss: tensor(0.4956)\n",
      "448 Training Loss: tensor(0.4956)\n",
      "449 Training Loss: tensor(0.4956)\n",
      "450 Training Loss: tensor(0.4956)\n",
      "451 Training Loss: tensor(0.4956)\n",
      "452 Training Loss: tensor(0.4956)\n",
      "453 Training Loss: tensor(0.4956)\n",
      "454 Training Loss: tensor(0.4956)\n",
      "455 Training Loss: tensor(0.4956)\n",
      "456 Training Loss: tensor(0.4956)\n",
      "457 Training Loss: tensor(0.4956)\n",
      "458 Training Loss: tensor(0.4956)\n",
      "459 Training Loss: tensor(0.4956)\n",
      "460 Training Loss: tensor(0.4956)\n",
      "461 Training Loss: tensor(0.4956)\n",
      "462 Training Loss: tensor(0.4956)\n",
      "463 Training Loss: tensor(0.4956)\n",
      "464 Training Loss: tensor(0.4956)\n",
      "465 Training Loss: tensor(0.4956)\n",
      "466 Training Loss: tensor(0.4956)\n",
      "467 Training Loss: tensor(0.4956)\n",
      "468 Training Loss: tensor(0.4956)\n",
      "469 Training Loss: tensor(0.4956)\n",
      "470 Training Loss: tensor(0.4956)\n",
      "471 Training Loss: tensor(0.4956)\n",
      "472 Training Loss: tensor(0.4956)\n",
      "473 Training Loss: tensor(0.4956)\n",
      "474 Training Loss: tensor(0.4956)\n",
      "475 Training Loss: tensor(0.4956)\n",
      "476 Training Loss: tensor(0.4956)\n",
      "477 Training Loss: tensor(0.4956)\n",
      "478 Training Loss: tensor(0.4956)\n",
      "479 Training Loss: tensor(0.4956)\n",
      "480 Training Loss: tensor(0.4956)\n",
      "481 Training Loss: tensor(0.4956)\n",
      "482 Training Loss: tensor(0.4956)\n",
      "483 Training Loss: tensor(0.4956)\n",
      "484 Training Loss: tensor(0.4956)\n",
      "485 Training Loss: tensor(0.4956)\n",
      "486 Training Loss: tensor(0.4956)\n",
      "487 Training Loss: tensor(0.4956)\n",
      "488 Training Loss: tensor(0.4956)\n",
      "489 Training Loss: tensor(0.4956)\n",
      "490 Training Loss: tensor(0.4956)\n",
      "491 Training Loss: tensor(0.4956)\n",
      "492 Training Loss: tensor(0.4956)\n",
      "493 Training Loss: tensor(0.4956)\n",
      "494 Training Loss: tensor(0.4956)\n",
      "495 Training Loss: tensor(0.4956)\n",
      "496 Training Loss: tensor(0.4956)\n",
      "497 Training Loss: tensor(0.4956)\n",
      "498 Training Loss: tensor(0.4956)\n",
      "499 Training Loss: tensor(0.4956)\n",
      "500 Training Loss: tensor(0.4956)\n",
      "501 Training Loss: tensor(0.4956)\n",
      "502 Training Loss: tensor(0.4956)\n",
      "503 Training Loss: tensor(0.4956)\n",
      "504 Training Loss: tensor(0.4956)\n",
      "505 Training Loss: tensor(0.4956)\n",
      "506 Training Loss: tensor(0.4956)\n",
      "507 Training Loss: tensor(0.4956)\n",
      "508 Training Loss: tensor(0.4956)\n",
      "509 Training Loss: tensor(0.4956)\n",
      "510 Training Loss: tensor(0.4956)\n",
      "511 Training Loss: tensor(0.4956)\n",
      "512 Training Loss: tensor(0.4956)\n",
      "513 Training Loss: tensor(0.4956)\n",
      "514 Training Loss: tensor(0.4956)\n",
      "515 Training Loss: tensor(0.4956)\n",
      "516 Training Loss: tensor(0.4956)\n",
      "517 Training Loss: tensor(0.4956)\n",
      "518 Training Loss: tensor(0.4956)\n",
      "519 Training Loss: tensor(0.4956)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520 Training Loss: tensor(0.4956)\n",
      "521 Training Loss: tensor(0.4956)\n",
      "522 Training Loss: tensor(0.4956)\n",
      "523 Training Loss: tensor(0.4956)\n",
      "524 Training Loss: tensor(0.4956)\n",
      "525 Training Loss: tensor(0.4956)\n",
      "526 Training Loss: tensor(0.4956)\n",
      "527 Training Loss: tensor(0.4956)\n",
      "528 Training Loss: tensor(0.4956)\n",
      "529 Training Loss: tensor(0.4956)\n",
      "530 Training Loss: tensor(0.4956)\n",
      "531 Training Loss: tensor(0.4956)\n",
      "532 Training Loss: tensor(0.4956)\n",
      "533 Training Loss: tensor(0.4956)\n",
      "534 Training Loss: tensor(0.4956)\n",
      "535 Training Loss: tensor(0.4956)\n",
      "536 Training Loss: tensor(0.4956)\n",
      "537 Training Loss: tensor(0.4956)\n",
      "538 Training Loss: tensor(0.4956)\n",
      "539 Training Loss: tensor(0.4956)\n",
      "540 Training Loss: tensor(0.4956)\n",
      "541 Training Loss: tensor(0.4956)\n",
      "542 Training Loss: tensor(0.4956)\n",
      "543 Training Loss: tensor(0.4956)\n",
      "544 Training Loss: tensor(0.4956)\n",
      "545 Training Loss: tensor(0.4956)\n",
      "546 Training Loss: tensor(0.4956)\n",
      "547 Training Loss: tensor(0.4956)\n",
      "548 Training Loss: tensor(0.4956)\n",
      "549 Training Loss: tensor(0.4956)\n",
      "550 Training Loss: tensor(0.4956)\n",
      "551 Training Loss: tensor(0.4956)\n",
      "552 Training Loss: tensor(0.4956)\n",
      "553 Training Loss: tensor(0.4956)\n",
      "554 Training Loss: tensor(0.4956)\n",
      "555 Training Loss: tensor(0.4956)\n",
      "556 Training Loss: tensor(0.4956)\n",
      "557 Training Loss: tensor(0.4956)\n",
      "558 Training Loss: tensor(0.4956)\n",
      "559 Training Loss: tensor(0.4956)\n",
      "560 Training Loss: tensor(0.4956)\n",
      "561 Training Loss: tensor(0.4956)\n",
      "562 Training Loss: tensor(0.4956)\n",
      "563 Training Loss: tensor(0.4956)\n",
      "564 Training Loss: tensor(0.4956)\n",
      "565 Training Loss: tensor(0.4956)\n",
      "566 Training Loss: tensor(0.4956)\n",
      "567 Training Loss: tensor(0.4956)\n",
      "568 Training Loss: tensor(0.4956)\n",
      "569 Training Loss: tensor(0.4956)\n",
      "570 Training Loss: tensor(0.4956)\n",
      "571 Training Loss: tensor(0.4956)\n",
      "572 Training Loss: tensor(0.4956)\n",
      "573 Training Loss: tensor(0.4956)\n",
      "574 Training Loss: tensor(0.4956)\n",
      "575 Training Loss: tensor(0.4956)\n",
      "576 Training Loss: tensor(0.4956)\n",
      "577 Training Loss: tensor(0.4956)\n",
      "578 Training Loss: tensor(0.4956)\n",
      "579 Training Loss: tensor(0.4956)\n",
      "580 Training Loss: tensor(0.4956)\n",
      "581 Training Loss: tensor(0.4956)\n",
      "582 Training Loss: tensor(0.4956)\n",
      "583 Training Loss: tensor(0.4956)\n",
      "584 Training Loss: tensor(0.4956)\n",
      "585 Training Loss: tensor(0.4956)\n",
      "586 Training Loss: tensor(0.4956)\n",
      "587 Training Loss: tensor(0.4956)\n",
      "588 Training Loss: tensor(0.4956)\n",
      "589 Training Loss: tensor(0.4956)\n",
      "590 Training Loss: tensor(0.4956)\n",
      "591 Training Loss: tensor(0.4956)\n",
      "592 Training Loss: tensor(0.4956)\n",
      "593 Training Loss: tensor(0.4956)\n",
      "594 Training Loss: tensor(0.4956)\n",
      "595 Training Loss: tensor(0.4956)\n",
      "596 Training Loss: tensor(0.4956)\n",
      "597 Training Loss: tensor(0.4956)\n",
      "598 Training Loss: tensor(0.4956)\n",
      "599 Training Loss: tensor(0.4956)\n",
      "600 Training Loss: tensor(0.4956)\n",
      "601 Training Loss: tensor(0.4956)\n",
      "602 Training Loss: tensor(0.4956)\n",
      "603 Training Loss: tensor(0.4956)\n",
      "604 Training Loss: tensor(0.4956)\n",
      "605 Training Loss: tensor(0.4956)\n",
      "606 Training Loss: tensor(0.4956)\n",
      "607 Training Loss: tensor(0.4956)\n",
      "608 Training Loss: tensor(0.4956)\n",
      "609 Training Loss: tensor(0.4956)\n",
      "610 Training Loss: tensor(0.4956)\n",
      "611 Training Loss: tensor(0.4956)\n",
      "612 Training Loss: tensor(0.4956)\n",
      "613 Training Loss: tensor(0.4956)\n",
      "614 Training Loss: tensor(0.4956)\n",
      "615 Training Loss: tensor(0.4956)\n",
      "616 Training Loss: tensor(0.4956)\n",
      "617 Training Loss: tensor(0.4956)\n",
      "618 Training Loss: tensor(0.4956)\n",
      "619 Training Loss: tensor(0.4956)\n",
      "620 Training Loss: tensor(0.4956)\n",
      "621 Training Loss: tensor(0.4956)\n",
      "622 Training Loss: tensor(0.4956)\n",
      "623 Training Loss: tensor(0.4956)\n",
      "624 Training Loss: tensor(0.4956)\n",
      "625 Training Loss: tensor(0.4956)\n",
      "626 Training Loss: tensor(0.4956)\n",
      "627 Training Loss: tensor(0.4956)\n",
      "628 Training Loss: tensor(0.4956)\n",
      "629 Training Loss: tensor(0.4956)\n",
      "630 Training Loss: tensor(0.4956)\n",
      "631 Training Loss: tensor(0.4956)\n",
      "632 Training Loss: tensor(0.4956)\n",
      "633 Training Loss: tensor(0.4956)\n",
      "634 Training Loss: tensor(0.4956)\n",
      "635 Training Loss: tensor(0.4956)\n",
      "636 Training Loss: tensor(0.4956)\n",
      "637 Training Loss: tensor(0.4956)\n",
      "638 Training Loss: tensor(0.4956)\n",
      "639 Training Loss: tensor(0.4956)\n",
      "640 Training Loss: tensor(0.4956)\n",
      "641 Training Loss: tensor(0.4956)\n",
      "642 Training Loss: tensor(0.4956)\n",
      "643 Training Loss: tensor(0.4956)\n",
      "644 Training Loss: tensor(0.4956)\n",
      "645 Training Loss: tensor(0.4956)\n",
      "646 Training Loss: tensor(0.4956)\n",
      "647 Training Loss: tensor(0.4956)\n",
      "648 Training Loss: tensor(0.4956)\n",
      "649 Training Loss: tensor(0.4956)\n",
      "650 Training Loss: tensor(0.4956)\n",
      "651 Training Loss: tensor(0.4956)\n",
      "652 Training Loss: tensor(0.4956)\n",
      "653 Training Loss: tensor(0.4956)\n",
      "654 Training Loss: tensor(0.4956)\n",
      "655 Training Loss: tensor(0.4956)\n",
      "656 Training Loss: tensor(0.4956)\n",
      "657 Training Loss: tensor(0.4956)\n",
      "658 Training Loss: tensor(0.4956)\n",
      "659 Training Loss: tensor(0.4956)\n",
      "660 Training Loss: tensor(0.4956)\n",
      "661 Training Loss: tensor(0.4956)\n",
      "662 Training Loss: tensor(0.4956)\n",
      "663 Training Loss: tensor(0.4956)\n",
      "664 Training Loss: tensor(0.4956)\n",
      "665 Training Loss: tensor(0.4956)\n",
      "666 Training Loss: tensor(0.4956)\n",
      "667 Training Loss: tensor(0.4956)\n",
      "668 Training Loss: tensor(0.4956)\n",
      "669 Training Loss: tensor(0.4956)\n",
      "670 Training Loss: tensor(0.4956)\n",
      "671 Training Loss: tensor(0.4956)\n",
      "672 Training Loss: tensor(0.4956)\n",
      "673 Training Loss: tensor(0.4956)\n",
      "674 Training Loss: tensor(0.4956)\n",
      "675 Training Loss: tensor(0.4956)\n",
      "676 Training Loss: tensor(0.4956)\n",
      "677 Training Loss: tensor(0.4956)\n",
      "678 Training Loss: tensor(0.4956)\n",
      "679 Training Loss: tensor(0.4956)\n",
      "680 Training Loss: tensor(0.4956)\n",
      "681 Training Loss: tensor(0.4956)\n",
      "682 Training Loss: tensor(0.4956)\n",
      "683 Training Loss: tensor(0.4956)\n",
      "684 Training Loss: tensor(0.4956)\n",
      "685 Training Loss: tensor(0.4956)\n",
      "686 Training Loss: tensor(0.4956)\n",
      "687 Training Loss: tensor(0.4956)\n",
      "688 Training Loss: tensor(0.4956)\n",
      "689 Training Loss: tensor(0.4956)\n",
      "690 Training Loss: tensor(0.4956)\n",
      "691 Training Loss: tensor(0.4956)\n",
      "692 Training Loss: tensor(0.4956)\n",
      "693 Training Loss: tensor(0.4956)\n",
      "694 Training Loss: tensor(0.4956)\n",
      "695 Training Loss: tensor(0.4956)\n",
      "696 Training Loss: tensor(0.4956)\n",
      "697 Training Loss: tensor(0.4956)\n",
      "698 Training Loss: tensor(0.4956)\n",
      "699 Training Loss: tensor(0.4956)\n",
      "700 Training Loss: tensor(0.4956)\n",
      "701 Training Loss: tensor(0.4956)\n",
      "702 Training Loss: tensor(0.4956)\n",
      "703 Training Loss: tensor(0.4956)\n",
      "704 Training Loss: tensor(0.4956)\n",
      "705 Training Loss: tensor(0.4956)\n",
      "706 Training Loss: tensor(0.4956)\n",
      "707 Training Loss: tensor(0.4956)\n",
      "708 Training Loss: tensor(0.4956)\n",
      "709 Training Loss: tensor(0.4956)\n",
      "710 Training Loss: tensor(0.4956)\n",
      "711 Training Loss: tensor(0.4956)\n",
      "712 Training Loss: tensor(0.4956)\n",
      "713 Training Loss: tensor(0.4956)\n",
      "714 Training Loss: tensor(0.4956)\n",
      "715 Training Loss: tensor(0.4956)\n",
      "716 Training Loss: tensor(0.4956)\n",
      "717 Training Loss: tensor(0.4956)\n",
      "718 Training Loss: tensor(0.4956)\n",
      "719 Training Loss: tensor(0.4956)\n",
      "720 Training Loss: tensor(0.4956)\n",
      "721 Training Loss: tensor(0.4956)\n",
      "722 Training Loss: tensor(0.4956)\n",
      "723 Training Loss: tensor(0.4956)\n",
      "724 Training Loss: tensor(0.4956)\n",
      "725 Training Loss: tensor(0.4956)\n",
      "726 Training Loss: tensor(0.4956)\n",
      "727 Training Loss: tensor(0.4956)\n",
      "728 Training Loss: tensor(0.4956)\n",
      "729 Training Loss: tensor(0.4956)\n",
      "730 Training Loss: tensor(0.4956)\n",
      "731 Training Loss: tensor(0.4956)\n",
      "732 Training Loss: tensor(0.4956)\n",
      "733 Training Loss: tensor(0.4956)\n",
      "734 Training Loss: tensor(0.4956)\n",
      "735 Training Loss: tensor(0.4956)\n",
      "736 Training Loss: tensor(0.4956)\n",
      "737 Training Loss: tensor(0.4956)\n",
      "738 Training Loss: tensor(0.4956)\n",
      "739 Training Loss: tensor(0.4956)\n",
      "740 Training Loss: tensor(0.4956)\n",
      "741 Training Loss: tensor(0.4956)\n",
      "742 Training Loss: tensor(0.4956)\n",
      "743 Training Loss: tensor(0.4956)\n",
      "744 Training Loss: tensor(0.4956)\n",
      "745 Training Loss: tensor(0.4956)\n",
      "746 Training Loss: tensor(0.4956)\n",
      "747 Training Loss: tensor(0.4956)\n",
      "748 Training Loss: tensor(0.4956)\n",
      "749 Training Loss: tensor(0.4956)\n",
      "750 Training Loss: tensor(0.4956)\n",
      "751 Training Loss: tensor(0.4956)\n",
      "752 Training Loss: tensor(0.4956)\n",
      "753 Training Loss: tensor(0.4956)\n",
      "754 Training Loss: tensor(0.4956)\n",
      "755 Training Loss: tensor(0.4956)\n",
      "756 Training Loss: tensor(0.4956)\n",
      "757 Training Loss: tensor(0.4956)\n",
      "758 Training Loss: tensor(0.4956)\n",
      "759 Training Loss: tensor(0.4956)\n",
      "760 Training Loss: tensor(0.4956)\n",
      "761 Training Loss: tensor(0.4956)\n",
      "762 Training Loss: tensor(0.4956)\n",
      "763 Training Loss: tensor(0.4956)\n",
      "764 Training Loss: tensor(0.4956)\n",
      "765 Training Loss: tensor(0.4956)\n",
      "766 Training Loss: tensor(0.4956)\n",
      "767 Training Loss: tensor(0.4956)\n",
      "768 Training Loss: tensor(0.4956)\n",
      "769 Training Loss: tensor(0.4956)\n",
      "770 Training Loss: tensor(0.4956)\n",
      "771 Training Loss: tensor(0.4956)\n",
      "772 Training Loss: tensor(0.4956)\n",
      "773 Training Loss: tensor(0.4956)\n",
      "774 Training Loss: tensor(0.4956)\n",
      "775 Training Loss: tensor(0.4956)\n",
      "776 Training Loss: tensor(0.4956)\n",
      "777 Training Loss: tensor(0.4956)\n",
      "778 Training Loss: tensor(0.4956)\n",
      "779 Training Loss: tensor(0.4956)\n",
      "780 Training Loss: tensor(0.4956)\n",
      "781 Training Loss: tensor(0.4956)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782 Training Loss: tensor(0.4956)\n",
      "783 Training Loss: tensor(0.4956)\n",
      "784 Training Loss: tensor(0.4956)\n",
      "785 Training Loss: tensor(0.4956)\n",
      "786 Training Loss: tensor(0.4956)\n",
      "787 Training Loss: tensor(0.4956)\n",
      "788 Training Loss: tensor(0.4956)\n",
      "789 Training Loss: tensor(0.4956)\n",
      "790 Training Loss: tensor(0.4956)\n",
      "791 Training Loss: tensor(0.4956)\n",
      "792 Training Loss: tensor(0.4956)\n",
      "793 Training Loss: tensor(0.4956)\n",
      "794 Training Loss: tensor(0.4956)\n",
      "795 Training Loss: tensor(0.4956)\n",
      "796 Training Loss: tensor(0.4956)\n",
      "797 Training Loss: tensor(0.4956)\n",
      "798 Training Loss: tensor(0.4956)\n",
      "799 Training Loss: tensor(0.4956)\n",
      "800 Training Loss: tensor(0.4956)\n",
      "801 Training Loss: tensor(0.4956)\n",
      "802 Training Loss: tensor(0.4956)\n",
      "803 Training Loss: tensor(0.4956)\n",
      "804 Training Loss: tensor(0.4956)\n",
      "805 Training Loss: tensor(0.4956)\n",
      "806 Training Loss: tensor(0.4956)\n",
      "807 Training Loss: tensor(0.4956)\n",
      "808 Training Loss: tensor(0.4956)\n",
      "809 Training Loss: tensor(0.4956)\n",
      "810 Training Loss: tensor(0.4956)\n",
      "811 Training Loss: tensor(0.4956)\n",
      "812 Training Loss: tensor(0.4956)\n",
      "813 Training Loss: tensor(0.4956)\n",
      "814 Training Loss: tensor(0.4956)\n",
      "815 Training Loss: tensor(0.4956)\n",
      "816 Training Loss: tensor(0.4956)\n",
      "817 Training Loss: tensor(0.4956)\n",
      "818 Training Loss: tensor(0.4956)\n",
      "819 Training Loss: tensor(0.4956)\n",
      "820 Training Loss: tensor(0.4956)\n",
      "821 Training Loss: tensor(0.4956)\n",
      "822 Training Loss: tensor(0.4956)\n",
      "823 Training Loss: tensor(0.4956)\n",
      "824 Training Loss: tensor(0.4956)\n",
      "825 Training Loss: tensor(0.4956)\n",
      "826 Training Loss: tensor(0.4956)\n",
      "827 Training Loss: tensor(0.4956)\n",
      "828 Training Loss: tensor(0.4956)\n",
      "829 Training Loss: tensor(0.4956)\n",
      "830 Training Loss: tensor(0.4956)\n",
      "831 Training Loss: tensor(0.4956)\n",
      "832 Training Loss: tensor(0.4956)\n",
      "833 Training Loss: tensor(0.4956)\n",
      "834 Training Loss: tensor(0.4956)\n",
      "835 Training Loss: tensor(0.4956)\n",
      "836 Training Loss: tensor(0.4956)\n",
      "837 Training Loss: tensor(0.4956)\n",
      "838 Training Loss: tensor(0.4956)\n",
      "839 Training Loss: tensor(0.4956)\n",
      "840 Training Loss: tensor(0.4956)\n",
      "841 Training Loss: tensor(0.4956)\n",
      "842 Training Loss: tensor(0.4956)\n",
      "843 Training Loss: tensor(0.4956)\n",
      "844 Training Loss: tensor(0.4956)\n",
      "845 Training Loss: tensor(0.4956)\n",
      "846 Training Loss: tensor(0.4956)\n",
      "847 Training Loss: tensor(0.4956)\n",
      "848 Training Loss: tensor(0.4956)\n",
      "849 Training Loss: tensor(0.4956)\n",
      "850 Training Loss: tensor(0.4956)\n",
      "851 Training Loss: tensor(0.4956)\n",
      "852 Training Loss: tensor(0.4956)\n",
      "853 Training Loss: tensor(0.4956)\n",
      "854 Training Loss: tensor(0.4956)\n",
      "855 Training Loss: tensor(0.4956)\n",
      "856 Training Loss: tensor(0.4956)\n",
      "857 Training Loss: tensor(0.4956)\n",
      "858 Training Loss: tensor(0.4956)\n",
      "859 Training Loss: tensor(0.4956)\n",
      "860 Training Loss: tensor(0.4956)\n",
      "861 Training Loss: tensor(0.4956)\n",
      "862 Training Loss: tensor(0.4956)\n",
      "863 Training Loss: tensor(0.4956)\n",
      "864 Training Loss: tensor(0.4956)\n",
      "865 Training Loss: tensor(0.4956)\n",
      "866 Training Loss: tensor(0.4956)\n",
      "867 Training Loss: tensor(0.4956)\n",
      "868 Training Loss: tensor(0.4956)\n",
      "869 Training Loss: tensor(0.4956)\n",
      "870 Training Loss: tensor(0.4956)\n",
      "871 Training Loss: tensor(0.4956)\n",
      "872 Training Loss: tensor(0.4956)\n",
      "873 Training Loss: tensor(0.4956)\n",
      "874 Training Loss: tensor(0.4956)\n",
      "875 Training Loss: tensor(0.4956)\n",
      "876 Training Loss: tensor(0.4956)\n",
      "877 Training Loss: tensor(0.4956)\n",
      "878 Training Loss: tensor(0.4956)\n",
      "879 Training Loss: tensor(0.4956)\n",
      "880 Training Loss: tensor(0.4956)\n",
      "881 Training Loss: tensor(0.4956)\n",
      "882 Training Loss: tensor(0.4956)\n",
      "883 Training Loss: tensor(0.4956)\n",
      "884 Training Loss: tensor(0.4956)\n",
      "885 Training Loss: tensor(0.4956)\n",
      "886 Training Loss: tensor(0.4956)\n",
      "887 Training Loss: tensor(0.4956)\n",
      "888 Training Loss: tensor(0.4956)\n",
      "889 Training Loss: tensor(0.4956)\n",
      "890 Training Loss: tensor(0.4956)\n",
      "891 Training Loss: tensor(0.4956)\n",
      "892 Training Loss: tensor(0.4956)\n",
      "893 Training Loss: tensor(0.4956)\n",
      "894 Training Loss: tensor(0.4956)\n",
      "895 Training Loss: tensor(0.4956)\n",
      "896 Training Loss: tensor(0.4956)\n",
      "897 Training Loss: tensor(0.4956)\n",
      "898 Training Loss: tensor(0.4956)\n",
      "899 Training Loss: tensor(0.4956)\n",
      "900 Training Loss: tensor(0.4956)\n",
      "901 Training Loss: tensor(0.4956)\n",
      "902 Training Loss: tensor(0.4956)\n",
      "903 Training Loss: tensor(0.4956)\n",
      "904 Training Loss: tensor(0.4956)\n",
      "905 Training Loss: tensor(0.4956)\n",
      "906 Training Loss: tensor(0.4956)\n",
      "907 Training Loss: tensor(0.4956)\n",
      "908 Training Loss: tensor(0.4956)\n",
      "909 Training Loss: tensor(0.4956)\n",
      "910 Training Loss: tensor(0.4956)\n",
      "911 Training Loss: tensor(0.4956)\n",
      "912 Training Loss: tensor(0.4956)\n",
      "913 Training Loss: tensor(0.4956)\n",
      "914 Training Loss: tensor(0.4956)\n",
      "915 Training Loss: tensor(0.4956)\n",
      "916 Training Loss: tensor(0.4956)\n",
      "917 Training Loss: tensor(0.4956)\n",
      "918 Training Loss: tensor(0.4956)\n",
      "919 Training Loss: tensor(0.4956)\n",
      "920 Training Loss: tensor(0.4956)\n",
      "921 Training Loss: tensor(0.4956)\n",
      "922 Training Loss: tensor(0.4956)\n",
      "923 Training Loss: tensor(0.4956)\n",
      "924 Training Loss: tensor(0.4956)\n",
      "925 Training Loss: tensor(0.4956)\n",
      "926 Training Loss: tensor(0.4956)\n",
      "927 Training Loss: tensor(0.4956)\n",
      "928 Training Loss: tensor(0.4956)\n",
      "929 Training Loss: tensor(0.4956)\n",
      "930 Training Loss: tensor(0.4956)\n",
      "931 Training Loss: tensor(0.4956)\n",
      "932 Training Loss: tensor(0.4956)\n",
      "933 Training Loss: tensor(0.4956)\n",
      "934 Training Loss: tensor(0.4956)\n",
      "935 Training Loss: tensor(0.4956)\n",
      "936 Training Loss: tensor(0.4956)\n",
      "937 Training Loss: tensor(0.4956)\n",
      "938 Training Loss: tensor(0.4956)\n",
      "939 Training Loss: tensor(0.4956)\n",
      "940 Training Loss: tensor(0.4956)\n",
      "941 Training Loss: tensor(0.4956)\n",
      "942 Training Loss: tensor(0.4956)\n",
      "943 Training Loss: tensor(0.4956)\n",
      "944 Training Loss: tensor(0.4956)\n",
      "945 Training Loss: tensor(0.4956)\n",
      "946 Training Loss: tensor(0.4956)\n",
      "947 Training Loss: tensor(0.4956)\n",
      "948 Training Loss: tensor(0.4956)\n",
      "949 Training Loss: tensor(0.4956)\n",
      "950 Training Loss: tensor(0.4956)\n",
      "951 Training Loss: tensor(0.4956)\n",
      "952 Training Loss: tensor(0.4956)\n",
      "953 Training Loss: tensor(0.4956)\n",
      "954 Training Loss: tensor(0.4956)\n",
      "955 Training Loss: tensor(0.4956)\n",
      "956 Training Loss: tensor(0.4956)\n",
      "957 Training Loss: tensor(0.4956)\n",
      "958 Training Loss: tensor(0.4956)\n",
      "959 Training Loss: tensor(0.4956)\n",
      "960 Training Loss: tensor(0.4956)\n",
      "961 Training Loss: tensor(0.4956)\n",
      "962 Training Loss: tensor(0.4956)\n",
      "963 Training Loss: tensor(0.4956)\n",
      "964 Training Loss: tensor(0.4956)\n",
      "965 Training Loss: tensor(0.4956)\n",
      "966 Training Loss: tensor(0.4956)\n",
      "967 Training Loss: tensor(0.4956)\n",
      "968 Training Loss: tensor(0.4956)\n",
      "969 Training Loss: tensor(0.4956)\n",
      "970 Training Loss: tensor(0.4956)\n",
      "971 Training Loss: tensor(0.4956)\n",
      "972 Training Loss: tensor(0.4956)\n",
      "973 Training Loss: tensor(0.4955)\n",
      "974 Training Loss: tensor(0.4955)\n",
      "975 Training Loss: tensor(0.4955)\n",
      "976 Training Loss: tensor(0.4955)\n",
      "977 Training Loss: tensor(0.4955)\n",
      "978 Training Loss: tensor(0.4955)\n",
      "979 Training Loss: tensor(0.4955)\n",
      "980 Training Loss: tensor(0.4955)\n",
      "981 Training Loss: tensor(0.4955)\n",
      "982 Training Loss: tensor(0.4955)\n",
      "983 Training Loss: tensor(0.4955)\n",
      "984 Training Loss: tensor(0.4955)\n",
      "985 Training Loss: tensor(0.4955)\n",
      "986 Training Loss: tensor(0.4955)\n",
      "987 Training Loss: tensor(0.4955)\n",
      "988 Training Loss: tensor(0.4955)\n",
      "989 Training Loss: tensor(0.4955)\n",
      "990 Training Loss: tensor(0.4955)\n",
      "991 Training Loss: tensor(0.4955)\n",
      "992 Training Loss: tensor(0.4955)\n",
      "993 Training Loss: tensor(0.4955)\n",
      "994 Training Loss: tensor(0.4955)\n",
      "995 Training Loss: tensor(0.4955)\n",
      "996 Training Loss: tensor(0.4955)\n",
      "997 Training Loss: tensor(0.4955)\n",
      "998 Training Loss: tensor(0.4955)\n",
      "999 Training Loss: tensor(0.4955)\n",
      "1000 Training Loss: tensor(0.4955)\n",
      "1001 Training Loss: tensor(0.4955)\n",
      "1002 Training Loss: tensor(0.4955)\n",
      "1003 Training Loss: tensor(0.4955)\n",
      "1004 Training Loss: tensor(0.4955)\n",
      "1005 Training Loss: tensor(0.4955)\n",
      "1006 Training Loss: tensor(0.4955)\n",
      "1007 Training Loss: tensor(0.4955)\n",
      "1008 Training Loss: tensor(0.4955)\n",
      "1009 Training Loss: tensor(0.4955)\n",
      "1010 Training Loss: tensor(0.4955)\n",
      "1011 Training Loss: tensor(0.4955)\n",
      "1012 Training Loss: tensor(0.4955)\n",
      "1013 Training Loss: tensor(0.4955)\n",
      "1014 Training Loss: tensor(0.4955)\n",
      "1015 Training Loss: tensor(0.4955)\n",
      "1016 Training Loss: tensor(0.4955)\n",
      "1017 Training Loss: tensor(0.4955)\n",
      "1018 Training Loss: tensor(0.4955)\n",
      "1019 Training Loss: tensor(0.4955)\n",
      "1020 Training Loss: tensor(0.4955)\n",
      "1021 Training Loss: tensor(0.4955)\n",
      "1022 Training Loss: tensor(0.4955)\n",
      "1023 Training Loss: tensor(0.4955)\n",
      "1024 Training Loss: tensor(0.4955)\n",
      "1025 Training Loss: tensor(0.4955)\n",
      "1026 Training Loss: tensor(0.4955)\n",
      "1027 Training Loss: tensor(0.4955)\n",
      "1028 Training Loss: tensor(0.4955)\n",
      "1029 Training Loss: tensor(0.4955)\n",
      "1030 Training Loss: tensor(0.4955)\n",
      "1031 Training Loss: tensor(0.4955)\n",
      "1032 Training Loss: tensor(0.4955)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033 Training Loss: tensor(0.4955)\n",
      "1034 Training Loss: tensor(0.4955)\n",
      "1035 Training Loss: tensor(0.4955)\n",
      "1036 Training Loss: tensor(0.4955)\n",
      "1037 Training Loss: tensor(0.4955)\n",
      "1038 Training Loss: tensor(0.4955)\n",
      "1039 Training Loss: tensor(0.4955)\n",
      "1040 Training Loss: tensor(0.4955)\n",
      "1041 Training Loss: tensor(0.4955)\n",
      "1042 Training Loss: tensor(0.4955)\n",
      "1043 Training Loss: tensor(0.4955)\n",
      "1044 Training Loss: tensor(0.4955)\n",
      "1045 Training Loss: tensor(0.4955)\n",
      "1046 Training Loss: tensor(0.4955)\n",
      "1047 Training Loss: tensor(0.4955)\n",
      "1048 Training Loss: tensor(0.4955)\n",
      "1049 Training Loss: tensor(0.4955)\n",
      "1050 Training Loss: tensor(0.4955)\n",
      "1051 Training Loss: tensor(0.4954)\n",
      "1052 Training Loss: tensor(0.4954)\n",
      "1053 Training Loss: tensor(0.4954)\n",
      "1054 Training Loss: tensor(0.4954)\n",
      "1055 Training Loss: tensor(0.4954)\n",
      "1056 Training Loss: tensor(0.4954)\n",
      "1057 Training Loss: tensor(0.4954)\n",
      "1058 Training Loss: tensor(0.4954)\n",
      "1059 Training Loss: tensor(0.4954)\n",
      "1060 Training Loss: tensor(0.4954)\n",
      "1061 Training Loss: tensor(0.4954)\n",
      "1062 Training Loss: tensor(0.4954)\n",
      "1063 Training Loss: tensor(0.4954)\n",
      "1064 Training Loss: tensor(0.4954)\n",
      "1065 Training Loss: tensor(0.4954)\n",
      "1066 Training Loss: tensor(0.4954)\n",
      "1067 Training Loss: tensor(0.4954)\n",
      "1068 Training Loss: tensor(0.4954)\n",
      "1069 Training Loss: tensor(0.4954)\n",
      "1070 Training Loss: tensor(0.4954)\n",
      "1071 Training Loss: tensor(0.4954)\n",
      "1072 Training Loss: tensor(0.4954)\n",
      "1073 Training Loss: tensor(0.4954)\n",
      "1074 Training Loss: tensor(0.4954)\n",
      "1075 Training Loss: tensor(0.4954)\n",
      "1076 Training Loss: tensor(0.4954)\n",
      "1077 Training Loss: tensor(0.4954)\n",
      "1078 Training Loss: tensor(0.4953)\n",
      "1079 Training Loss: tensor(0.4953)\n",
      "1080 Training Loss: tensor(0.4953)\n",
      "1081 Training Loss: tensor(0.4953)\n",
      "1082 Training Loss: tensor(0.4953)\n",
      "1083 Training Loss: tensor(0.4953)\n",
      "1084 Training Loss: tensor(0.4953)\n",
      "1085 Training Loss: tensor(0.4953)\n",
      "1086 Training Loss: tensor(0.4953)\n",
      "1087 Training Loss: tensor(0.4953)\n",
      "1088 Training Loss: tensor(0.4953)\n",
      "1089 Training Loss: tensor(0.4953)\n",
      "1090 Training Loss: tensor(0.4953)\n",
      "1091 Training Loss: tensor(0.4953)\n",
      "1092 Training Loss: tensor(0.4953)\n",
      "1093 Training Loss: tensor(0.4953)\n",
      "1094 Training Loss: tensor(0.4952)\n",
      "1095 Training Loss: tensor(0.4952)\n",
      "1096 Training Loss: tensor(0.4952)\n",
      "1097 Training Loss: tensor(0.4952)\n",
      "1098 Training Loss: tensor(0.4952)\n",
      "1099 Training Loss: tensor(0.4952)\n",
      "1100 Training Loss: tensor(0.4952)\n",
      "1101 Training Loss: tensor(0.4952)\n",
      "1102 Training Loss: tensor(0.4952)\n",
      "1103 Training Loss: tensor(0.4952)\n",
      "1104 Training Loss: tensor(0.4952)\n",
      "1105 Training Loss: tensor(0.4952)\n",
      "1106 Training Loss: tensor(0.4951)\n",
      "1107 Training Loss: tensor(0.4951)\n",
      "1108 Training Loss: tensor(0.4951)\n",
      "1109 Training Loss: tensor(0.4951)\n",
      "1110 Training Loss: tensor(0.4951)\n",
      "1111 Training Loss: tensor(0.4951)\n",
      "1112 Training Loss: tensor(0.4951)\n",
      "1113 Training Loss: tensor(0.4951)\n",
      "1114 Training Loss: tensor(0.4951)\n",
      "1115 Training Loss: tensor(0.4951)\n",
      "1116 Training Loss: tensor(0.4950)\n",
      "1117 Training Loss: tensor(0.4950)\n",
      "1118 Training Loss: tensor(0.4950)\n",
      "1119 Training Loss: tensor(0.4950)\n",
      "1120 Training Loss: tensor(0.4950)\n",
      "1121 Training Loss: tensor(0.4950)\n",
      "1122 Training Loss: tensor(0.4950)\n",
      "1123 Training Loss: tensor(0.4950)\n",
      "1124 Training Loss: tensor(0.4949)\n",
      "1125 Training Loss: tensor(0.4949)\n",
      "1126 Training Loss: tensor(0.4949)\n",
      "1127 Training Loss: tensor(0.4949)\n",
      "1128 Training Loss: tensor(0.4949)\n",
      "1129 Training Loss: tensor(0.4949)\n",
      "1130 Training Loss: tensor(0.4949)\n",
      "1131 Training Loss: tensor(0.4949)\n",
      "1132 Training Loss: tensor(0.4948)\n",
      "1133 Training Loss: tensor(0.4948)\n",
      "1134 Training Loss: tensor(0.4948)\n",
      "1135 Training Loss: tensor(0.4948)\n",
      "1136 Training Loss: tensor(0.4948)\n",
      "1137 Training Loss: tensor(0.4948)\n",
      "1138 Training Loss: tensor(0.4947)\n",
      "1139 Training Loss: tensor(0.4947)\n",
      "1140 Training Loss: tensor(0.4947)\n",
      "1141 Training Loss: tensor(0.4947)\n",
      "1142 Training Loss: tensor(0.4947)\n",
      "1143 Training Loss: tensor(0.4947)\n",
      "1144 Training Loss: tensor(0.4946)\n",
      "1145 Training Loss: tensor(0.4946)\n",
      "1146 Training Loss: tensor(0.4946)\n",
      "1147 Training Loss: tensor(0.4946)\n",
      "1148 Training Loss: tensor(0.4946)\n",
      "1149 Training Loss: tensor(0.4945)\n",
      "1150 Training Loss: tensor(0.4945)\n",
      "1151 Training Loss: tensor(0.4945)\n",
      "1152 Training Loss: tensor(0.4945)\n",
      "1153 Training Loss: tensor(0.4944)\n",
      "1154 Training Loss: tensor(0.4944)\n",
      "1155 Training Loss: tensor(0.4944)\n",
      "1156 Training Loss: tensor(0.4944)\n",
      "1157 Training Loss: tensor(0.4944)\n",
      "1158 Training Loss: tensor(0.4944)\n",
      "1159 Training Loss: tensor(0.4943)\n",
      "1160 Training Loss: tensor(0.4943)\n",
      "1161 Training Loss: tensor(0.4943)\n",
      "1162 Training Loss: tensor(0.4943)\n",
      "1163 Training Loss: tensor(0.4942)\n",
      "1164 Training Loss: tensor(0.4942)\n",
      "1165 Training Loss: tensor(0.4942)\n",
      "1166 Training Loss: tensor(0.4942)\n",
      "1167 Training Loss: tensor(0.4941)\n",
      "1168 Training Loss: tensor(0.4941)\n",
      "1169 Training Loss: tensor(0.4941)\n",
      "1170 Training Loss: tensor(0.4941)\n",
      "1171 Training Loss: tensor(0.4941)\n",
      "1172 Training Loss: tensor(0.4940)\n",
      "1173 Training Loss: tensor(0.4940)\n",
      "1174 Training Loss: tensor(0.4940)\n",
      "1175 Training Loss: tensor(0.4940)\n",
      "1176 Training Loss: tensor(0.4939)\n",
      "1177 Training Loss: tensor(0.4939)\n",
      "1178 Training Loss: tensor(0.4939)\n",
      "1179 Training Loss: tensor(0.4939)\n",
      "1180 Training Loss: tensor(0.4938)\n",
      "1181 Training Loss: tensor(0.4938)\n",
      "1182 Training Loss: tensor(0.4938)\n",
      "1183 Training Loss: tensor(0.4937)\n",
      "1184 Training Loss: tensor(0.4937)\n",
      "1185 Training Loss: tensor(0.4937)\n",
      "1186 Training Loss: tensor(0.4937)\n",
      "1187 Training Loss: tensor(0.4936)\n",
      "1188 Training Loss: tensor(0.4936)\n",
      "1189 Training Loss: tensor(0.4936)\n",
      "1190 Training Loss: tensor(0.4935)\n",
      "1191 Training Loss: tensor(0.4936)\n",
      "1192 Training Loss: tensor(0.4935)\n",
      "1193 Training Loss: tensor(0.4935)\n",
      "1194 Training Loss: tensor(0.4935)\n",
      "1195 Training Loss: tensor(0.4934)\n",
      "1196 Training Loss: tensor(0.4934)\n",
      "1197 Training Loss: tensor(0.4934)\n",
      "1198 Training Loss: tensor(0.4933)\n",
      "1199 Training Loss: tensor(0.4933)\n",
      "1200 Training Loss: tensor(0.4933)\n",
      "1201 Training Loss: tensor(0.4932)\n",
      "1202 Training Loss: tensor(0.4932)\n",
      "1203 Training Loss: tensor(0.4933)\n",
      "1204 Training Loss: tensor(0.4931)\n",
      "1205 Training Loss: tensor(0.4932)\n",
      "1206 Training Loss: tensor(0.4931)\n",
      "1207 Training Loss: tensor(0.4930)\n",
      "1208 Training Loss: tensor(0.4930)\n",
      "1209 Training Loss: tensor(0.4930)\n",
      "1210 Training Loss: tensor(0.4930)\n",
      "1211 Training Loss: tensor(0.4930)\n",
      "1212 Training Loss: tensor(0.4929)\n",
      "1213 Training Loss: tensor(0.4930)\n",
      "1214 Training Loss: tensor(0.4929)\n",
      "1215 Training Loss: tensor(0.4928)\n",
      "1216 Training Loss: tensor(0.4928)\n",
      "1217 Training Loss: tensor(0.4927)\n",
      "1218 Training Loss: tensor(0.4927)\n",
      "1219 Training Loss: tensor(0.4926)\n",
      "1220 Training Loss: tensor(0.4927)\n",
      "1221 Training Loss: tensor(0.4925)\n",
      "1222 Training Loss: tensor(0.4925)\n",
      "1223 Training Loss: tensor(0.4925)\n",
      "1224 Training Loss: tensor(0.4925)\n",
      "1225 Training Loss: tensor(0.4925)\n",
      "1226 Training Loss: tensor(0.4924)\n",
      "1227 Training Loss: tensor(0.4924)\n",
      "1228 Training Loss: tensor(0.4924)\n",
      "1229 Training Loss: tensor(0.4924)\n",
      "1230 Training Loss: tensor(0.4922)\n",
      "1231 Training Loss: tensor(0.4922)\n",
      "1232 Training Loss: tensor(0.4921)\n",
      "1233 Training Loss: tensor(0.4921)\n",
      "1234 Training Loss: tensor(0.4920)\n",
      "1235 Training Loss: tensor(0.4920)\n",
      "1236 Training Loss: tensor(0.4920)\n",
      "1237 Training Loss: tensor(0.4919)\n",
      "1238 Training Loss: tensor(0.4919)\n",
      "1239 Training Loss: tensor(0.4919)\n",
      "1240 Training Loss: tensor(0.4918)\n",
      "1241 Training Loss: tensor(0.4916)\n",
      "1242 Training Loss: tensor(0.4918)\n",
      "1243 Training Loss: tensor(0.4918)\n",
      "1244 Training Loss: tensor(0.4916)\n",
      "1245 Training Loss: tensor(0.4915)\n",
      "1246 Training Loss: tensor(0.4916)\n",
      "1247 Training Loss: tensor(0.4915)\n",
      "1248 Training Loss: tensor(0.4914)\n",
      "1249 Training Loss: tensor(0.4914)\n",
      "1250 Training Loss: tensor(0.4912)\n",
      "1251 Training Loss: tensor(0.4913)\n",
      "1252 Training Loss: tensor(0.4912)\n",
      "1253 Training Loss: tensor(0.4912)\n",
      "1254 Training Loss: tensor(0.4911)\n",
      "1255 Training Loss: tensor(0.4910)\n",
      "1256 Training Loss: tensor(0.4911)\n",
      "1257 Training Loss: tensor(0.4909)\n",
      "1258 Training Loss: tensor(0.4909)\n",
      "1259 Training Loss: tensor(0.4908)\n",
      "1260 Training Loss: tensor(0.4907)\n",
      "1261 Training Loss: tensor(0.4909)\n",
      "1262 Training Loss: tensor(0.4907)\n",
      "1263 Training Loss: tensor(0.4905)\n",
      "1264 Training Loss: tensor(0.4905)\n",
      "1265 Training Loss: tensor(0.4905)\n",
      "1266 Training Loss: tensor(0.4905)\n",
      "1267 Training Loss: tensor(0.4905)\n",
      "1268 Training Loss: tensor(0.4904)\n",
      "1269 Training Loss: tensor(0.4903)\n",
      "1270 Training Loss: tensor(0.4903)\n",
      "1271 Training Loss: tensor(0.4902)\n",
      "1272 Training Loss: tensor(0.4900)\n",
      "1273 Training Loss: tensor(0.4899)\n",
      "1274 Training Loss: tensor(0.4899)\n",
      "1275 Training Loss: tensor(0.4898)\n",
      "1276 Training Loss: tensor(0.4899)\n",
      "1277 Training Loss: tensor(0.4896)\n",
      "1278 Training Loss: tensor(0.4895)\n",
      "1279 Training Loss: tensor(0.4897)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 Training Loss: tensor(0.4895)\n",
      "1281 Training Loss: tensor(0.4894)\n",
      "1282 Training Loss: tensor(0.4893)\n",
      "1283 Training Loss: tensor(0.4893)\n",
      "1284 Training Loss: tensor(0.4893)\n",
      "1285 Training Loss: tensor(0.4893)\n",
      "1286 Training Loss: tensor(0.4892)\n",
      "1287 Training Loss: tensor(0.4891)\n",
      "1288 Training Loss: tensor(0.4889)\n",
      "1289 Training Loss: tensor(0.4889)\n",
      "1290 Training Loss: tensor(0.4887)\n",
      "1291 Training Loss: tensor(0.4887)\n",
      "1292 Training Loss: tensor(0.4887)\n",
      "1293 Training Loss: tensor(0.4884)\n",
      "1294 Training Loss: tensor(0.4885)\n",
      "1295 Training Loss: tensor(0.4883)\n",
      "1296 Training Loss: tensor(0.4882)\n",
      "1297 Training Loss: tensor(0.4882)\n",
      "1298 Training Loss: tensor(0.4882)\n",
      "1299 Training Loss: tensor(0.4882)\n",
      "1300 Training Loss: tensor(0.4878)\n",
      "1301 Training Loss: tensor(0.4880)\n",
      "1302 Training Loss: tensor(0.4878)\n",
      "1303 Training Loss: tensor(0.4877)\n",
      "1304 Training Loss: tensor(0.4876)\n",
      "1305 Training Loss: tensor(0.4875)\n",
      "1306 Training Loss: tensor(0.4874)\n",
      "1307 Training Loss: tensor(0.4874)\n",
      "1308 Training Loss: tensor(0.4872)\n",
      "1309 Training Loss: tensor(0.4871)\n",
      "1310 Training Loss: tensor(0.4872)\n",
      "1311 Training Loss: tensor(0.4868)\n",
      "1312 Training Loss: tensor(0.4870)\n",
      "1313 Training Loss: tensor(0.4868)\n",
      "1314 Training Loss: tensor(0.4866)\n",
      "1315 Training Loss: tensor(0.4863)\n",
      "1316 Training Loss: tensor(0.4863)\n",
      "1317 Training Loss: tensor(0.4863)\n",
      "1318 Training Loss: tensor(0.4862)\n",
      "1319 Training Loss: tensor(0.4862)\n",
      "1320 Training Loss: tensor(0.4859)\n",
      "1321 Training Loss: tensor(0.4860)\n",
      "1322 Training Loss: tensor(0.4858)\n",
      "1323 Training Loss: tensor(0.4858)\n",
      "1324 Training Loss: tensor(0.4853)\n",
      "1325 Training Loss: tensor(0.4852)\n",
      "1326 Training Loss: tensor(0.4852)\n",
      "1327 Training Loss: tensor(0.4852)\n",
      "1328 Training Loss: tensor(0.4852)\n",
      "1329 Training Loss: tensor(0.4848)\n",
      "1330 Training Loss: tensor(0.4849)\n",
      "1331 Training Loss: tensor(0.4846)\n",
      "1332 Training Loss: tensor(0.4844)\n",
      "1333 Training Loss: tensor(0.4844)\n",
      "1334 Training Loss: tensor(0.4844)\n",
      "1335 Training Loss: tensor(0.4843)\n",
      "1336 Training Loss: tensor(0.4841)\n",
      "1337 Training Loss: tensor(0.4837)\n",
      "1338 Training Loss: tensor(0.4837)\n",
      "1339 Training Loss: tensor(0.4840)\n",
      "1340 Training Loss: tensor(0.4834)\n",
      "1341 Training Loss: tensor(0.4839)\n",
      "1342 Training Loss: tensor(0.4833)\n",
      "1343 Training Loss: tensor(0.4833)\n",
      "1344 Training Loss: tensor(0.4830)\n",
      "1345 Training Loss: tensor(0.4828)\n",
      "1346 Training Loss: tensor(0.4827)\n",
      "1347 Training Loss: tensor(0.4824)\n",
      "1348 Training Loss: tensor(0.4823)\n",
      "1349 Training Loss: tensor(0.4821)\n",
      "1350 Training Loss: tensor(0.4820)\n",
      "1351 Training Loss: tensor(0.4822)\n",
      "1352 Training Loss: tensor(0.4815)\n",
      "1353 Training Loss: tensor(0.4815)\n",
      "1354 Training Loss: tensor(0.4812)\n",
      "1355 Training Loss: tensor(0.4813)\n",
      "1356 Training Loss: tensor(0.4811)\n",
      "1357 Training Loss: tensor(0.4810)\n",
      "1358 Training Loss: tensor(0.4810)\n",
      "1359 Training Loss: tensor(0.4805)\n",
      "1360 Training Loss: tensor(0.4806)\n",
      "1361 Training Loss: tensor(0.4800)\n",
      "1362 Training Loss: tensor(0.4797)\n",
      "1363 Training Loss: tensor(0.4799)\n",
      "1364 Training Loss: tensor(0.4794)\n",
      "1365 Training Loss: tensor(0.4792)\n",
      "1366 Training Loss: tensor(0.4788)\n",
      "1367 Training Loss: tensor(0.4796)\n",
      "1368 Training Loss: tensor(0.4792)\n",
      "1369 Training Loss: tensor(0.4785)\n",
      "1370 Training Loss: tensor(0.4787)\n",
      "1371 Training Loss: tensor(0.4783)\n",
      "1372 Training Loss: tensor(0.4784)\n",
      "1373 Training Loss: tensor(0.4781)\n",
      "1374 Training Loss: tensor(0.4780)\n",
      "1375 Training Loss: tensor(0.4775)\n",
      "1376 Training Loss: tensor(0.4776)\n",
      "1377 Training Loss: tensor(0.4776)\n",
      "1378 Training Loss: tensor(0.4769)\n",
      "1379 Training Loss: tensor(0.4767)\n",
      "1380 Training Loss: tensor(0.4761)\n",
      "1381 Training Loss: tensor(0.4766)\n",
      "1382 Training Loss: tensor(0.4768)\n",
      "1383 Training Loss: tensor(0.4762)\n",
      "1384 Training Loss: tensor(0.4759)\n",
      "1385 Training Loss: tensor(0.4755)\n",
      "1386 Training Loss: tensor(0.4753)\n",
      "1387 Training Loss: tensor(0.4756)\n",
      "1388 Training Loss: tensor(0.4750)\n",
      "1389 Training Loss: tensor(0.4747)\n",
      "1390 Training Loss: tensor(0.4747)\n",
      "1391 Training Loss: tensor(0.4744)\n",
      "1392 Training Loss: tensor(0.4740)\n",
      "1393 Training Loss: tensor(0.4737)\n",
      "1394 Training Loss: tensor(0.4737)\n",
      "1395 Training Loss: tensor(0.4733)\n",
      "1396 Training Loss: tensor(0.4731)\n",
      "1397 Training Loss: tensor(0.4724)\n",
      "1398 Training Loss: tensor(0.4724)\n",
      "1399 Training Loss: tensor(0.4731)\n",
      "1400 Training Loss: tensor(0.4730)\n",
      "1401 Training Loss: tensor(0.4714)\n",
      "1402 Training Loss: tensor(0.4715)\n",
      "1403 Training Loss: tensor(0.4711)\n",
      "1404 Training Loss: tensor(0.4720)\n",
      "1405 Training Loss: tensor(0.4705)\n",
      "1406 Training Loss: tensor(0.4701)\n",
      "1407 Training Loss: tensor(0.4705)\n",
      "1408 Training Loss: tensor(0.4699)\n",
      "1409 Training Loss: tensor(0.4693)\n",
      "1410 Training Loss: tensor(0.4696)\n",
      "1411 Training Loss: tensor(0.4692)\n",
      "1412 Training Loss: tensor(0.4693)\n",
      "1413 Training Loss: tensor(0.4695)\n",
      "1414 Training Loss: tensor(0.4686)\n",
      "1415 Training Loss: tensor(0.4692)\n",
      "1416 Training Loss: tensor(0.4672)\n",
      "1417 Training Loss: tensor(0.4677)\n",
      "1418 Training Loss: tensor(0.4678)\n",
      "1419 Training Loss: tensor(0.4675)\n",
      "1420 Training Loss: tensor(0.4668)\n",
      "1421 Training Loss: tensor(0.4660)\n",
      "1422 Training Loss: tensor(0.4663)\n",
      "1423 Training Loss: tensor(0.4670)\n",
      "1424 Training Loss: tensor(0.4657)\n",
      "1425 Training Loss: tensor(0.4658)\n",
      "1426 Training Loss: tensor(0.4657)\n",
      "1427 Training Loss: tensor(0.4661)\n",
      "1428 Training Loss: tensor(0.4645)\n",
      "1429 Training Loss: tensor(0.4646)\n",
      "1430 Training Loss: tensor(0.4636)\n",
      "1431 Training Loss: tensor(0.4641)\n",
      "1432 Training Loss: tensor(0.4649)\n",
      "1433 Training Loss: tensor(0.4633)\n",
      "1434 Training Loss: tensor(0.4629)\n",
      "1435 Training Loss: tensor(0.4633)\n",
      "1436 Training Loss: tensor(0.4630)\n",
      "1437 Training Loss: tensor(0.4623)\n",
      "1438 Training Loss: tensor(0.4629)\n",
      "1439 Training Loss: tensor(0.4621)\n",
      "1440 Training Loss: tensor(0.4616)\n",
      "1441 Training Loss: tensor(0.4609)\n",
      "1442 Training Loss: tensor(0.4620)\n",
      "1443 Training Loss: tensor(0.4616)\n",
      "1444 Training Loss: tensor(0.4602)\n",
      "1445 Training Loss: tensor(0.4599)\n",
      "1446 Training Loss: tensor(0.4601)\n",
      "1447 Training Loss: tensor(0.4595)\n",
      "1448 Training Loss: tensor(0.4597)\n",
      "1449 Training Loss: tensor(0.4595)\n",
      "1450 Training Loss: tensor(0.4601)\n",
      "1451 Training Loss: tensor(0.4587)\n",
      "1452 Training Loss: tensor(0.4579)\n",
      "1453 Training Loss: tensor(0.4568)\n",
      "1454 Training Loss: tensor(0.4565)\n",
      "1455 Training Loss: tensor(0.4581)\n",
      "1456 Training Loss: tensor(0.4564)\n",
      "1457 Training Loss: tensor(0.4564)\n",
      "1458 Training Loss: tensor(0.4556)\n",
      "1459 Training Loss: tensor(0.4553)\n",
      "1460 Training Loss: tensor(0.4553)\n",
      "1461 Training Loss: tensor(0.4563)\n",
      "1462 Training Loss: tensor(0.4544)\n",
      "1463 Training Loss: tensor(0.4550)\n",
      "1464 Training Loss: tensor(0.4542)\n",
      "1465 Training Loss: tensor(0.4534)\n",
      "1466 Training Loss: tensor(0.4530)\n",
      "1467 Training Loss: tensor(0.4552)\n",
      "1468 Training Loss: tensor(0.4523)\n",
      "1469 Training Loss: tensor(0.4537)\n",
      "1470 Training Loss: tensor(0.4517)\n",
      "1471 Training Loss: tensor(0.4524)\n",
      "1472 Training Loss: tensor(0.4535)\n",
      "1473 Training Loss: tensor(0.4524)\n",
      "1474 Training Loss: tensor(0.4513)\n",
      "1475 Training Loss: tensor(0.4514)\n",
      "1476 Training Loss: tensor(0.4499)\n",
      "1477 Training Loss: tensor(0.4505)\n",
      "1478 Training Loss: tensor(0.4500)\n",
      "1479 Training Loss: tensor(0.4483)\n",
      "1480 Training Loss: tensor(0.4473)\n",
      "1481 Training Loss: tensor(0.4491)\n",
      "1482 Training Loss: tensor(0.4494)\n",
      "1483 Training Loss: tensor(0.4496)\n",
      "1484 Training Loss: tensor(0.4487)\n",
      "1485 Training Loss: tensor(0.4472)\n",
      "1486 Training Loss: tensor(0.4484)\n",
      "1487 Training Loss: tensor(0.4467)\n",
      "1488 Training Loss: tensor(0.4468)\n",
      "1489 Training Loss: tensor(0.4469)\n",
      "1490 Training Loss: tensor(0.4458)\n",
      "1491 Training Loss: tensor(0.4467)\n",
      "1492 Training Loss: tensor(0.4456)\n",
      "1493 Training Loss: tensor(0.4457)\n",
      "1494 Training Loss: tensor(0.4444)\n",
      "1495 Training Loss: tensor(0.4433)\n",
      "1496 Training Loss: tensor(0.4464)\n",
      "1497 Training Loss: tensor(0.4438)\n",
      "1498 Training Loss: tensor(0.4445)\n",
      "1499 Training Loss: tensor(0.4437)\n",
      "1500 Training Loss: tensor(0.4437)\n",
      "1501 Training Loss: tensor(0.4419)\n",
      "1502 Training Loss: tensor(0.4419)\n",
      "1503 Training Loss: tensor(0.4419)\n",
      "1504 Training Loss: tensor(0.4411)\n",
      "1505 Training Loss: tensor(0.4426)\n",
      "1506 Training Loss: tensor(0.4411)\n",
      "1507 Training Loss: tensor(0.4409)\n",
      "1508 Training Loss: tensor(0.4419)\n",
      "1509 Training Loss: tensor(0.4408)\n",
      "1510 Training Loss: tensor(0.4393)\n",
      "1511 Training Loss: tensor(0.4382)\n",
      "1512 Training Loss: tensor(0.4390)\n",
      "1513 Training Loss: tensor(0.4377)\n",
      "1514 Training Loss: tensor(0.4380)\n",
      "1515 Training Loss: tensor(0.4399)\n",
      "1516 Training Loss: tensor(0.4392)\n",
      "1517 Training Loss: tensor(0.4379)\n",
      "1518 Training Loss: tensor(0.4389)\n",
      "1519 Training Loss: tensor(0.4353)\n",
      "1520 Training Loss: tensor(0.4396)\n",
      "1521 Training Loss: tensor(0.4356)\n",
      "1522 Training Loss: tensor(0.4373)\n",
      "1523 Training Loss: tensor(0.4357)\n",
      "1524 Training Loss: tensor(0.4341)\n",
      "1525 Training Loss: tensor(0.4361)\n",
      "1526 Training Loss: tensor(0.4364)\n",
      "1527 Training Loss: tensor(0.4350)\n",
      "1528 Training Loss: tensor(0.4357)\n",
      "1529 Training Loss: tensor(0.4345)\n",
      "1530 Training Loss: tensor(0.4336)\n",
      "1531 Training Loss: tensor(0.4331)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532 Training Loss: tensor(0.4340)\n",
      "1533 Training Loss: tensor(0.4343)\n",
      "1534 Training Loss: tensor(0.4333)\n",
      "1535 Training Loss: tensor(0.4344)\n",
      "1536 Training Loss: tensor(0.4345)\n",
      "1537 Training Loss: tensor(0.4310)\n",
      "1538 Training Loss: tensor(0.4309)\n",
      "1539 Training Loss: tensor(0.4314)\n",
      "1540 Training Loss: tensor(0.4309)\n",
      "1541 Training Loss: tensor(0.4300)\n",
      "1542 Training Loss: tensor(0.4319)\n",
      "1543 Training Loss: tensor(0.4292)\n",
      "1544 Training Loss: tensor(0.4296)\n",
      "1545 Training Loss: tensor(0.4284)\n",
      "1546 Training Loss: tensor(0.4299)\n",
      "1547 Training Loss: tensor(0.4297)\n",
      "1548 Training Loss: tensor(0.4301)\n",
      "1549 Training Loss: tensor(0.4299)\n",
      "1550 Training Loss: tensor(0.4289)\n",
      "1551 Training Loss: tensor(0.4259)\n",
      "1552 Training Loss: tensor(0.4276)\n",
      "1553 Training Loss: tensor(0.4269)\n",
      "1554 Training Loss: tensor(0.4250)\n",
      "1555 Training Loss: tensor(0.4258)\n",
      "1556 Training Loss: tensor(0.4265)\n",
      "1557 Training Loss: tensor(0.4310)\n",
      "1558 Training Loss: tensor(0.4261)\n",
      "1559 Training Loss: tensor(0.4272)\n",
      "1560 Training Loss: tensor(0.4255)\n",
      "1561 Training Loss: tensor(0.4254)\n",
      "1562 Training Loss: tensor(0.4282)\n",
      "1563 Training Loss: tensor(0.4254)\n",
      "1564 Training Loss: tensor(0.4265)\n",
      "1565 Training Loss: tensor(0.4240)\n",
      "1566 Training Loss: tensor(0.4244)\n",
      "1567 Training Loss: tensor(0.4244)\n",
      "1568 Training Loss: tensor(0.4226)\n",
      "1569 Training Loss: tensor(0.4221)\n",
      "1570 Training Loss: tensor(0.4218)\n",
      "1571 Training Loss: tensor(0.4233)\n",
      "1572 Training Loss: tensor(0.4227)\n",
      "1573 Training Loss: tensor(0.4215)\n",
      "1574 Training Loss: tensor(0.4257)\n",
      "1575 Training Loss: tensor(0.4206)\n",
      "1576 Training Loss: tensor(0.4230)\n",
      "1577 Training Loss: tensor(0.4217)\n",
      "1578 Training Loss: tensor(0.4205)\n",
      "1579 Training Loss: tensor(0.4215)\n",
      "1580 Training Loss: tensor(0.4197)\n",
      "1581 Training Loss: tensor(0.4242)\n",
      "1582 Training Loss: tensor(0.4242)\n",
      "1583 Training Loss: tensor(0.4209)\n",
      "1584 Training Loss: tensor(0.4224)\n",
      "1585 Training Loss: tensor(0.4211)\n",
      "1586 Training Loss: tensor(0.4201)\n",
      "1587 Training Loss: tensor(0.4222)\n",
      "1588 Training Loss: tensor(0.4210)\n",
      "1589 Training Loss: tensor(0.4208)\n",
      "1590 Training Loss: tensor(0.4212)\n",
      "1591 Training Loss: tensor(0.4198)\n",
      "1592 Training Loss: tensor(0.4190)\n",
      "1593 Training Loss: tensor(0.4205)\n",
      "1594 Training Loss: tensor(0.4196)\n",
      "1595 Training Loss: tensor(0.4188)\n",
      "1596 Training Loss: tensor(0.4188)\n",
      "1597 Training Loss: tensor(0.4181)\n",
      "1598 Training Loss: tensor(0.4171)\n",
      "1599 Training Loss: tensor(0.4212)\n",
      "1600 Training Loss: tensor(0.4160)\n",
      "1601 Training Loss: tensor(0.4184)\n",
      "1602 Training Loss: tensor(0.4167)\n",
      "1603 Training Loss: tensor(0.4165)\n",
      "1604 Training Loss: tensor(0.4181)\n",
      "1605 Training Loss: tensor(0.4167)\n",
      "1606 Training Loss: tensor(0.4154)\n",
      "1607 Training Loss: tensor(0.4174)\n",
      "1608 Training Loss: tensor(0.4150)\n",
      "1609 Training Loss: tensor(0.4136)\n",
      "1610 Training Loss: tensor(0.4154)\n",
      "1611 Training Loss: tensor(0.4134)\n",
      "1612 Training Loss: tensor(0.4161)\n",
      "1613 Training Loss: tensor(0.4165)\n",
      "1614 Training Loss: tensor(0.4168)\n",
      "1615 Training Loss: tensor(0.4158)\n",
      "1616 Training Loss: tensor(0.4168)\n",
      "1617 Training Loss: tensor(0.4121)\n",
      "1618 Training Loss: tensor(0.4135)\n",
      "1619 Training Loss: tensor(0.4180)\n",
      "1620 Training Loss: tensor(0.4151)\n",
      "1621 Training Loss: tensor(0.4128)\n",
      "1622 Training Loss: tensor(0.4150)\n",
      "1623 Training Loss: tensor(0.4144)\n",
      "1624 Training Loss: tensor(0.4153)\n",
      "1625 Training Loss: tensor(0.4118)\n",
      "1626 Training Loss: tensor(0.4133)\n",
      "1627 Training Loss: tensor(0.4115)\n",
      "1628 Training Loss: tensor(0.4132)\n",
      "1629 Training Loss: tensor(0.4110)\n",
      "1630 Training Loss: tensor(0.4118)\n",
      "1631 Training Loss: tensor(0.4130)\n",
      "1632 Training Loss: tensor(0.4121)\n",
      "1633 Training Loss: tensor(0.4153)\n",
      "1634 Training Loss: tensor(0.4130)\n",
      "1635 Training Loss: tensor(0.4144)\n",
      "1636 Training Loss: tensor(0.4132)\n",
      "1637 Training Loss: tensor(0.4124)\n",
      "1638 Training Loss: tensor(0.4081)\n",
      "1639 Training Loss: tensor(0.4115)\n",
      "1640 Training Loss: tensor(0.4125)\n",
      "1641 Training Loss: tensor(0.4114)\n",
      "1642 Training Loss: tensor(0.4111)\n",
      "1643 Training Loss: tensor(0.4117)\n",
      "1644 Training Loss: tensor(0.4096)\n",
      "1645 Training Loss: tensor(0.4111)\n",
      "1646 Training Loss: tensor(0.4096)\n",
      "1647 Training Loss: tensor(0.4065)\n",
      "1648 Training Loss: tensor(0.4109)\n",
      "1649 Training Loss: tensor(0.4065)\n",
      "1650 Training Loss: tensor(0.4113)\n",
      "1651 Training Loss: tensor(0.4112)\n",
      "1652 Training Loss: tensor(0.4091)\n",
      "1653 Training Loss: tensor(0.4061)\n",
      "1654 Training Loss: tensor(0.4072)\n",
      "1655 Training Loss: tensor(0.4097)\n",
      "1656 Training Loss: tensor(0.4094)\n",
      "1657 Training Loss: tensor(0.4072)\n",
      "1658 Training Loss: tensor(0.4069)\n",
      "1659 Training Loss: tensor(0.4089)\n",
      "1660 Training Loss: tensor(0.4105)\n",
      "1661 Training Loss: tensor(0.4085)\n",
      "1662 Training Loss: tensor(0.4072)\n",
      "1663 Training Loss: tensor(0.4070)\n",
      "1664 Training Loss: tensor(0.4116)\n",
      "1665 Training Loss: tensor(0.4117)\n",
      "1666 Training Loss: tensor(0.4053)\n",
      "1667 Training Loss: tensor(0.4067)\n",
      "1668 Training Loss: tensor(0.4079)\n",
      "1669 Training Loss: tensor(0.4081)\n",
      "1670 Training Loss: tensor(0.4071)\n",
      "1671 Training Loss: tensor(0.4072)\n",
      "1672 Training Loss: tensor(0.4051)\n",
      "1673 Training Loss: tensor(0.4084)\n",
      "1674 Training Loss: tensor(0.4067)\n",
      "1675 Training Loss: tensor(0.4079)\n",
      "1676 Training Loss: tensor(0.4042)\n",
      "1677 Training Loss: tensor(0.4087)\n",
      "1678 Training Loss: tensor(0.4082)\n",
      "1679 Training Loss: tensor(0.4080)\n",
      "1680 Training Loss: tensor(0.4072)\n",
      "1681 Training Loss: tensor(0.4078)\n",
      "1682 Training Loss: tensor(0.4066)\n",
      "1683 Training Loss: tensor(0.4039)\n",
      "1684 Training Loss: tensor(0.4050)\n",
      "1685 Training Loss: tensor(0.4081)\n",
      "1686 Training Loss: tensor(0.4077)\n",
      "1687 Training Loss: tensor(0.4044)\n",
      "1688 Training Loss: tensor(0.4050)\n",
      "1689 Training Loss: tensor(0.4068)\n",
      "1690 Training Loss: tensor(0.4051)\n",
      "1691 Training Loss: tensor(0.4101)\n",
      "1692 Training Loss: tensor(0.4078)\n",
      "1693 Training Loss: tensor(0.4057)\n",
      "1694 Training Loss: tensor(0.4054)\n",
      "1695 Training Loss: tensor(0.4061)\n",
      "1696 Training Loss: tensor(0.4059)\n",
      "1697 Training Loss: tensor(0.4040)\n",
      "1698 Training Loss: tensor(0.4038)\n",
      "1699 Training Loss: tensor(0.4044)\n",
      "1700 Training Loss: tensor(0.4072)\n",
      "1701 Training Loss: tensor(0.4066)\n",
      "1702 Training Loss: tensor(0.4050)\n",
      "1703 Training Loss: tensor(0.4031)\n",
      "1704 Training Loss: tensor(0.4085)\n",
      "1705 Training Loss: tensor(0.4079)\n",
      "1706 Training Loss: tensor(0.4098)\n",
      "1707 Training Loss: tensor(0.4028)\n",
      "1708 Training Loss: tensor(0.4087)\n",
      "1709 Training Loss: tensor(0.4038)\n",
      "1710 Training Loss: tensor(0.4044)\n",
      "1711 Training Loss: tensor(0.4037)\n",
      "1712 Training Loss: tensor(0.4058)\n",
      "1713 Training Loss: tensor(0.4042)\n",
      "1714 Training Loss: tensor(0.4081)\n",
      "1715 Training Loss: tensor(0.4032)\n",
      "1716 Training Loss: tensor(0.4055)\n",
      "1717 Training Loss: tensor(0.4057)\n",
      "1718 Training Loss: tensor(0.4027)\n",
      "1719 Training Loss: tensor(0.4034)\n",
      "1720 Training Loss: tensor(0.4024)\n",
      "1721 Training Loss: tensor(0.4072)\n",
      "1722 Training Loss: tensor(0.4041)\n",
      "1723 Training Loss: tensor(0.4044)\n",
      "1724 Training Loss: tensor(0.4035)\n",
      "1725 Training Loss: tensor(0.4064)\n",
      "1726 Training Loss: tensor(0.4052)\n",
      "1727 Training Loss: tensor(0.4039)\n",
      "1728 Training Loss: tensor(0.4035)\n",
      "1729 Training Loss: tensor(0.4025)\n",
      "1730 Training Loss: tensor(0.4049)\n",
      "1731 Training Loss: tensor(0.4020)\n",
      "1732 Training Loss: tensor(0.4044)\n",
      "1733 Training Loss: tensor(0.4010)\n",
      "1734 Training Loss: tensor(0.4012)\n",
      "1735 Training Loss: tensor(0.4050)\n",
      "1736 Training Loss: tensor(0.4061)\n",
      "1737 Training Loss: tensor(0.4029)\n",
      "1738 Training Loss: tensor(0.4014)\n",
      "1739 Training Loss: tensor(0.4024)\n",
      "1740 Training Loss: tensor(0.4029)\n",
      "1741 Training Loss: tensor(0.3999)\n",
      "1742 Training Loss: tensor(0.4037)\n",
      "1743 Training Loss: tensor(0.4006)\n",
      "1744 Training Loss: tensor(0.3995)\n",
      "1745 Training Loss: tensor(0.3989)\n",
      "1746 Training Loss: tensor(0.4014)\n",
      "1747 Training Loss: tensor(0.4032)\n",
      "1748 Training Loss: tensor(0.4082)\n",
      "1749 Training Loss: tensor(0.4007)\n",
      "1750 Training Loss: tensor(0.4033)\n",
      "1751 Training Loss: tensor(0.3987)\n",
      "1752 Training Loss: tensor(0.4015)\n",
      "1753 Training Loss: tensor(0.4000)\n",
      "1754 Training Loss: tensor(0.4041)\n",
      "1755 Training Loss: tensor(0.4050)\n",
      "1756 Training Loss: tensor(0.4020)\n",
      "1757 Training Loss: tensor(0.4019)\n",
      "1758 Training Loss: tensor(0.4026)\n",
      "1759 Training Loss: tensor(0.4005)\n",
      "1760 Training Loss: tensor(0.4043)\n",
      "1761 Training Loss: tensor(0.4005)\n",
      "1762 Training Loss: tensor(0.4008)\n",
      "1763 Training Loss: tensor(0.3987)\n",
      "1764 Training Loss: tensor(0.4008)\n",
      "1765 Training Loss: tensor(0.4038)\n",
      "1766 Training Loss: tensor(0.4017)\n",
      "1767 Training Loss: tensor(0.4002)\n",
      "1768 Training Loss: tensor(0.4014)\n",
      "1769 Training Loss: tensor(0.3992)\n",
      "1770 Training Loss: tensor(0.4005)\n",
      "1771 Training Loss: tensor(0.3991)\n",
      "1772 Training Loss: tensor(0.4054)\n",
      "1773 Training Loss: tensor(0.4014)\n",
      "1774 Training Loss: tensor(0.3994)\n",
      "1775 Training Loss: tensor(0.4035)\n",
      "1776 Training Loss: tensor(0.4012)\n",
      "1777 Training Loss: tensor(0.4018)\n",
      "1778 Training Loss: tensor(0.3984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1779 Training Loss: tensor(0.3975)\n",
      "1780 Training Loss: tensor(0.4015)\n",
      "1781 Training Loss: tensor(0.3993)\n",
      "1782 Training Loss: tensor(0.4004)\n",
      "1783 Training Loss: tensor(0.3997)\n",
      "1784 Training Loss: tensor(0.4004)\n",
      "1785 Training Loss: tensor(0.4025)\n",
      "1786 Training Loss: tensor(0.4022)\n",
      "1787 Training Loss: tensor(0.4010)\n",
      "1788 Training Loss: tensor(0.3984)\n",
      "1789 Training Loss: tensor(0.4040)\n",
      "1790 Training Loss: tensor(0.4023)\n",
      "1791 Training Loss: tensor(0.3975)\n",
      "1792 Training Loss: tensor(0.3989)\n",
      "1793 Training Loss: tensor(0.3998)\n",
      "1794 Training Loss: tensor(0.3994)\n",
      "1795 Training Loss: tensor(0.3965)\n",
      "1796 Training Loss: tensor(0.3978)\n",
      "1797 Training Loss: tensor(0.4013)\n",
      "1798 Training Loss: tensor(0.4002)\n",
      "1799 Training Loss: tensor(0.4039)\n",
      "1800 Training Loss: tensor(0.4031)\n",
      "1801 Training Loss: tensor(0.4007)\n",
      "1802 Training Loss: tensor(0.3991)\n",
      "1803 Training Loss: tensor(0.4004)\n",
      "1804 Training Loss: tensor(0.4011)\n",
      "1805 Training Loss: tensor(0.3995)\n",
      "1806 Training Loss: tensor(0.4004)\n",
      "1807 Training Loss: tensor(0.3995)\n",
      "1808 Training Loss: tensor(0.4013)\n",
      "1809 Training Loss: tensor(0.3985)\n",
      "1810 Training Loss: tensor(0.3975)\n",
      "1811 Training Loss: tensor(0.3998)\n",
      "1812 Training Loss: tensor(0.3989)\n",
      "1813 Training Loss: tensor(0.4028)\n",
      "1814 Training Loss: tensor(0.3969)\n",
      "1815 Training Loss: tensor(0.3980)\n",
      "1816 Training Loss: tensor(0.3992)\n",
      "1817 Training Loss: tensor(0.3970)\n",
      "1818 Training Loss: tensor(0.4013)\n",
      "1819 Training Loss: tensor(0.3999)\n",
      "1820 Training Loss: tensor(0.3998)\n",
      "1821 Training Loss: tensor(0.3982)\n",
      "1822 Training Loss: tensor(0.3993)\n",
      "1823 Training Loss: tensor(0.3983)\n",
      "1824 Training Loss: tensor(0.4004)\n",
      "1825 Training Loss: tensor(0.3991)\n",
      "1826 Training Loss: tensor(0.4001)\n",
      "1827 Training Loss: tensor(0.3968)\n",
      "1828 Training Loss: tensor(0.3968)\n",
      "1829 Training Loss: tensor(0.3965)\n",
      "1830 Training Loss: tensor(0.4023)\n",
      "1831 Training Loss: tensor(0.3971)\n",
      "1832 Training Loss: tensor(0.3947)\n",
      "1833 Training Loss: tensor(0.3969)\n",
      "1834 Training Loss: tensor(0.4010)\n",
      "1835 Training Loss: tensor(0.3979)\n",
      "1836 Training Loss: tensor(0.3980)\n",
      "1837 Training Loss: tensor(0.3979)\n",
      "1838 Training Loss: tensor(0.3981)\n",
      "1839 Training Loss: tensor(0.4004)\n",
      "1840 Training Loss: tensor(0.3979)\n",
      "1841 Training Loss: tensor(0.4007)\n",
      "1842 Training Loss: tensor(0.4001)\n",
      "1843 Training Loss: tensor(0.3964)\n",
      "1844 Training Loss: tensor(0.3977)\n",
      "1845 Training Loss: tensor(0.3967)\n",
      "1846 Training Loss: tensor(0.3983)\n",
      "1847 Training Loss: tensor(0.3989)\n",
      "1848 Training Loss: tensor(0.3978)\n",
      "1849 Training Loss: tensor(0.3977)\n",
      "1850 Training Loss: tensor(0.3991)\n",
      "1851 Training Loss: tensor(0.3994)\n",
      "1852 Training Loss: tensor(0.3946)\n",
      "1853 Training Loss: tensor(0.3987)\n",
      "1854 Training Loss: tensor(0.3992)\n",
      "1855 Training Loss: tensor(0.3984)\n",
      "1856 Training Loss: tensor(0.3971)\n",
      "1857 Training Loss: tensor(0.4011)\n",
      "1858 Training Loss: tensor(0.3979)\n",
      "1859 Training Loss: tensor(0.4006)\n",
      "1860 Training Loss: tensor(0.3959)\n",
      "1861 Training Loss: tensor(0.3995)\n",
      "1862 Training Loss: tensor(0.3962)\n",
      "1863 Training Loss: tensor(0.3991)\n",
      "1864 Training Loss: tensor(0.4005)\n",
      "1865 Training Loss: tensor(0.3963)\n",
      "1866 Training Loss: tensor(0.3964)\n",
      "1867 Training Loss: tensor(0.3977)\n",
      "1868 Training Loss: tensor(0.3967)\n",
      "1869 Training Loss: tensor(0.3979)\n",
      "1870 Training Loss: tensor(0.3962)\n",
      "1871 Training Loss: tensor(0.3980)\n",
      "1872 Training Loss: tensor(0.3958)\n",
      "1873 Training Loss: tensor(0.3972)\n",
      "1874 Training Loss: tensor(0.4004)\n",
      "1875 Training Loss: tensor(0.3992)\n",
      "1876 Training Loss: tensor(0.3955)\n",
      "1877 Training Loss: tensor(0.3956)\n",
      "1878 Training Loss: tensor(0.3970)\n",
      "1879 Training Loss: tensor(0.3992)\n",
      "1880 Training Loss: tensor(0.3956)\n",
      "1881 Training Loss: tensor(0.3963)\n",
      "1882 Training Loss: tensor(0.3976)\n",
      "1883 Training Loss: tensor(0.3998)\n",
      "1884 Training Loss: tensor(0.3986)\n",
      "1885 Training Loss: tensor(0.3954)\n",
      "1886 Training Loss: tensor(0.3995)\n",
      "1887 Training Loss: tensor(0.3969)\n",
      "1888 Training Loss: tensor(0.3975)\n",
      "1889 Training Loss: tensor(0.3984)\n",
      "1890 Training Loss: tensor(0.3978)\n",
      "1891 Training Loss: tensor(0.3965)\n",
      "1892 Training Loss: tensor(0.3958)\n",
      "1893 Training Loss: tensor(0.3956)\n",
      "1894 Training Loss: tensor(0.3947)\n",
      "1895 Training Loss: tensor(0.3943)\n",
      "1896 Training Loss: tensor(0.3946)\n",
      "1897 Training Loss: tensor(0.3993)\n",
      "1898 Training Loss: tensor(0.4005)\n",
      "1899 Training Loss: tensor(0.3972)\n",
      "1900 Training Loss: tensor(0.3976)\n",
      "1901 Training Loss: tensor(0.3948)\n",
      "1902 Training Loss: tensor(0.3970)\n",
      "1903 Training Loss: tensor(0.3936)\n",
      "1904 Training Loss: tensor(0.3945)\n",
      "1905 Training Loss: tensor(0.3962)\n",
      "1906 Training Loss: tensor(0.3960)\n",
      "1907 Training Loss: tensor(0.3983)\n",
      "1908 Training Loss: tensor(0.3951)\n",
      "1909 Training Loss: tensor(0.3981)\n",
      "1910 Training Loss: tensor(0.3965)\n",
      "1911 Training Loss: tensor(0.3991)\n",
      "1912 Training Loss: tensor(0.3964)\n",
      "1913 Training Loss: tensor(0.3971)\n",
      "1914 Training Loss: tensor(0.3953)\n",
      "1915 Training Loss: tensor(0.3972)\n",
      "1916 Training Loss: tensor(0.3941)\n",
      "1917 Training Loss: tensor(0.3974)\n",
      "1918 Training Loss: tensor(0.3951)\n",
      "1919 Training Loss: tensor(0.3925)\n",
      "1920 Training Loss: tensor(0.3958)\n",
      "1921 Training Loss: tensor(0.3957)\n",
      "1922 Training Loss: tensor(0.3937)\n",
      "1923 Training Loss: tensor(0.3971)\n",
      "1924 Training Loss: tensor(0.3995)\n",
      "1925 Training Loss: tensor(0.3960)\n",
      "1926 Training Loss: tensor(0.3966)\n",
      "1927 Training Loss: tensor(0.3981)\n",
      "1928 Training Loss: tensor(0.3923)\n",
      "1929 Training Loss: tensor(0.3953)\n",
      "1930 Training Loss: tensor(0.3939)\n",
      "1931 Training Loss: tensor(0.3969)\n",
      "1932 Training Loss: tensor(0.3986)\n",
      "1933 Training Loss: tensor(0.3971)\n",
      "1934 Training Loss: tensor(0.3965)\n",
      "1935 Training Loss: tensor(0.3945)\n",
      "1936 Training Loss: tensor(0.3977)\n",
      "1937 Training Loss: tensor(0.3966)\n",
      "1938 Training Loss: tensor(0.3969)\n",
      "1939 Training Loss: tensor(0.3995)\n",
      "1940 Training Loss: tensor(0.3946)\n",
      "1941 Training Loss: tensor(0.3964)\n",
      "1942 Training Loss: tensor(0.3968)\n",
      "1943 Training Loss: tensor(0.3964)\n",
      "1944 Training Loss: tensor(0.3975)\n",
      "1945 Training Loss: tensor(0.3956)\n",
      "1946 Training Loss: tensor(0.3971)\n",
      "1947 Training Loss: tensor(0.3991)\n",
      "1948 Training Loss: tensor(0.3952)\n",
      "1949 Training Loss: tensor(0.3950)\n",
      "1950 Training Loss: tensor(0.3976)\n",
      "1951 Training Loss: tensor(0.3959)\n",
      "1952 Training Loss: tensor(0.3971)\n",
      "1953 Training Loss: tensor(0.3937)\n",
      "1954 Training Loss: tensor(0.3980)\n",
      "1955 Training Loss: tensor(0.3926)\n",
      "1956 Training Loss: tensor(0.3995)\n",
      "1957 Training Loss: tensor(0.3930)\n",
      "1958 Training Loss: tensor(0.3934)\n",
      "1959 Training Loss: tensor(0.3989)\n",
      "1960 Training Loss: tensor(0.3964)\n",
      "1961 Training Loss: tensor(0.3944)\n",
      "1962 Training Loss: tensor(0.3949)\n",
      "1963 Training Loss: tensor(0.3946)\n",
      "1964 Training Loss: tensor(0.3952)\n",
      "1965 Training Loss: tensor(0.3968)\n",
      "1966 Training Loss: tensor(0.3927)\n",
      "1967 Training Loss: tensor(0.3931)\n",
      "1968 Training Loss: tensor(0.3948)\n",
      "1969 Training Loss: tensor(0.3993)\n",
      "1970 Training Loss: tensor(0.3938)\n",
      "1971 Training Loss: tensor(0.3938)\n",
      "1972 Training Loss: tensor(0.3951)\n",
      "1973 Training Loss: tensor(0.3982)\n",
      "1974 Training Loss: tensor(0.4002)\n",
      "1975 Training Loss: tensor(0.3966)\n",
      "1976 Training Loss: tensor(0.3965)\n",
      "1977 Training Loss: tensor(0.3955)\n",
      "1978 Training Loss: tensor(0.3923)\n",
      "1979 Training Loss: tensor(0.3947)\n",
      "1980 Training Loss: tensor(0.3957)\n",
      "1981 Training Loss: tensor(0.3953)\n",
      "1982 Training Loss: tensor(0.3956)\n",
      "1983 Training Loss: tensor(0.3945)\n",
      "1984 Training Loss: tensor(0.3929)\n",
      "1985 Training Loss: tensor(0.3956)\n",
      "1986 Training Loss: tensor(0.3938)\n",
      "1987 Training Loss: tensor(0.3952)\n",
      "1988 Training Loss: tensor(0.3923)\n",
      "1989 Training Loss: tensor(0.3973)\n",
      "1990 Training Loss: tensor(0.4007)\n",
      "1991 Training Loss: tensor(0.3949)\n",
      "1992 Training Loss: tensor(0.3954)\n",
      "1993 Training Loss: tensor(0.3910)\n",
      "1994 Training Loss: tensor(0.3995)\n",
      "1995 Training Loss: tensor(0.3926)\n",
      "1996 Training Loss: tensor(0.3949)\n",
      "1997 Training Loss: tensor(0.3964)\n",
      "1998 Training Loss: tensor(0.3956)\n",
      "1999 Training Loss: tensor(0.3944)\n",
      "2000 Training Loss: tensor(0.3946)\n",
      "2001 Training Loss: tensor(0.3964)\n",
      "2002 Training Loss: tensor(0.3966)\n",
      "2003 Training Loss: tensor(0.3926)\n",
      "2004 Training Loss: tensor(0.3966)\n",
      "2005 Training Loss: tensor(0.3957)\n",
      "2006 Training Loss: tensor(0.3953)\n",
      "2007 Training Loss: tensor(0.3983)\n",
      "2008 Training Loss: tensor(0.3964)\n",
      "2009 Training Loss: tensor(0.3950)\n",
      "2010 Training Loss: tensor(0.3915)\n",
      "2011 Training Loss: tensor(0.3939)\n",
      "2012 Training Loss: tensor(0.3955)\n",
      "2013 Training Loss: tensor(0.3937)\n",
      "2014 Training Loss: tensor(0.3926)\n",
      "2015 Training Loss: tensor(0.3941)\n",
      "2016 Training Loss: tensor(0.3953)\n",
      "2017 Training Loss: tensor(0.3972)\n",
      "2018 Training Loss: tensor(0.3962)\n",
      "2019 Training Loss: tensor(0.3923)\n",
      "2020 Training Loss: tensor(0.3957)\n",
      "2021 Training Loss: tensor(0.3975)\n",
      "2022 Training Loss: tensor(0.3986)\n",
      "2023 Training Loss: tensor(0.3927)\n",
      "2024 Training Loss: tensor(0.3932)\n",
      "2025 Training Loss: tensor(0.3966)\n",
      "2026 Training Loss: tensor(0.3929)\n",
      "2027 Training Loss: tensor(0.3971)\n",
      "2028 Training Loss: tensor(0.3976)\n",
      "2029 Training Loss: tensor(0.3979)\n",
      "2030 Training Loss: tensor(0.3938)\n",
      "2031 Training Loss: tensor(0.3967)\n",
      "2032 Training Loss: tensor(0.3934)\n",
      "2033 Training Loss: tensor(0.3945)\n",
      "2034 Training Loss: tensor(0.3927)\n",
      "2035 Training Loss: tensor(0.3989)\n",
      "2036 Training Loss: tensor(0.3955)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2037 Training Loss: tensor(0.3922)\n",
      "2038 Training Loss: tensor(0.3961)\n",
      "2039 Training Loss: tensor(0.3937)\n",
      "2040 Training Loss: tensor(0.3955)\n",
      "2041 Training Loss: tensor(0.3939)\n",
      "2042 Training Loss: tensor(0.3936)\n",
      "2043 Training Loss: tensor(0.3925)\n",
      "2044 Training Loss: tensor(0.3935)\n",
      "2045 Training Loss: tensor(0.3938)\n",
      "2046 Training Loss: tensor(0.3935)\n",
      "2047 Training Loss: tensor(0.3991)\n",
      "2048 Training Loss: tensor(0.3952)\n",
      "2049 Training Loss: tensor(0.3957)\n",
      "2050 Training Loss: tensor(0.3951)\n",
      "2051 Training Loss: tensor(0.3949)\n",
      "2052 Training Loss: tensor(0.3908)\n",
      "2053 Training Loss: tensor(0.3935)\n",
      "2054 Training Loss: tensor(0.3955)\n",
      "2055 Training Loss: tensor(0.3941)\n",
      "2056 Training Loss: tensor(0.3935)\n",
      "2057 Training Loss: tensor(0.3945)\n",
      "2058 Training Loss: tensor(0.3947)\n",
      "2059 Training Loss: tensor(0.3959)\n",
      "2060 Training Loss: tensor(0.3943)\n",
      "2061 Training Loss: tensor(0.3948)\n",
      "2062 Training Loss: tensor(0.3947)\n",
      "2063 Training Loss: tensor(0.3955)\n",
      "2064 Training Loss: tensor(0.3940)\n",
      "2065 Training Loss: tensor(0.3915)\n",
      "2066 Training Loss: tensor(0.3903)\n",
      "2067 Training Loss: tensor(0.3940)\n",
      "2068 Training Loss: tensor(0.3929)\n",
      "2069 Training Loss: tensor(0.3912)\n",
      "2070 Training Loss: tensor(0.3938)\n",
      "2071 Training Loss: tensor(0.3973)\n",
      "2072 Training Loss: tensor(0.3944)\n",
      "2073 Training Loss: tensor(0.3917)\n",
      "2074 Training Loss: tensor(0.3950)\n",
      "2075 Training Loss: tensor(0.3945)\n",
      "2076 Training Loss: tensor(0.3933)\n",
      "2077 Training Loss: tensor(0.3940)\n",
      "2078 Training Loss: tensor(0.3948)\n",
      "2079 Training Loss: tensor(0.3899)\n",
      "2080 Training Loss: tensor(0.3945)\n",
      "2081 Training Loss: tensor(0.3954)\n",
      "2082 Training Loss: tensor(0.3950)\n",
      "2083 Training Loss: tensor(0.3951)\n",
      "2084 Training Loss: tensor(0.3930)\n",
      "2085 Training Loss: tensor(0.3939)\n",
      "2086 Training Loss: tensor(0.3950)\n",
      "2087 Training Loss: tensor(0.3916)\n",
      "2088 Training Loss: tensor(0.3930)\n",
      "2089 Training Loss: tensor(0.3930)\n",
      "2090 Training Loss: tensor(0.3959)\n",
      "2091 Training Loss: tensor(0.3939)\n",
      "2092 Training Loss: tensor(0.3974)\n",
      "2093 Training Loss: tensor(0.3913)\n",
      "2094 Training Loss: tensor(0.3921)\n",
      "2095 Training Loss: tensor(0.3951)\n",
      "2096 Training Loss: tensor(0.3929)\n",
      "2097 Training Loss: tensor(0.3920)\n",
      "2098 Training Loss: tensor(0.3923)\n",
      "2099 Training Loss: tensor(0.3913)\n",
      "2100 Training Loss: tensor(0.3939)\n",
      "2101 Training Loss: tensor(0.3951)\n",
      "2102 Training Loss: tensor(0.3948)\n",
      "2103 Training Loss: tensor(0.3905)\n",
      "2104 Training Loss: tensor(0.3960)\n",
      "2105 Training Loss: tensor(0.3930)\n",
      "2106 Training Loss: tensor(0.3935)\n",
      "2107 Training Loss: tensor(0.3906)\n",
      "2108 Training Loss: tensor(0.3963)\n",
      "2109 Training Loss: tensor(0.3923)\n",
      "2110 Training Loss: tensor(0.3945)\n",
      "2111 Training Loss: tensor(0.3950)\n",
      "2112 Training Loss: tensor(0.3923)\n",
      "2113 Training Loss: tensor(0.3921)\n",
      "2114 Training Loss: tensor(0.3937)\n",
      "2115 Training Loss: tensor(0.3928)\n",
      "2116 Training Loss: tensor(0.3937)\n",
      "2117 Training Loss: tensor(0.3920)\n",
      "2118 Training Loss: tensor(0.3915)\n",
      "2119 Training Loss: tensor(0.3933)\n",
      "2120 Training Loss: tensor(0.3932)\n",
      "2121 Training Loss: tensor(0.3946)\n",
      "2122 Training Loss: tensor(0.3948)\n",
      "2123 Training Loss: tensor(0.3932)\n",
      "2124 Training Loss: tensor(0.3946)\n",
      "2125 Training Loss: tensor(0.3942)\n",
      "2126 Training Loss: tensor(0.3930)\n",
      "2127 Training Loss: tensor(0.3946)\n",
      "2128 Training Loss: tensor(0.3917)\n",
      "2129 Training Loss: tensor(0.3899)\n",
      "2130 Training Loss: tensor(0.3926)\n",
      "2131 Training Loss: tensor(0.3941)\n",
      "2132 Training Loss: tensor(0.3917)\n",
      "2133 Training Loss: tensor(0.3931)\n",
      "2134 Training Loss: tensor(0.3918)\n",
      "2135 Training Loss: tensor(0.3896)\n",
      "2136 Training Loss: tensor(0.3962)\n",
      "2137 Training Loss: tensor(0.3916)\n",
      "2138 Training Loss: tensor(0.3953)\n",
      "2139 Training Loss: tensor(0.3960)\n",
      "2140 Training Loss: tensor(0.3950)\n",
      "2141 Training Loss: tensor(0.3937)\n",
      "2142 Training Loss: tensor(0.3937)\n",
      "2143 Training Loss: tensor(0.3899)\n",
      "2144 Training Loss: tensor(0.4002)\n",
      "2145 Training Loss: tensor(0.3947)\n",
      "2146 Training Loss: tensor(0.3942)\n",
      "2147 Training Loss: tensor(0.3985)\n",
      "2148 Training Loss: tensor(0.3950)\n",
      "2149 Training Loss: tensor(0.3952)\n",
      "2150 Training Loss: tensor(0.3913)\n",
      "2151 Training Loss: tensor(0.3935)\n",
      "2152 Training Loss: tensor(0.3921)\n",
      "2153 Training Loss: tensor(0.3945)\n",
      "2154 Training Loss: tensor(0.3939)\n",
      "2155 Training Loss: tensor(0.4007)\n",
      "2156 Training Loss: tensor(0.3958)\n",
      "2157 Training Loss: tensor(0.3928)\n",
      "2158 Training Loss: tensor(0.3920)\n",
      "2159 Training Loss: tensor(0.3934)\n",
      "2160 Training Loss: tensor(0.3929)\n",
      "2161 Training Loss: tensor(0.3909)\n",
      "2162 Training Loss: tensor(0.3939)\n",
      "2163 Training Loss: tensor(0.3954)\n",
      "2164 Training Loss: tensor(0.3904)\n",
      "2165 Training Loss: tensor(0.3929)\n",
      "2166 Training Loss: tensor(0.3944)\n",
      "2167 Training Loss: tensor(0.3915)\n",
      "2168 Training Loss: tensor(0.3929)\n",
      "2169 Training Loss: tensor(0.3956)\n",
      "2170 Training Loss: tensor(0.3934)\n",
      "2171 Training Loss: tensor(0.3924)\n",
      "2172 Training Loss: tensor(0.3927)\n",
      "2173 Training Loss: tensor(0.3916)\n",
      "2174 Training Loss: tensor(0.3942)\n",
      "2175 Training Loss: tensor(0.3946)\n",
      "2176 Training Loss: tensor(0.3929)\n",
      "2177 Training Loss: tensor(0.3944)\n",
      "2178 Training Loss: tensor(0.3934)\n",
      "2179 Training Loss: tensor(0.3929)\n",
      "2180 Training Loss: tensor(0.3916)\n",
      "2181 Training Loss: tensor(0.3954)\n",
      "2182 Training Loss: tensor(0.3952)\n",
      "2183 Training Loss: tensor(0.3925)\n",
      "2184 Training Loss: tensor(0.3921)\n",
      "2185 Training Loss: tensor(0.3941)\n",
      "2186 Training Loss: tensor(0.3925)\n",
      "2187 Training Loss: tensor(0.3943)\n",
      "2188 Training Loss: tensor(0.3916)\n",
      "2189 Training Loss: tensor(0.3924)\n",
      "2190 Training Loss: tensor(0.3937)\n",
      "2191 Training Loss: tensor(0.3928)\n",
      "2192 Training Loss: tensor(0.3957)\n",
      "2193 Training Loss: tensor(0.3921)\n",
      "2194 Training Loss: tensor(0.3929)\n",
      "2195 Training Loss: tensor(0.3906)\n",
      "2196 Training Loss: tensor(0.3937)\n",
      "2197 Training Loss: tensor(0.3943)\n",
      "2198 Training Loss: tensor(0.3883)\n",
      "2199 Training Loss: tensor(0.3914)\n",
      "2200 Training Loss: tensor(0.3982)\n",
      "2201 Training Loss: tensor(0.3949)\n",
      "2202 Training Loss: tensor(0.3908)\n",
      "2203 Training Loss: tensor(0.3923)\n",
      "2204 Training Loss: tensor(0.3984)\n",
      "2205 Training Loss: tensor(0.3933)\n",
      "2206 Training Loss: tensor(0.3942)\n",
      "2207 Training Loss: tensor(0.3956)\n",
      "2208 Training Loss: tensor(0.3892)\n",
      "2209 Training Loss: tensor(0.3915)\n",
      "2210 Training Loss: tensor(0.3930)\n",
      "2211 Training Loss: tensor(0.3912)\n",
      "2212 Training Loss: tensor(0.3926)\n",
      "2213 Training Loss: tensor(0.3948)\n",
      "2214 Training Loss: tensor(0.3893)\n",
      "2215 Training Loss: tensor(0.3918)\n",
      "2216 Training Loss: tensor(0.3923)\n",
      "2217 Training Loss: tensor(0.3920)\n",
      "2218 Training Loss: tensor(0.3907)\n",
      "2219 Training Loss: tensor(0.3930)\n",
      "2220 Training Loss: tensor(0.3918)\n",
      "2221 Training Loss: tensor(0.3927)\n",
      "2222 Training Loss: tensor(0.3921)\n",
      "2223 Training Loss: tensor(0.3912)\n",
      "2224 Training Loss: tensor(0.3910)\n",
      "2225 Training Loss: tensor(0.3926)\n",
      "2226 Training Loss: tensor(0.3966)\n",
      "2227 Training Loss: tensor(0.3945)\n",
      "2228 Training Loss: tensor(0.3903)\n",
      "2229 Training Loss: tensor(0.3936)\n",
      "2230 Training Loss: tensor(0.3915)\n",
      "2231 Training Loss: tensor(0.3918)\n",
      "2232 Training Loss: tensor(0.3913)\n",
      "2233 Training Loss: tensor(0.3912)\n",
      "2234 Training Loss: tensor(0.3902)\n",
      "2235 Training Loss: tensor(0.3884)\n",
      "2236 Training Loss: tensor(0.3920)\n",
      "2237 Training Loss: tensor(0.3910)\n",
      "2238 Training Loss: tensor(0.3936)\n",
      "2239 Training Loss: tensor(0.3996)\n",
      "2240 Training Loss: tensor(0.3926)\n",
      "2241 Training Loss: tensor(0.3920)\n",
      "2242 Training Loss: tensor(0.3925)\n",
      "2243 Training Loss: tensor(0.3930)\n",
      "2244 Training Loss: tensor(0.3916)\n",
      "2245 Training Loss: tensor(0.3913)\n",
      "2246 Training Loss: tensor(0.3935)\n",
      "2247 Training Loss: tensor(0.3957)\n",
      "2248 Training Loss: tensor(0.3939)\n",
      "2249 Training Loss: tensor(0.3904)\n",
      "2250 Training Loss: tensor(0.3922)\n",
      "2251 Training Loss: tensor(0.3900)\n",
      "2252 Training Loss: tensor(0.3916)\n",
      "2253 Training Loss: tensor(0.3911)\n",
      "2254 Training Loss: tensor(0.3946)\n",
      "2255 Training Loss: tensor(0.3907)\n",
      "2256 Training Loss: tensor(0.3954)\n",
      "2257 Training Loss: tensor(0.3908)\n",
      "2258 Training Loss: tensor(0.3941)\n",
      "2259 Training Loss: tensor(0.3919)\n",
      "2260 Training Loss: tensor(0.3939)\n",
      "2261 Training Loss: tensor(0.3928)\n",
      "2262 Training Loss: tensor(0.3900)\n",
      "2263 Training Loss: tensor(0.3923)\n",
      "2264 Training Loss: tensor(0.3893)\n",
      "2265 Training Loss: tensor(0.3904)\n",
      "2266 Training Loss: tensor(0.3927)\n",
      "2267 Training Loss: tensor(0.3946)\n",
      "2268 Training Loss: tensor(0.3924)\n",
      "2269 Training Loss: tensor(0.3940)\n",
      "2270 Training Loss: tensor(0.3927)\n",
      "2271 Training Loss: tensor(0.3911)\n",
      "2272 Training Loss: tensor(0.3910)\n",
      "2273 Training Loss: tensor(0.3969)\n",
      "2274 Training Loss: tensor(0.3892)\n",
      "2275 Training Loss: tensor(0.3932)\n",
      "2276 Training Loss: tensor(0.3917)\n",
      "2277 Training Loss: tensor(0.3934)\n",
      "2278 Training Loss: tensor(0.3897)\n",
      "2279 Training Loss: tensor(0.3909)\n",
      "2280 Training Loss: tensor(0.3900)\n",
      "2281 Training Loss: tensor(0.3901)\n",
      "2282 Training Loss: tensor(0.3895)\n",
      "2283 Training Loss: tensor(0.3916)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284 Training Loss: tensor(0.3924)\n",
      "2285 Training Loss: tensor(0.3883)\n",
      "2286 Training Loss: tensor(0.3923)\n",
      "2287 Training Loss: tensor(0.3916)\n",
      "2288 Training Loss: tensor(0.3939)\n",
      "2289 Training Loss: tensor(0.3931)\n",
      "2290 Training Loss: tensor(0.3940)\n",
      "2291 Training Loss: tensor(0.3900)\n",
      "2292 Training Loss: tensor(0.3898)\n",
      "2293 Training Loss: tensor(0.3899)\n",
      "2294 Training Loss: tensor(0.3900)\n",
      "2295 Training Loss: tensor(0.3912)\n",
      "2296 Training Loss: tensor(0.3941)\n",
      "2297 Training Loss: tensor(0.3898)\n",
      "2298 Training Loss: tensor(0.3917)\n",
      "2299 Training Loss: tensor(0.3888)\n",
      "2300 Training Loss: tensor(0.3892)\n",
      "2301 Training Loss: tensor(0.3932)\n",
      "2302 Training Loss: tensor(0.3925)\n",
      "2303 Training Loss: tensor(0.3926)\n",
      "2304 Training Loss: tensor(0.3899)\n",
      "2305 Training Loss: tensor(0.3895)\n",
      "2306 Training Loss: tensor(0.3928)\n",
      "2307 Training Loss: tensor(0.3906)\n",
      "2308 Training Loss: tensor(0.3926)\n",
      "2309 Training Loss: tensor(0.3914)\n",
      "2310 Training Loss: tensor(0.3907)\n",
      "2311 Training Loss: tensor(0.3900)\n",
      "2312 Training Loss: tensor(0.3898)\n",
      "2313 Training Loss: tensor(0.3927)\n",
      "2314 Training Loss: tensor(0.3907)\n",
      "2315 Training Loss: tensor(0.3941)\n",
      "2316 Training Loss: tensor(0.3883)\n",
      "2317 Training Loss: tensor(0.3903)\n",
      "2318 Training Loss: tensor(0.3904)\n",
      "2319 Training Loss: tensor(0.3926)\n",
      "2320 Training Loss: tensor(0.3893)\n",
      "2321 Training Loss: tensor(0.3905)\n",
      "2322 Training Loss: tensor(0.3891)\n",
      "2323 Training Loss: tensor(0.3937)\n",
      "2324 Training Loss: tensor(0.3904)\n",
      "2325 Training Loss: tensor(0.3948)\n",
      "2326 Training Loss: tensor(0.3904)\n",
      "2327 Training Loss: tensor(0.3908)\n",
      "2328 Training Loss: tensor(0.3937)\n",
      "2329 Training Loss: tensor(0.3908)\n",
      "2330 Training Loss: tensor(0.3912)\n",
      "2331 Training Loss: tensor(0.3886)\n",
      "2332 Training Loss: tensor(0.3964)\n",
      "2333 Training Loss: tensor(0.3898)\n",
      "2334 Training Loss: tensor(0.3930)\n",
      "2335 Training Loss: tensor(0.3897)\n",
      "2336 Training Loss: tensor(0.3903)\n",
      "2337 Training Loss: tensor(0.3888)\n",
      "2338 Training Loss: tensor(0.3906)\n",
      "2339 Training Loss: tensor(0.3894)\n",
      "2340 Training Loss: tensor(0.3912)\n",
      "2341 Training Loss: tensor(0.3919)\n",
      "2342 Training Loss: tensor(0.3924)\n",
      "2343 Training Loss: tensor(0.3916)\n",
      "2344 Training Loss: tensor(0.3902)\n",
      "2345 Training Loss: tensor(0.3923)\n",
      "2346 Training Loss: tensor(0.3899)\n",
      "2347 Training Loss: tensor(0.3933)\n",
      "2348 Training Loss: tensor(0.3966)\n",
      "2349 Training Loss: tensor(0.3890)\n",
      "2350 Training Loss: tensor(0.3926)\n",
      "2351 Training Loss: tensor(0.3946)\n",
      "2352 Training Loss: tensor(0.3907)\n",
      "2353 Training Loss: tensor(0.3921)\n",
      "2354 Training Loss: tensor(0.3904)\n",
      "2355 Training Loss: tensor(0.3903)\n",
      "2356 Training Loss: tensor(0.3946)\n",
      "2357 Training Loss: tensor(0.3902)\n",
      "2358 Training Loss: tensor(0.3893)\n",
      "2359 Training Loss: tensor(0.3900)\n",
      "2360 Training Loss: tensor(0.3867)\n",
      "2361 Training Loss: tensor(0.3911)\n",
      "2362 Training Loss: tensor(0.3879)\n",
      "2363 Training Loss: tensor(0.3953)\n",
      "2364 Training Loss: tensor(0.3929)\n",
      "2365 Training Loss: tensor(0.3932)\n",
      "2366 Training Loss: tensor(0.3951)\n",
      "2367 Training Loss: tensor(0.3879)\n",
      "2368 Training Loss: tensor(0.3909)\n",
      "2369 Training Loss: tensor(0.3913)\n",
      "2370 Training Loss: tensor(0.3909)\n",
      "2371 Training Loss: tensor(0.3925)\n",
      "2372 Training Loss: tensor(0.3940)\n",
      "2373 Training Loss: tensor(0.3897)\n",
      "2374 Training Loss: tensor(0.3909)\n",
      "2375 Training Loss: tensor(0.3896)\n",
      "2376 Training Loss: tensor(0.3908)\n",
      "2377 Training Loss: tensor(0.3895)\n",
      "2378 Training Loss: tensor(0.3935)\n",
      "2379 Training Loss: tensor(0.3900)\n",
      "2380 Training Loss: tensor(0.3907)\n",
      "2381 Training Loss: tensor(0.3871)\n",
      "2382 Training Loss: tensor(0.3897)\n",
      "2383 Training Loss: tensor(0.3910)\n",
      "2384 Training Loss: tensor(0.3876)\n",
      "2385 Training Loss: tensor(0.3936)\n",
      "2386 Training Loss: tensor(0.3929)\n",
      "2387 Training Loss: tensor(0.3880)\n",
      "2388 Training Loss: tensor(0.3924)\n",
      "2389 Training Loss: tensor(0.3879)\n",
      "2390 Training Loss: tensor(0.3868)\n",
      "2391 Training Loss: tensor(0.3900)\n",
      "2392 Training Loss: tensor(0.3906)\n",
      "2393 Training Loss: tensor(0.3871)\n",
      "2394 Training Loss: tensor(0.3895)\n",
      "2395 Training Loss: tensor(0.3871)\n",
      "2396 Training Loss: tensor(0.3944)\n",
      "2397 Training Loss: tensor(0.3952)\n",
      "2398 Training Loss: tensor(0.3902)\n",
      "2399 Training Loss: tensor(0.3905)\n",
      "2400 Training Loss: tensor(0.3891)\n",
      "2401 Training Loss: tensor(0.3913)\n",
      "2402 Training Loss: tensor(0.3861)\n",
      "2403 Training Loss: tensor(0.3911)\n",
      "2404 Training Loss: tensor(0.3896)\n",
      "2405 Training Loss: tensor(0.3911)\n",
      "2406 Training Loss: tensor(0.3924)\n",
      "2407 Training Loss: tensor(0.3881)\n",
      "2408 Training Loss: tensor(0.3908)\n",
      "2409 Training Loss: tensor(0.3949)\n",
      "2410 Training Loss: tensor(0.3892)\n",
      "2411 Training Loss: tensor(0.3914)\n",
      "2412 Training Loss: tensor(0.3886)\n",
      "2413 Training Loss: tensor(0.3898)\n",
      "2414 Training Loss: tensor(0.3898)\n",
      "2415 Training Loss: tensor(0.3880)\n",
      "2416 Training Loss: tensor(0.3907)\n",
      "2417 Training Loss: tensor(0.3913)\n",
      "2418 Training Loss: tensor(0.3909)\n",
      "2419 Training Loss: tensor(0.3902)\n",
      "2420 Training Loss: tensor(0.3914)\n",
      "2421 Training Loss: tensor(0.3948)\n",
      "2422 Training Loss: tensor(0.3910)\n",
      "2423 Training Loss: tensor(0.3936)\n",
      "2424 Training Loss: tensor(0.3915)\n",
      "2425 Training Loss: tensor(0.3897)\n",
      "2426 Training Loss: tensor(0.3906)\n",
      "2427 Training Loss: tensor(0.3893)\n",
      "2428 Training Loss: tensor(0.3935)\n",
      "2429 Training Loss: tensor(0.3895)\n",
      "2430 Training Loss: tensor(0.3923)\n",
      "2431 Training Loss: tensor(0.3903)\n",
      "2432 Training Loss: tensor(0.3869)\n",
      "2433 Training Loss: tensor(0.3920)\n",
      "2434 Training Loss: tensor(0.3881)\n",
      "2435 Training Loss: tensor(0.3912)\n",
      "2436 Training Loss: tensor(0.3943)\n",
      "2437 Training Loss: tensor(0.3910)\n",
      "2438 Training Loss: tensor(0.3891)\n",
      "2439 Training Loss: tensor(0.3897)\n",
      "2440 Training Loss: tensor(0.3964)\n",
      "2441 Training Loss: tensor(0.3868)\n",
      "2442 Training Loss: tensor(0.3889)\n",
      "2443 Training Loss: tensor(0.3921)\n",
      "2444 Training Loss: tensor(0.3925)\n",
      "2445 Training Loss: tensor(0.3903)\n",
      "2446 Training Loss: tensor(0.3904)\n",
      "2447 Training Loss: tensor(0.3907)\n",
      "2448 Training Loss: tensor(0.3891)\n",
      "2449 Training Loss: tensor(0.3889)\n",
      "2450 Training Loss: tensor(0.3887)\n",
      "2451 Training Loss: tensor(0.3914)\n",
      "2452 Training Loss: tensor(0.3883)\n",
      "2453 Training Loss: tensor(0.3898)\n",
      "2454 Training Loss: tensor(0.3895)\n",
      "2455 Training Loss: tensor(0.3869)\n",
      "2456 Training Loss: tensor(0.3896)\n",
      "2457 Training Loss: tensor(0.3869)\n",
      "2458 Training Loss: tensor(0.3875)\n",
      "2459 Training Loss: tensor(0.3921)\n",
      "2460 Training Loss: tensor(0.3891)\n",
      "2461 Training Loss: tensor(0.3901)\n",
      "2462 Training Loss: tensor(0.3879)\n",
      "2463 Training Loss: tensor(0.3922)\n",
      "2464 Training Loss: tensor(0.3915)\n",
      "2465 Training Loss: tensor(0.3906)\n",
      "2466 Training Loss: tensor(0.3915)\n",
      "2467 Training Loss: tensor(0.3899)\n",
      "2468 Training Loss: tensor(0.3895)\n",
      "2469 Training Loss: tensor(0.3896)\n",
      "2470 Training Loss: tensor(0.3899)\n",
      "2471 Training Loss: tensor(0.3933)\n",
      "2472 Training Loss: tensor(0.3913)\n",
      "2473 Training Loss: tensor(0.3904)\n",
      "2474 Training Loss: tensor(0.3897)\n",
      "2475 Training Loss: tensor(0.3889)\n",
      "2476 Training Loss: tensor(0.3931)\n",
      "2477 Training Loss: tensor(0.3916)\n",
      "2478 Training Loss: tensor(0.3877)\n",
      "2479 Training Loss: tensor(0.3897)\n",
      "2480 Training Loss: tensor(0.3894)\n",
      "2481 Training Loss: tensor(0.3885)\n",
      "2482 Training Loss: tensor(0.3874)\n",
      "2483 Training Loss: tensor(0.3847)\n",
      "2484 Training Loss: tensor(0.3872)\n",
      "2485 Training Loss: tensor(0.3937)\n",
      "2486 Training Loss: tensor(0.3914)\n",
      "2487 Training Loss: tensor(0.3894)\n",
      "2488 Training Loss: tensor(0.3907)\n",
      "2489 Training Loss: tensor(0.3900)\n",
      "2490 Training Loss: tensor(0.3925)\n",
      "2491 Training Loss: tensor(0.3906)\n",
      "2492 Training Loss: tensor(0.3881)\n",
      "2493 Training Loss: tensor(0.3901)\n",
      "2494 Training Loss: tensor(0.3898)\n",
      "2495 Training Loss: tensor(0.3942)\n",
      "2496 Training Loss: tensor(0.3917)\n",
      "2497 Training Loss: tensor(0.3915)\n",
      "2498 Training Loss: tensor(0.3886)\n",
      "2499 Training Loss: tensor(0.3882)\n",
      "2500 Training Loss: tensor(0.3900)\n",
      "2501 Training Loss: tensor(0.3938)\n",
      "2502 Training Loss: tensor(0.3894)\n",
      "2503 Training Loss: tensor(0.3868)\n",
      "2504 Training Loss: tensor(0.3903)\n",
      "2505 Training Loss: tensor(0.3912)\n",
      "2506 Training Loss: tensor(0.3910)\n",
      "2507 Training Loss: tensor(0.3918)\n",
      "2508 Training Loss: tensor(0.3901)\n",
      "2509 Training Loss: tensor(0.3929)\n",
      "2510 Training Loss: tensor(0.3909)\n",
      "2511 Training Loss: tensor(0.3890)\n",
      "2512 Training Loss: tensor(0.3915)\n",
      "2513 Training Loss: tensor(0.3890)\n",
      "2514 Training Loss: tensor(0.3906)\n",
      "2515 Training Loss: tensor(0.3909)\n",
      "2516 Training Loss: tensor(0.3858)\n",
      "2517 Training Loss: tensor(0.3908)\n",
      "2518 Training Loss: tensor(0.3898)\n",
      "2519 Training Loss: tensor(0.3869)\n",
      "2520 Training Loss: tensor(0.3896)\n",
      "2521 Training Loss: tensor(0.3899)\n",
      "2522 Training Loss: tensor(0.3856)\n",
      "2523 Training Loss: tensor(0.3880)\n",
      "2524 Training Loss: tensor(0.3896)\n",
      "2525 Training Loss: tensor(0.3887)\n",
      "2526 Training Loss: tensor(0.3899)\n",
      "2527 Training Loss: tensor(0.3860)\n",
      "2528 Training Loss: tensor(0.3890)\n",
      "2529 Training Loss: tensor(0.3879)\n",
      "2530 Training Loss: tensor(0.3864)\n",
      "2531 Training Loss: tensor(0.3873)\n",
      "2532 Training Loss: tensor(0.3907)\n",
      "2533 Training Loss: tensor(0.3932)\n",
      "2534 Training Loss: tensor(0.3911)\n",
      "2535 Training Loss: tensor(0.3918)\n",
      "2536 Training Loss: tensor(0.3921)\n",
      "2537 Training Loss: tensor(0.3897)\n",
      "2538 Training Loss: tensor(0.3858)\n",
      "2539 Training Loss: tensor(0.3907)\n",
      "2540 Training Loss: tensor(0.3915)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2541 Training Loss: tensor(0.3928)\n",
      "2542 Training Loss: tensor(0.3915)\n",
      "2543 Training Loss: tensor(0.3898)\n",
      "2544 Training Loss: tensor(0.3911)\n",
      "2545 Training Loss: tensor(0.3866)\n",
      "2546 Training Loss: tensor(0.3891)\n",
      "2547 Training Loss: tensor(0.3898)\n",
      "2548 Training Loss: tensor(0.3897)\n",
      "2549 Training Loss: tensor(0.3925)\n",
      "2550 Training Loss: tensor(0.3886)\n",
      "2551 Training Loss: tensor(0.3890)\n",
      "2552 Training Loss: tensor(0.3901)\n",
      "2553 Training Loss: tensor(0.3889)\n",
      "2554 Training Loss: tensor(0.3868)\n",
      "2555 Training Loss: tensor(0.3877)\n",
      "2556 Training Loss: tensor(0.3926)\n",
      "2557 Training Loss: tensor(0.3902)\n",
      "2558 Training Loss: tensor(0.3910)\n",
      "2559 Training Loss: tensor(0.3959)\n",
      "2560 Training Loss: tensor(0.3907)\n",
      "2561 Training Loss: tensor(0.3902)\n",
      "2562 Training Loss: tensor(0.3887)\n",
      "2563 Training Loss: tensor(0.3921)\n",
      "2564 Training Loss: tensor(0.3894)\n",
      "2565 Training Loss: tensor(0.3882)\n",
      "2566 Training Loss: tensor(0.3870)\n",
      "2567 Training Loss: tensor(0.3874)\n",
      "2568 Training Loss: tensor(0.3882)\n",
      "2569 Training Loss: tensor(0.3880)\n",
      "2570 Training Loss: tensor(0.3882)\n",
      "2571 Training Loss: tensor(0.3861)\n",
      "2572 Training Loss: tensor(0.3862)\n",
      "2573 Training Loss: tensor(0.3929)\n",
      "2574 Training Loss: tensor(0.3895)\n",
      "2575 Training Loss: tensor(0.3876)\n",
      "2576 Training Loss: tensor(0.3900)\n",
      "2577 Training Loss: tensor(0.3879)\n",
      "2578 Training Loss: tensor(0.3852)\n",
      "2579 Training Loss: tensor(0.3906)\n",
      "2580 Training Loss: tensor(0.3873)\n",
      "2581 Training Loss: tensor(0.3895)\n",
      "2582 Training Loss: tensor(0.3933)\n",
      "2583 Training Loss: tensor(0.3916)\n",
      "2584 Training Loss: tensor(0.3892)\n",
      "2585 Training Loss: tensor(0.3875)\n",
      "2586 Training Loss: tensor(0.3914)\n",
      "2587 Training Loss: tensor(0.3866)\n",
      "2588 Training Loss: tensor(0.3887)\n",
      "2589 Training Loss: tensor(0.3902)\n",
      "2590 Training Loss: tensor(0.3909)\n",
      "2591 Training Loss: tensor(0.3916)\n",
      "2592 Training Loss: tensor(0.3877)\n",
      "2593 Training Loss: tensor(0.3892)\n",
      "2594 Training Loss: tensor(0.3866)\n",
      "2595 Training Loss: tensor(0.3893)\n",
      "2596 Training Loss: tensor(0.3897)\n",
      "2597 Training Loss: tensor(0.3863)\n",
      "2598 Training Loss: tensor(0.3880)\n",
      "2599 Training Loss: tensor(0.3869)\n",
      "2600 Training Loss: tensor(0.3921)\n",
      "2601 Training Loss: tensor(0.3870)\n",
      "2602 Training Loss: tensor(0.3911)\n",
      "2603 Training Loss: tensor(0.3883)\n",
      "2604 Training Loss: tensor(0.3908)\n",
      "2605 Training Loss: tensor(0.3914)\n",
      "2606 Training Loss: tensor(0.3888)\n",
      "2607 Training Loss: tensor(0.3894)\n",
      "2608 Training Loss: tensor(0.3875)\n",
      "2609 Training Loss: tensor(0.3883)\n",
      "2610 Training Loss: tensor(0.3902)\n",
      "2611 Training Loss: tensor(0.3903)\n",
      "2612 Training Loss: tensor(0.3866)\n",
      "2613 Training Loss: tensor(0.3883)\n",
      "2614 Training Loss: tensor(0.3883)\n",
      "2615 Training Loss: tensor(0.3885)\n",
      "2616 Training Loss: tensor(0.3885)\n",
      "2617 Training Loss: tensor(0.3896)\n",
      "2618 Training Loss: tensor(0.3895)\n",
      "2619 Training Loss: tensor(0.3879)\n",
      "2620 Training Loss: tensor(0.3858)\n",
      "2621 Training Loss: tensor(0.3894)\n",
      "2622 Training Loss: tensor(0.3865)\n",
      "2623 Training Loss: tensor(0.3866)\n",
      "2624 Training Loss: tensor(0.3875)\n",
      "2625 Training Loss: tensor(0.3876)\n",
      "2626 Training Loss: tensor(0.3895)\n",
      "2627 Training Loss: tensor(0.3892)\n",
      "2628 Training Loss: tensor(0.3867)\n",
      "2629 Training Loss: tensor(0.3886)\n",
      "2630 Training Loss: tensor(0.3874)\n",
      "2631 Training Loss: tensor(0.3934)\n",
      "2632 Training Loss: tensor(0.3908)\n",
      "2633 Training Loss: tensor(0.3864)\n",
      "2634 Training Loss: tensor(0.3861)\n",
      "2635 Training Loss: tensor(0.3899)\n",
      "2636 Training Loss: tensor(0.3866)\n",
      "2637 Training Loss: tensor(0.3875)\n",
      "2638 Training Loss: tensor(0.3865)\n",
      "2639 Training Loss: tensor(0.3881)\n",
      "2640 Training Loss: tensor(0.3923)\n",
      "2641 Training Loss: tensor(0.3882)\n",
      "2642 Training Loss: tensor(0.3878)\n",
      "2643 Training Loss: tensor(0.3883)\n",
      "2644 Training Loss: tensor(0.3879)\n",
      "2645 Training Loss: tensor(0.3863)\n",
      "2646 Training Loss: tensor(0.3866)\n",
      "2647 Training Loss: tensor(0.3870)\n",
      "2648 Training Loss: tensor(0.3906)\n",
      "2649 Training Loss: tensor(0.3862)\n",
      "2650 Training Loss: tensor(0.3845)\n",
      "2651 Training Loss: tensor(0.3869)\n",
      "2652 Training Loss: tensor(0.3873)\n",
      "2653 Training Loss: tensor(0.3922)\n",
      "2654 Training Loss: tensor(0.3879)\n",
      "2655 Training Loss: tensor(0.3862)\n",
      "2656 Training Loss: tensor(0.3890)\n",
      "2657 Training Loss: tensor(0.3869)\n",
      "2658 Training Loss: tensor(0.3884)\n",
      "2659 Training Loss: tensor(0.3859)\n",
      "2660 Training Loss: tensor(0.3886)\n",
      "2661 Training Loss: tensor(0.3892)\n",
      "2662 Training Loss: tensor(0.3903)\n",
      "2663 Training Loss: tensor(0.3905)\n",
      "2664 Training Loss: tensor(0.3891)\n",
      "2665 Training Loss: tensor(0.3918)\n",
      "2666 Training Loss: tensor(0.3877)\n",
      "2667 Training Loss: tensor(0.3890)\n",
      "2668 Training Loss: tensor(0.3886)\n",
      "2669 Training Loss: tensor(0.3866)\n",
      "2670 Training Loss: tensor(0.3866)\n",
      "2671 Training Loss: tensor(0.3931)\n",
      "2672 Training Loss: tensor(0.3876)\n",
      "2673 Training Loss: tensor(0.3862)\n",
      "2674 Training Loss: tensor(0.3911)\n",
      "2675 Training Loss: tensor(0.3880)\n",
      "2676 Training Loss: tensor(0.3855)\n",
      "2677 Training Loss: tensor(0.3965)\n",
      "2678 Training Loss: tensor(0.3924)\n",
      "2679 Training Loss: tensor(0.3924)\n",
      "2680 Training Loss: tensor(0.3888)\n",
      "2681 Training Loss: tensor(0.3901)\n",
      "2682 Training Loss: tensor(0.3877)\n",
      "2683 Training Loss: tensor(0.3896)\n",
      "2684 Training Loss: tensor(0.3855)\n",
      "2685 Training Loss: tensor(0.3891)\n",
      "2686 Training Loss: tensor(0.3869)\n",
      "2687 Training Loss: tensor(0.3901)\n",
      "2688 Training Loss: tensor(0.3892)\n",
      "2689 Training Loss: tensor(0.3885)\n",
      "2690 Training Loss: tensor(0.3913)\n",
      "2691 Training Loss: tensor(0.3906)\n",
      "2692 Training Loss: tensor(0.3885)\n",
      "2693 Training Loss: tensor(0.3896)\n",
      "2694 Training Loss: tensor(0.3868)\n",
      "2695 Training Loss: tensor(0.3885)\n",
      "2696 Training Loss: tensor(0.3877)\n",
      "2697 Training Loss: tensor(0.3939)\n",
      "2698 Training Loss: tensor(0.3881)\n",
      "2699 Training Loss: tensor(0.3890)\n",
      "2700 Training Loss: tensor(0.3872)\n",
      "2701 Training Loss: tensor(0.3915)\n",
      "2702 Training Loss: tensor(0.3848)\n",
      "2703 Training Loss: tensor(0.3872)\n",
      "2704 Training Loss: tensor(0.3876)\n",
      "2705 Training Loss: tensor(0.3867)\n",
      "2706 Training Loss: tensor(0.3849)\n",
      "2707 Training Loss: tensor(0.3850)\n",
      "2708 Training Loss: tensor(0.3887)\n",
      "2709 Training Loss: tensor(0.3887)\n",
      "2710 Training Loss: tensor(0.3893)\n",
      "2711 Training Loss: tensor(0.3961)\n",
      "2712 Training Loss: tensor(0.3910)\n",
      "2713 Training Loss: tensor(0.3924)\n",
      "2714 Training Loss: tensor(0.3890)\n",
      "2715 Training Loss: tensor(0.3879)\n",
      "2716 Training Loss: tensor(0.3879)\n",
      "2717 Training Loss: tensor(0.3901)\n",
      "2718 Training Loss: tensor(0.3886)\n",
      "2719 Training Loss: tensor(0.3920)\n",
      "2720 Training Loss: tensor(0.3883)\n",
      "2721 Training Loss: tensor(0.3898)\n",
      "2722 Training Loss: tensor(0.3923)\n",
      "2723 Training Loss: tensor(0.3865)\n",
      "2724 Training Loss: tensor(0.3944)\n",
      "2725 Training Loss: tensor(0.3878)\n",
      "2726 Training Loss: tensor(0.3892)\n",
      "2727 Training Loss: tensor(0.3885)\n",
      "2728 Training Loss: tensor(0.3854)\n",
      "2729 Training Loss: tensor(0.3885)\n",
      "2730 Training Loss: tensor(0.3868)\n",
      "2731 Training Loss: tensor(0.3925)\n",
      "2732 Training Loss: tensor(0.3850)\n",
      "2733 Training Loss: tensor(0.3877)\n",
      "2734 Training Loss: tensor(0.3856)\n",
      "2735 Training Loss: tensor(0.3884)\n",
      "2736 Training Loss: tensor(0.3878)\n",
      "2737 Training Loss: tensor(0.3906)\n",
      "2738 Training Loss: tensor(0.3869)\n",
      "2739 Training Loss: tensor(0.3923)\n",
      "2740 Training Loss: tensor(0.3894)\n",
      "2741 Training Loss: tensor(0.3917)\n",
      "2742 Training Loss: tensor(0.3853)\n",
      "2743 Training Loss: tensor(0.3906)\n",
      "2744 Training Loss: tensor(0.3880)\n",
      "2745 Training Loss: tensor(0.3897)\n",
      "2746 Training Loss: tensor(0.3883)\n",
      "2747 Training Loss: tensor(0.3871)\n",
      "2748 Training Loss: tensor(0.3858)\n",
      "2749 Training Loss: tensor(0.3863)\n",
      "2750 Training Loss: tensor(0.3858)\n",
      "2751 Training Loss: tensor(0.3880)\n",
      "2752 Training Loss: tensor(0.3881)\n",
      "2753 Training Loss: tensor(0.3897)\n",
      "2754 Training Loss: tensor(0.3925)\n",
      "2755 Training Loss: tensor(0.3847)\n",
      "2756 Training Loss: tensor(0.3872)\n",
      "2757 Training Loss: tensor(0.3881)\n",
      "2758 Training Loss: tensor(0.3938)\n",
      "2759 Training Loss: tensor(0.3841)\n",
      "2760 Training Loss: tensor(0.3897)\n",
      "2761 Training Loss: tensor(0.3895)\n",
      "2762 Training Loss: tensor(0.3897)\n",
      "2763 Training Loss: tensor(0.3882)\n",
      "2764 Training Loss: tensor(0.3884)\n",
      "2765 Training Loss: tensor(0.3857)\n",
      "2766 Training Loss: tensor(0.3898)\n",
      "2767 Training Loss: tensor(0.3869)\n",
      "2768 Training Loss: tensor(0.3878)\n",
      "2769 Training Loss: tensor(0.3890)\n",
      "2770 Training Loss: tensor(0.3891)\n",
      "2771 Training Loss: tensor(0.3896)\n",
      "2772 Training Loss: tensor(0.3912)\n",
      "2773 Training Loss: tensor(0.3874)\n",
      "2774 Training Loss: tensor(0.3861)\n",
      "2775 Training Loss: tensor(0.3861)\n",
      "2776 Training Loss: tensor(0.3903)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2777 Training Loss: tensor(0.3854)\n",
      "2778 Training Loss: tensor(0.3880)\n",
      "2779 Training Loss: tensor(0.3882)\n",
      "2780 Training Loss: tensor(0.3860)\n",
      "2781 Training Loss: tensor(0.3895)\n",
      "2782 Training Loss: tensor(0.3880)\n",
      "2783 Training Loss: tensor(0.3866)\n",
      "2784 Training Loss: tensor(0.3864)\n",
      "2785 Training Loss: tensor(0.3874)\n",
      "2786 Training Loss: tensor(0.3865)\n",
      "2787 Training Loss: tensor(0.3865)\n",
      "2788 Training Loss: tensor(0.3853)\n",
      "2789 Training Loss: tensor(0.3882)\n",
      "2790 Training Loss: tensor(0.3917)\n",
      "2791 Training Loss: tensor(0.3857)\n",
      "2792 Training Loss: tensor(0.3888)\n",
      "2793 Training Loss: tensor(0.3851)\n",
      "2794 Training Loss: tensor(0.3939)\n",
      "2795 Training Loss: tensor(0.3871)\n",
      "2796 Training Loss: tensor(0.3908)\n",
      "2797 Training Loss: tensor(0.3883)\n",
      "2798 Training Loss: tensor(0.3878)\n",
      "2799 Training Loss: tensor(0.3898)\n",
      "2800 Training Loss: tensor(0.3876)\n",
      "2801 Training Loss: tensor(0.3877)\n",
      "2802 Training Loss: tensor(0.3885)\n",
      "2803 Training Loss: tensor(0.3868)\n",
      "2804 Training Loss: tensor(0.3879)\n",
      "2805 Training Loss: tensor(0.3886)\n",
      "2806 Training Loss: tensor(0.3885)\n",
      "2807 Training Loss: tensor(0.3880)\n",
      "2808 Training Loss: tensor(0.3901)\n",
      "2809 Training Loss: tensor(0.3879)\n",
      "2810 Training Loss: tensor(0.3900)\n",
      "2811 Training Loss: tensor(0.3871)\n",
      "2812 Training Loss: tensor(0.3871)\n",
      "2813 Training Loss: tensor(0.3866)\n",
      "2814 Training Loss: tensor(0.3866)\n",
      "2815 Training Loss: tensor(0.3914)\n",
      "2816 Training Loss: tensor(0.3886)\n",
      "2817 Training Loss: tensor(0.3872)\n",
      "2818 Training Loss: tensor(0.3877)\n",
      "2819 Training Loss: tensor(0.3895)\n",
      "2820 Training Loss: tensor(0.3902)\n",
      "2821 Training Loss: tensor(0.3865)\n",
      "2822 Training Loss: tensor(0.3869)\n",
      "2823 Training Loss: tensor(0.3887)\n",
      "2824 Training Loss: tensor(0.3884)\n",
      "2825 Training Loss: tensor(0.3862)\n",
      "2826 Training Loss: tensor(0.3906)\n",
      "2827 Training Loss: tensor(0.3884)\n",
      "2828 Training Loss: tensor(0.3868)\n",
      "2829 Training Loss: tensor(0.3844)\n",
      "2830 Training Loss: tensor(0.3894)\n",
      "2831 Training Loss: tensor(0.3887)\n",
      "2832 Training Loss: tensor(0.3878)\n",
      "2833 Training Loss: tensor(0.3839)\n",
      "2834 Training Loss: tensor(0.3864)\n",
      "2835 Training Loss: tensor(0.3854)\n",
      "2836 Training Loss: tensor(0.3890)\n",
      "2837 Training Loss: tensor(0.3851)\n",
      "2838 Training Loss: tensor(0.3873)\n",
      "2839 Training Loss: tensor(0.3863)\n",
      "2840 Training Loss: tensor(0.3894)\n",
      "2841 Training Loss: tensor(0.3890)\n",
      "2842 Training Loss: tensor(0.3866)\n",
      "2843 Training Loss: tensor(0.3894)\n",
      "2844 Training Loss: tensor(0.3926)\n",
      "2845 Training Loss: tensor(0.3869)\n",
      "2846 Training Loss: tensor(0.3838)\n",
      "2847 Training Loss: tensor(0.3855)\n",
      "2848 Training Loss: tensor(0.3895)\n",
      "2849 Training Loss: tensor(0.3898)\n",
      "2850 Training Loss: tensor(0.3858)\n",
      "2851 Training Loss: tensor(0.3883)\n",
      "2852 Training Loss: tensor(0.3880)\n",
      "2853 Training Loss: tensor(0.3897)\n",
      "2854 Training Loss: tensor(0.3860)\n",
      "2855 Training Loss: tensor(0.3897)\n",
      "2856 Training Loss: tensor(0.3931)\n",
      "2857 Training Loss: tensor(0.3887)\n",
      "2858 Training Loss: tensor(0.3903)\n",
      "2859 Training Loss: tensor(0.3887)\n",
      "2860 Training Loss: tensor(0.3856)\n",
      "2861 Training Loss: tensor(0.3896)\n",
      "2862 Training Loss: tensor(0.3847)\n",
      "2863 Training Loss: tensor(0.3873)\n",
      "2864 Training Loss: tensor(0.3878)\n",
      "2865 Training Loss: tensor(0.3906)\n",
      "2866 Training Loss: tensor(0.3862)\n",
      "2867 Training Loss: tensor(0.3889)\n",
      "2868 Training Loss: tensor(0.3860)\n",
      "2869 Training Loss: tensor(0.3853)\n",
      "2870 Training Loss: tensor(0.3870)\n",
      "2871 Training Loss: tensor(0.3861)\n",
      "2872 Training Loss: tensor(0.3899)\n",
      "2873 Training Loss: tensor(0.3868)\n",
      "2874 Training Loss: tensor(0.3878)\n",
      "2875 Training Loss: tensor(0.3882)\n",
      "2876 Training Loss: tensor(0.3855)\n",
      "2877 Training Loss: tensor(0.3879)\n",
      "2878 Training Loss: tensor(0.3855)\n",
      "2879 Training Loss: tensor(0.3883)\n",
      "2880 Training Loss: tensor(0.3903)\n",
      "2881 Training Loss: tensor(0.3883)\n",
      "2882 Training Loss: tensor(0.3875)\n",
      "2883 Training Loss: tensor(0.3876)\n",
      "2884 Training Loss: tensor(0.3859)\n",
      "2885 Training Loss: tensor(0.3847)\n",
      "2886 Training Loss: tensor(0.3892)\n",
      "2887 Training Loss: tensor(0.3872)\n",
      "2888 Training Loss: tensor(0.3895)\n",
      "2889 Training Loss: tensor(0.3872)\n",
      "2890 Training Loss: tensor(0.3897)\n",
      "2891 Training Loss: tensor(0.3861)\n",
      "2892 Training Loss: tensor(0.3871)\n",
      "2893 Training Loss: tensor(0.3875)\n",
      "2894 Training Loss: tensor(0.3860)\n",
      "2895 Training Loss: tensor(0.3884)\n",
      "2896 Training Loss: tensor(0.3914)\n",
      "2897 Training Loss: tensor(0.3906)\n",
      "2898 Training Loss: tensor(0.3843)\n",
      "2899 Training Loss: tensor(0.3870)\n",
      "2900 Training Loss: tensor(0.3851)\n",
      "2901 Training Loss: tensor(0.3894)\n",
      "2902 Training Loss: tensor(0.3864)\n",
      "2903 Training Loss: tensor(0.3876)\n",
      "2904 Training Loss: tensor(0.3899)\n",
      "2905 Training Loss: tensor(0.3881)\n",
      "2906 Training Loss: tensor(0.3865)\n",
      "2907 Training Loss: tensor(0.3900)\n",
      "2908 Training Loss: tensor(0.3863)\n",
      "2909 Training Loss: tensor(0.3858)\n",
      "2910 Training Loss: tensor(0.3876)\n",
      "2911 Training Loss: tensor(0.3861)\n",
      "2912 Training Loss: tensor(0.3908)\n",
      "2913 Training Loss: tensor(0.3882)\n",
      "2914 Training Loss: tensor(0.3878)\n",
      "2915 Training Loss: tensor(0.3861)\n",
      "2916 Training Loss: tensor(0.3922)\n",
      "2917 Training Loss: tensor(0.3874)\n",
      "2918 Training Loss: tensor(0.3872)\n",
      "2919 Training Loss: tensor(0.3857)\n",
      "2920 Training Loss: tensor(0.3845)\n",
      "2921 Training Loss: tensor(0.3863)\n",
      "2922 Training Loss: tensor(0.3872)\n",
      "2923 Training Loss: tensor(0.3903)\n",
      "2924 Training Loss: tensor(0.3852)\n",
      "2925 Training Loss: tensor(0.3861)\n",
      "2926 Training Loss: tensor(0.3860)\n",
      "2927 Training Loss: tensor(0.3906)\n",
      "2928 Training Loss: tensor(0.3884)\n",
      "2929 Training Loss: tensor(0.3929)\n",
      "2930 Training Loss: tensor(0.3858)\n",
      "2931 Training Loss: tensor(0.3840)\n",
      "2932 Training Loss: tensor(0.3876)\n",
      "2933 Training Loss: tensor(0.3879)\n",
      "2934 Training Loss: tensor(0.3892)\n",
      "2935 Training Loss: tensor(0.3853)\n",
      "2936 Training Loss: tensor(0.3863)\n",
      "2937 Training Loss: tensor(0.3869)\n",
      "2938 Training Loss: tensor(0.3881)\n",
      "2939 Training Loss: tensor(0.3847)\n",
      "2940 Training Loss: tensor(0.3864)\n",
      "2941 Training Loss: tensor(0.3874)\n",
      "2942 Training Loss: tensor(0.3949)\n",
      "2943 Training Loss: tensor(0.3910)\n",
      "2944 Training Loss: tensor(0.3886)\n",
      "2945 Training Loss: tensor(0.3889)\n",
      "2946 Training Loss: tensor(0.3873)\n",
      "2947 Training Loss: tensor(0.3877)\n",
      "2948 Training Loss: tensor(0.3863)\n",
      "2949 Training Loss: tensor(0.3848)\n",
      "2950 Training Loss: tensor(0.3879)\n",
      "2951 Training Loss: tensor(0.3865)\n",
      "2952 Training Loss: tensor(0.3864)\n",
      "2953 Training Loss: tensor(0.3856)\n",
      "2954 Training Loss: tensor(0.3873)\n",
      "2955 Training Loss: tensor(0.3873)\n",
      "2956 Training Loss: tensor(0.3852)\n",
      "2957 Training Loss: tensor(0.3861)\n",
      "2958 Training Loss: tensor(0.3844)\n",
      "2959 Training Loss: tensor(0.3857)\n",
      "2960 Training Loss: tensor(0.3899)\n",
      "2961 Training Loss: tensor(0.3847)\n",
      "2962 Training Loss: tensor(0.3858)\n",
      "2963 Training Loss: tensor(0.3876)\n",
      "2964 Training Loss: tensor(0.3874)\n",
      "2965 Training Loss: tensor(0.3848)\n",
      "2966 Training Loss: tensor(0.3877)\n",
      "2967 Training Loss: tensor(0.3843)\n",
      "2968 Training Loss: tensor(0.3884)\n",
      "2969 Training Loss: tensor(0.3883)\n",
      "2970 Training Loss: tensor(0.3875)\n",
      "2971 Training Loss: tensor(0.3836)\n",
      "2972 Training Loss: tensor(0.3858)\n",
      "2973 Training Loss: tensor(0.3917)\n",
      "2974 Training Loss: tensor(0.3901)\n",
      "2975 Training Loss: tensor(0.3877)\n",
      "2976 Training Loss: tensor(0.3854)\n",
      "2977 Training Loss: tensor(0.3878)\n",
      "2978 Training Loss: tensor(0.3900)\n",
      "2979 Training Loss: tensor(0.3841)\n",
      "2980 Training Loss: tensor(0.3866)\n",
      "2981 Training Loss: tensor(0.3856)\n",
      "2982 Training Loss: tensor(0.3870)\n",
      "2983 Training Loss: tensor(0.3870)\n",
      "2984 Training Loss: tensor(0.3871)\n",
      "2985 Training Loss: tensor(0.3858)\n",
      "2986 Training Loss: tensor(0.3854)\n",
      "2987 Training Loss: tensor(0.3866)\n",
      "2988 Training Loss: tensor(0.3850)\n",
      "2989 Training Loss: tensor(0.3867)\n",
      "2990 Training Loss: tensor(0.3870)\n",
      "2991 Training Loss: tensor(0.3882)\n",
      "2992 Training Loss: tensor(0.3881)\n",
      "2993 Training Loss: tensor(0.3870)\n",
      "2994 Training Loss: tensor(0.3843)\n",
      "2995 Training Loss: tensor(0.3846)\n",
      "2996 Training Loss: tensor(0.3851)\n",
      "2997 Training Loss: tensor(0.3903)\n",
      "2998 Training Loss: tensor(0.3856)\n",
      "2999 Training Loss: tensor(0.3881)\n",
      "3000 Training Loss: tensor(0.3894)\n",
      "3001 Training Loss: tensor(0.3875)\n",
      "3002 Training Loss: tensor(0.3844)\n",
      "3003 Training Loss: tensor(0.3887)\n",
      "3004 Training Loss: tensor(0.3838)\n",
      "3005 Training Loss: tensor(0.3908)\n",
      "3006 Training Loss: tensor(0.3916)\n",
      "3007 Training Loss: tensor(0.3881)\n",
      "3008 Training Loss: tensor(0.3841)\n",
      "3009 Training Loss: tensor(0.3883)\n",
      "3010 Training Loss: tensor(0.3860)\n",
      "3011 Training Loss: tensor(0.3872)\n",
      "3012 Training Loss: tensor(0.3853)\n",
      "3013 Training Loss: tensor(0.3844)\n",
      "3014 Training Loss: tensor(0.3865)\n",
      "3015 Training Loss: tensor(0.3827)\n",
      "3016 Training Loss: tensor(0.3848)\n",
      "3017 Training Loss: tensor(0.3874)\n",
      "3018 Training Loss: tensor(0.3858)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019 Training Loss: tensor(0.3844)\n",
      "3020 Training Loss: tensor(0.3860)\n",
      "3021 Training Loss: tensor(0.3868)\n",
      "3022 Training Loss: tensor(0.3861)\n",
      "3023 Training Loss: tensor(0.3846)\n",
      "3024 Training Loss: tensor(0.3911)\n",
      "3025 Training Loss: tensor(0.3877)\n",
      "3026 Training Loss: tensor(0.3856)\n",
      "3027 Training Loss: tensor(0.3836)\n",
      "3028 Training Loss: tensor(0.3920)\n",
      "3029 Training Loss: tensor(0.3848)\n",
      "3030 Training Loss: tensor(0.3881)\n",
      "3031 Training Loss: tensor(0.3868)\n",
      "3032 Training Loss: tensor(0.3853)\n",
      "3033 Training Loss: tensor(0.3855)\n",
      "3034 Training Loss: tensor(0.3871)\n",
      "3035 Training Loss: tensor(0.3839)\n",
      "3036 Training Loss: tensor(0.3831)\n",
      "3037 Training Loss: tensor(0.3873)\n",
      "3038 Training Loss: tensor(0.3834)\n",
      "3039 Training Loss: tensor(0.3852)\n",
      "3040 Training Loss: tensor(0.3892)\n",
      "3041 Training Loss: tensor(0.3837)\n",
      "3042 Training Loss: tensor(0.3837)\n",
      "3043 Training Loss: tensor(0.3849)\n",
      "3044 Training Loss: tensor(0.3864)\n",
      "3045 Training Loss: tensor(0.3874)\n",
      "3046 Training Loss: tensor(0.3864)\n",
      "3047 Training Loss: tensor(0.3837)\n",
      "3048 Training Loss: tensor(0.3868)\n",
      "3049 Training Loss: tensor(0.3861)\n",
      "3050 Training Loss: tensor(0.3868)\n",
      "3051 Training Loss: tensor(0.3857)\n",
      "3052 Training Loss: tensor(0.3841)\n",
      "3053 Training Loss: tensor(0.3856)\n",
      "3054 Training Loss: tensor(0.3851)\n",
      "3055 Training Loss: tensor(0.3887)\n",
      "3056 Training Loss: tensor(0.3894)\n",
      "3057 Training Loss: tensor(0.3896)\n",
      "3058 Training Loss: tensor(0.3880)\n",
      "3059 Training Loss: tensor(0.3853)\n",
      "3060 Training Loss: tensor(0.3868)\n",
      "3061 Training Loss: tensor(0.3850)\n",
      "3062 Training Loss: tensor(0.3869)\n",
      "3063 Training Loss: tensor(0.3847)\n",
      "3064 Training Loss: tensor(0.3845)\n",
      "3065 Training Loss: tensor(0.3908)\n",
      "3066 Training Loss: tensor(0.3884)\n",
      "3067 Training Loss: tensor(0.3865)\n",
      "3068 Training Loss: tensor(0.3868)\n",
      "3069 Training Loss: tensor(0.3889)\n",
      "3070 Training Loss: tensor(0.3872)\n",
      "3071 Training Loss: tensor(0.3839)\n",
      "3072 Training Loss: tensor(0.3957)\n",
      "3073 Training Loss: tensor(0.3854)\n",
      "3074 Training Loss: tensor(0.3887)\n",
      "3075 Training Loss: tensor(0.3874)\n",
      "3076 Training Loss: tensor(0.3857)\n",
      "3077 Training Loss: tensor(0.3868)\n",
      "3078 Training Loss: tensor(0.3882)\n",
      "3079 Training Loss: tensor(0.3866)\n",
      "3080 Training Loss: tensor(0.3841)\n",
      "3081 Training Loss: tensor(0.3862)\n",
      "3082 Training Loss: tensor(0.3839)\n",
      "3083 Training Loss: tensor(0.3844)\n",
      "3084 Training Loss: tensor(0.3873)\n",
      "3085 Training Loss: tensor(0.3842)\n",
      "3086 Training Loss: tensor(0.3832)\n",
      "3087 Training Loss: tensor(0.3883)\n",
      "3088 Training Loss: tensor(0.3906)\n",
      "3089 Training Loss: tensor(0.3878)\n",
      "3090 Training Loss: tensor(0.3867)\n",
      "3091 Training Loss: tensor(0.3868)\n",
      "3092 Training Loss: tensor(0.3848)\n",
      "3093 Training Loss: tensor(0.3832)\n",
      "3094 Training Loss: tensor(0.3841)\n",
      "3095 Training Loss: tensor(0.3856)\n",
      "3096 Training Loss: tensor(0.3849)\n",
      "3097 Training Loss: tensor(0.3843)\n",
      "3098 Training Loss: tensor(0.3862)\n",
      "3099 Training Loss: tensor(0.3874)\n",
      "3100 Training Loss: tensor(0.3836)\n",
      "3101 Training Loss: tensor(0.3860)\n",
      "3102 Training Loss: tensor(0.3851)\n",
      "3103 Training Loss: tensor(0.3878)\n",
      "3104 Training Loss: tensor(0.3907)\n",
      "3105 Training Loss: tensor(0.3900)\n",
      "3106 Training Loss: tensor(0.3862)\n",
      "3107 Training Loss: tensor(0.3872)\n",
      "3108 Training Loss: tensor(0.3869)\n",
      "3109 Training Loss: tensor(0.3862)\n",
      "3110 Training Loss: tensor(0.3872)\n",
      "3111 Training Loss: tensor(0.3864)\n",
      "3112 Training Loss: tensor(0.3868)\n",
      "3113 Training Loss: tensor(0.3877)\n",
      "3114 Training Loss: tensor(0.3848)\n",
      "3115 Training Loss: tensor(0.3888)\n",
      "3116 Training Loss: tensor(0.3861)\n",
      "3117 Training Loss: tensor(0.3849)\n",
      "3118 Training Loss: tensor(0.3867)\n",
      "3119 Training Loss: tensor(0.3885)\n",
      "3120 Training Loss: tensor(0.3865)\n",
      "3121 Training Loss: tensor(0.3877)\n",
      "3122 Training Loss: tensor(0.3889)\n",
      "3123 Training Loss: tensor(0.3870)\n",
      "3124 Training Loss: tensor(0.3865)\n",
      "3125 Training Loss: tensor(0.3836)\n",
      "3126 Training Loss: tensor(0.3862)\n",
      "3127 Training Loss: tensor(0.3881)\n",
      "3128 Training Loss: tensor(0.3851)\n",
      "3129 Training Loss: tensor(0.3867)\n",
      "3130 Training Loss: tensor(0.3847)\n",
      "3131 Training Loss: tensor(0.3843)\n",
      "3132 Training Loss: tensor(0.3835)\n",
      "3133 Training Loss: tensor(0.3864)\n",
      "3134 Training Loss: tensor(0.3837)\n",
      "3135 Training Loss: tensor(0.3844)\n",
      "3136 Training Loss: tensor(0.3850)\n",
      "3137 Training Loss: tensor(0.3863)\n",
      "3138 Training Loss: tensor(0.3885)\n",
      "3139 Training Loss: tensor(0.3853)\n",
      "3140 Training Loss: tensor(0.3866)\n",
      "3141 Training Loss: tensor(0.3827)\n",
      "3142 Training Loss: tensor(0.3882)\n",
      "3143 Training Loss: tensor(0.3864)\n",
      "3144 Training Loss: tensor(0.3841)\n",
      "3145 Training Loss: tensor(0.3847)\n",
      "3146 Training Loss: tensor(0.3843)\n",
      "3147 Training Loss: tensor(0.3867)\n",
      "3148 Training Loss: tensor(0.3895)\n",
      "3149 Training Loss: tensor(0.3818)\n",
      "3150 Training Loss: tensor(0.3834)\n",
      "3151 Training Loss: tensor(0.3840)\n",
      "3152 Training Loss: tensor(0.3858)\n",
      "3153 Training Loss: tensor(0.3825)\n",
      "3154 Training Loss: tensor(0.3851)\n",
      "3155 Training Loss: tensor(0.3848)\n",
      "3156 Training Loss: tensor(0.3890)\n",
      "3157 Training Loss: tensor(0.3887)\n",
      "3158 Training Loss: tensor(0.3847)\n",
      "3159 Training Loss: tensor(0.3891)\n",
      "3160 Training Loss: tensor(0.3889)\n",
      "3161 Training Loss: tensor(0.3895)\n",
      "3162 Training Loss: tensor(0.3872)\n",
      "3163 Training Loss: tensor(0.3846)\n",
      "3164 Training Loss: tensor(0.3840)\n",
      "3165 Training Loss: tensor(0.3809)\n",
      "3166 Training Loss: tensor(0.3866)\n",
      "3167 Training Loss: tensor(0.3864)\n",
      "3168 Training Loss: tensor(0.3876)\n",
      "3169 Training Loss: tensor(0.3836)\n",
      "3170 Training Loss: tensor(0.3869)\n",
      "3171 Training Loss: tensor(0.3811)\n",
      "3172 Training Loss: tensor(0.3875)\n",
      "3173 Training Loss: tensor(0.3838)\n",
      "3174 Training Loss: tensor(0.3861)\n",
      "3175 Training Loss: tensor(0.3860)\n",
      "3176 Training Loss: tensor(0.3887)\n",
      "3177 Training Loss: tensor(0.3834)\n",
      "3178 Training Loss: tensor(0.3845)\n",
      "3179 Training Loss: tensor(0.3844)\n",
      "3180 Training Loss: tensor(0.3849)\n",
      "3181 Training Loss: tensor(0.3879)\n",
      "3182 Training Loss: tensor(0.3830)\n",
      "3183 Training Loss: tensor(0.3867)\n",
      "3184 Training Loss: tensor(0.3846)\n",
      "3185 Training Loss: tensor(0.3848)\n",
      "3186 Training Loss: tensor(0.3841)\n",
      "3187 Training Loss: tensor(0.3843)\n",
      "3188 Training Loss: tensor(0.3863)\n",
      "3189 Training Loss: tensor(0.3833)\n",
      "3190 Training Loss: tensor(0.3838)\n",
      "3191 Training Loss: tensor(0.3858)\n",
      "3192 Training Loss: tensor(0.3864)\n",
      "3193 Training Loss: tensor(0.3832)\n",
      "3194 Training Loss: tensor(0.3826)\n",
      "3195 Training Loss: tensor(0.3858)\n",
      "3196 Training Loss: tensor(0.3825)\n",
      "3197 Training Loss: tensor(0.3893)\n",
      "3198 Training Loss: tensor(0.3819)\n",
      "3199 Training Loss: tensor(0.3835)\n",
      "3200 Training Loss: tensor(0.3923)\n",
      "3201 Training Loss: tensor(0.3840)\n",
      "3202 Training Loss: tensor(0.3890)\n",
      "3203 Training Loss: tensor(0.3859)\n",
      "3204 Training Loss: tensor(0.3880)\n",
      "3205 Training Loss: tensor(0.3889)\n",
      "3206 Training Loss: tensor(0.3860)\n",
      "3207 Training Loss: tensor(0.3826)\n",
      "3208 Training Loss: tensor(0.3877)\n",
      "3209 Training Loss: tensor(0.3854)\n",
      "3210 Training Loss: tensor(0.3849)\n",
      "3211 Training Loss: tensor(0.3876)\n",
      "3212 Training Loss: tensor(0.3859)\n",
      "3213 Training Loss: tensor(0.3868)\n",
      "3214 Training Loss: tensor(0.3831)\n",
      "3215 Training Loss: tensor(0.3852)\n",
      "3216 Training Loss: tensor(0.3859)\n",
      "3217 Training Loss: tensor(0.3850)\n",
      "3218 Training Loss: tensor(0.3857)\n",
      "3219 Training Loss: tensor(0.3847)\n",
      "3220 Training Loss: tensor(0.3852)\n",
      "3221 Training Loss: tensor(0.3864)\n",
      "3222 Training Loss: tensor(0.3870)\n",
      "3223 Training Loss: tensor(0.3865)\n",
      "3224 Training Loss: tensor(0.3858)\n",
      "3225 Training Loss: tensor(0.3816)\n",
      "3226 Training Loss: tensor(0.3867)\n",
      "3227 Training Loss: tensor(0.3859)\n",
      "3228 Training Loss: tensor(0.3837)\n",
      "3229 Training Loss: tensor(0.3894)\n",
      "3230 Training Loss: tensor(0.3868)\n",
      "3231 Training Loss: tensor(0.3837)\n",
      "3232 Training Loss: tensor(0.3862)\n",
      "3233 Training Loss: tensor(0.3872)\n",
      "3234 Training Loss: tensor(0.3871)\n",
      "3235 Training Loss: tensor(0.3844)\n",
      "3236 Training Loss: tensor(0.3836)\n",
      "3237 Training Loss: tensor(0.3902)\n",
      "3238 Training Loss: tensor(0.3868)\n",
      "3239 Training Loss: tensor(0.3881)\n",
      "3240 Training Loss: tensor(0.3854)\n",
      "3241 Training Loss: tensor(0.3824)\n",
      "3242 Training Loss: tensor(0.3860)\n",
      "3243 Training Loss: tensor(0.3875)\n",
      "3244 Training Loss: tensor(0.3859)\n",
      "3245 Training Loss: tensor(0.3854)\n",
      "3246 Training Loss: tensor(0.3845)\n",
      "3247 Training Loss: tensor(0.3851)\n",
      "3248 Training Loss: tensor(0.3830)\n",
      "3249 Training Loss: tensor(0.3851)\n",
      "3250 Training Loss: tensor(0.3896)\n",
      "3251 Training Loss: tensor(0.3852)\n",
      "3252 Training Loss: tensor(0.3837)\n",
      "3253 Training Loss: tensor(0.3873)\n",
      "3254 Training Loss: tensor(0.3892)\n",
      "3255 Training Loss: tensor(0.3840)\n",
      "3256 Training Loss: tensor(0.3827)\n",
      "3257 Training Loss: tensor(0.3865)\n",
      "3258 Training Loss: tensor(0.3912)\n",
      "3259 Training Loss: tensor(0.3859)\n",
      "3260 Training Loss: tensor(0.3869)\n",
      "3261 Training Loss: tensor(0.3910)\n",
      "3262 Training Loss: tensor(0.3846)\n",
      "3263 Training Loss: tensor(0.3822)\n",
      "3264 Training Loss: tensor(0.3851)\n",
      "3265 Training Loss: tensor(0.3854)\n",
      "3266 Training Loss: tensor(0.3876)\n",
      "3267 Training Loss: tensor(0.3854)\n",
      "3268 Training Loss: tensor(0.3830)\n",
      "3269 Training Loss: tensor(0.3845)\n",
      "3270 Training Loss: tensor(0.3863)\n",
      "3271 Training Loss: tensor(0.3862)\n",
      "3272 Training Loss: tensor(0.3852)\n",
      "3273 Training Loss: tensor(0.3842)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3274 Training Loss: tensor(0.3817)\n",
      "3275 Training Loss: tensor(0.3843)\n",
      "3276 Training Loss: tensor(0.3875)\n",
      "3277 Training Loss: tensor(0.3861)\n",
      "3278 Training Loss: tensor(0.3837)\n",
      "3279 Training Loss: tensor(0.3884)\n",
      "3280 Training Loss: tensor(0.3837)\n",
      "3281 Training Loss: tensor(0.3861)\n",
      "3282 Training Loss: tensor(0.3815)\n",
      "3283 Training Loss: tensor(0.3849)\n",
      "3284 Training Loss: tensor(0.3857)\n",
      "3285 Training Loss: tensor(0.3828)\n",
      "3286 Training Loss: tensor(0.3859)\n",
      "3287 Training Loss: tensor(0.3877)\n",
      "3288 Training Loss: tensor(0.3847)\n",
      "3289 Training Loss: tensor(0.3882)\n",
      "3290 Training Loss: tensor(0.3838)\n",
      "3291 Training Loss: tensor(0.3850)\n",
      "3292 Training Loss: tensor(0.3815)\n",
      "3293 Training Loss: tensor(0.3834)\n",
      "3294 Training Loss: tensor(0.3855)\n",
      "3295 Training Loss: tensor(0.3871)\n",
      "3296 Training Loss: tensor(0.3880)\n",
      "3297 Training Loss: tensor(0.3835)\n",
      "3298 Training Loss: tensor(0.3820)\n",
      "3299 Training Loss: tensor(0.3907)\n",
      "3300 Training Loss: tensor(0.3817)\n",
      "3301 Training Loss: tensor(0.3854)\n",
      "3302 Training Loss: tensor(0.3855)\n",
      "3303 Training Loss: tensor(0.3862)\n",
      "3304 Training Loss: tensor(0.3830)\n",
      "3305 Training Loss: tensor(0.3848)\n",
      "3306 Training Loss: tensor(0.3848)\n",
      "3307 Training Loss: tensor(0.3876)\n",
      "3308 Training Loss: tensor(0.3872)\n",
      "3309 Training Loss: tensor(0.3856)\n",
      "3310 Training Loss: tensor(0.3838)\n",
      "3311 Training Loss: tensor(0.3865)\n",
      "3312 Training Loss: tensor(0.3829)\n",
      "3313 Training Loss: tensor(0.3817)\n",
      "3314 Training Loss: tensor(0.3835)\n",
      "3315 Training Loss: tensor(0.3868)\n",
      "3316 Training Loss: tensor(0.3877)\n",
      "3317 Training Loss: tensor(0.3848)\n",
      "3318 Training Loss: tensor(0.3856)\n",
      "3319 Training Loss: tensor(0.3828)\n",
      "3320 Training Loss: tensor(0.3868)\n",
      "3321 Training Loss: tensor(0.3834)\n",
      "3322 Training Loss: tensor(0.3824)\n",
      "3323 Training Loss: tensor(0.3904)\n",
      "3324 Training Loss: tensor(0.3811)\n",
      "3325 Training Loss: tensor(0.3883)\n",
      "3326 Training Loss: tensor(0.3832)\n",
      "3327 Training Loss: tensor(0.3864)\n",
      "3328 Training Loss: tensor(0.3859)\n",
      "3329 Training Loss: tensor(0.3852)\n",
      "3330 Training Loss: tensor(0.3881)\n",
      "3331 Training Loss: tensor(0.3831)\n",
      "3332 Training Loss: tensor(0.3877)\n",
      "3333 Training Loss: tensor(0.3836)\n",
      "3334 Training Loss: tensor(0.3814)\n",
      "3335 Training Loss: tensor(0.3899)\n",
      "3336 Training Loss: tensor(0.3837)\n",
      "3337 Training Loss: tensor(0.3841)\n",
      "3338 Training Loss: tensor(0.3843)\n",
      "3339 Training Loss: tensor(0.3842)\n",
      "3340 Training Loss: tensor(0.3841)\n",
      "3341 Training Loss: tensor(0.3851)\n",
      "3342 Training Loss: tensor(0.3850)\n",
      "3343 Training Loss: tensor(0.3813)\n",
      "3344 Training Loss: tensor(0.3835)\n",
      "3345 Training Loss: tensor(0.3854)\n",
      "3346 Training Loss: tensor(0.3876)\n",
      "3347 Training Loss: tensor(0.3836)\n",
      "3348 Training Loss: tensor(0.3851)\n",
      "3349 Training Loss: tensor(0.3860)\n",
      "3350 Training Loss: tensor(0.3866)\n",
      "3351 Training Loss: tensor(0.3818)\n",
      "3352 Training Loss: tensor(0.3824)\n",
      "3353 Training Loss: tensor(0.3853)\n",
      "3354 Training Loss: tensor(0.3848)\n",
      "3355 Training Loss: tensor(0.3858)\n",
      "3356 Training Loss: tensor(0.3854)\n",
      "3357 Training Loss: tensor(0.3839)\n",
      "3358 Training Loss: tensor(0.3847)\n",
      "3359 Training Loss: tensor(0.3833)\n",
      "3360 Training Loss: tensor(0.3846)\n",
      "3361 Training Loss: tensor(0.3896)\n",
      "3362 Training Loss: tensor(0.3869)\n",
      "3363 Training Loss: tensor(0.3837)\n",
      "3364 Training Loss: tensor(0.3892)\n",
      "3365 Training Loss: tensor(0.3826)\n",
      "3366 Training Loss: tensor(0.3814)\n",
      "3367 Training Loss: tensor(0.3821)\n",
      "3368 Training Loss: tensor(0.3839)\n",
      "3369 Training Loss: tensor(0.3818)\n",
      "3370 Training Loss: tensor(0.3885)\n",
      "3371 Training Loss: tensor(0.3863)\n",
      "3372 Training Loss: tensor(0.3835)\n",
      "3373 Training Loss: tensor(0.3844)\n",
      "3374 Training Loss: tensor(0.3838)\n",
      "3375 Training Loss: tensor(0.3839)\n",
      "3376 Training Loss: tensor(0.3826)\n",
      "3377 Training Loss: tensor(0.3832)\n",
      "3378 Training Loss: tensor(0.3857)\n",
      "3379 Training Loss: tensor(0.3836)\n",
      "3380 Training Loss: tensor(0.3851)\n",
      "3381 Training Loss: tensor(0.3850)\n",
      "3382 Training Loss: tensor(0.3877)\n",
      "3383 Training Loss: tensor(0.3807)\n",
      "3384 Training Loss: tensor(0.3859)\n",
      "3385 Training Loss: tensor(0.3828)\n",
      "3386 Training Loss: tensor(0.3842)\n",
      "3387 Training Loss: tensor(0.3850)\n",
      "3388 Training Loss: tensor(0.3857)\n",
      "3389 Training Loss: tensor(0.3810)\n",
      "3390 Training Loss: tensor(0.3860)\n",
      "3391 Training Loss: tensor(0.3903)\n",
      "3392 Training Loss: tensor(0.3892)\n",
      "3393 Training Loss: tensor(0.3801)\n",
      "3394 Training Loss: tensor(0.3866)\n",
      "3395 Training Loss: tensor(0.3831)\n",
      "3396 Training Loss: tensor(0.3830)\n",
      "3397 Training Loss: tensor(0.3884)\n",
      "3398 Training Loss: tensor(0.3854)\n",
      "3399 Training Loss: tensor(0.3851)\n",
      "3400 Training Loss: tensor(0.3828)\n",
      "3401 Training Loss: tensor(0.3879)\n",
      "3402 Training Loss: tensor(0.3850)\n",
      "3403 Training Loss: tensor(0.3837)\n",
      "3404 Training Loss: tensor(0.3831)\n",
      "3405 Training Loss: tensor(0.3861)\n",
      "3406 Training Loss: tensor(0.3841)\n",
      "3407 Training Loss: tensor(0.3827)\n",
      "3408 Training Loss: tensor(0.3849)\n",
      "3409 Training Loss: tensor(0.3878)\n",
      "3410 Training Loss: tensor(0.3890)\n",
      "3411 Training Loss: tensor(0.3869)\n",
      "3412 Training Loss: tensor(0.3839)\n",
      "3413 Training Loss: tensor(0.3823)\n",
      "3414 Training Loss: tensor(0.3868)\n",
      "3415 Training Loss: tensor(0.3846)\n",
      "3416 Training Loss: tensor(0.3878)\n",
      "3417 Training Loss: tensor(0.3859)\n",
      "3418 Training Loss: tensor(0.3832)\n",
      "3419 Training Loss: tensor(0.3859)\n",
      "3420 Training Loss: tensor(0.3902)\n",
      "3421 Training Loss: tensor(0.3863)\n",
      "3422 Training Loss: tensor(0.3821)\n",
      "3423 Training Loss: tensor(0.3839)\n",
      "3424 Training Loss: tensor(0.3825)\n",
      "3425 Training Loss: tensor(0.3865)\n",
      "3426 Training Loss: tensor(0.3853)\n",
      "3427 Training Loss: tensor(0.3884)\n",
      "3428 Training Loss: tensor(0.3836)\n",
      "3429 Training Loss: tensor(0.3847)\n",
      "3430 Training Loss: tensor(0.3840)\n",
      "3431 Training Loss: tensor(0.3849)\n",
      "3432 Training Loss: tensor(0.3835)\n",
      "3433 Training Loss: tensor(0.3813)\n",
      "3434 Training Loss: tensor(0.3853)\n",
      "3435 Training Loss: tensor(0.3830)\n",
      "3436 Training Loss: tensor(0.3835)\n",
      "3437 Training Loss: tensor(0.3893)\n",
      "3438 Training Loss: tensor(0.3823)\n",
      "3439 Training Loss: tensor(0.3810)\n",
      "3440 Training Loss: tensor(0.3878)\n",
      "3441 Training Loss: tensor(0.3838)\n",
      "3442 Training Loss: tensor(0.3820)\n",
      "3443 Training Loss: tensor(0.3897)\n",
      "3444 Training Loss: tensor(0.3873)\n",
      "3445 Training Loss: tensor(0.3829)\n",
      "3446 Training Loss: tensor(0.3878)\n",
      "3447 Training Loss: tensor(0.3889)\n",
      "3448 Training Loss: tensor(0.3885)\n",
      "3449 Training Loss: tensor(0.3850)\n",
      "3450 Training Loss: tensor(0.3837)\n",
      "3451 Training Loss: tensor(0.3862)\n",
      "3452 Training Loss: tensor(0.3834)\n",
      "3453 Training Loss: tensor(0.3864)\n",
      "3454 Training Loss: tensor(0.3850)\n",
      "3455 Training Loss: tensor(0.3834)\n",
      "3456 Training Loss: tensor(0.3852)\n",
      "3457 Training Loss: tensor(0.3857)\n",
      "3458 Training Loss: tensor(0.3857)\n",
      "3459 Training Loss: tensor(0.3856)\n",
      "3460 Training Loss: tensor(0.3851)\n",
      "3461 Training Loss: tensor(0.3854)\n",
      "3462 Training Loss: tensor(0.3828)\n",
      "3463 Training Loss: tensor(0.3846)\n",
      "3464 Training Loss: tensor(0.3863)\n",
      "3465 Training Loss: tensor(0.3824)\n",
      "3466 Training Loss: tensor(0.3850)\n",
      "3467 Training Loss: tensor(0.3859)\n",
      "3468 Training Loss: tensor(0.3830)\n",
      "3469 Training Loss: tensor(0.3819)\n",
      "3470 Training Loss: tensor(0.3841)\n",
      "3471 Training Loss: tensor(0.3843)\n",
      "3472 Training Loss: tensor(0.3849)\n",
      "3473 Training Loss: tensor(0.3846)\n",
      "3474 Training Loss: tensor(0.3823)\n",
      "3475 Training Loss: tensor(0.3856)\n",
      "3476 Training Loss: tensor(0.3802)\n",
      "3477 Training Loss: tensor(0.3850)\n",
      "3478 Training Loss: tensor(0.3863)\n",
      "3479 Training Loss: tensor(0.3844)\n",
      "3480 Training Loss: tensor(0.3835)\n",
      "3481 Training Loss: tensor(0.3829)\n",
      "3482 Training Loss: tensor(0.3843)\n",
      "3483 Training Loss: tensor(0.3798)\n",
      "3484 Training Loss: tensor(0.3855)\n",
      "3485 Training Loss: tensor(0.3857)\n",
      "3486 Training Loss: tensor(0.3837)\n",
      "3487 Training Loss: tensor(0.3854)\n",
      "3488 Training Loss: tensor(0.3810)\n",
      "3489 Training Loss: tensor(0.3842)\n",
      "3490 Training Loss: tensor(0.3864)\n",
      "3491 Training Loss: tensor(0.3863)\n",
      "3492 Training Loss: tensor(0.3826)\n",
      "3493 Training Loss: tensor(0.3827)\n",
      "3494 Training Loss: tensor(0.3820)\n",
      "3495 Training Loss: tensor(0.3815)\n",
      "3496 Training Loss: tensor(0.3857)\n",
      "3497 Training Loss: tensor(0.3815)\n",
      "3498 Training Loss: tensor(0.3824)\n",
      "3499 Training Loss: tensor(0.3829)\n",
      "3500 Training Loss: tensor(0.3842)\n",
      "3501 Training Loss: tensor(0.3811)\n",
      "3502 Training Loss: tensor(0.3860)\n",
      "3503 Training Loss: tensor(0.3821)\n",
      "3504 Training Loss: tensor(0.3899)\n",
      "3505 Training Loss: tensor(0.3837)\n",
      "3506 Training Loss: tensor(0.3832)\n",
      "3507 Training Loss: tensor(0.3839)\n",
      "3508 Training Loss: tensor(0.3862)\n",
      "3509 Training Loss: tensor(0.3846)\n",
      "3510 Training Loss: tensor(0.3833)\n",
      "3511 Training Loss: tensor(0.3836)\n",
      "3512 Training Loss: tensor(0.3864)\n",
      "3513 Training Loss: tensor(0.3796)\n",
      "3514 Training Loss: tensor(0.3883)\n",
      "3515 Training Loss: tensor(0.3824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516 Training Loss: tensor(0.3828)\n",
      "3517 Training Loss: tensor(0.3817)\n",
      "3518 Training Loss: tensor(0.3885)\n",
      "3519 Training Loss: tensor(0.3802)\n",
      "3520 Training Loss: tensor(0.3857)\n",
      "3521 Training Loss: tensor(0.3903)\n",
      "3522 Training Loss: tensor(0.3861)\n",
      "3523 Training Loss: tensor(0.3853)\n",
      "3524 Training Loss: tensor(0.3822)\n",
      "3525 Training Loss: tensor(0.3837)\n",
      "3526 Training Loss: tensor(0.3870)\n",
      "3527 Training Loss: tensor(0.3874)\n",
      "3528 Training Loss: tensor(0.3823)\n",
      "3529 Training Loss: tensor(0.3831)\n",
      "3530 Training Loss: tensor(0.3839)\n",
      "3531 Training Loss: tensor(0.3849)\n",
      "3532 Training Loss: tensor(0.3822)\n",
      "3533 Training Loss: tensor(0.3855)\n",
      "3534 Training Loss: tensor(0.3853)\n",
      "3535 Training Loss: tensor(0.3840)\n",
      "3536 Training Loss: tensor(0.3830)\n",
      "3537 Training Loss: tensor(0.3830)\n",
      "3538 Training Loss: tensor(0.3859)\n",
      "3539 Training Loss: tensor(0.3838)\n",
      "3540 Training Loss: tensor(0.3871)\n",
      "3541 Training Loss: tensor(0.3841)\n",
      "3542 Training Loss: tensor(0.3858)\n",
      "3543 Training Loss: tensor(0.3823)\n",
      "3544 Training Loss: tensor(0.3830)\n",
      "3545 Training Loss: tensor(0.3832)\n",
      "3546 Training Loss: tensor(0.3842)\n",
      "3547 Training Loss: tensor(0.3830)\n",
      "3548 Training Loss: tensor(0.3820)\n",
      "3549 Training Loss: tensor(0.3832)\n",
      "3550 Training Loss: tensor(0.3893)\n",
      "3551 Training Loss: tensor(0.3853)\n",
      "3552 Training Loss: tensor(0.3858)\n",
      "3553 Training Loss: tensor(0.3801)\n",
      "3554 Training Loss: tensor(0.3860)\n",
      "3555 Training Loss: tensor(0.3832)\n",
      "3556 Training Loss: tensor(0.3855)\n",
      "3557 Training Loss: tensor(0.3836)\n",
      "3558 Training Loss: tensor(0.3851)\n",
      "3559 Training Loss: tensor(0.3827)\n",
      "3560 Training Loss: tensor(0.3822)\n",
      "3561 Training Loss: tensor(0.3835)\n",
      "3562 Training Loss: tensor(0.3832)\n",
      "3563 Training Loss: tensor(0.3851)\n",
      "3564 Training Loss: tensor(0.3877)\n",
      "3565 Training Loss: tensor(0.3856)\n",
      "3566 Training Loss: tensor(0.3879)\n",
      "3567 Training Loss: tensor(0.3820)\n",
      "3568 Training Loss: tensor(0.3823)\n",
      "3569 Training Loss: tensor(0.3835)\n",
      "3570 Training Loss: tensor(0.3895)\n",
      "3571 Training Loss: tensor(0.3830)\n",
      "3572 Training Loss: tensor(0.3825)\n",
      "3573 Training Loss: tensor(0.3816)\n",
      "3574 Training Loss: tensor(0.3843)\n",
      "3575 Training Loss: tensor(0.3834)\n",
      "3576 Training Loss: tensor(0.3817)\n",
      "3577 Training Loss: tensor(0.3841)\n",
      "3578 Training Loss: tensor(0.3847)\n",
      "3579 Training Loss: tensor(0.3824)\n",
      "3580 Training Loss: tensor(0.3822)\n",
      "3581 Training Loss: tensor(0.3828)\n",
      "3582 Training Loss: tensor(0.3837)\n",
      "3583 Training Loss: tensor(0.3830)\n",
      "3584 Training Loss: tensor(0.3827)\n",
      "3585 Training Loss: tensor(0.3856)\n",
      "3586 Training Loss: tensor(0.3839)\n",
      "3587 Training Loss: tensor(0.3811)\n",
      "3588 Training Loss: tensor(0.3830)\n",
      "3589 Training Loss: tensor(0.3830)\n",
      "3590 Training Loss: tensor(0.3861)\n",
      "3591 Training Loss: tensor(0.3871)\n",
      "3592 Training Loss: tensor(0.3853)\n",
      "3593 Training Loss: tensor(0.3837)\n",
      "3594 Training Loss: tensor(0.3881)\n",
      "3595 Training Loss: tensor(0.3852)\n",
      "3596 Training Loss: tensor(0.3822)\n",
      "3597 Training Loss: tensor(0.3830)\n",
      "3598 Training Loss: tensor(0.3823)\n",
      "3599 Training Loss: tensor(0.3795)\n",
      "3600 Training Loss: tensor(0.3863)\n",
      "3601 Training Loss: tensor(0.3809)\n",
      "3602 Training Loss: tensor(0.3837)\n",
      "3603 Training Loss: tensor(0.3799)\n",
      "3604 Training Loss: tensor(0.3843)\n",
      "3605 Training Loss: tensor(0.3830)\n",
      "3606 Training Loss: tensor(0.3837)\n",
      "3607 Training Loss: tensor(0.3823)\n",
      "3608 Training Loss: tensor(0.3857)\n",
      "3609 Training Loss: tensor(0.3844)\n",
      "3610 Training Loss: tensor(0.3843)\n",
      "3611 Training Loss: tensor(0.3824)\n",
      "3612 Training Loss: tensor(0.3848)\n",
      "3613 Training Loss: tensor(0.3869)\n",
      "3614 Training Loss: tensor(0.3813)\n",
      "3615 Training Loss: tensor(0.3799)\n",
      "3616 Training Loss: tensor(0.3879)\n",
      "3617 Training Loss: tensor(0.3826)\n",
      "3618 Training Loss: tensor(0.3870)\n",
      "3619 Training Loss: tensor(0.3890)\n",
      "3620 Training Loss: tensor(0.3821)\n",
      "3621 Training Loss: tensor(0.3880)\n",
      "3622 Training Loss: tensor(0.3866)\n",
      "3623 Training Loss: tensor(0.3821)\n",
      "3624 Training Loss: tensor(0.3812)\n",
      "3625 Training Loss: tensor(0.3821)\n",
      "3626 Training Loss: tensor(0.3826)\n",
      "3627 Training Loss: tensor(0.3825)\n",
      "3628 Training Loss: tensor(0.3859)\n",
      "3629 Training Loss: tensor(0.3843)\n",
      "3630 Training Loss: tensor(0.3822)\n",
      "3631 Training Loss: tensor(0.3834)\n",
      "3632 Training Loss: tensor(0.3817)\n",
      "3633 Training Loss: tensor(0.3820)\n",
      "3634 Training Loss: tensor(0.3864)\n",
      "3635 Training Loss: tensor(0.3842)\n",
      "3636 Training Loss: tensor(0.3854)\n",
      "3637 Training Loss: tensor(0.3825)\n",
      "3638 Training Loss: tensor(0.3852)\n",
      "3639 Training Loss: tensor(0.3836)\n",
      "3640 Training Loss: tensor(0.3848)\n",
      "3641 Training Loss: tensor(0.3877)\n",
      "3642 Training Loss: tensor(0.3817)\n",
      "3643 Training Loss: tensor(0.3812)\n",
      "3644 Training Loss: tensor(0.3886)\n",
      "3645 Training Loss: tensor(0.3849)\n",
      "3646 Training Loss: tensor(0.3816)\n",
      "3647 Training Loss: tensor(0.3847)\n",
      "3648 Training Loss: tensor(0.3836)\n",
      "3649 Training Loss: tensor(0.3808)\n",
      "3650 Training Loss: tensor(0.3852)\n",
      "3651 Training Loss: tensor(0.3828)\n",
      "3652 Training Loss: tensor(0.3818)\n",
      "3653 Training Loss: tensor(0.3819)\n",
      "3654 Training Loss: tensor(0.3850)\n",
      "3655 Training Loss: tensor(0.3819)\n",
      "3656 Training Loss: tensor(0.3840)\n",
      "3657 Training Loss: tensor(0.3826)\n",
      "3658 Training Loss: tensor(0.3801)\n",
      "3659 Training Loss: tensor(0.3827)\n",
      "3660 Training Loss: tensor(0.3843)\n",
      "3661 Training Loss: tensor(0.3796)\n",
      "3662 Training Loss: tensor(0.3815)\n",
      "3663 Training Loss: tensor(0.3818)\n",
      "3664 Training Loss: tensor(0.3861)\n",
      "3665 Training Loss: tensor(0.3822)\n",
      "3666 Training Loss: tensor(0.3792)\n",
      "3667 Training Loss: tensor(0.3798)\n",
      "3668 Training Loss: tensor(0.3790)\n",
      "3669 Training Loss: tensor(0.3841)\n",
      "3670 Training Loss: tensor(0.3826)\n",
      "3671 Training Loss: tensor(0.3801)\n",
      "3672 Training Loss: tensor(0.3827)\n",
      "3673 Training Loss: tensor(0.3884)\n",
      "3674 Training Loss: tensor(0.3822)\n",
      "3675 Training Loss: tensor(0.3855)\n",
      "3676 Training Loss: tensor(0.3798)\n",
      "3677 Training Loss: tensor(0.3899)\n",
      "3678 Training Loss: tensor(0.3854)\n",
      "3679 Training Loss: tensor(0.3837)\n",
      "3680 Training Loss: tensor(0.3808)\n",
      "3681 Training Loss: tensor(0.3815)\n",
      "3682 Training Loss: tensor(0.3871)\n",
      "3683 Training Loss: tensor(0.3842)\n",
      "3684 Training Loss: tensor(0.3854)\n",
      "3685 Training Loss: tensor(0.3817)\n",
      "3686 Training Loss: tensor(0.3823)\n",
      "3687 Training Loss: tensor(0.3840)\n",
      "3688 Training Loss: tensor(0.3810)\n",
      "3689 Training Loss: tensor(0.3828)\n",
      "3690 Training Loss: tensor(0.3822)\n",
      "3691 Training Loss: tensor(0.3818)\n",
      "3692 Training Loss: tensor(0.3844)\n",
      "3693 Training Loss: tensor(0.3818)\n",
      "3694 Training Loss: tensor(0.3832)\n",
      "3695 Training Loss: tensor(0.3878)\n",
      "3696 Training Loss: tensor(0.3830)\n",
      "3697 Training Loss: tensor(0.3874)\n",
      "3698 Training Loss: tensor(0.3803)\n",
      "3699 Training Loss: tensor(0.3808)\n",
      "3700 Training Loss: tensor(0.3845)\n",
      "3701 Training Loss: tensor(0.3822)\n",
      "3702 Training Loss: tensor(0.3832)\n",
      "3703 Training Loss: tensor(0.3855)\n",
      "3704 Training Loss: tensor(0.3828)\n",
      "3705 Training Loss: tensor(0.3813)\n",
      "3706 Training Loss: tensor(0.3828)\n",
      "3707 Training Loss: tensor(0.3842)\n",
      "3708 Training Loss: tensor(0.3827)\n",
      "3709 Training Loss: tensor(0.3810)\n",
      "3710 Training Loss: tensor(0.3886)\n",
      "3711 Training Loss: tensor(0.3828)\n",
      "3712 Training Loss: tensor(0.3816)\n",
      "3713 Training Loss: tensor(0.3831)\n",
      "3714 Training Loss: tensor(0.3865)\n",
      "3715 Training Loss: tensor(0.3835)\n",
      "3716 Training Loss: tensor(0.3844)\n",
      "3717 Training Loss: tensor(0.3816)\n",
      "3718 Training Loss: tensor(0.3823)\n",
      "3719 Training Loss: tensor(0.3824)\n",
      "3720 Training Loss: tensor(0.3858)\n",
      "3721 Training Loss: tensor(0.3782)\n",
      "3722 Training Loss: tensor(0.3833)\n",
      "3723 Training Loss: tensor(0.3805)\n",
      "3724 Training Loss: tensor(0.3818)\n",
      "3725 Training Loss: tensor(0.3862)\n",
      "3726 Training Loss: tensor(0.3841)\n",
      "3727 Training Loss: tensor(0.3848)\n",
      "3728 Training Loss: tensor(0.3830)\n",
      "3729 Training Loss: tensor(0.3815)\n",
      "3730 Training Loss: tensor(0.3854)\n",
      "3731 Training Loss: tensor(0.3843)\n",
      "3732 Training Loss: tensor(0.3802)\n",
      "3733 Training Loss: tensor(0.3853)\n",
      "3734 Training Loss: tensor(0.3816)\n",
      "3735 Training Loss: tensor(0.3817)\n",
      "3736 Training Loss: tensor(0.3828)\n",
      "3737 Training Loss: tensor(0.3849)\n",
      "3738 Training Loss: tensor(0.3821)\n",
      "3739 Training Loss: tensor(0.3815)\n",
      "3740 Training Loss: tensor(0.3827)\n",
      "3741 Training Loss: tensor(0.3814)\n",
      "3742 Training Loss: tensor(0.3850)\n",
      "3743 Training Loss: tensor(0.3843)\n",
      "3744 Training Loss: tensor(0.3823)\n",
      "3745 Training Loss: tensor(0.3818)\n",
      "3746 Training Loss: tensor(0.3849)\n",
      "3747 Training Loss: tensor(0.3824)\n",
      "3748 Training Loss: tensor(0.3807)\n",
      "3749 Training Loss: tensor(0.3867)\n",
      "3750 Training Loss: tensor(0.3846)\n",
      "3751 Training Loss: tensor(0.3810)\n",
      "3752 Training Loss: tensor(0.3850)\n",
      "3753 Training Loss: tensor(0.3828)\n",
      "3754 Training Loss: tensor(0.3900)\n",
      "3755 Training Loss: tensor(0.3803)\n",
      "3756 Training Loss: tensor(0.3825)\n",
      "3757 Training Loss: tensor(0.3869)\n",
      "3758 Training Loss: tensor(0.3830)\n",
      "3759 Training Loss: tensor(0.3828)\n",
      "3760 Training Loss: tensor(0.3824)\n",
      "3761 Training Loss: tensor(0.3846)\n",
      "3762 Training Loss: tensor(0.3827)\n",
      "3763 Training Loss: tensor(0.3841)\n",
      "3764 Training Loss: tensor(0.3856)\n",
      "3765 Training Loss: tensor(0.3809)\n",
      "3766 Training Loss: tensor(0.3829)\n",
      "3767 Training Loss: tensor(0.3806)\n",
      "3768 Training Loss: tensor(0.3846)\n",
      "3769 Training Loss: tensor(0.3849)\n",
      "3770 Training Loss: tensor(0.3812)\n",
      "3771 Training Loss: tensor(0.3825)\n",
      "3772 Training Loss: tensor(0.3816)\n",
      "3773 Training Loss: tensor(0.3795)\n",
      "3774 Training Loss: tensor(0.3826)\n",
      "3775 Training Loss: tensor(0.3812)\n",
      "3776 Training Loss: tensor(0.3862)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3777 Training Loss: tensor(0.3809)\n",
      "3778 Training Loss: tensor(0.3825)\n",
      "3779 Training Loss: tensor(0.3805)\n",
      "3780 Training Loss: tensor(0.3847)\n",
      "3781 Training Loss: tensor(0.3821)\n",
      "3782 Training Loss: tensor(0.3831)\n",
      "3783 Training Loss: tensor(0.3817)\n",
      "3784 Training Loss: tensor(0.3837)\n",
      "3785 Training Loss: tensor(0.3815)\n",
      "3786 Training Loss: tensor(0.3821)\n",
      "3787 Training Loss: tensor(0.3816)\n",
      "3788 Training Loss: tensor(0.3822)\n",
      "3789 Training Loss: tensor(0.3829)\n",
      "3790 Training Loss: tensor(0.3803)\n",
      "3791 Training Loss: tensor(0.3844)\n",
      "3792 Training Loss: tensor(0.3810)\n",
      "3793 Training Loss: tensor(0.3807)\n",
      "3794 Training Loss: tensor(0.3833)\n",
      "3795 Training Loss: tensor(0.3836)\n",
      "3796 Training Loss: tensor(0.3809)\n",
      "3797 Training Loss: tensor(0.3801)\n",
      "3798 Training Loss: tensor(0.3817)\n",
      "3799 Training Loss: tensor(0.3835)\n",
      "3800 Training Loss: tensor(0.3831)\n",
      "3801 Training Loss: tensor(0.3827)\n",
      "3802 Training Loss: tensor(0.3829)\n",
      "3803 Training Loss: tensor(0.3862)\n",
      "3804 Training Loss: tensor(0.3818)\n",
      "3805 Training Loss: tensor(0.3826)\n",
      "3806 Training Loss: tensor(0.3808)\n",
      "3807 Training Loss: tensor(0.3845)\n",
      "3808 Training Loss: tensor(0.3829)\n",
      "3809 Training Loss: tensor(0.3810)\n",
      "3810 Training Loss: tensor(0.3832)\n",
      "3811 Training Loss: tensor(0.3824)\n",
      "3812 Training Loss: tensor(0.3827)\n",
      "3813 Training Loss: tensor(0.3877)\n",
      "3814 Training Loss: tensor(0.3912)\n",
      "3815 Training Loss: tensor(0.3845)\n",
      "3816 Training Loss: tensor(0.3803)\n",
      "3817 Training Loss: tensor(0.3852)\n",
      "3818 Training Loss: tensor(0.3816)\n",
      "3819 Training Loss: tensor(0.3842)\n",
      "3820 Training Loss: tensor(0.3836)\n",
      "3821 Training Loss: tensor(0.3814)\n",
      "3822 Training Loss: tensor(0.3840)\n",
      "3823 Training Loss: tensor(0.3808)\n",
      "3824 Training Loss: tensor(0.3844)\n",
      "3825 Training Loss: tensor(0.3818)\n",
      "3826 Training Loss: tensor(0.3838)\n",
      "3827 Training Loss: tensor(0.3821)\n",
      "3828 Training Loss: tensor(0.3818)\n",
      "3829 Training Loss: tensor(0.3805)\n",
      "3830 Training Loss: tensor(0.3834)\n",
      "3831 Training Loss: tensor(0.3807)\n",
      "3832 Training Loss: tensor(0.3822)\n",
      "3833 Training Loss: tensor(0.3828)\n",
      "3834 Training Loss: tensor(0.3844)\n",
      "3835 Training Loss: tensor(0.3870)\n",
      "3836 Training Loss: tensor(0.3813)\n",
      "3837 Training Loss: tensor(0.3848)\n",
      "3838 Training Loss: tensor(0.3805)\n",
      "3839 Training Loss: tensor(0.3846)\n",
      "3840 Training Loss: tensor(0.3805)\n",
      "3841 Training Loss: tensor(0.3911)\n",
      "3842 Training Loss: tensor(0.3863)\n",
      "3843 Training Loss: tensor(0.3860)\n",
      "3844 Training Loss: tensor(0.3831)\n",
      "3845 Training Loss: tensor(0.3789)\n",
      "3846 Training Loss: tensor(0.3860)\n",
      "3847 Training Loss: tensor(0.3845)\n",
      "3848 Training Loss: tensor(0.3806)\n",
      "3849 Training Loss: tensor(0.3828)\n",
      "3850 Training Loss: tensor(0.3815)\n",
      "3851 Training Loss: tensor(0.3853)\n",
      "3852 Training Loss: tensor(0.3832)\n",
      "3853 Training Loss: tensor(0.3836)\n",
      "3854 Training Loss: tensor(0.3831)\n",
      "3855 Training Loss: tensor(0.3836)\n",
      "3856 Training Loss: tensor(0.3879)\n",
      "3857 Training Loss: tensor(0.3843)\n",
      "3858 Training Loss: tensor(0.3838)\n",
      "3859 Training Loss: tensor(0.3810)\n",
      "3860 Training Loss: tensor(0.3809)\n",
      "3861 Training Loss: tensor(0.3836)\n",
      "3862 Training Loss: tensor(0.3835)\n",
      "3863 Training Loss: tensor(0.3828)\n",
      "3864 Training Loss: tensor(0.3894)\n",
      "3865 Training Loss: tensor(0.3851)\n",
      "3866 Training Loss: tensor(0.3814)\n",
      "3867 Training Loss: tensor(0.3798)\n",
      "3868 Training Loss: tensor(0.3827)\n",
      "3869 Training Loss: tensor(0.3844)\n",
      "3870 Training Loss: tensor(0.3822)\n",
      "3871 Training Loss: tensor(0.3824)\n",
      "3872 Training Loss: tensor(0.3849)\n",
      "3873 Training Loss: tensor(0.3859)\n",
      "3874 Training Loss: tensor(0.3810)\n",
      "3875 Training Loss: tensor(0.3804)\n",
      "3876 Training Loss: tensor(0.3806)\n",
      "3877 Training Loss: tensor(0.3804)\n",
      "3878 Training Loss: tensor(0.3812)\n",
      "3879 Training Loss: tensor(0.3811)\n",
      "3880 Training Loss: tensor(0.3843)\n",
      "3881 Training Loss: tensor(0.3802)\n",
      "3882 Training Loss: tensor(0.3819)\n",
      "3883 Training Loss: tensor(0.3822)\n",
      "3884 Training Loss: tensor(0.3813)\n",
      "3885 Training Loss: tensor(0.3844)\n",
      "3886 Training Loss: tensor(0.3826)\n",
      "3887 Training Loss: tensor(0.3783)\n",
      "3888 Training Loss: tensor(0.3826)\n",
      "3889 Training Loss: tensor(0.3822)\n",
      "3890 Training Loss: tensor(0.3819)\n",
      "3891 Training Loss: tensor(0.3809)\n",
      "3892 Training Loss: tensor(0.3803)\n",
      "3893 Training Loss: tensor(0.3861)\n",
      "3894 Training Loss: tensor(0.3853)\n",
      "3895 Training Loss: tensor(0.3823)\n",
      "3896 Training Loss: tensor(0.3837)\n",
      "3897 Training Loss: tensor(0.3811)\n",
      "3898 Training Loss: tensor(0.3838)\n",
      "3899 Training Loss: tensor(0.3802)\n",
      "3900 Training Loss: tensor(0.3801)\n",
      "3901 Training Loss: tensor(0.3795)\n",
      "3902 Training Loss: tensor(0.3841)\n",
      "3903 Training Loss: tensor(0.3803)\n",
      "3904 Training Loss: tensor(0.3809)\n",
      "3905 Training Loss: tensor(0.3825)\n",
      "3906 Training Loss: tensor(0.3826)\n",
      "3907 Training Loss: tensor(0.3860)\n",
      "3908 Training Loss: tensor(0.3824)\n",
      "3909 Training Loss: tensor(0.3804)\n",
      "3910 Training Loss: tensor(0.3842)\n",
      "3911 Training Loss: tensor(0.3813)\n",
      "3912 Training Loss: tensor(0.3842)\n",
      "3913 Training Loss: tensor(0.3811)\n",
      "3914 Training Loss: tensor(0.3813)\n",
      "3915 Training Loss: tensor(0.3803)\n",
      "3916 Training Loss: tensor(0.3827)\n",
      "3917 Training Loss: tensor(0.3835)\n",
      "3918 Training Loss: tensor(0.3845)\n",
      "3919 Training Loss: tensor(0.3864)\n",
      "3920 Training Loss: tensor(0.3850)\n",
      "3921 Training Loss: tensor(0.3823)\n",
      "3922 Training Loss: tensor(0.3843)\n",
      "3923 Training Loss: tensor(0.3846)\n",
      "3924 Training Loss: tensor(0.3855)\n",
      "3925 Training Loss: tensor(0.3825)\n",
      "3926 Training Loss: tensor(0.3835)\n",
      "3927 Training Loss: tensor(0.3812)\n",
      "3928 Training Loss: tensor(0.3812)\n",
      "3929 Training Loss: tensor(0.3809)\n",
      "3930 Training Loss: tensor(0.3813)\n",
      "3931 Training Loss: tensor(0.3838)\n",
      "3932 Training Loss: tensor(0.3831)\n",
      "3933 Training Loss: tensor(0.3826)\n",
      "3934 Training Loss: tensor(0.3850)\n",
      "3935 Training Loss: tensor(0.3837)\n",
      "3936 Training Loss: tensor(0.3837)\n",
      "3937 Training Loss: tensor(0.3831)\n",
      "3938 Training Loss: tensor(0.3804)\n",
      "3939 Training Loss: tensor(0.3835)\n",
      "3940 Training Loss: tensor(0.3801)\n",
      "3941 Training Loss: tensor(0.3843)\n",
      "3942 Training Loss: tensor(0.3848)\n",
      "3943 Training Loss: tensor(0.3823)\n",
      "3944 Training Loss: tensor(0.3794)\n",
      "3945 Training Loss: tensor(0.3853)\n",
      "3946 Training Loss: tensor(0.3817)\n",
      "3947 Training Loss: tensor(0.3848)\n",
      "3948 Training Loss: tensor(0.3810)\n",
      "3949 Training Loss: tensor(0.3810)\n",
      "3950 Training Loss: tensor(0.3813)\n",
      "3951 Training Loss: tensor(0.3795)\n",
      "3952 Training Loss: tensor(0.3832)\n",
      "3953 Training Loss: tensor(0.3820)\n",
      "3954 Training Loss: tensor(0.3810)\n",
      "3955 Training Loss: tensor(0.3807)\n",
      "3956 Training Loss: tensor(0.3826)\n",
      "3957 Training Loss: tensor(0.3831)\n",
      "3958 Training Loss: tensor(0.3818)\n",
      "3959 Training Loss: tensor(0.3854)\n",
      "3960 Training Loss: tensor(0.3797)\n",
      "3961 Training Loss: tensor(0.3863)\n",
      "3962 Training Loss: tensor(0.3799)\n",
      "3963 Training Loss: tensor(0.3819)\n",
      "3964 Training Loss: tensor(0.3856)\n",
      "3965 Training Loss: tensor(0.3849)\n",
      "3966 Training Loss: tensor(0.3820)\n",
      "3967 Training Loss: tensor(0.3809)\n",
      "3968 Training Loss: tensor(0.3820)\n",
      "3969 Training Loss: tensor(0.3807)\n",
      "3970 Training Loss: tensor(0.3834)\n",
      "3971 Training Loss: tensor(0.3844)\n",
      "3972 Training Loss: tensor(0.3811)\n",
      "3973 Training Loss: tensor(0.3802)\n",
      "3974 Training Loss: tensor(0.3801)\n",
      "3975 Training Loss: tensor(0.3823)\n",
      "3976 Training Loss: tensor(0.3809)\n",
      "3977 Training Loss: tensor(0.3841)\n",
      "3978 Training Loss: tensor(0.3841)\n",
      "3979 Training Loss: tensor(0.3821)\n",
      "3980 Training Loss: tensor(0.3853)\n",
      "3981 Training Loss: tensor(0.3805)\n",
      "3982 Training Loss: tensor(0.3822)\n",
      "3983 Training Loss: tensor(0.3794)\n",
      "3984 Training Loss: tensor(0.3830)\n",
      "3985 Training Loss: tensor(0.3840)\n",
      "3986 Training Loss: tensor(0.3803)\n",
      "3987 Training Loss: tensor(0.3853)\n",
      "3988 Training Loss: tensor(0.3843)\n",
      "3989 Training Loss: tensor(0.3833)\n",
      "3990 Training Loss: tensor(0.3798)\n",
      "3991 Training Loss: tensor(0.3800)\n",
      "3992 Training Loss: tensor(0.3852)\n",
      "3993 Training Loss: tensor(0.3837)\n",
      "3994 Training Loss: tensor(0.3805)\n",
      "3995 Training Loss: tensor(0.3827)\n",
      "3996 Training Loss: tensor(0.3836)\n",
      "3997 Training Loss: tensor(0.3835)\n",
      "3998 Training Loss: tensor(0.3811)\n",
      "3999 Training Loss: tensor(0.3793)\n",
      "4000 Training Loss: tensor(0.3801)\n",
      "4001 Training Loss: tensor(0.3810)\n",
      "4002 Training Loss: tensor(0.3806)\n",
      "4003 Training Loss: tensor(0.3793)\n",
      "4004 Training Loss: tensor(0.3813)\n",
      "4005 Training Loss: tensor(0.3809)\n",
      "4006 Training Loss: tensor(0.3819)\n",
      "4007 Training Loss: tensor(0.3809)\n",
      "4008 Training Loss: tensor(0.3813)\n",
      "4009 Training Loss: tensor(0.3827)\n",
      "4010 Training Loss: tensor(0.3826)\n",
      "4011 Training Loss: tensor(0.3825)\n",
      "4012 Training Loss: tensor(0.3838)\n",
      "4013 Training Loss: tensor(0.3807)\n",
      "4014 Training Loss: tensor(0.3828)\n",
      "4015 Training Loss: tensor(0.3848)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4016 Training Loss: tensor(0.3838)\n",
      "4017 Training Loss: tensor(0.3832)\n",
      "4018 Training Loss: tensor(0.3805)\n",
      "4019 Training Loss: tensor(0.3808)\n",
      "4020 Training Loss: tensor(0.3797)\n",
      "4021 Training Loss: tensor(0.3795)\n",
      "4022 Training Loss: tensor(0.3809)\n",
      "4023 Training Loss: tensor(0.3789)\n",
      "4024 Training Loss: tensor(0.3854)\n",
      "4025 Training Loss: tensor(0.3791)\n",
      "4026 Training Loss: tensor(0.3829)\n",
      "4027 Training Loss: tensor(0.3799)\n",
      "4028 Training Loss: tensor(0.3836)\n",
      "4029 Training Loss: tensor(0.3828)\n",
      "4030 Training Loss: tensor(0.3810)\n",
      "4031 Training Loss: tensor(0.3819)\n",
      "4032 Training Loss: tensor(0.3830)\n",
      "4033 Training Loss: tensor(0.3810)\n",
      "4034 Training Loss: tensor(0.3788)\n",
      "4035 Training Loss: tensor(0.3782)\n",
      "4036 Training Loss: tensor(0.3878)\n",
      "4037 Training Loss: tensor(0.3807)\n",
      "4038 Training Loss: tensor(0.3822)\n",
      "4039 Training Loss: tensor(0.3836)\n",
      "4040 Training Loss: tensor(0.3840)\n",
      "4041 Training Loss: tensor(0.3826)\n",
      "4042 Training Loss: tensor(0.3834)\n",
      "4043 Training Loss: tensor(0.3807)\n",
      "4044 Training Loss: tensor(0.3819)\n",
      "4045 Training Loss: tensor(0.3813)\n",
      "4046 Training Loss: tensor(0.3844)\n",
      "4047 Training Loss: tensor(0.3803)\n",
      "4048 Training Loss: tensor(0.3807)\n",
      "4049 Training Loss: tensor(0.3848)\n",
      "4050 Training Loss: tensor(0.3797)\n",
      "4051 Training Loss: tensor(0.3813)\n",
      "4052 Training Loss: tensor(0.3819)\n",
      "4053 Training Loss: tensor(0.3862)\n",
      "4054 Training Loss: tensor(0.3868)\n",
      "4055 Training Loss: tensor(0.3849)\n",
      "4056 Training Loss: tensor(0.3817)\n",
      "4057 Training Loss: tensor(0.3803)\n",
      "4058 Training Loss: tensor(0.3831)\n",
      "4059 Training Loss: tensor(0.3816)\n",
      "4060 Training Loss: tensor(0.3818)\n",
      "4061 Training Loss: tensor(0.3821)\n",
      "4062 Training Loss: tensor(0.3872)\n",
      "4063 Training Loss: tensor(0.3837)\n",
      "4064 Training Loss: tensor(0.3840)\n",
      "4065 Training Loss: tensor(0.3795)\n",
      "4066 Training Loss: tensor(0.3809)\n",
      "4067 Training Loss: tensor(0.3904)\n",
      "4068 Training Loss: tensor(0.3792)\n",
      "4069 Training Loss: tensor(0.3820)\n",
      "4070 Training Loss: tensor(0.3889)\n",
      "4071 Training Loss: tensor(0.3859)\n",
      "4072 Training Loss: tensor(0.3807)\n",
      "4073 Training Loss: tensor(0.3861)\n",
      "4074 Training Loss: tensor(0.3815)\n",
      "4075 Training Loss: tensor(0.3802)\n",
      "4076 Training Loss: tensor(0.3781)\n",
      "4077 Training Loss: tensor(0.3845)\n",
      "4078 Training Loss: tensor(0.3825)\n",
      "4079 Training Loss: tensor(0.3834)\n",
      "4080 Training Loss: tensor(0.3836)\n",
      "4081 Training Loss: tensor(0.3849)\n",
      "4082 Training Loss: tensor(0.3846)\n",
      "4083 Training Loss: tensor(0.3825)\n",
      "4084 Training Loss: tensor(0.3810)\n",
      "4085 Training Loss: tensor(0.3809)\n",
      "4086 Training Loss: tensor(0.3844)\n",
      "4087 Training Loss: tensor(0.3848)\n",
      "4088 Training Loss: tensor(0.3828)\n",
      "4089 Training Loss: tensor(0.3852)\n",
      "4090 Training Loss: tensor(0.3878)\n",
      "4091 Training Loss: tensor(0.3821)\n",
      "4092 Training Loss: tensor(0.3894)\n",
      "4093 Training Loss: tensor(0.3825)\n",
      "4094 Training Loss: tensor(0.3799)\n",
      "4095 Training Loss: tensor(0.3820)\n",
      "4096 Training Loss: tensor(0.3853)\n",
      "4097 Training Loss: tensor(0.3846)\n",
      "4098 Training Loss: tensor(0.3823)\n",
      "4099 Training Loss: tensor(0.3821)\n",
      "4100 Training Loss: tensor(0.3808)\n",
      "4101 Training Loss: tensor(0.3839)\n",
      "4102 Training Loss: tensor(0.3831)\n",
      "4103 Training Loss: tensor(0.3826)\n",
      "4104 Training Loss: tensor(0.3808)\n",
      "4105 Training Loss: tensor(0.3825)\n",
      "4106 Training Loss: tensor(0.3825)\n",
      "4107 Training Loss: tensor(0.3801)\n",
      "4108 Training Loss: tensor(0.3816)\n",
      "4109 Training Loss: tensor(0.3809)\n",
      "4110 Training Loss: tensor(0.3811)\n",
      "4111 Training Loss: tensor(0.3816)\n",
      "4112 Training Loss: tensor(0.3813)\n",
      "4113 Training Loss: tensor(0.3815)\n",
      "4114 Training Loss: tensor(0.3789)\n",
      "4115 Training Loss: tensor(0.3814)\n",
      "4116 Training Loss: tensor(0.3809)\n",
      "4117 Training Loss: tensor(0.3830)\n",
      "4118 Training Loss: tensor(0.3809)\n",
      "4119 Training Loss: tensor(0.3833)\n",
      "4120 Training Loss: tensor(0.3846)\n",
      "4121 Training Loss: tensor(0.3788)\n",
      "4122 Training Loss: tensor(0.3857)\n",
      "4123 Training Loss: tensor(0.3877)\n",
      "4124 Training Loss: tensor(0.3796)\n",
      "4125 Training Loss: tensor(0.3793)\n",
      "4126 Training Loss: tensor(0.3786)\n",
      "4127 Training Loss: tensor(0.3848)\n",
      "4128 Training Loss: tensor(0.3789)\n",
      "4129 Training Loss: tensor(0.3811)\n",
      "4130 Training Loss: tensor(0.3781)\n",
      "4131 Training Loss: tensor(0.3815)\n",
      "4132 Training Loss: tensor(0.3825)\n",
      "4133 Training Loss: tensor(0.3824)\n",
      "4134 Training Loss: tensor(0.3821)\n",
      "4135 Training Loss: tensor(0.3853)\n",
      "4136 Training Loss: tensor(0.3834)\n",
      "4137 Training Loss: tensor(0.3847)\n",
      "4138 Training Loss: tensor(0.3826)\n",
      "4139 Training Loss: tensor(0.3848)\n",
      "4140 Training Loss: tensor(0.3787)\n",
      "4141 Training Loss: tensor(0.3797)\n",
      "4142 Training Loss: tensor(0.3837)\n",
      "4143 Training Loss: tensor(0.3840)\n",
      "4144 Training Loss: tensor(0.3804)\n",
      "4145 Training Loss: tensor(0.3788)\n",
      "4146 Training Loss: tensor(0.3818)\n",
      "4147 Training Loss: tensor(0.3818)\n",
      "4148 Training Loss: tensor(0.3832)\n",
      "4149 Training Loss: tensor(0.3797)\n",
      "4150 Training Loss: tensor(0.3802)\n",
      "4151 Training Loss: tensor(0.3838)\n",
      "4152 Training Loss: tensor(0.3857)\n",
      "4153 Training Loss: tensor(0.3809)\n",
      "4154 Training Loss: tensor(0.3832)\n",
      "4155 Training Loss: tensor(0.3828)\n",
      "4156 Training Loss: tensor(0.3788)\n",
      "4157 Training Loss: tensor(0.3861)\n",
      "4158 Training Loss: tensor(0.3803)\n",
      "4159 Training Loss: tensor(0.3834)\n",
      "4160 Training Loss: tensor(0.3824)\n",
      "4161 Training Loss: tensor(0.3809)\n",
      "4162 Training Loss: tensor(0.3841)\n",
      "4163 Training Loss: tensor(0.3821)\n",
      "4164 Training Loss: tensor(0.3809)\n",
      "4165 Training Loss: tensor(0.3813)\n",
      "4166 Training Loss: tensor(0.3791)\n",
      "4167 Training Loss: tensor(0.3847)\n",
      "4168 Training Loss: tensor(0.3823)\n",
      "4169 Training Loss: tensor(0.3825)\n",
      "4170 Training Loss: tensor(0.3796)\n",
      "4171 Training Loss: tensor(0.3826)\n",
      "4172 Training Loss: tensor(0.3783)\n",
      "4173 Training Loss: tensor(0.3792)\n",
      "4174 Training Loss: tensor(0.3848)\n",
      "4175 Training Loss: tensor(0.3829)\n",
      "4176 Training Loss: tensor(0.3798)\n",
      "4177 Training Loss: tensor(0.3836)\n",
      "4178 Training Loss: tensor(0.3835)\n",
      "4179 Training Loss: tensor(0.3802)\n",
      "4180 Training Loss: tensor(0.3821)\n",
      "4181 Training Loss: tensor(0.3805)\n",
      "4182 Training Loss: tensor(0.3802)\n",
      "4183 Training Loss: tensor(0.3850)\n",
      "4184 Training Loss: tensor(0.3804)\n",
      "4185 Training Loss: tensor(0.3807)\n",
      "4186 Training Loss: tensor(0.3802)\n",
      "4187 Training Loss: tensor(0.3848)\n",
      "4188 Training Loss: tensor(0.3832)\n",
      "4189 Training Loss: tensor(0.3812)\n",
      "4190 Training Loss: tensor(0.3832)\n",
      "4191 Training Loss: tensor(0.3802)\n",
      "4192 Training Loss: tensor(0.3789)\n",
      "4193 Training Loss: tensor(0.3828)\n",
      "4194 Training Loss: tensor(0.3834)\n",
      "4195 Training Loss: tensor(0.3835)\n",
      "4196 Training Loss: tensor(0.3835)\n",
      "4197 Training Loss: tensor(0.3811)\n",
      "4198 Training Loss: tensor(0.3794)\n",
      "4199 Training Loss: tensor(0.3790)\n",
      "4200 Training Loss: tensor(0.3816)\n",
      "4201 Training Loss: tensor(0.3866)\n",
      "4202 Training Loss: tensor(0.3877)\n",
      "4203 Training Loss: tensor(0.3811)\n",
      "4204 Training Loss: tensor(0.3816)\n",
      "4205 Training Loss: tensor(0.3832)\n",
      "4206 Training Loss: tensor(0.3780)\n",
      "4207 Training Loss: tensor(0.3831)\n",
      "4208 Training Loss: tensor(0.3852)\n",
      "4209 Training Loss: tensor(0.3848)\n",
      "4210 Training Loss: tensor(0.3800)\n",
      "4211 Training Loss: tensor(0.3825)\n",
      "4212 Training Loss: tensor(0.3820)\n",
      "4213 Training Loss: tensor(0.3791)\n",
      "4214 Training Loss: tensor(0.3835)\n",
      "4215 Training Loss: tensor(0.3836)\n",
      "4216 Training Loss: tensor(0.3802)\n",
      "4217 Training Loss: tensor(0.3821)\n",
      "4218 Training Loss: tensor(0.3813)\n",
      "4219 Training Loss: tensor(0.3787)\n",
      "4220 Training Loss: tensor(0.3788)\n",
      "4221 Training Loss: tensor(0.3798)\n",
      "4222 Training Loss: tensor(0.3808)\n",
      "4223 Training Loss: tensor(0.3791)\n",
      "4224 Training Loss: tensor(0.3799)\n",
      "4225 Training Loss: tensor(0.3781)\n",
      "4226 Training Loss: tensor(0.3832)\n",
      "4227 Training Loss: tensor(0.3796)\n",
      "4228 Training Loss: tensor(0.3824)\n",
      "4229 Training Loss: tensor(0.3810)\n",
      "4230 Training Loss: tensor(0.3822)\n",
      "4231 Training Loss: tensor(0.3798)\n",
      "4232 Training Loss: tensor(0.3826)\n",
      "4233 Training Loss: tensor(0.3814)\n",
      "4234 Training Loss: tensor(0.3794)\n",
      "4235 Training Loss: tensor(0.3794)\n",
      "4236 Training Loss: tensor(0.3788)\n",
      "4237 Training Loss: tensor(0.3819)\n",
      "4238 Training Loss: tensor(0.3787)\n",
      "4239 Training Loss: tensor(0.3814)\n",
      "4240 Training Loss: tensor(0.3861)\n",
      "4241 Training Loss: tensor(0.3806)\n",
      "4242 Training Loss: tensor(0.3790)\n",
      "4243 Training Loss: tensor(0.3810)\n",
      "4244 Training Loss: tensor(0.3830)\n",
      "4245 Training Loss: tensor(0.3822)\n",
      "4246 Training Loss: tensor(0.3844)\n",
      "4247 Training Loss: tensor(0.3800)\n",
      "4248 Training Loss: tensor(0.3780)\n",
      "4249 Training Loss: tensor(0.3809)\n",
      "4250 Training Loss: tensor(0.3816)\n",
      "4251 Training Loss: tensor(0.3790)\n",
      "4252 Training Loss: tensor(0.3807)\n",
      "4253 Training Loss: tensor(0.3814)\n",
      "4254 Training Loss: tensor(0.3828)\n",
      "4255 Training Loss: tensor(0.3817)\n",
      "4256 Training Loss: tensor(0.3843)\n",
      "4257 Training Loss: tensor(0.3839)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258 Training Loss: tensor(0.3818)\n",
      "4259 Training Loss: tensor(0.3811)\n",
      "4260 Training Loss: tensor(0.3822)\n",
      "4261 Training Loss: tensor(0.3840)\n",
      "4262 Training Loss: tensor(0.3771)\n",
      "4263 Training Loss: tensor(0.3804)\n",
      "4264 Training Loss: tensor(0.3810)\n",
      "4265 Training Loss: tensor(0.3815)\n",
      "4266 Training Loss: tensor(0.3813)\n",
      "4267 Training Loss: tensor(0.3805)\n",
      "4268 Training Loss: tensor(0.3805)\n",
      "4269 Training Loss: tensor(0.3855)\n",
      "4270 Training Loss: tensor(0.3805)\n",
      "4271 Training Loss: tensor(0.3809)\n",
      "4272 Training Loss: tensor(0.3807)\n",
      "4273 Training Loss: tensor(0.3794)\n",
      "4274 Training Loss: tensor(0.3812)\n",
      "4275 Training Loss: tensor(0.3827)\n",
      "4276 Training Loss: tensor(0.3797)\n",
      "4277 Training Loss: tensor(0.3840)\n",
      "4278 Training Loss: tensor(0.3818)\n",
      "4279 Training Loss: tensor(0.3803)\n",
      "4280 Training Loss: tensor(0.3813)\n",
      "4281 Training Loss: tensor(0.3814)\n",
      "4282 Training Loss: tensor(0.3786)\n",
      "4283 Training Loss: tensor(0.3820)\n",
      "4284 Training Loss: tensor(0.3781)\n",
      "4285 Training Loss: tensor(0.3822)\n",
      "4286 Training Loss: tensor(0.3784)\n",
      "4287 Training Loss: tensor(0.3773)\n",
      "4288 Training Loss: tensor(0.3756)\n",
      "4289 Training Loss: tensor(0.3794)\n",
      "4290 Training Loss: tensor(0.3840)\n",
      "4291 Training Loss: tensor(0.3792)\n",
      "4292 Training Loss: tensor(0.3804)\n",
      "4293 Training Loss: tensor(0.3815)\n",
      "4294 Training Loss: tensor(0.3782)\n",
      "4295 Training Loss: tensor(0.3819)\n",
      "4296 Training Loss: tensor(0.3892)\n",
      "4297 Training Loss: tensor(0.3823)\n",
      "4298 Training Loss: tensor(0.3794)\n",
      "4299 Training Loss: tensor(0.3818)\n",
      "4300 Training Loss: tensor(0.3880)\n",
      "4301 Training Loss: tensor(0.3812)\n",
      "4302 Training Loss: tensor(0.3876)\n",
      "4303 Training Loss: tensor(0.3821)\n",
      "4304 Training Loss: tensor(0.3812)\n",
      "4305 Training Loss: tensor(0.3790)\n",
      "4306 Training Loss: tensor(0.3820)\n",
      "4307 Training Loss: tensor(0.3794)\n",
      "4308 Training Loss: tensor(0.3819)\n",
      "4309 Training Loss: tensor(0.3799)\n",
      "4310 Training Loss: tensor(0.3823)\n",
      "4311 Training Loss: tensor(0.3797)\n",
      "4312 Training Loss: tensor(0.3799)\n",
      "4313 Training Loss: tensor(0.3819)\n",
      "4314 Training Loss: tensor(0.3801)\n",
      "4315 Training Loss: tensor(0.3780)\n",
      "4316 Training Loss: tensor(0.3829)\n",
      "4317 Training Loss: tensor(0.3796)\n",
      "4318 Training Loss: tensor(0.3785)\n",
      "4319 Training Loss: tensor(0.3821)\n",
      "4320 Training Loss: tensor(0.3800)\n",
      "4321 Training Loss: tensor(0.3815)\n",
      "4322 Training Loss: tensor(0.3800)\n",
      "4323 Training Loss: tensor(0.3865)\n",
      "4324 Training Loss: tensor(0.3782)\n",
      "4325 Training Loss: tensor(0.3840)\n",
      "4326 Training Loss: tensor(0.3811)\n",
      "4327 Training Loss: tensor(0.3820)\n",
      "4328 Training Loss: tensor(0.3826)\n",
      "4329 Training Loss: tensor(0.3818)\n",
      "4330 Training Loss: tensor(0.3874)\n",
      "4331 Training Loss: tensor(0.3832)\n",
      "4332 Training Loss: tensor(0.3800)\n",
      "4333 Training Loss: tensor(0.3816)\n",
      "4334 Training Loss: tensor(0.3781)\n",
      "4335 Training Loss: tensor(0.3833)\n",
      "4336 Training Loss: tensor(0.3829)\n",
      "4337 Training Loss: tensor(0.3794)\n",
      "4338 Training Loss: tensor(0.3847)\n",
      "4339 Training Loss: tensor(0.3824)\n",
      "4340 Training Loss: tensor(0.3826)\n",
      "4341 Training Loss: tensor(0.3797)\n",
      "4342 Training Loss: tensor(0.3779)\n",
      "4343 Training Loss: tensor(0.3801)\n",
      "4344 Training Loss: tensor(0.3813)\n",
      "4345 Training Loss: tensor(0.3789)\n",
      "4346 Training Loss: tensor(0.3795)\n",
      "4347 Training Loss: tensor(0.3776)\n",
      "4348 Training Loss: tensor(0.3787)\n",
      "4349 Training Loss: tensor(0.3812)\n",
      "4350 Training Loss: tensor(0.3818)\n",
      "4351 Training Loss: tensor(0.3841)\n",
      "4352 Training Loss: tensor(0.3829)\n",
      "4353 Training Loss: tensor(0.3820)\n",
      "4354 Training Loss: tensor(0.3786)\n",
      "4355 Training Loss: tensor(0.3817)\n",
      "4356 Training Loss: tensor(0.3787)\n",
      "4357 Training Loss: tensor(0.3834)\n",
      "4358 Training Loss: tensor(0.3843)\n",
      "4359 Training Loss: tensor(0.3808)\n",
      "4360 Training Loss: tensor(0.3798)\n",
      "4361 Training Loss: tensor(0.3829)\n",
      "4362 Training Loss: tensor(0.3841)\n",
      "4363 Training Loss: tensor(0.3816)\n",
      "4364 Training Loss: tensor(0.3820)\n",
      "4365 Training Loss: tensor(0.3778)\n",
      "4366 Training Loss: tensor(0.3846)\n",
      "4367 Training Loss: tensor(0.3809)\n",
      "4368 Training Loss: tensor(0.3846)\n",
      "4369 Training Loss: tensor(0.3816)\n",
      "4370 Training Loss: tensor(0.3809)\n",
      "4371 Training Loss: tensor(0.3815)\n",
      "4372 Training Loss: tensor(0.3799)\n",
      "4373 Training Loss: tensor(0.3791)\n",
      "4374 Training Loss: tensor(0.3824)\n",
      "4375 Training Loss: tensor(0.3825)\n",
      "4376 Training Loss: tensor(0.3800)\n",
      "4377 Training Loss: tensor(0.3795)\n",
      "4378 Training Loss: tensor(0.3800)\n",
      "4379 Training Loss: tensor(0.3808)\n",
      "4380 Training Loss: tensor(0.3783)\n",
      "4381 Training Loss: tensor(0.3836)\n",
      "4382 Training Loss: tensor(0.3816)\n",
      "4383 Training Loss: tensor(0.3784)\n",
      "4384 Training Loss: tensor(0.3793)\n",
      "4385 Training Loss: tensor(0.3880)\n",
      "4386 Training Loss: tensor(0.3801)\n",
      "4387 Training Loss: tensor(0.3827)\n",
      "4388 Training Loss: tensor(0.3821)\n",
      "4389 Training Loss: tensor(0.3778)\n",
      "4390 Training Loss: tensor(0.3751)\n",
      "4391 Training Loss: tensor(0.3786)\n",
      "4392 Training Loss: tensor(0.3831)\n",
      "4393 Training Loss: tensor(0.3794)\n",
      "4394 Training Loss: tensor(0.3775)\n",
      "4395 Training Loss: tensor(0.3819)\n",
      "4396 Training Loss: tensor(0.3863)\n",
      "4397 Training Loss: tensor(0.3801)\n",
      "4398 Training Loss: tensor(0.3823)\n",
      "4399 Training Loss: tensor(0.3795)\n",
      "4400 Training Loss: tensor(0.3797)\n",
      "4401 Training Loss: tensor(0.3850)\n",
      "4402 Training Loss: tensor(0.3797)\n",
      "4403 Training Loss: tensor(0.3792)\n",
      "4404 Training Loss: tensor(0.3808)\n",
      "4405 Training Loss: tensor(0.3793)\n",
      "4406 Training Loss: tensor(0.3820)\n",
      "4407 Training Loss: tensor(0.3799)\n",
      "4408 Training Loss: tensor(0.3821)\n",
      "4409 Training Loss: tensor(0.3791)\n",
      "4410 Training Loss: tensor(0.3817)\n",
      "4411 Training Loss: tensor(0.3823)\n",
      "4412 Training Loss: tensor(0.3829)\n",
      "4413 Training Loss: tensor(0.3824)\n",
      "4414 Training Loss: tensor(0.3801)\n",
      "4415 Training Loss: tensor(0.3812)\n",
      "4416 Training Loss: tensor(0.3816)\n",
      "4417 Training Loss: tensor(0.3840)\n",
      "4418 Training Loss: tensor(0.3792)\n",
      "4419 Training Loss: tensor(0.3809)\n",
      "4420 Training Loss: tensor(0.3793)\n",
      "4421 Training Loss: tensor(0.3785)\n",
      "4422 Training Loss: tensor(0.3832)\n",
      "4423 Training Loss: tensor(0.3854)\n",
      "4424 Training Loss: tensor(0.3833)\n",
      "4425 Training Loss: tensor(0.3804)\n",
      "4426 Training Loss: tensor(0.3806)\n",
      "4427 Training Loss: tensor(0.3809)\n",
      "4428 Training Loss: tensor(0.3815)\n",
      "4429 Training Loss: tensor(0.3781)\n",
      "4430 Training Loss: tensor(0.3795)\n",
      "4431 Training Loss: tensor(0.3821)\n",
      "4432 Training Loss: tensor(0.3799)\n",
      "4433 Training Loss: tensor(0.3801)\n",
      "4434 Training Loss: tensor(0.3873)\n",
      "4435 Training Loss: tensor(0.3782)\n",
      "4436 Training Loss: tensor(0.3806)\n",
      "4437 Training Loss: tensor(0.3822)\n",
      "4438 Training Loss: tensor(0.3754)\n",
      "4439 Training Loss: tensor(0.3812)\n",
      "4440 Training Loss: tensor(0.3816)\n",
      "4441 Training Loss: tensor(0.3801)\n",
      "4442 Training Loss: tensor(0.3783)\n",
      "4443 Training Loss: tensor(0.3832)\n",
      "4444 Training Loss: tensor(0.3823)\n",
      "4445 Training Loss: tensor(0.3826)\n",
      "4446 Training Loss: tensor(0.3847)\n",
      "4447 Training Loss: tensor(0.3775)\n",
      "4448 Training Loss: tensor(0.3783)\n",
      "4449 Training Loss: tensor(0.3827)\n",
      "4450 Training Loss: tensor(0.3823)\n",
      "4451 Training Loss: tensor(0.3806)\n",
      "4452 Training Loss: tensor(0.3816)\n",
      "4453 Training Loss: tensor(0.3804)\n",
      "4454 Training Loss: tensor(0.3810)\n",
      "4455 Training Loss: tensor(0.3832)\n",
      "4456 Training Loss: tensor(0.3782)\n",
      "4457 Training Loss: tensor(0.3820)\n",
      "4458 Training Loss: tensor(0.3787)\n",
      "4459 Training Loss: tensor(0.3797)\n",
      "4460 Training Loss: tensor(0.3785)\n",
      "4461 Training Loss: tensor(0.3791)\n",
      "4462 Training Loss: tensor(0.3843)\n",
      "4463 Training Loss: tensor(0.3823)\n",
      "4464 Training Loss: tensor(0.3790)\n",
      "4465 Training Loss: tensor(0.3824)\n",
      "4466 Training Loss: tensor(0.3801)\n",
      "4467 Training Loss: tensor(0.3768)\n",
      "4468 Training Loss: tensor(0.3819)\n",
      "4469 Training Loss: tensor(0.3802)\n",
      "4470 Training Loss: tensor(0.3765)\n",
      "4471 Training Loss: tensor(0.3811)\n",
      "4472 Training Loss: tensor(0.3838)\n",
      "4473 Training Loss: tensor(0.3788)\n",
      "4474 Training Loss: tensor(0.3775)\n",
      "4475 Training Loss: tensor(0.3795)\n",
      "4476 Training Loss: tensor(0.3821)\n",
      "4477 Training Loss: tensor(0.3804)\n",
      "4478 Training Loss: tensor(0.3820)\n",
      "4479 Training Loss: tensor(0.3817)\n",
      "4480 Training Loss: tensor(0.3814)\n",
      "4481 Training Loss: tensor(0.3826)\n",
      "4482 Training Loss: tensor(0.3799)\n",
      "4483 Training Loss: tensor(0.3814)\n",
      "4484 Training Loss: tensor(0.3789)\n",
      "4485 Training Loss: tensor(0.3837)\n",
      "4486 Training Loss: tensor(0.3802)\n",
      "4487 Training Loss: tensor(0.3805)\n",
      "4488 Training Loss: tensor(0.3818)\n",
      "4489 Training Loss: tensor(0.3788)\n",
      "4490 Training Loss: tensor(0.3829)\n",
      "4491 Training Loss: tensor(0.3810)\n",
      "4492 Training Loss: tensor(0.3809)\n",
      "4493 Training Loss: tensor(0.3796)\n",
      "4494 Training Loss: tensor(0.3803)\n",
      "4495 Training Loss: tensor(0.3791)\n",
      "4496 Training Loss: tensor(0.3837)\n",
      "4497 Training Loss: tensor(0.3816)\n",
      "4498 Training Loss: tensor(0.3805)\n",
      "4499 Training Loss: tensor(0.3798)\n",
      "4500 Training Loss: tensor(0.3814)\n",
      "4501 Training Loss: tensor(0.3815)\n",
      "4502 Training Loss: tensor(0.3830)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4503 Training Loss: tensor(0.3842)\n",
      "4504 Training Loss: tensor(0.3824)\n",
      "4505 Training Loss: tensor(0.3793)\n",
      "4506 Training Loss: tensor(0.3787)\n",
      "4507 Training Loss: tensor(0.3801)\n",
      "4508 Training Loss: tensor(0.3766)\n",
      "4509 Training Loss: tensor(0.3805)\n",
      "4510 Training Loss: tensor(0.3796)\n",
      "4511 Training Loss: tensor(0.3817)\n",
      "4512 Training Loss: tensor(0.3830)\n",
      "4513 Training Loss: tensor(0.3791)\n",
      "4514 Training Loss: tensor(0.3803)\n",
      "4515 Training Loss: tensor(0.3800)\n",
      "4516 Training Loss: tensor(0.3805)\n",
      "4517 Training Loss: tensor(0.3800)\n",
      "4518 Training Loss: tensor(0.3786)\n",
      "4519 Training Loss: tensor(0.3805)\n",
      "4520 Training Loss: tensor(0.3831)\n",
      "4521 Training Loss: tensor(0.3798)\n",
      "4522 Training Loss: tensor(0.3764)\n",
      "4523 Training Loss: tensor(0.3767)\n",
      "4524 Training Loss: tensor(0.3784)\n",
      "4525 Training Loss: tensor(0.3801)\n",
      "4526 Training Loss: tensor(0.3828)\n",
      "4527 Training Loss: tensor(0.3779)\n",
      "4528 Training Loss: tensor(0.3810)\n",
      "4529 Training Loss: tensor(0.3835)\n",
      "4530 Training Loss: tensor(0.3823)\n",
      "4531 Training Loss: tensor(0.3764)\n",
      "4532 Training Loss: tensor(0.3823)\n",
      "4533 Training Loss: tensor(0.3809)\n",
      "4534 Training Loss: tensor(0.3780)\n",
      "4535 Training Loss: tensor(0.3815)\n",
      "4536 Training Loss: tensor(0.3767)\n",
      "4537 Training Loss: tensor(0.3799)\n",
      "4538 Training Loss: tensor(0.3811)\n",
      "4539 Training Loss: tensor(0.3810)\n",
      "4540 Training Loss: tensor(0.3816)\n",
      "4541 Training Loss: tensor(0.3763)\n",
      "4542 Training Loss: tensor(0.3799)\n",
      "4543 Training Loss: tensor(0.3795)\n",
      "4544 Training Loss: tensor(0.3783)\n",
      "4545 Training Loss: tensor(0.3843)\n",
      "4546 Training Loss: tensor(0.3789)\n",
      "4547 Training Loss: tensor(0.3807)\n",
      "4548 Training Loss: tensor(0.3781)\n",
      "4549 Training Loss: tensor(0.3813)\n",
      "4550 Training Loss: tensor(0.3815)\n",
      "4551 Training Loss: tensor(0.3809)\n",
      "4552 Training Loss: tensor(0.3776)\n",
      "4553 Training Loss: tensor(0.3849)\n",
      "4554 Training Loss: tensor(0.3766)\n",
      "4555 Training Loss: tensor(0.3802)\n",
      "4556 Training Loss: tensor(0.3783)\n",
      "4557 Training Loss: tensor(0.3763)\n",
      "4558 Training Loss: tensor(0.3801)\n",
      "4559 Training Loss: tensor(0.3811)\n",
      "4560 Training Loss: tensor(0.3788)\n",
      "4561 Training Loss: tensor(0.3799)\n",
      "4562 Training Loss: tensor(0.3801)\n",
      "4563 Training Loss: tensor(0.3805)\n",
      "4564 Training Loss: tensor(0.3824)\n",
      "4565 Training Loss: tensor(0.3805)\n",
      "4566 Training Loss: tensor(0.3837)\n",
      "4567 Training Loss: tensor(0.3840)\n",
      "4568 Training Loss: tensor(0.3818)\n",
      "4569 Training Loss: tensor(0.3810)\n",
      "4570 Training Loss: tensor(0.3789)\n",
      "4571 Training Loss: tensor(0.3825)\n",
      "4572 Training Loss: tensor(0.3775)\n",
      "4573 Training Loss: tensor(0.3801)\n",
      "4574 Training Loss: tensor(0.3801)\n",
      "4575 Training Loss: tensor(0.3811)\n",
      "4576 Training Loss: tensor(0.3805)\n",
      "4577 Training Loss: tensor(0.3810)\n",
      "4578 Training Loss: tensor(0.3793)\n",
      "4579 Training Loss: tensor(0.3812)\n",
      "4580 Training Loss: tensor(0.3783)\n",
      "4581 Training Loss: tensor(0.3804)\n",
      "4582 Training Loss: tensor(0.3785)\n",
      "4583 Training Loss: tensor(0.3815)\n",
      "4584 Training Loss: tensor(0.3778)\n",
      "4585 Training Loss: tensor(0.3849)\n",
      "4586 Training Loss: tensor(0.3847)\n",
      "4587 Training Loss: tensor(0.3820)\n",
      "4588 Training Loss: tensor(0.3860)\n",
      "4589 Training Loss: tensor(0.3791)\n",
      "4590 Training Loss: tensor(0.3815)\n",
      "4591 Training Loss: tensor(0.3799)\n",
      "4592 Training Loss: tensor(0.3780)\n",
      "4593 Training Loss: tensor(0.3812)\n",
      "4594 Training Loss: tensor(0.3782)\n",
      "4595 Training Loss: tensor(0.3806)\n",
      "4596 Training Loss: tensor(0.3805)\n",
      "4597 Training Loss: tensor(0.3775)\n",
      "4598 Training Loss: tensor(0.3797)\n",
      "4599 Training Loss: tensor(0.3781)\n",
      "4600 Training Loss: tensor(0.3816)\n",
      "4601 Training Loss: tensor(0.3795)\n",
      "4602 Training Loss: tensor(0.3789)\n",
      "4603 Training Loss: tensor(0.3803)\n",
      "4604 Training Loss: tensor(0.3859)\n",
      "4605 Training Loss: tensor(0.3870)\n",
      "4606 Training Loss: tensor(0.3812)\n",
      "4607 Training Loss: tensor(0.3798)\n",
      "4608 Training Loss: tensor(0.3814)\n",
      "4609 Training Loss: tensor(0.3766)\n",
      "4610 Training Loss: tensor(0.3783)\n",
      "4611 Training Loss: tensor(0.3783)\n",
      "4612 Training Loss: tensor(0.3765)\n",
      "4613 Training Loss: tensor(0.3818)\n",
      "4614 Training Loss: tensor(0.3799)\n",
      "4615 Training Loss: tensor(0.3800)\n",
      "4616 Training Loss: tensor(0.3773)\n",
      "4617 Training Loss: tensor(0.3807)\n",
      "4618 Training Loss: tensor(0.3777)\n",
      "4619 Training Loss: tensor(0.3787)\n",
      "4620 Training Loss: tensor(0.3798)\n",
      "4621 Training Loss: tensor(0.3802)\n",
      "4622 Training Loss: tensor(0.3820)\n",
      "4623 Training Loss: tensor(0.3785)\n",
      "4624 Training Loss: tensor(0.3776)\n",
      "4625 Training Loss: tensor(0.3801)\n",
      "4626 Training Loss: tensor(0.3857)\n",
      "4627 Training Loss: tensor(0.3779)\n",
      "4628 Training Loss: tensor(0.3832)\n",
      "4629 Training Loss: tensor(0.3829)\n",
      "4630 Training Loss: tensor(0.3781)\n",
      "4631 Training Loss: tensor(0.3814)\n",
      "4632 Training Loss: tensor(0.3806)\n",
      "4633 Training Loss: tensor(0.3800)\n",
      "4634 Training Loss: tensor(0.3792)\n",
      "4635 Training Loss: tensor(0.3818)\n",
      "4636 Training Loss: tensor(0.3830)\n",
      "4637 Training Loss: tensor(0.3806)\n",
      "4638 Training Loss: tensor(0.3805)\n",
      "4639 Training Loss: tensor(0.3788)\n",
      "4640 Training Loss: tensor(0.3774)\n",
      "4641 Training Loss: tensor(0.3778)\n",
      "4642 Training Loss: tensor(0.3801)\n",
      "4643 Training Loss: tensor(0.3798)\n",
      "4644 Training Loss: tensor(0.3796)\n",
      "4645 Training Loss: tensor(0.3807)\n",
      "4646 Training Loss: tensor(0.3823)\n",
      "4647 Training Loss: tensor(0.3797)\n",
      "4648 Training Loss: tensor(0.3773)\n",
      "4649 Training Loss: tensor(0.3779)\n",
      "4650 Training Loss: tensor(0.3766)\n",
      "4651 Training Loss: tensor(0.3855)\n",
      "4652 Training Loss: tensor(0.3822)\n",
      "4653 Training Loss: tensor(0.3810)\n",
      "4654 Training Loss: tensor(0.3786)\n",
      "4655 Training Loss: tensor(0.3800)\n",
      "4656 Training Loss: tensor(0.3786)\n",
      "4657 Training Loss: tensor(0.3781)\n",
      "4658 Training Loss: tensor(0.3789)\n",
      "4659 Training Loss: tensor(0.3797)\n",
      "4660 Training Loss: tensor(0.3793)\n",
      "4661 Training Loss: tensor(0.3827)\n",
      "4662 Training Loss: tensor(0.3793)\n",
      "4663 Training Loss: tensor(0.3834)\n",
      "4664 Training Loss: tensor(0.3811)\n",
      "4665 Training Loss: tensor(0.3799)\n",
      "4666 Training Loss: tensor(0.3777)\n",
      "4667 Training Loss: tensor(0.3772)\n",
      "4668 Training Loss: tensor(0.3843)\n",
      "4669 Training Loss: tensor(0.3766)\n",
      "4670 Training Loss: tensor(0.3779)\n",
      "4671 Training Loss: tensor(0.3799)\n",
      "4672 Training Loss: tensor(0.3812)\n",
      "4673 Training Loss: tensor(0.3838)\n",
      "4674 Training Loss: tensor(0.3785)\n",
      "4675 Training Loss: tensor(0.3809)\n",
      "4676 Training Loss: tensor(0.3776)\n",
      "4677 Training Loss: tensor(0.3803)\n",
      "4678 Training Loss: tensor(0.3795)\n",
      "4679 Training Loss: tensor(0.3770)\n",
      "4680 Training Loss: tensor(0.3765)\n",
      "4681 Training Loss: tensor(0.3788)\n",
      "4682 Training Loss: tensor(0.3809)\n",
      "4683 Training Loss: tensor(0.3782)\n",
      "4684 Training Loss: tensor(0.3826)\n",
      "4685 Training Loss: tensor(0.3825)\n",
      "4686 Training Loss: tensor(0.3801)\n",
      "4687 Training Loss: tensor(0.3800)\n",
      "4688 Training Loss: tensor(0.3801)\n",
      "4689 Training Loss: tensor(0.3840)\n",
      "4690 Training Loss: tensor(0.3845)\n",
      "4691 Training Loss: tensor(0.3754)\n",
      "4692 Training Loss: tensor(0.3842)\n",
      "4693 Training Loss: tensor(0.3804)\n",
      "4694 Training Loss: tensor(0.3783)\n",
      "4695 Training Loss: tensor(0.3822)\n",
      "4696 Training Loss: tensor(0.3781)\n",
      "4697 Training Loss: tensor(0.3802)\n",
      "4698 Training Loss: tensor(0.3786)\n",
      "4699 Training Loss: tensor(0.3784)\n",
      "4700 Training Loss: tensor(0.3780)\n",
      "4701 Training Loss: tensor(0.3798)\n",
      "4702 Training Loss: tensor(0.3801)\n",
      "4703 Training Loss: tensor(0.3827)\n",
      "4704 Training Loss: tensor(0.3790)\n",
      "4705 Training Loss: tensor(0.3819)\n",
      "4706 Training Loss: tensor(0.3766)\n",
      "4707 Training Loss: tensor(0.3776)\n",
      "4708 Training Loss: tensor(0.3791)\n",
      "4709 Training Loss: tensor(0.3811)\n",
      "4710 Training Loss: tensor(0.3860)\n",
      "4711 Training Loss: tensor(0.3786)\n",
      "4712 Training Loss: tensor(0.3774)\n",
      "4713 Training Loss: tensor(0.3800)\n",
      "4714 Training Loss: tensor(0.3816)\n",
      "4715 Training Loss: tensor(0.3811)\n",
      "4716 Training Loss: tensor(0.3782)\n",
      "4717 Training Loss: tensor(0.3796)\n",
      "4718 Training Loss: tensor(0.3778)\n",
      "4719 Training Loss: tensor(0.3806)\n",
      "4720 Training Loss: tensor(0.3767)\n",
      "4721 Training Loss: tensor(0.3786)\n",
      "4722 Training Loss: tensor(0.3780)\n",
      "4723 Training Loss: tensor(0.3794)\n",
      "4724 Training Loss: tensor(0.3802)\n",
      "4725 Training Loss: tensor(0.3813)\n",
      "4726 Training Loss: tensor(0.3815)\n",
      "4727 Training Loss: tensor(0.3780)\n",
      "4728 Training Loss: tensor(0.3808)\n",
      "4729 Training Loss: tensor(0.3801)\n",
      "4730 Training Loss: tensor(0.3764)\n",
      "4731 Training Loss: tensor(0.3785)\n",
      "4732 Training Loss: tensor(0.3774)\n",
      "4733 Training Loss: tensor(0.3774)\n",
      "4734 Training Loss: tensor(0.3817)\n",
      "4735 Training Loss: tensor(0.3795)\n",
      "4736 Training Loss: tensor(0.3816)\n",
      "4737 Training Loss: tensor(0.3813)\n",
      "4738 Training Loss: tensor(0.3783)\n",
      "4739 Training Loss: tensor(0.3791)\n",
      "4740 Training Loss: tensor(0.3775)\n",
      "4741 Training Loss: tensor(0.3784)\n",
      "4742 Training Loss: tensor(0.3820)\n",
      "4743 Training Loss: tensor(0.3812)\n",
      "4744 Training Loss: tensor(0.3798)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4745 Training Loss: tensor(0.3791)\n",
      "4746 Training Loss: tensor(0.3795)\n",
      "4747 Training Loss: tensor(0.3819)\n",
      "4748 Training Loss: tensor(0.3823)\n",
      "4749 Training Loss: tensor(0.3787)\n",
      "4750 Training Loss: tensor(0.3814)\n",
      "4751 Training Loss: tensor(0.3782)\n",
      "4752 Training Loss: tensor(0.3797)\n",
      "4753 Training Loss: tensor(0.3821)\n",
      "4754 Training Loss: tensor(0.3807)\n",
      "4755 Training Loss: tensor(0.3782)\n",
      "4756 Training Loss: tensor(0.3855)\n",
      "4757 Training Loss: tensor(0.3806)\n",
      "4758 Training Loss: tensor(0.3785)\n",
      "4759 Training Loss: tensor(0.3765)\n",
      "4760 Training Loss: tensor(0.3807)\n",
      "4761 Training Loss: tensor(0.3789)\n",
      "4762 Training Loss: tensor(0.3779)\n",
      "4763 Training Loss: tensor(0.3776)\n",
      "4764 Training Loss: tensor(0.3804)\n",
      "4765 Training Loss: tensor(0.3800)\n",
      "4766 Training Loss: tensor(0.3788)\n",
      "4767 Training Loss: tensor(0.3792)\n",
      "4768 Training Loss: tensor(0.3785)\n",
      "4769 Training Loss: tensor(0.3785)\n",
      "4770 Training Loss: tensor(0.3779)\n",
      "4771 Training Loss: tensor(0.3770)\n",
      "4772 Training Loss: tensor(0.3851)\n",
      "4773 Training Loss: tensor(0.3771)\n",
      "4774 Training Loss: tensor(0.3788)\n",
      "4775 Training Loss: tensor(0.3782)\n",
      "4776 Training Loss: tensor(0.3801)\n",
      "4777 Training Loss: tensor(0.3782)\n",
      "4778 Training Loss: tensor(0.3770)\n",
      "4779 Training Loss: tensor(0.3800)\n",
      "4780 Training Loss: tensor(0.3790)\n",
      "4781 Training Loss: tensor(0.3779)\n",
      "4782 Training Loss: tensor(0.3804)\n",
      "4783 Training Loss: tensor(0.3764)\n",
      "4784 Training Loss: tensor(0.3779)\n",
      "4785 Training Loss: tensor(0.3831)\n",
      "4786 Training Loss: tensor(0.3785)\n",
      "4787 Training Loss: tensor(0.3764)\n",
      "4788 Training Loss: tensor(0.3779)\n",
      "4789 Training Loss: tensor(0.3793)\n",
      "4790 Training Loss: tensor(0.3753)\n",
      "4791 Training Loss: tensor(0.3815)\n",
      "4792 Training Loss: tensor(0.3777)\n",
      "4793 Training Loss: tensor(0.3857)\n",
      "4794 Training Loss: tensor(0.3811)\n",
      "4795 Training Loss: tensor(0.3806)\n",
      "4796 Training Loss: tensor(0.3764)\n",
      "4797 Training Loss: tensor(0.3802)\n",
      "4798 Training Loss: tensor(0.3792)\n",
      "4799 Training Loss: tensor(0.3812)\n",
      "4800 Training Loss: tensor(0.3797)\n",
      "4801 Training Loss: tensor(0.3786)\n",
      "4802 Training Loss: tensor(0.3794)\n",
      "4803 Training Loss: tensor(0.3848)\n",
      "4804 Training Loss: tensor(0.3768)\n",
      "4805 Training Loss: tensor(0.3791)\n",
      "4806 Training Loss: tensor(0.3813)\n",
      "4807 Training Loss: tensor(0.3806)\n",
      "4808 Training Loss: tensor(0.3828)\n",
      "4809 Training Loss: tensor(0.3794)\n",
      "4810 Training Loss: tensor(0.3779)\n",
      "4811 Training Loss: tensor(0.3787)\n",
      "4812 Training Loss: tensor(0.3765)\n",
      "4813 Training Loss: tensor(0.3800)\n",
      "4814 Training Loss: tensor(0.3773)\n",
      "4815 Training Loss: tensor(0.3776)\n",
      "4816 Training Loss: tensor(0.3802)\n",
      "4817 Training Loss: tensor(0.3767)\n",
      "4818 Training Loss: tensor(0.3773)\n",
      "4819 Training Loss: tensor(0.3794)\n",
      "4820 Training Loss: tensor(0.3811)\n",
      "4821 Training Loss: tensor(0.3771)\n",
      "4822 Training Loss: tensor(0.3773)\n",
      "4823 Training Loss: tensor(0.3763)\n",
      "4824 Training Loss: tensor(0.3800)\n",
      "4825 Training Loss: tensor(0.3793)\n",
      "4826 Training Loss: tensor(0.3781)\n",
      "4827 Training Loss: tensor(0.3774)\n",
      "4828 Training Loss: tensor(0.3760)\n",
      "4829 Training Loss: tensor(0.3805)\n",
      "4830 Training Loss: tensor(0.3792)\n",
      "4831 Training Loss: tensor(0.3825)\n",
      "4832 Training Loss: tensor(0.3789)\n",
      "4833 Training Loss: tensor(0.3794)\n",
      "4834 Training Loss: tensor(0.3821)\n",
      "4835 Training Loss: tensor(0.3790)\n",
      "4836 Training Loss: tensor(0.3763)\n",
      "4837 Training Loss: tensor(0.3801)\n",
      "4838 Training Loss: tensor(0.3857)\n",
      "4839 Training Loss: tensor(0.3794)\n",
      "4840 Training Loss: tensor(0.3826)\n",
      "4841 Training Loss: tensor(0.3791)\n",
      "4842 Training Loss: tensor(0.3775)\n",
      "4843 Training Loss: tensor(0.3761)\n",
      "4844 Training Loss: tensor(0.3804)\n",
      "4845 Training Loss: tensor(0.3814)\n",
      "4846 Training Loss: tensor(0.3766)\n",
      "4847 Training Loss: tensor(0.3790)\n",
      "4848 Training Loss: tensor(0.3794)\n",
      "4849 Training Loss: tensor(0.3780)\n",
      "4850 Training Loss: tensor(0.3797)\n",
      "4851 Training Loss: tensor(0.3780)\n",
      "4852 Training Loss: tensor(0.3788)\n",
      "4853 Training Loss: tensor(0.3765)\n",
      "4854 Training Loss: tensor(0.3782)\n",
      "4855 Training Loss: tensor(0.3832)\n",
      "4856 Training Loss: tensor(0.3814)\n",
      "4857 Training Loss: tensor(0.3752)\n",
      "4858 Training Loss: tensor(0.3785)\n",
      "4859 Training Loss: tensor(0.3768)\n",
      "4860 Training Loss: tensor(0.3827)\n",
      "4861 Training Loss: tensor(0.3788)\n",
      "4862 Training Loss: tensor(0.3781)\n",
      "4863 Training Loss: tensor(0.3792)\n",
      "4864 Training Loss: tensor(0.3775)\n",
      "4865 Training Loss: tensor(0.3801)\n",
      "4866 Training Loss: tensor(0.3777)\n",
      "4867 Training Loss: tensor(0.3805)\n",
      "4868 Training Loss: tensor(0.3808)\n",
      "4869 Training Loss: tensor(0.3856)\n",
      "4870 Training Loss: tensor(0.3775)\n",
      "4871 Training Loss: tensor(0.3799)\n",
      "4872 Training Loss: tensor(0.3763)\n",
      "4873 Training Loss: tensor(0.3757)\n",
      "4874 Training Loss: tensor(0.3762)\n",
      "4875 Training Loss: tensor(0.3764)\n",
      "4876 Training Loss: tensor(0.3810)\n",
      "4877 Training Loss: tensor(0.3801)\n",
      "4878 Training Loss: tensor(0.3817)\n",
      "4879 Training Loss: tensor(0.3776)\n",
      "4880 Training Loss: tensor(0.3785)\n",
      "4881 Training Loss: tensor(0.3816)\n",
      "4882 Training Loss: tensor(0.3782)\n",
      "4883 Training Loss: tensor(0.3808)\n",
      "4884 Training Loss: tensor(0.3793)\n",
      "4885 Training Loss: tensor(0.3843)\n",
      "4886 Training Loss: tensor(0.3860)\n",
      "4887 Training Loss: tensor(0.3796)\n",
      "4888 Training Loss: tensor(0.3793)\n",
      "4889 Training Loss: tensor(0.3789)\n",
      "4890 Training Loss: tensor(0.3822)\n",
      "4891 Training Loss: tensor(0.3787)\n",
      "4892 Training Loss: tensor(0.3794)\n",
      "4893 Training Loss: tensor(0.3807)\n",
      "4894 Training Loss: tensor(0.3816)\n",
      "4895 Training Loss: tensor(0.3802)\n",
      "4896 Training Loss: tensor(0.3776)\n",
      "4897 Training Loss: tensor(0.3772)\n",
      "4898 Training Loss: tensor(0.3816)\n",
      "4899 Training Loss: tensor(0.3802)\n",
      "4900 Training Loss: tensor(0.3846)\n",
      "4901 Training Loss: tensor(0.3783)\n",
      "4902 Training Loss: tensor(0.3794)\n",
      "4903 Training Loss: tensor(0.3800)\n",
      "4904 Training Loss: tensor(0.3776)\n",
      "4905 Training Loss: tensor(0.3770)\n",
      "4906 Training Loss: tensor(0.3757)\n",
      "4907 Training Loss: tensor(0.3808)\n",
      "4908 Training Loss: tensor(0.3788)\n",
      "4909 Training Loss: tensor(0.3757)\n",
      "4910 Training Loss: tensor(0.3766)\n",
      "4911 Training Loss: tensor(0.3803)\n",
      "4912 Training Loss: tensor(0.3800)\n",
      "4913 Training Loss: tensor(0.3762)\n",
      "4914 Training Loss: tensor(0.3799)\n",
      "4915 Training Loss: tensor(0.3815)\n",
      "4916 Training Loss: tensor(0.3743)\n",
      "4917 Training Loss: tensor(0.3787)\n",
      "4918 Training Loss: tensor(0.3800)\n",
      "4919 Training Loss: tensor(0.3765)\n",
      "4920 Training Loss: tensor(0.3843)\n",
      "4921 Training Loss: tensor(0.3764)\n",
      "4922 Training Loss: tensor(0.3776)\n",
      "4923 Training Loss: tensor(0.3809)\n",
      "4924 Training Loss: tensor(0.3782)\n",
      "4925 Training Loss: tensor(0.3770)\n",
      "4926 Training Loss: tensor(0.3783)\n",
      "4927 Training Loss: tensor(0.3799)\n",
      "4928 Training Loss: tensor(0.3799)\n",
      "4929 Training Loss: tensor(0.3791)\n",
      "4930 Training Loss: tensor(0.3806)\n",
      "4931 Training Loss: tensor(0.3808)\n",
      "4932 Training Loss: tensor(0.3767)\n",
      "4933 Training Loss: tensor(0.3805)\n",
      "4934 Training Loss: tensor(0.3780)\n",
      "4935 Training Loss: tensor(0.3766)\n",
      "4936 Training Loss: tensor(0.3763)\n",
      "4937 Training Loss: tensor(0.3756)\n",
      "4938 Training Loss: tensor(0.3805)\n",
      "4939 Training Loss: tensor(0.3783)\n",
      "4940 Training Loss: tensor(0.3800)\n",
      "4941 Training Loss: tensor(0.3776)\n",
      "4942 Training Loss: tensor(0.3754)\n",
      "4943 Training Loss: tensor(0.3781)\n",
      "4944 Training Loss: tensor(0.3795)\n",
      "4945 Training Loss: tensor(0.3830)\n",
      "4946 Training Loss: tensor(0.3805)\n",
      "4947 Training Loss: tensor(0.3776)\n",
      "4948 Training Loss: tensor(0.3834)\n",
      "4949 Training Loss: tensor(0.3801)\n",
      "4950 Training Loss: tensor(0.3765)\n",
      "4951 Training Loss: tensor(0.3776)\n",
      "4952 Training Loss: tensor(0.3803)\n",
      "4953 Training Loss: tensor(0.3785)\n",
      "4954 Training Loss: tensor(0.3781)\n",
      "4955 Training Loss: tensor(0.3823)\n",
      "4956 Training Loss: tensor(0.3795)\n",
      "4957 Training Loss: tensor(0.3782)\n",
      "4958 Training Loss: tensor(0.3789)\n",
      "4959 Training Loss: tensor(0.3803)\n",
      "4960 Training Loss: tensor(0.3800)\n",
      "4961 Training Loss: tensor(0.3780)\n",
      "4962 Training Loss: tensor(0.3800)\n",
      "4963 Training Loss: tensor(0.3772)\n",
      "4964 Training Loss: tensor(0.3762)\n",
      "4965 Training Loss: tensor(0.3789)\n",
      "4966 Training Loss: tensor(0.3786)\n",
      "4967 Training Loss: tensor(0.3831)\n",
      "4968 Training Loss: tensor(0.3763)\n",
      "4969 Training Loss: tensor(0.3807)\n",
      "4970 Training Loss: tensor(0.3809)\n",
      "4971 Training Loss: tensor(0.3755)\n",
      "4972 Training Loss: tensor(0.3815)\n",
      "4973 Training Loss: tensor(0.3758)\n",
      "4974 Training Loss: tensor(0.3823)\n",
      "4975 Training Loss: tensor(0.3803)\n",
      "4976 Training Loss: tensor(0.3797)\n",
      "4977 Training Loss: tensor(0.3785)\n",
      "4978 Training Loss: tensor(0.3788)\n",
      "4979 Training Loss: tensor(0.3798)\n",
      "4980 Training Loss: tensor(0.3859)\n",
      "4981 Training Loss: tensor(0.3779)\n",
      "4982 Training Loss: tensor(0.3809)\n",
      "4983 Training Loss: tensor(0.3782)\n",
      "4984 Training Loss: tensor(0.3819)\n",
      "4985 Training Loss: tensor(0.3759)\n",
      "4986 Training Loss: tensor(0.3795)\n",
      "4987 Training Loss: tensor(0.3858)\n",
      "4988 Training Loss: tensor(0.3792)\n",
      "4989 Training Loss: tensor(0.3795)\n",
      "4990 Training Loss: tensor(0.3781)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4991 Training Loss: tensor(0.3771)\n",
      "4992 Training Loss: tensor(0.3785)\n",
      "4993 Training Loss: tensor(0.3781)\n",
      "4994 Training Loss: tensor(0.3798)\n",
      "4995 Training Loss: tensor(0.3825)\n",
      "4996 Training Loss: tensor(0.3811)\n",
      "4997 Training Loss: tensor(0.3771)\n",
      "4998 Training Loss: tensor(0.3800)\n",
      "4999 Training Loss: tensor(0.3786)\n",
      "5000 Training Loss: tensor(0.3768)\n",
      "5001 Training Loss: tensor(0.3803)\n",
      "5002 Training Loss: tensor(0.3786)\n",
      "5003 Training Loss: tensor(0.3776)\n",
      "5004 Training Loss: tensor(0.3785)\n",
      "5005 Training Loss: tensor(0.3798)\n",
      "5006 Training Loss: tensor(0.3769)\n",
      "5007 Training Loss: tensor(0.3788)\n",
      "5008 Training Loss: tensor(0.3791)\n",
      "5009 Training Loss: tensor(0.3793)\n",
      "5010 Training Loss: tensor(0.3763)\n",
      "5011 Training Loss: tensor(0.3784)\n",
      "5012 Training Loss: tensor(0.3803)\n",
      "5013 Training Loss: tensor(0.3814)\n",
      "5014 Training Loss: tensor(0.3760)\n",
      "5015 Training Loss: tensor(0.3799)\n",
      "5016 Training Loss: tensor(0.3772)\n",
      "5017 Training Loss: tensor(0.3773)\n",
      "5018 Training Loss: tensor(0.3770)\n",
      "5019 Training Loss: tensor(0.3814)\n",
      "5020 Training Loss: tensor(0.3807)\n",
      "5021 Training Loss: tensor(0.3766)\n",
      "5022 Training Loss: tensor(0.3805)\n",
      "5023 Training Loss: tensor(0.3783)\n",
      "5024 Training Loss: tensor(0.3808)\n",
      "5025 Training Loss: tensor(0.3804)\n",
      "5026 Training Loss: tensor(0.3796)\n",
      "5027 Training Loss: tensor(0.3779)\n",
      "5028 Training Loss: tensor(0.3755)\n",
      "5029 Training Loss: tensor(0.3796)\n",
      "5030 Training Loss: tensor(0.3831)\n",
      "5031 Training Loss: tensor(0.3773)\n",
      "5032 Training Loss: tensor(0.3802)\n",
      "5033 Training Loss: tensor(0.3790)\n",
      "5034 Training Loss: tensor(0.3801)\n",
      "5035 Training Loss: tensor(0.3767)\n",
      "5036 Training Loss: tensor(0.3768)\n",
      "5037 Training Loss: tensor(0.3800)\n",
      "5038 Training Loss: tensor(0.3767)\n",
      "5039 Training Loss: tensor(0.3791)\n",
      "5040 Training Loss: tensor(0.3743)\n",
      "5041 Training Loss: tensor(0.3820)\n",
      "5042 Training Loss: tensor(0.3809)\n",
      "5043 Training Loss: tensor(0.3833)\n",
      "5044 Training Loss: tensor(0.3833)\n",
      "5045 Training Loss: tensor(0.3762)\n",
      "5046 Training Loss: tensor(0.3783)\n",
      "5047 Training Loss: tensor(0.3804)\n",
      "5048 Training Loss: tensor(0.3791)\n",
      "5049 Training Loss: tensor(0.3788)\n",
      "5050 Training Loss: tensor(0.3781)\n",
      "5051 Training Loss: tensor(0.3792)\n",
      "5052 Training Loss: tensor(0.3771)\n",
      "5053 Training Loss: tensor(0.3835)\n",
      "5054 Training Loss: tensor(0.3781)\n",
      "5055 Training Loss: tensor(0.3776)\n",
      "5056 Training Loss: tensor(0.3765)\n",
      "5057 Training Loss: tensor(0.3803)\n",
      "5058 Training Loss: tensor(0.3777)\n",
      "5059 Training Loss: tensor(0.3753)\n",
      "5060 Training Loss: tensor(0.3774)\n",
      "5061 Training Loss: tensor(0.3815)\n",
      "5062 Training Loss: tensor(0.3810)\n",
      "5063 Training Loss: tensor(0.3811)\n",
      "5064 Training Loss: tensor(0.3787)\n",
      "5065 Training Loss: tensor(0.3766)\n",
      "5066 Training Loss: tensor(0.3791)\n",
      "5067 Training Loss: tensor(0.3788)\n",
      "5068 Training Loss: tensor(0.3823)\n",
      "5069 Training Loss: tensor(0.3783)\n",
      "5070 Training Loss: tensor(0.3802)\n",
      "5071 Training Loss: tensor(0.3764)\n",
      "5072 Training Loss: tensor(0.3780)\n",
      "5073 Training Loss: tensor(0.3796)\n",
      "5074 Training Loss: tensor(0.3839)\n",
      "5075 Training Loss: tensor(0.3763)\n",
      "5076 Training Loss: tensor(0.3815)\n",
      "5077 Training Loss: tensor(0.3753)\n",
      "5078 Training Loss: tensor(0.3815)\n",
      "5079 Training Loss: tensor(0.3837)\n",
      "5080 Training Loss: tensor(0.3772)\n",
      "5081 Training Loss: tensor(0.3793)\n",
      "5082 Training Loss: tensor(0.3759)\n",
      "5083 Training Loss: tensor(0.3772)\n",
      "5084 Training Loss: tensor(0.3759)\n",
      "5085 Training Loss: tensor(0.3777)\n",
      "5086 Training Loss: tensor(0.3759)\n",
      "5087 Training Loss: tensor(0.3817)\n",
      "5088 Training Loss: tensor(0.3792)\n",
      "5089 Training Loss: tensor(0.3785)\n",
      "5090 Training Loss: tensor(0.3795)\n",
      "5091 Training Loss: tensor(0.3783)\n",
      "5092 Training Loss: tensor(0.3765)\n",
      "5093 Training Loss: tensor(0.3799)\n",
      "5094 Training Loss: tensor(0.3785)\n",
      "5095 Training Loss: tensor(0.3783)\n",
      "5096 Training Loss: tensor(0.3798)\n",
      "5097 Training Loss: tensor(0.3743)\n",
      "5098 Training Loss: tensor(0.3813)\n",
      "5099 Training Loss: tensor(0.3755)\n",
      "5100 Training Loss: tensor(0.3765)\n",
      "5101 Training Loss: tensor(0.3774)\n",
      "5102 Training Loss: tensor(0.3755)\n",
      "5103 Training Loss: tensor(0.3781)\n",
      "5104 Training Loss: tensor(0.3751)\n",
      "5105 Training Loss: tensor(0.3777)\n",
      "5106 Training Loss: tensor(0.3759)\n",
      "5107 Training Loss: tensor(0.3798)\n",
      "5108 Training Loss: tensor(0.3768)\n",
      "5109 Training Loss: tensor(0.3813)\n",
      "5110 Training Loss: tensor(0.3810)\n",
      "5111 Training Loss: tensor(0.3778)\n",
      "5112 Training Loss: tensor(0.3787)\n",
      "5113 Training Loss: tensor(0.3791)\n",
      "5114 Training Loss: tensor(0.3800)\n",
      "5115 Training Loss: tensor(0.3767)\n",
      "5116 Training Loss: tensor(0.3786)\n",
      "5117 Training Loss: tensor(0.3789)\n",
      "5118 Training Loss: tensor(0.3765)\n",
      "5119 Training Loss: tensor(0.3823)\n",
      "5120 Training Loss: tensor(0.3773)\n",
      "5121 Training Loss: tensor(0.3799)\n",
      "5122 Training Loss: tensor(0.3808)\n",
      "5123 Training Loss: tensor(0.3750)\n",
      "5124 Training Loss: tensor(0.3844)\n",
      "5125 Training Loss: tensor(0.3779)\n",
      "5126 Training Loss: tensor(0.3765)\n",
      "5127 Training Loss: tensor(0.3782)\n",
      "5128 Training Loss: tensor(0.3801)\n",
      "5129 Training Loss: tensor(0.3802)\n",
      "5130 Training Loss: tensor(0.3775)\n",
      "5131 Training Loss: tensor(0.3787)\n",
      "5132 Training Loss: tensor(0.3788)\n",
      "5133 Training Loss: tensor(0.3793)\n",
      "5134 Training Loss: tensor(0.3799)\n",
      "5135 Training Loss: tensor(0.3800)\n",
      "5136 Training Loss: tensor(0.3785)\n",
      "5137 Training Loss: tensor(0.3793)\n",
      "5138 Training Loss: tensor(0.3784)\n",
      "5139 Training Loss: tensor(0.3782)\n",
      "5140 Training Loss: tensor(0.3769)\n",
      "5141 Training Loss: tensor(0.3783)\n",
      "5142 Training Loss: tensor(0.3792)\n",
      "5143 Training Loss: tensor(0.3756)\n",
      "5144 Training Loss: tensor(0.3772)\n",
      "5145 Training Loss: tensor(0.3769)\n",
      "5146 Training Loss: tensor(0.3784)\n",
      "5147 Training Loss: tensor(0.3845)\n",
      "5148 Training Loss: tensor(0.3822)\n",
      "5149 Training Loss: tensor(0.3798)\n",
      "5150 Training Loss: tensor(0.3800)\n",
      "5151 Training Loss: tensor(0.3832)\n",
      "5152 Training Loss: tensor(0.3812)\n",
      "5153 Training Loss: tensor(0.3779)\n",
      "5154 Training Loss: tensor(0.3798)\n",
      "5155 Training Loss: tensor(0.3792)\n",
      "5156 Training Loss: tensor(0.3787)\n",
      "5157 Training Loss: tensor(0.3764)\n",
      "5158 Training Loss: tensor(0.3796)\n",
      "5159 Training Loss: tensor(0.3814)\n",
      "5160 Training Loss: tensor(0.3763)\n",
      "5161 Training Loss: tensor(0.3798)\n",
      "5162 Training Loss: tensor(0.3764)\n",
      "5163 Training Loss: tensor(0.3769)\n",
      "5164 Training Loss: tensor(0.3813)\n",
      "5165 Training Loss: tensor(0.3766)\n",
      "5166 Training Loss: tensor(0.3788)\n",
      "5167 Training Loss: tensor(0.3779)\n",
      "5168 Training Loss: tensor(0.3789)\n",
      "5169 Training Loss: tensor(0.3798)\n",
      "5170 Training Loss: tensor(0.3806)\n",
      "5171 Training Loss: tensor(0.3751)\n",
      "5172 Training Loss: tensor(0.3779)\n",
      "5173 Training Loss: tensor(0.3772)\n",
      "5174 Training Loss: tensor(0.3773)\n",
      "5175 Training Loss: tensor(0.3839)\n",
      "5176 Training Loss: tensor(0.3762)\n",
      "5177 Training Loss: tensor(0.3821)\n",
      "5178 Training Loss: tensor(0.3786)\n",
      "5179 Training Loss: tensor(0.3840)\n",
      "5180 Training Loss: tensor(0.3776)\n",
      "5181 Training Loss: tensor(0.3786)\n",
      "5182 Training Loss: tensor(0.3797)\n",
      "5183 Training Loss: tensor(0.3816)\n",
      "5184 Training Loss: tensor(0.3794)\n",
      "5185 Training Loss: tensor(0.3844)\n",
      "5186 Training Loss: tensor(0.3749)\n",
      "5187 Training Loss: tensor(0.3772)\n",
      "5188 Training Loss: tensor(0.3766)\n",
      "5189 Training Loss: tensor(0.3784)\n",
      "5190 Training Loss: tensor(0.3741)\n",
      "5191 Training Loss: tensor(0.3769)\n",
      "5192 Training Loss: tensor(0.3764)\n",
      "5193 Training Loss: tensor(0.3789)\n",
      "5194 Training Loss: tensor(0.3798)\n",
      "5195 Training Loss: tensor(0.3758)\n",
      "5196 Training Loss: tensor(0.3816)\n",
      "5197 Training Loss: tensor(0.3755)\n",
      "5198 Training Loss: tensor(0.3773)\n",
      "5199 Training Loss: tensor(0.3773)\n",
      "5200 Training Loss: tensor(0.3780)\n",
      "5201 Training Loss: tensor(0.3769)\n",
      "5202 Training Loss: tensor(0.3799)\n",
      "5203 Training Loss: tensor(0.3851)\n",
      "5204 Training Loss: tensor(0.3800)\n",
      "5205 Training Loss: tensor(0.3827)\n",
      "5206 Training Loss: tensor(0.3830)\n",
      "5207 Training Loss: tensor(0.3765)\n",
      "5208 Training Loss: tensor(0.3778)\n",
      "5209 Training Loss: tensor(0.3788)\n",
      "5210 Training Loss: tensor(0.3789)\n",
      "5211 Training Loss: tensor(0.3775)\n",
      "5212 Training Loss: tensor(0.3782)\n",
      "5213 Training Loss: tensor(0.3772)\n",
      "5214 Training Loss: tensor(0.3796)\n",
      "5215 Training Loss: tensor(0.3770)\n",
      "5216 Training Loss: tensor(0.3805)\n",
      "5217 Training Loss: tensor(0.3804)\n",
      "5218 Training Loss: tensor(0.3762)\n",
      "5219 Training Loss: tensor(0.3822)\n",
      "5220 Training Loss: tensor(0.3755)\n",
      "5221 Training Loss: tensor(0.3806)\n",
      "5222 Training Loss: tensor(0.3834)\n",
      "5223 Training Loss: tensor(0.3788)\n",
      "5224 Training Loss: tensor(0.3765)\n",
      "5225 Training Loss: tensor(0.3795)\n",
      "5226 Training Loss: tensor(0.3791)\n",
      "5227 Training Loss: tensor(0.3762)\n",
      "5228 Training Loss: tensor(0.3768)\n",
      "5229 Training Loss: tensor(0.3750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5230 Training Loss: tensor(0.3818)\n",
      "5231 Training Loss: tensor(0.3749)\n",
      "5232 Training Loss: tensor(0.3781)\n",
      "5233 Training Loss: tensor(0.3746)\n",
      "5234 Training Loss: tensor(0.3794)\n",
      "5235 Training Loss: tensor(0.3767)\n",
      "5236 Training Loss: tensor(0.3844)\n",
      "5237 Training Loss: tensor(0.3812)\n",
      "5238 Training Loss: tensor(0.3763)\n",
      "5239 Training Loss: tensor(0.3784)\n",
      "5240 Training Loss: tensor(0.3742)\n",
      "5241 Training Loss: tensor(0.3768)\n",
      "5242 Training Loss: tensor(0.3794)\n",
      "5243 Training Loss: tensor(0.3761)\n",
      "5244 Training Loss: tensor(0.3763)\n",
      "5245 Training Loss: tensor(0.3763)\n",
      "5246 Training Loss: tensor(0.3743)\n",
      "5247 Training Loss: tensor(0.3785)\n",
      "5248 Training Loss: tensor(0.3781)\n",
      "5249 Training Loss: tensor(0.3784)\n",
      "5250 Training Loss: tensor(0.3772)\n",
      "5251 Training Loss: tensor(0.3787)\n",
      "5252 Training Loss: tensor(0.3774)\n",
      "5253 Training Loss: tensor(0.3766)\n",
      "5254 Training Loss: tensor(0.3815)\n",
      "5255 Training Loss: tensor(0.3816)\n",
      "5256 Training Loss: tensor(0.3777)\n",
      "5257 Training Loss: tensor(0.3789)\n",
      "5258 Training Loss: tensor(0.3789)\n",
      "5259 Training Loss: tensor(0.3802)\n",
      "5260 Training Loss: tensor(0.3828)\n",
      "5261 Training Loss: tensor(0.3798)\n",
      "5262 Training Loss: tensor(0.3786)\n",
      "5263 Training Loss: tensor(0.3814)\n",
      "5264 Training Loss: tensor(0.3778)\n",
      "5265 Training Loss: tensor(0.3792)\n",
      "5266 Training Loss: tensor(0.3811)\n",
      "5267 Training Loss: tensor(0.3792)\n",
      "5268 Training Loss: tensor(0.3813)\n",
      "5269 Training Loss: tensor(0.3775)\n",
      "5270 Training Loss: tensor(0.3782)\n",
      "5271 Training Loss: tensor(0.3784)\n",
      "5272 Training Loss: tensor(0.3819)\n",
      "5273 Training Loss: tensor(0.3832)\n",
      "5274 Training Loss: tensor(0.3788)\n",
      "5275 Training Loss: tensor(0.3773)\n",
      "5276 Training Loss: tensor(0.3792)\n",
      "5277 Training Loss: tensor(0.3784)\n",
      "5278 Training Loss: tensor(0.3781)\n",
      "5279 Training Loss: tensor(0.3777)\n",
      "5280 Training Loss: tensor(0.3786)\n",
      "5281 Training Loss: tensor(0.3773)\n",
      "5282 Training Loss: tensor(0.3778)\n",
      "5283 Training Loss: tensor(0.3806)\n",
      "5284 Training Loss: tensor(0.3798)\n",
      "5285 Training Loss: tensor(0.3835)\n",
      "5286 Training Loss: tensor(0.3799)\n",
      "5287 Training Loss: tensor(0.3765)\n",
      "5288 Training Loss: tensor(0.3776)\n",
      "5289 Training Loss: tensor(0.3753)\n",
      "5290 Training Loss: tensor(0.3836)\n",
      "5291 Training Loss: tensor(0.3782)\n",
      "5292 Training Loss: tensor(0.3773)\n",
      "5293 Training Loss: tensor(0.3799)\n",
      "5294 Training Loss: tensor(0.3751)\n",
      "5295 Training Loss: tensor(0.3771)\n",
      "5296 Training Loss: tensor(0.3789)\n",
      "5297 Training Loss: tensor(0.3783)\n",
      "5298 Training Loss: tensor(0.3764)\n",
      "5299 Training Loss: tensor(0.3792)\n",
      "5300 Training Loss: tensor(0.3765)\n",
      "5301 Training Loss: tensor(0.3769)\n",
      "5302 Training Loss: tensor(0.3776)\n",
      "5303 Training Loss: tensor(0.3788)\n",
      "5304 Training Loss: tensor(0.3803)\n",
      "5305 Training Loss: tensor(0.3757)\n",
      "5306 Training Loss: tensor(0.3746)\n",
      "5307 Training Loss: tensor(0.3774)\n",
      "5308 Training Loss: tensor(0.3837)\n",
      "5309 Training Loss: tensor(0.3768)\n",
      "5310 Training Loss: tensor(0.3766)\n",
      "5311 Training Loss: tensor(0.3791)\n",
      "5312 Training Loss: tensor(0.3750)\n",
      "5313 Training Loss: tensor(0.3901)\n",
      "5314 Training Loss: tensor(0.3806)\n",
      "5315 Training Loss: tensor(0.3811)\n",
      "5316 Training Loss: tensor(0.3779)\n",
      "5317 Training Loss: tensor(0.3779)\n",
      "5318 Training Loss: tensor(0.3797)\n",
      "5319 Training Loss: tensor(0.3778)\n",
      "5320 Training Loss: tensor(0.3803)\n",
      "5321 Training Loss: tensor(0.3789)\n",
      "5322 Training Loss: tensor(0.3820)\n",
      "5323 Training Loss: tensor(0.3770)\n",
      "5324 Training Loss: tensor(0.3801)\n",
      "5325 Training Loss: tensor(0.3770)\n",
      "5326 Training Loss: tensor(0.3776)\n",
      "5327 Training Loss: tensor(0.3833)\n",
      "5328 Training Loss: tensor(0.3807)\n",
      "5329 Training Loss: tensor(0.3773)\n",
      "5330 Training Loss: tensor(0.3780)\n",
      "5331 Training Loss: tensor(0.3815)\n",
      "5332 Training Loss: tensor(0.3772)\n",
      "5333 Training Loss: tensor(0.3762)\n",
      "5334 Training Loss: tensor(0.3748)\n",
      "5335 Training Loss: tensor(0.3828)\n",
      "5336 Training Loss: tensor(0.3763)\n",
      "5337 Training Loss: tensor(0.3779)\n",
      "5338 Training Loss: tensor(0.3728)\n",
      "5339 Training Loss: tensor(0.3775)\n",
      "5340 Training Loss: tensor(0.3836)\n",
      "5341 Training Loss: tensor(0.3825)\n",
      "5342 Training Loss: tensor(0.3813)\n",
      "5343 Training Loss: tensor(0.3786)\n",
      "5344 Training Loss: tensor(0.3763)\n",
      "5345 Training Loss: tensor(0.3742)\n",
      "5346 Training Loss: tensor(0.3769)\n",
      "5347 Training Loss: tensor(0.3766)\n",
      "5348 Training Loss: tensor(0.3773)\n",
      "5349 Training Loss: tensor(0.3751)\n",
      "5350 Training Loss: tensor(0.3733)\n",
      "5351 Training Loss: tensor(0.3800)\n",
      "5352 Training Loss: tensor(0.3760)\n",
      "5353 Training Loss: tensor(0.3804)\n",
      "5354 Training Loss: tensor(0.3791)\n",
      "5355 Training Loss: tensor(0.3814)\n",
      "5356 Training Loss: tensor(0.3847)\n",
      "5357 Training Loss: tensor(0.3826)\n",
      "5358 Training Loss: tensor(0.3802)\n",
      "5359 Training Loss: tensor(0.3763)\n",
      "5360 Training Loss: tensor(0.3856)\n",
      "5361 Training Loss: tensor(0.3818)\n",
      "5362 Training Loss: tensor(0.3794)\n",
      "5363 Training Loss: tensor(0.3749)\n",
      "5364 Training Loss: tensor(0.3758)\n",
      "5365 Training Loss: tensor(0.3793)\n",
      "5366 Training Loss: tensor(0.3775)\n",
      "5367 Training Loss: tensor(0.3826)\n",
      "5368 Training Loss: tensor(0.3788)\n",
      "5369 Training Loss: tensor(0.3796)\n",
      "5370 Training Loss: tensor(0.3784)\n",
      "5371 Training Loss: tensor(0.3827)\n",
      "5372 Training Loss: tensor(0.3780)\n",
      "5373 Training Loss: tensor(0.3785)\n",
      "5374 Training Loss: tensor(0.3792)\n",
      "5375 Training Loss: tensor(0.3796)\n",
      "5376 Training Loss: tensor(0.3757)\n",
      "5377 Training Loss: tensor(0.3765)\n",
      "5378 Training Loss: tensor(0.3791)\n",
      "5379 Training Loss: tensor(0.3767)\n",
      "5380 Training Loss: tensor(0.3771)\n",
      "5381 Training Loss: tensor(0.3765)\n",
      "5382 Training Loss: tensor(0.3753)\n",
      "5383 Training Loss: tensor(0.3772)\n",
      "5384 Training Loss: tensor(0.3774)\n",
      "5385 Training Loss: tensor(0.3765)\n",
      "5386 Training Loss: tensor(0.3765)\n",
      "5387 Training Loss: tensor(0.3829)\n",
      "5388 Training Loss: tensor(0.3784)\n",
      "5389 Training Loss: tensor(0.3730)\n",
      "5390 Training Loss: tensor(0.3752)\n",
      "5391 Training Loss: tensor(0.3815)\n",
      "5392 Training Loss: tensor(0.3783)\n",
      "5393 Training Loss: tensor(0.3829)\n",
      "5394 Training Loss: tensor(0.3742)\n",
      "5395 Training Loss: tensor(0.3812)\n",
      "5396 Training Loss: tensor(0.3740)\n",
      "5397 Training Loss: tensor(0.3775)\n",
      "5398 Training Loss: tensor(0.3775)\n",
      "5399 Training Loss: tensor(0.3774)\n",
      "5400 Training Loss: tensor(0.3774)\n",
      "5401 Training Loss: tensor(0.3760)\n",
      "5402 Training Loss: tensor(0.3820)\n",
      "5403 Training Loss: tensor(0.3748)\n",
      "5404 Training Loss: tensor(0.3760)\n",
      "5405 Training Loss: tensor(0.3775)\n",
      "5406 Training Loss: tensor(0.3776)\n",
      "5407 Training Loss: tensor(0.3801)\n",
      "5408 Training Loss: tensor(0.3831)\n",
      "5409 Training Loss: tensor(0.3770)\n",
      "5410 Training Loss: tensor(0.3798)\n",
      "5411 Training Loss: tensor(0.3832)\n",
      "5412 Training Loss: tensor(0.3765)\n",
      "5413 Training Loss: tensor(0.3733)\n",
      "5414 Training Loss: tensor(0.3823)\n",
      "5415 Training Loss: tensor(0.3788)\n",
      "5416 Training Loss: tensor(0.3784)\n",
      "5417 Training Loss: tensor(0.3782)\n",
      "5418 Training Loss: tensor(0.3746)\n",
      "5419 Training Loss: tensor(0.3782)\n",
      "5420 Training Loss: tensor(0.3758)\n",
      "5421 Training Loss: tensor(0.3808)\n",
      "5422 Training Loss: tensor(0.3797)\n",
      "5423 Training Loss: tensor(0.3782)\n",
      "5424 Training Loss: tensor(0.3805)\n",
      "5425 Training Loss: tensor(0.3753)\n",
      "5426 Training Loss: tensor(0.3810)\n",
      "5427 Training Loss: tensor(0.3773)\n",
      "5428 Training Loss: tensor(0.3750)\n",
      "5429 Training Loss: tensor(0.3814)\n",
      "5430 Training Loss: tensor(0.3794)\n",
      "5431 Training Loss: tensor(0.3771)\n",
      "5432 Training Loss: tensor(0.3768)\n",
      "5433 Training Loss: tensor(0.3796)\n",
      "5434 Training Loss: tensor(0.3769)\n",
      "5435 Training Loss: tensor(0.3812)\n",
      "5436 Training Loss: tensor(0.3781)\n",
      "5437 Training Loss: tensor(0.3765)\n",
      "5438 Training Loss: tensor(0.3807)\n",
      "5439 Training Loss: tensor(0.3752)\n",
      "5440 Training Loss: tensor(0.3770)\n",
      "5441 Training Loss: tensor(0.3773)\n",
      "5442 Training Loss: tensor(0.3792)\n",
      "5443 Training Loss: tensor(0.3807)\n",
      "5444 Training Loss: tensor(0.3772)\n",
      "5445 Training Loss: tensor(0.3793)\n",
      "5446 Training Loss: tensor(0.3767)\n",
      "5447 Training Loss: tensor(0.3740)\n",
      "5448 Training Loss: tensor(0.3797)\n",
      "5449 Training Loss: tensor(0.3768)\n",
      "5450 Training Loss: tensor(0.3775)\n",
      "5451 Training Loss: tensor(0.3797)\n",
      "5452 Training Loss: tensor(0.3785)\n",
      "5453 Training Loss: tensor(0.3803)\n",
      "5454 Training Loss: tensor(0.3770)\n",
      "5455 Training Loss: tensor(0.3745)\n",
      "5456 Training Loss: tensor(0.3759)\n",
      "5457 Training Loss: tensor(0.3743)\n",
      "5458 Training Loss: tensor(0.3791)\n",
      "5459 Training Loss: tensor(0.3780)\n",
      "5460 Training Loss: tensor(0.3762)\n",
      "5461 Training Loss: tensor(0.3758)\n",
      "5462 Training Loss: tensor(0.3775)\n",
      "5463 Training Loss: tensor(0.3804)\n",
      "5464 Training Loss: tensor(0.3822)\n",
      "5465 Training Loss: tensor(0.3773)\n",
      "5466 Training Loss: tensor(0.3835)\n",
      "5467 Training Loss: tensor(0.3852)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468 Training Loss: tensor(0.3762)\n",
      "5469 Training Loss: tensor(0.3759)\n",
      "5470 Training Loss: tensor(0.3764)\n",
      "5471 Training Loss: tensor(0.3781)\n",
      "5472 Training Loss: tensor(0.3795)\n",
      "5473 Training Loss: tensor(0.3772)\n",
      "5474 Training Loss: tensor(0.3764)\n",
      "5475 Training Loss: tensor(0.3774)\n",
      "5476 Training Loss: tensor(0.3787)\n",
      "5477 Training Loss: tensor(0.3802)\n",
      "5478 Training Loss: tensor(0.3789)\n",
      "5479 Training Loss: tensor(0.3802)\n",
      "5480 Training Loss: tensor(0.3784)\n",
      "5481 Training Loss: tensor(0.3775)\n",
      "5482 Training Loss: tensor(0.3814)\n",
      "5483 Training Loss: tensor(0.3783)\n",
      "5484 Training Loss: tensor(0.3837)\n",
      "5485 Training Loss: tensor(0.3765)\n",
      "5486 Training Loss: tensor(0.3770)\n",
      "5487 Training Loss: tensor(0.3760)\n",
      "5488 Training Loss: tensor(0.3787)\n",
      "5489 Training Loss: tensor(0.3783)\n",
      "5490 Training Loss: tensor(0.3771)\n",
      "5491 Training Loss: tensor(0.3796)\n",
      "5492 Training Loss: tensor(0.3774)\n",
      "5493 Training Loss: tensor(0.3772)\n",
      "5494 Training Loss: tensor(0.3815)\n",
      "5495 Training Loss: tensor(0.3774)\n",
      "5496 Training Loss: tensor(0.3783)\n",
      "5497 Training Loss: tensor(0.3819)\n",
      "5498 Training Loss: tensor(0.3789)\n",
      "5499 Training Loss: tensor(0.3778)\n",
      "5500 Training Loss: tensor(0.3807)\n",
      "5501 Training Loss: tensor(0.3806)\n",
      "5502 Training Loss: tensor(0.3768)\n",
      "5503 Training Loss: tensor(0.3778)\n",
      "5504 Training Loss: tensor(0.3794)\n",
      "5505 Training Loss: tensor(0.3797)\n",
      "5506 Training Loss: tensor(0.3757)\n",
      "5507 Training Loss: tensor(0.3756)\n",
      "5508 Training Loss: tensor(0.3768)\n",
      "5509 Training Loss: tensor(0.3801)\n",
      "5510 Training Loss: tensor(0.3758)\n",
      "5511 Training Loss: tensor(0.3755)\n",
      "5512 Training Loss: tensor(0.3781)\n",
      "5513 Training Loss: tensor(0.3805)\n",
      "5514 Training Loss: tensor(0.3757)\n",
      "5515 Training Loss: tensor(0.3792)\n",
      "5516 Training Loss: tensor(0.3800)\n",
      "5517 Training Loss: tensor(0.3776)\n",
      "5518 Training Loss: tensor(0.3789)\n",
      "5519 Training Loss: tensor(0.3733)\n",
      "5520 Training Loss: tensor(0.3730)\n",
      "5521 Training Loss: tensor(0.3751)\n",
      "5522 Training Loss: tensor(0.3798)\n",
      "5523 Training Loss: tensor(0.3794)\n",
      "5524 Training Loss: tensor(0.3756)\n",
      "5525 Training Loss: tensor(0.3823)\n",
      "5526 Training Loss: tensor(0.3752)\n",
      "5527 Training Loss: tensor(0.3806)\n",
      "5528 Training Loss: tensor(0.3833)\n",
      "5529 Training Loss: tensor(0.3810)\n",
      "5530 Training Loss: tensor(0.3771)\n",
      "5531 Training Loss: tensor(0.3790)\n",
      "5532 Training Loss: tensor(0.3788)\n",
      "5533 Training Loss: tensor(0.3773)\n",
      "5534 Training Loss: tensor(0.3761)\n",
      "5535 Training Loss: tensor(0.3797)\n",
      "5536 Training Loss: tensor(0.3797)\n",
      "5537 Training Loss: tensor(0.3806)\n",
      "5538 Training Loss: tensor(0.3782)\n",
      "5539 Training Loss: tensor(0.3776)\n",
      "5540 Training Loss: tensor(0.3750)\n",
      "5541 Training Loss: tensor(0.3834)\n",
      "5542 Training Loss: tensor(0.3808)\n",
      "5543 Training Loss: tensor(0.3794)\n",
      "5544 Training Loss: tensor(0.3765)\n",
      "5545 Training Loss: tensor(0.3766)\n",
      "5546 Training Loss: tensor(0.3774)\n",
      "5547 Training Loss: tensor(0.3784)\n",
      "5548 Training Loss: tensor(0.3761)\n",
      "5549 Training Loss: tensor(0.3796)\n",
      "5550 Training Loss: tensor(0.3784)\n",
      "5551 Training Loss: tensor(0.3781)\n",
      "5552 Training Loss: tensor(0.3785)\n",
      "5553 Training Loss: tensor(0.3799)\n",
      "5554 Training Loss: tensor(0.3764)\n",
      "5555 Training Loss: tensor(0.3767)\n",
      "5556 Training Loss: tensor(0.3775)\n",
      "5557 Training Loss: tensor(0.3766)\n",
      "5558 Training Loss: tensor(0.3758)\n",
      "5559 Training Loss: tensor(0.3759)\n",
      "5560 Training Loss: tensor(0.3762)\n",
      "5561 Training Loss: tensor(0.3784)\n",
      "5562 Training Loss: tensor(0.3822)\n",
      "5563 Training Loss: tensor(0.3737)\n",
      "5564 Training Loss: tensor(0.3761)\n",
      "5565 Training Loss: tensor(0.3731)\n",
      "5566 Training Loss: tensor(0.3761)\n",
      "5567 Training Loss: tensor(0.3822)\n",
      "5568 Training Loss: tensor(0.3822)\n",
      "5569 Training Loss: tensor(0.3821)\n",
      "5570 Training Loss: tensor(0.3755)\n",
      "5571 Training Loss: tensor(0.3772)\n",
      "5572 Training Loss: tensor(0.3761)\n",
      "5573 Training Loss: tensor(0.3780)\n",
      "5574 Training Loss: tensor(0.3803)\n",
      "5575 Training Loss: tensor(0.3797)\n",
      "5576 Training Loss: tensor(0.3784)\n",
      "5577 Training Loss: tensor(0.3769)\n",
      "5578 Training Loss: tensor(0.3754)\n",
      "5579 Training Loss: tensor(0.3763)\n",
      "5580 Training Loss: tensor(0.3752)\n",
      "5581 Training Loss: tensor(0.3804)\n",
      "5582 Training Loss: tensor(0.3783)\n",
      "5583 Training Loss: tensor(0.3751)\n",
      "5584 Training Loss: tensor(0.3789)\n",
      "5585 Training Loss: tensor(0.3754)\n",
      "5586 Training Loss: tensor(0.3784)\n",
      "5587 Training Loss: tensor(0.3783)\n",
      "5588 Training Loss: tensor(0.3805)\n",
      "5589 Training Loss: tensor(0.3735)\n",
      "5590 Training Loss: tensor(0.3790)\n",
      "5591 Training Loss: tensor(0.3761)\n",
      "5592 Training Loss: tensor(0.3769)\n",
      "5593 Training Loss: tensor(0.3773)\n",
      "5594 Training Loss: tensor(0.3783)\n",
      "5595 Training Loss: tensor(0.3756)\n",
      "5596 Training Loss: tensor(0.3756)\n",
      "5597 Training Loss: tensor(0.3765)\n",
      "5598 Training Loss: tensor(0.3804)\n",
      "5599 Training Loss: tensor(0.3785)\n",
      "5600 Training Loss: tensor(0.3824)\n",
      "5601 Training Loss: tensor(0.3768)\n",
      "5602 Training Loss: tensor(0.3797)\n",
      "5603 Training Loss: tensor(0.3841)\n",
      "5604 Training Loss: tensor(0.3757)\n",
      "5605 Training Loss: tensor(0.3776)\n",
      "5606 Training Loss: tensor(0.3829)\n",
      "5607 Training Loss: tensor(0.3800)\n",
      "5608 Training Loss: tensor(0.3800)\n",
      "5609 Training Loss: tensor(0.3783)\n",
      "5610 Training Loss: tensor(0.3798)\n",
      "5611 Training Loss: tensor(0.3801)\n",
      "5612 Training Loss: tensor(0.3782)\n",
      "5613 Training Loss: tensor(0.3778)\n",
      "5614 Training Loss: tensor(0.3819)\n",
      "5615 Training Loss: tensor(0.3793)\n",
      "5616 Training Loss: tensor(0.3777)\n",
      "5617 Training Loss: tensor(0.3795)\n",
      "5618 Training Loss: tensor(0.3793)\n",
      "5619 Training Loss: tensor(0.3773)\n",
      "5620 Training Loss: tensor(0.3809)\n",
      "5621 Training Loss: tensor(0.3787)\n",
      "5622 Training Loss: tensor(0.3786)\n",
      "5623 Training Loss: tensor(0.3804)\n",
      "5624 Training Loss: tensor(0.3760)\n",
      "5625 Training Loss: tensor(0.3737)\n",
      "5626 Training Loss: tensor(0.3820)\n",
      "5627 Training Loss: tensor(0.3751)\n",
      "5628 Training Loss: tensor(0.3750)\n",
      "5629 Training Loss: tensor(0.3773)\n",
      "5630 Training Loss: tensor(0.3778)\n",
      "5631 Training Loss: tensor(0.3803)\n",
      "5632 Training Loss: tensor(0.3799)\n",
      "5633 Training Loss: tensor(0.3758)\n",
      "5634 Training Loss: tensor(0.3780)\n",
      "5635 Training Loss: tensor(0.3761)\n",
      "5636 Training Loss: tensor(0.3780)\n",
      "5637 Training Loss: tensor(0.3788)\n",
      "5638 Training Loss: tensor(0.3764)\n",
      "5639 Training Loss: tensor(0.3769)\n",
      "5640 Training Loss: tensor(0.3782)\n",
      "5641 Training Loss: tensor(0.3769)\n",
      "5642 Training Loss: tensor(0.3783)\n",
      "5643 Training Loss: tensor(0.3772)\n",
      "5644 Training Loss: tensor(0.3765)\n",
      "5645 Training Loss: tensor(0.3804)\n",
      "5646 Training Loss: tensor(0.3777)\n",
      "5647 Training Loss: tensor(0.3762)\n",
      "5648 Training Loss: tensor(0.3784)\n",
      "5649 Training Loss: tensor(0.3746)\n",
      "5650 Training Loss: tensor(0.3737)\n",
      "5651 Training Loss: tensor(0.3751)\n",
      "5652 Training Loss: tensor(0.3767)\n",
      "5653 Training Loss: tensor(0.3754)\n",
      "5654 Training Loss: tensor(0.3753)\n",
      "5655 Training Loss: tensor(0.3773)\n",
      "5656 Training Loss: tensor(0.3791)\n",
      "5657 Training Loss: tensor(0.3768)\n",
      "5658 Training Loss: tensor(0.3775)\n",
      "5659 Training Loss: tensor(0.3815)\n",
      "5660 Training Loss: tensor(0.3785)\n",
      "5661 Training Loss: tensor(0.3784)\n",
      "5662 Training Loss: tensor(0.3754)\n",
      "5663 Training Loss: tensor(0.3769)\n",
      "5664 Training Loss: tensor(0.3785)\n",
      "5665 Training Loss: tensor(0.3783)\n",
      "5666 Training Loss: tensor(0.3815)\n",
      "5667 Training Loss: tensor(0.3744)\n",
      "5668 Training Loss: tensor(0.3773)\n",
      "5669 Training Loss: tensor(0.3820)\n",
      "5670 Training Loss: tensor(0.3797)\n",
      "5671 Training Loss: tensor(0.3765)\n",
      "5672 Training Loss: tensor(0.3756)\n",
      "5673 Training Loss: tensor(0.3778)\n",
      "5674 Training Loss: tensor(0.3782)\n",
      "5675 Training Loss: tensor(0.3750)\n",
      "5676 Training Loss: tensor(0.3772)\n",
      "5677 Training Loss: tensor(0.3749)\n",
      "5678 Training Loss: tensor(0.3778)\n",
      "5679 Training Loss: tensor(0.3739)\n",
      "5680 Training Loss: tensor(0.3799)\n",
      "5681 Training Loss: tensor(0.3763)\n",
      "5682 Training Loss: tensor(0.3774)\n",
      "5683 Training Loss: tensor(0.3819)\n",
      "5684 Training Loss: tensor(0.3757)\n",
      "5685 Training Loss: tensor(0.3811)\n",
      "5686 Training Loss: tensor(0.3774)\n",
      "5687 Training Loss: tensor(0.3815)\n",
      "5688 Training Loss: tensor(0.3751)\n",
      "5689 Training Loss: tensor(0.3780)\n",
      "5690 Training Loss: tensor(0.3788)\n",
      "5691 Training Loss: tensor(0.3739)\n",
      "5692 Training Loss: tensor(0.3739)\n",
      "5693 Training Loss: tensor(0.3761)\n",
      "5694 Training Loss: tensor(0.3777)\n",
      "5695 Training Loss: tensor(0.3785)\n",
      "5696 Training Loss: tensor(0.3800)\n",
      "5697 Training Loss: tensor(0.3768)\n",
      "5698 Training Loss: tensor(0.3792)\n",
      "5699 Training Loss: tensor(0.3794)\n",
      "5700 Training Loss: tensor(0.3782)\n",
      "5701 Training Loss: tensor(0.3756)\n",
      "5702 Training Loss: tensor(0.3777)\n",
      "5703 Training Loss: tensor(0.3817)\n",
      "5704 Training Loss: tensor(0.3736)\n",
      "5705 Training Loss: tensor(0.3744)\n",
      "5706 Training Loss: tensor(0.3808)\n",
      "5707 Training Loss: tensor(0.3771)\n",
      "5708 Training Loss: tensor(0.3791)\n",
      "5709 Training Loss: tensor(0.3794)\n",
      "5710 Training Loss: tensor(0.3800)\n",
      "5711 Training Loss: tensor(0.3815)\n",
      "5712 Training Loss: tensor(0.3746)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5713 Training Loss: tensor(0.3755)\n",
      "5714 Training Loss: tensor(0.3761)\n",
      "5715 Training Loss: tensor(0.3768)\n",
      "5716 Training Loss: tensor(0.3783)\n",
      "5717 Training Loss: tensor(0.3802)\n",
      "5718 Training Loss: tensor(0.3808)\n",
      "5719 Training Loss: tensor(0.3785)\n",
      "5720 Training Loss: tensor(0.3783)\n",
      "5721 Training Loss: tensor(0.3775)\n",
      "5722 Training Loss: tensor(0.3752)\n",
      "5723 Training Loss: tensor(0.3746)\n",
      "5724 Training Loss: tensor(0.3742)\n",
      "5725 Training Loss: tensor(0.3730)\n",
      "5726 Training Loss: tensor(0.3763)\n",
      "5727 Training Loss: tensor(0.3814)\n",
      "5728 Training Loss: tensor(0.3754)\n",
      "5729 Training Loss: tensor(0.3798)\n",
      "5730 Training Loss: tensor(0.3763)\n",
      "5731 Training Loss: tensor(0.3782)\n",
      "5732 Training Loss: tensor(0.3793)\n",
      "5733 Training Loss: tensor(0.3807)\n",
      "5734 Training Loss: tensor(0.3804)\n",
      "5735 Training Loss: tensor(0.3809)\n",
      "5736 Training Loss: tensor(0.3771)\n",
      "5737 Training Loss: tensor(0.3758)\n",
      "5738 Training Loss: tensor(0.3761)\n",
      "5739 Training Loss: tensor(0.3842)\n",
      "5740 Training Loss: tensor(0.3832)\n",
      "5741 Training Loss: tensor(0.3769)\n",
      "5742 Training Loss: tensor(0.3731)\n",
      "5743 Training Loss: tensor(0.3784)\n",
      "5744 Training Loss: tensor(0.3789)\n",
      "5745 Training Loss: tensor(0.3784)\n",
      "5746 Training Loss: tensor(0.3809)\n",
      "5747 Training Loss: tensor(0.3761)\n",
      "5748 Training Loss: tensor(0.3797)\n",
      "5749 Training Loss: tensor(0.3740)\n",
      "5750 Training Loss: tensor(0.3777)\n",
      "5751 Training Loss: tensor(0.3766)\n",
      "5752 Training Loss: tensor(0.3787)\n",
      "5753 Training Loss: tensor(0.3824)\n",
      "5754 Training Loss: tensor(0.3771)\n",
      "5755 Training Loss: tensor(0.3742)\n",
      "5756 Training Loss: tensor(0.3755)\n",
      "5757 Training Loss: tensor(0.3807)\n",
      "5758 Training Loss: tensor(0.3733)\n",
      "5759 Training Loss: tensor(0.3807)\n",
      "5760 Training Loss: tensor(0.3775)\n",
      "5761 Training Loss: tensor(0.3764)\n",
      "5762 Training Loss: tensor(0.3781)\n",
      "5763 Training Loss: tensor(0.3748)\n",
      "5764 Training Loss: tensor(0.3740)\n",
      "5765 Training Loss: tensor(0.3803)\n",
      "5766 Training Loss: tensor(0.3742)\n",
      "5767 Training Loss: tensor(0.3779)\n",
      "5768 Training Loss: tensor(0.3772)\n",
      "5769 Training Loss: tensor(0.3791)\n",
      "5770 Training Loss: tensor(0.3782)\n",
      "5771 Training Loss: tensor(0.3877)\n",
      "5772 Training Loss: tensor(0.3829)\n",
      "5773 Training Loss: tensor(0.3795)\n",
      "5774 Training Loss: tensor(0.3756)\n",
      "5775 Training Loss: tensor(0.3781)\n",
      "5776 Training Loss: tensor(0.3767)\n",
      "5777 Training Loss: tensor(0.3755)\n",
      "5778 Training Loss: tensor(0.3763)\n",
      "5779 Training Loss: tensor(0.3739)\n",
      "5780 Training Loss: tensor(0.3775)\n",
      "5781 Training Loss: tensor(0.3765)\n",
      "5782 Training Loss: tensor(0.3746)\n",
      "5783 Training Loss: tensor(0.3754)\n",
      "5784 Training Loss: tensor(0.3771)\n",
      "5785 Training Loss: tensor(0.3782)\n",
      "5786 Training Loss: tensor(0.3745)\n",
      "5787 Training Loss: tensor(0.3787)\n",
      "5788 Training Loss: tensor(0.3787)\n",
      "5789 Training Loss: tensor(0.3805)\n",
      "5790 Training Loss: tensor(0.3749)\n",
      "5791 Training Loss: tensor(0.3760)\n",
      "5792 Training Loss: tensor(0.3772)\n",
      "5793 Training Loss: tensor(0.3787)\n",
      "5794 Training Loss: tensor(0.3748)\n",
      "5795 Training Loss: tensor(0.3745)\n",
      "5796 Training Loss: tensor(0.3758)\n",
      "5797 Training Loss: tensor(0.3781)\n",
      "5798 Training Loss: tensor(0.3743)\n",
      "5799 Training Loss: tensor(0.3748)\n",
      "5800 Training Loss: tensor(0.3800)\n",
      "5801 Training Loss: tensor(0.3811)\n",
      "5802 Training Loss: tensor(0.3853)\n",
      "5803 Training Loss: tensor(0.3734)\n",
      "5804 Training Loss: tensor(0.3739)\n",
      "5805 Training Loss: tensor(0.3771)\n",
      "5806 Training Loss: tensor(0.3740)\n",
      "5807 Training Loss: tensor(0.3793)\n",
      "5808 Training Loss: tensor(0.3783)\n",
      "5809 Training Loss: tensor(0.3753)\n",
      "5810 Training Loss: tensor(0.3751)\n",
      "5811 Training Loss: tensor(0.3787)\n",
      "5812 Training Loss: tensor(0.3743)\n",
      "5813 Training Loss: tensor(0.3762)\n",
      "5814 Training Loss: tensor(0.3775)\n",
      "5815 Training Loss: tensor(0.3758)\n",
      "5816 Training Loss: tensor(0.3810)\n",
      "5817 Training Loss: tensor(0.3797)\n",
      "5818 Training Loss: tensor(0.3750)\n",
      "5819 Training Loss: tensor(0.3779)\n",
      "5820 Training Loss: tensor(0.3786)\n",
      "5821 Training Loss: tensor(0.3761)\n",
      "5822 Training Loss: tensor(0.3762)\n",
      "5823 Training Loss: tensor(0.3753)\n",
      "5824 Training Loss: tensor(0.3741)\n",
      "5825 Training Loss: tensor(0.3778)\n",
      "5826 Training Loss: tensor(0.3743)\n",
      "5827 Training Loss: tensor(0.3782)\n",
      "5828 Training Loss: tensor(0.3758)\n",
      "5829 Training Loss: tensor(0.3774)\n",
      "5830 Training Loss: tensor(0.3777)\n",
      "5831 Training Loss: tensor(0.3793)\n",
      "5832 Training Loss: tensor(0.3764)\n",
      "5833 Training Loss: tensor(0.3810)\n",
      "5834 Training Loss: tensor(0.3768)\n",
      "5835 Training Loss: tensor(0.3762)\n",
      "5836 Training Loss: tensor(0.3739)\n",
      "5837 Training Loss: tensor(0.3788)\n",
      "5838 Training Loss: tensor(0.3760)\n",
      "5839 Training Loss: tensor(0.3734)\n",
      "5840 Training Loss: tensor(0.3728)\n",
      "5841 Training Loss: tensor(0.3752)\n",
      "5842 Training Loss: tensor(0.3762)\n",
      "5843 Training Loss: tensor(0.3758)\n",
      "5844 Training Loss: tensor(0.3825)\n",
      "5845 Training Loss: tensor(0.3779)\n",
      "5846 Training Loss: tensor(0.3793)\n",
      "5847 Training Loss: tensor(0.3767)\n",
      "5848 Training Loss: tensor(0.3800)\n",
      "5849 Training Loss: tensor(0.3753)\n",
      "5850 Training Loss: tensor(0.3827)\n",
      "5851 Training Loss: tensor(0.3768)\n",
      "5852 Training Loss: tensor(0.3741)\n",
      "5853 Training Loss: tensor(0.3764)\n",
      "5854 Training Loss: tensor(0.3781)\n",
      "5855 Training Loss: tensor(0.3788)\n",
      "5856 Training Loss: tensor(0.3794)\n",
      "5857 Training Loss: tensor(0.3811)\n",
      "5858 Training Loss: tensor(0.3753)\n",
      "5859 Training Loss: tensor(0.3785)\n",
      "5860 Training Loss: tensor(0.3752)\n",
      "5861 Training Loss: tensor(0.3744)\n",
      "5862 Training Loss: tensor(0.3735)\n",
      "5863 Training Loss: tensor(0.3770)\n",
      "5864 Training Loss: tensor(0.3784)\n",
      "5865 Training Loss: tensor(0.3794)\n",
      "5866 Training Loss: tensor(0.3757)\n",
      "5867 Training Loss: tensor(0.3791)\n",
      "5868 Training Loss: tensor(0.3757)\n",
      "5869 Training Loss: tensor(0.3739)\n",
      "5870 Training Loss: tensor(0.3772)\n",
      "5871 Training Loss: tensor(0.3750)\n",
      "5872 Training Loss: tensor(0.3760)\n",
      "5873 Training Loss: tensor(0.3815)\n",
      "5874 Training Loss: tensor(0.3773)\n",
      "5875 Training Loss: tensor(0.3764)\n",
      "5876 Training Loss: tensor(0.3777)\n",
      "5877 Training Loss: tensor(0.3762)\n",
      "5878 Training Loss: tensor(0.3749)\n",
      "5879 Training Loss: tensor(0.3808)\n",
      "5880 Training Loss: tensor(0.3780)\n",
      "5881 Training Loss: tensor(0.3743)\n",
      "5882 Training Loss: tensor(0.3768)\n",
      "5883 Training Loss: tensor(0.3741)\n",
      "5884 Training Loss: tensor(0.3757)\n",
      "5885 Training Loss: tensor(0.3781)\n",
      "5886 Training Loss: tensor(0.3761)\n",
      "5887 Training Loss: tensor(0.3780)\n",
      "5888 Training Loss: tensor(0.3760)\n",
      "5889 Training Loss: tensor(0.3768)\n",
      "5890 Training Loss: tensor(0.3796)\n",
      "5891 Training Loss: tensor(0.3754)\n",
      "5892 Training Loss: tensor(0.3769)\n",
      "5893 Training Loss: tensor(0.3779)\n",
      "5894 Training Loss: tensor(0.3758)\n",
      "5895 Training Loss: tensor(0.3772)\n",
      "5896 Training Loss: tensor(0.3795)\n",
      "5897 Training Loss: tensor(0.3750)\n",
      "5898 Training Loss: tensor(0.3785)\n",
      "5899 Training Loss: tensor(0.3759)\n",
      "5900 Training Loss: tensor(0.3805)\n",
      "5901 Training Loss: tensor(0.3766)\n",
      "5902 Training Loss: tensor(0.3803)\n",
      "5903 Training Loss: tensor(0.3776)\n",
      "5904 Training Loss: tensor(0.3760)\n",
      "5905 Training Loss: tensor(0.3769)\n",
      "5906 Training Loss: tensor(0.3811)\n",
      "5907 Training Loss: tensor(0.3774)\n",
      "5908 Training Loss: tensor(0.3763)\n",
      "5909 Training Loss: tensor(0.3745)\n",
      "5910 Training Loss: tensor(0.3767)\n",
      "5911 Training Loss: tensor(0.3802)\n",
      "5912 Training Loss: tensor(0.3740)\n",
      "5913 Training Loss: tensor(0.3795)\n",
      "5914 Training Loss: tensor(0.3790)\n",
      "5915 Training Loss: tensor(0.3753)\n",
      "5916 Training Loss: tensor(0.3765)\n",
      "5917 Training Loss: tensor(0.3770)\n",
      "5918 Training Loss: tensor(0.3770)\n",
      "5919 Training Loss: tensor(0.3777)\n",
      "5920 Training Loss: tensor(0.3786)\n",
      "5921 Training Loss: tensor(0.3778)\n",
      "5922 Training Loss: tensor(0.3790)\n",
      "5923 Training Loss: tensor(0.3741)\n",
      "5924 Training Loss: tensor(0.3780)\n",
      "5925 Training Loss: tensor(0.3875)\n",
      "5926 Training Loss: tensor(0.3773)\n",
      "5927 Training Loss: tensor(0.3815)\n",
      "5928 Training Loss: tensor(0.3779)\n",
      "5929 Training Loss: tensor(0.3767)\n",
      "5930 Training Loss: tensor(0.3779)\n",
      "5931 Training Loss: tensor(0.3815)\n",
      "5932 Training Loss: tensor(0.3764)\n",
      "5933 Training Loss: tensor(0.3764)\n",
      "5934 Training Loss: tensor(0.3787)\n",
      "5935 Training Loss: tensor(0.3748)\n",
      "5936 Training Loss: tensor(0.3793)\n",
      "5937 Training Loss: tensor(0.3760)\n",
      "5938 Training Loss: tensor(0.3761)\n",
      "5939 Training Loss: tensor(0.3779)\n",
      "5940 Training Loss: tensor(0.3760)\n",
      "5941 Training Loss: tensor(0.3730)\n",
      "5942 Training Loss: tensor(0.3783)\n",
      "5943 Training Loss: tensor(0.3745)\n",
      "5944 Training Loss: tensor(0.3754)\n",
      "5945 Training Loss: tensor(0.3751)\n",
      "5946 Training Loss: tensor(0.3788)\n",
      "5947 Training Loss: tensor(0.3751)\n",
      "5948 Training Loss: tensor(0.3755)\n",
      "5949 Training Loss: tensor(0.3763)\n",
      "5950 Training Loss: tensor(0.3758)\n",
      "5951 Training Loss: tensor(0.3758)\n",
      "5952 Training Loss: tensor(0.3751)\n",
      "5953 Training Loss: tensor(0.3750)\n",
      "5954 Training Loss: tensor(0.3839)\n",
      "5955 Training Loss: tensor(0.3793)\n",
      "5956 Training Loss: tensor(0.3774)\n",
      "5957 Training Loss: tensor(0.3811)\n",
      "5958 Training Loss: tensor(0.3779)\n",
      "5959 Training Loss: tensor(0.3786)\n",
      "5960 Training Loss: tensor(0.3767)\n",
      "5961 Training Loss: tensor(0.3744)\n",
      "5962 Training Loss: tensor(0.3793)\n",
      "5963 Training Loss: tensor(0.3762)\n",
      "5964 Training Loss: tensor(0.3821)\n",
      "5965 Training Loss: tensor(0.3790)\n",
      "5966 Training Loss: tensor(0.3737)\n",
      "5967 Training Loss: tensor(0.3788)\n",
      "5968 Training Loss: tensor(0.3751)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5969 Training Loss: tensor(0.3768)\n",
      "5970 Training Loss: tensor(0.3781)\n",
      "5971 Training Loss: tensor(0.3767)\n",
      "5972 Training Loss: tensor(0.3746)\n",
      "5973 Training Loss: tensor(0.3770)\n",
      "5974 Training Loss: tensor(0.3756)\n",
      "5975 Training Loss: tensor(0.3745)\n",
      "5976 Training Loss: tensor(0.3760)\n",
      "5977 Training Loss: tensor(0.3748)\n",
      "5978 Training Loss: tensor(0.3771)\n",
      "5979 Training Loss: tensor(0.3762)\n",
      "5980 Training Loss: tensor(0.3747)\n",
      "5981 Training Loss: tensor(0.3744)\n",
      "5982 Training Loss: tensor(0.3746)\n",
      "5983 Training Loss: tensor(0.3793)\n",
      "5984 Training Loss: tensor(0.3802)\n",
      "5985 Training Loss: tensor(0.3777)\n",
      "5986 Training Loss: tensor(0.3776)\n",
      "5987 Training Loss: tensor(0.3794)\n",
      "5988 Training Loss: tensor(0.3754)\n",
      "5989 Training Loss: tensor(0.3760)\n",
      "5990 Training Loss: tensor(0.3770)\n",
      "5991 Training Loss: tensor(0.3799)\n",
      "5992 Training Loss: tensor(0.3796)\n",
      "5993 Training Loss: tensor(0.3770)\n",
      "5994 Training Loss: tensor(0.3735)\n",
      "5995 Training Loss: tensor(0.3752)\n",
      "5996 Training Loss: tensor(0.3743)\n",
      "5997 Training Loss: tensor(0.3781)\n",
      "5998 Training Loss: tensor(0.3776)\n",
      "5999 Training Loss: tensor(0.3792)\n",
      "6000 Training Loss: tensor(0.3778)\n",
      "6001 Training Loss: tensor(0.3765)\n",
      "6002 Training Loss: tensor(0.3795)\n",
      "6003 Training Loss: tensor(0.3770)\n",
      "6004 Training Loss: tensor(0.3730)\n",
      "6005 Training Loss: tensor(0.3753)\n",
      "6006 Training Loss: tensor(0.3749)\n",
      "6007 Training Loss: tensor(0.3777)\n",
      "6008 Training Loss: tensor(0.3784)\n",
      "6009 Training Loss: tensor(0.3807)\n",
      "6010 Training Loss: tensor(0.3808)\n",
      "6011 Training Loss: tensor(0.3777)\n",
      "6012 Training Loss: tensor(0.3773)\n",
      "6013 Training Loss: tensor(0.3781)\n",
      "6014 Training Loss: tensor(0.3779)\n",
      "6015 Training Loss: tensor(0.3797)\n",
      "6016 Training Loss: tensor(0.3778)\n",
      "6017 Training Loss: tensor(0.3748)\n",
      "6018 Training Loss: tensor(0.3800)\n",
      "6019 Training Loss: tensor(0.3758)\n",
      "6020 Training Loss: tensor(0.3795)\n",
      "6021 Training Loss: tensor(0.3764)\n",
      "6022 Training Loss: tensor(0.3779)\n",
      "6023 Training Loss: tensor(0.3808)\n",
      "6024 Training Loss: tensor(0.3759)\n",
      "6025 Training Loss: tensor(0.3747)\n",
      "6026 Training Loss: tensor(0.3775)\n",
      "6027 Training Loss: tensor(0.3744)\n",
      "6028 Training Loss: tensor(0.3737)\n",
      "6029 Training Loss: tensor(0.3760)\n",
      "6030 Training Loss: tensor(0.3743)\n",
      "6031 Training Loss: tensor(0.3766)\n",
      "6032 Training Loss: tensor(0.3749)\n",
      "6033 Training Loss: tensor(0.3737)\n",
      "6034 Training Loss: tensor(0.3783)\n",
      "6035 Training Loss: tensor(0.3779)\n",
      "6036 Training Loss: tensor(0.3766)\n",
      "6037 Training Loss: tensor(0.3776)\n",
      "6038 Training Loss: tensor(0.3739)\n",
      "6039 Training Loss: tensor(0.3785)\n",
      "6040 Training Loss: tensor(0.3765)\n",
      "6041 Training Loss: tensor(0.3793)\n",
      "6042 Training Loss: tensor(0.3724)\n",
      "6043 Training Loss: tensor(0.3717)\n",
      "6044 Training Loss: tensor(0.3750)\n",
      "6045 Training Loss: tensor(0.3767)\n",
      "6046 Training Loss: tensor(0.3748)\n",
      "6047 Training Loss: tensor(0.3781)\n",
      "6048 Training Loss: tensor(0.3745)\n",
      "6049 Training Loss: tensor(0.3819)\n",
      "6050 Training Loss: tensor(0.3716)\n",
      "6051 Training Loss: tensor(0.3774)\n",
      "6052 Training Loss: tensor(0.3754)\n",
      "6053 Training Loss: tensor(0.3774)\n",
      "6054 Training Loss: tensor(0.3748)\n",
      "6055 Training Loss: tensor(0.3751)\n",
      "6056 Training Loss: tensor(0.3780)\n",
      "6057 Training Loss: tensor(0.3750)\n",
      "6058 Training Loss: tensor(0.3815)\n",
      "6059 Training Loss: tensor(0.3763)\n",
      "6060 Training Loss: tensor(0.3743)\n",
      "6061 Training Loss: tensor(0.3745)\n",
      "6062 Training Loss: tensor(0.3754)\n",
      "6063 Training Loss: tensor(0.3763)\n",
      "6064 Training Loss: tensor(0.3771)\n",
      "6065 Training Loss: tensor(0.3778)\n",
      "6066 Training Loss: tensor(0.3805)\n",
      "6067 Training Loss: tensor(0.3758)\n",
      "6068 Training Loss: tensor(0.3782)\n",
      "6069 Training Loss: tensor(0.3787)\n",
      "6070 Training Loss: tensor(0.3797)\n",
      "6071 Training Loss: tensor(0.3766)\n",
      "6072 Training Loss: tensor(0.3755)\n",
      "6073 Training Loss: tensor(0.3764)\n",
      "6074 Training Loss: tensor(0.3770)\n",
      "6075 Training Loss: tensor(0.3760)\n",
      "6076 Training Loss: tensor(0.3785)\n",
      "6077 Training Loss: tensor(0.3763)\n",
      "6078 Training Loss: tensor(0.3757)\n",
      "6079 Training Loss: tensor(0.3752)\n",
      "6080 Training Loss: tensor(0.3813)\n",
      "6081 Training Loss: tensor(0.3761)\n",
      "6082 Training Loss: tensor(0.3793)\n",
      "6083 Training Loss: tensor(0.3769)\n",
      "6084 Training Loss: tensor(0.3789)\n",
      "6085 Training Loss: tensor(0.3760)\n",
      "6086 Training Loss: tensor(0.3770)\n",
      "6087 Training Loss: tensor(0.3758)\n",
      "6088 Training Loss: tensor(0.3777)\n",
      "6089 Training Loss: tensor(0.3763)\n",
      "6090 Training Loss: tensor(0.3799)\n",
      "6091 Training Loss: tensor(0.3788)\n",
      "6092 Training Loss: tensor(0.3787)\n",
      "6093 Training Loss: tensor(0.3784)\n",
      "6094 Training Loss: tensor(0.3773)\n",
      "6095 Training Loss: tensor(0.3743)\n",
      "6096 Training Loss: tensor(0.3772)\n",
      "6097 Training Loss: tensor(0.3750)\n",
      "6098 Training Loss: tensor(0.3799)\n",
      "6099 Training Loss: tensor(0.3791)\n",
      "6100 Training Loss: tensor(0.3786)\n",
      "6101 Training Loss: tensor(0.3772)\n",
      "6102 Training Loss: tensor(0.3756)\n",
      "6103 Training Loss: tensor(0.3738)\n",
      "6104 Training Loss: tensor(0.3824)\n",
      "6105 Training Loss: tensor(0.3774)\n",
      "6106 Training Loss: tensor(0.3766)\n",
      "6107 Training Loss: tensor(0.3761)\n",
      "6108 Training Loss: tensor(0.3765)\n",
      "6109 Training Loss: tensor(0.3784)\n",
      "6110 Training Loss: tensor(0.3754)\n",
      "6111 Training Loss: tensor(0.3743)\n",
      "6112 Training Loss: tensor(0.3782)\n",
      "6113 Training Loss: tensor(0.3747)\n",
      "6114 Training Loss: tensor(0.3731)\n",
      "6115 Training Loss: tensor(0.3772)\n",
      "6116 Training Loss: tensor(0.3759)\n",
      "6117 Training Loss: tensor(0.3742)\n",
      "6118 Training Loss: tensor(0.3751)\n",
      "6119 Training Loss: tensor(0.3737)\n",
      "6120 Training Loss: tensor(0.3761)\n",
      "6121 Training Loss: tensor(0.3724)\n",
      "6122 Training Loss: tensor(0.3741)\n",
      "6123 Training Loss: tensor(0.3730)\n",
      "6124 Training Loss: tensor(0.3793)\n",
      "6125 Training Loss: tensor(0.3785)\n",
      "6126 Training Loss: tensor(0.3793)\n",
      "6127 Training Loss: tensor(0.3766)\n",
      "6128 Training Loss: tensor(0.3754)\n",
      "6129 Training Loss: tensor(0.3774)\n",
      "6130 Training Loss: tensor(0.3726)\n",
      "6131 Training Loss: tensor(0.3741)\n",
      "6132 Training Loss: tensor(0.3742)\n",
      "6133 Training Loss: tensor(0.3800)\n",
      "6134 Training Loss: tensor(0.3742)\n",
      "6135 Training Loss: tensor(0.3728)\n",
      "6136 Training Loss: tensor(0.3748)\n",
      "6137 Training Loss: tensor(0.3806)\n",
      "6138 Training Loss: tensor(0.3771)\n",
      "6139 Training Loss: tensor(0.3756)\n",
      "6140 Training Loss: tensor(0.3764)\n",
      "6141 Training Loss: tensor(0.3763)\n",
      "6142 Training Loss: tensor(0.3789)\n",
      "6143 Training Loss: tensor(0.3780)\n",
      "6144 Training Loss: tensor(0.3765)\n",
      "6145 Training Loss: tensor(0.3746)\n",
      "6146 Training Loss: tensor(0.3808)\n",
      "6147 Training Loss: tensor(0.3781)\n",
      "6148 Training Loss: tensor(0.3735)\n",
      "6149 Training Loss: tensor(0.3749)\n",
      "6150 Training Loss: tensor(0.3790)\n",
      "6151 Training Loss: tensor(0.3749)\n",
      "6152 Training Loss: tensor(0.3746)\n",
      "6153 Training Loss: tensor(0.3754)\n",
      "6154 Training Loss: tensor(0.3763)\n",
      "6155 Training Loss: tensor(0.3755)\n",
      "6156 Training Loss: tensor(0.3786)\n",
      "6157 Training Loss: tensor(0.3801)\n",
      "6158 Training Loss: tensor(0.3752)\n",
      "6159 Training Loss: tensor(0.3779)\n",
      "6160 Training Loss: tensor(0.3821)\n",
      "6161 Training Loss: tensor(0.3756)\n",
      "6162 Training Loss: tensor(0.3747)\n",
      "6163 Training Loss: tensor(0.3782)\n",
      "6164 Training Loss: tensor(0.3793)\n",
      "6165 Training Loss: tensor(0.3754)\n",
      "6166 Training Loss: tensor(0.3809)\n",
      "6167 Training Loss: tensor(0.3744)\n",
      "6168 Training Loss: tensor(0.3797)\n",
      "6169 Training Loss: tensor(0.3739)\n",
      "6170 Training Loss: tensor(0.3726)\n",
      "6171 Training Loss: tensor(0.3751)\n",
      "6172 Training Loss: tensor(0.3728)\n",
      "6173 Training Loss: tensor(0.3781)\n",
      "6174 Training Loss: tensor(0.3759)\n",
      "6175 Training Loss: tensor(0.3729)\n",
      "6176 Training Loss: tensor(0.3758)\n",
      "6177 Training Loss: tensor(0.3759)\n",
      "6178 Training Loss: tensor(0.3766)\n",
      "6179 Training Loss: tensor(0.3765)\n",
      "6180 Training Loss: tensor(0.3734)\n",
      "6181 Training Loss: tensor(0.3813)\n",
      "6182 Training Loss: tensor(0.3814)\n",
      "6183 Training Loss: tensor(0.3760)\n",
      "6184 Training Loss: tensor(0.3807)\n",
      "6185 Training Loss: tensor(0.3815)\n",
      "6186 Training Loss: tensor(0.3823)\n",
      "6187 Training Loss: tensor(0.3728)\n",
      "6188 Training Loss: tensor(0.3776)\n",
      "6189 Training Loss: tensor(0.3761)\n",
      "6190 Training Loss: tensor(0.3770)\n",
      "6191 Training Loss: tensor(0.3736)\n",
      "6192 Training Loss: tensor(0.3735)\n",
      "6193 Training Loss: tensor(0.3782)\n",
      "6194 Training Loss: tensor(0.3766)\n",
      "6195 Training Loss: tensor(0.3748)\n",
      "6196 Training Loss: tensor(0.3788)\n",
      "6197 Training Loss: tensor(0.3745)\n",
      "6198 Training Loss: tensor(0.3803)\n",
      "6199 Training Loss: tensor(0.3760)\n",
      "6200 Training Loss: tensor(0.3753)\n",
      "6201 Training Loss: tensor(0.3748)\n",
      "6202 Training Loss: tensor(0.3810)\n",
      "6203 Training Loss: tensor(0.3828)\n",
      "6204 Training Loss: tensor(0.3727)\n",
      "6205 Training Loss: tensor(0.3754)\n",
      "6206 Training Loss: tensor(0.3767)\n",
      "6207 Training Loss: tensor(0.3749)\n",
      "6208 Training Loss: tensor(0.3781)\n",
      "6209 Training Loss: tensor(0.3756)\n",
      "6210 Training Loss: tensor(0.3756)\n",
      "6211 Training Loss: tensor(0.3748)\n",
      "6212 Training Loss: tensor(0.3829)\n",
      "6213 Training Loss: tensor(0.3733)\n",
      "6214 Training Loss: tensor(0.3783)\n",
      "6215 Training Loss: tensor(0.3724)\n",
      "6216 Training Loss: tensor(0.3801)\n",
      "6217 Training Loss: tensor(0.3808)\n",
      "6218 Training Loss: tensor(0.3753)\n",
      "6219 Training Loss: tensor(0.3730)\n",
      "6220 Training Loss: tensor(0.3801)\n",
      "6221 Training Loss: tensor(0.3789)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6222 Training Loss: tensor(0.3758)\n",
      "6223 Training Loss: tensor(0.3767)\n",
      "6224 Training Loss: tensor(0.3792)\n",
      "6225 Training Loss: tensor(0.3759)\n",
      "6226 Training Loss: tensor(0.3769)\n",
      "6227 Training Loss: tensor(0.3742)\n",
      "6228 Training Loss: tensor(0.3740)\n",
      "6229 Training Loss: tensor(0.3769)\n",
      "6230 Training Loss: tensor(0.3796)\n",
      "6231 Training Loss: tensor(0.3741)\n",
      "6232 Training Loss: tensor(0.3762)\n",
      "6233 Training Loss: tensor(0.3755)\n",
      "6234 Training Loss: tensor(0.3752)\n",
      "6235 Training Loss: tensor(0.3762)\n",
      "6236 Training Loss: tensor(0.3793)\n",
      "6237 Training Loss: tensor(0.3732)\n",
      "6238 Training Loss: tensor(0.3788)\n",
      "6239 Training Loss: tensor(0.3807)\n",
      "6240 Training Loss: tensor(0.3718)\n",
      "6241 Training Loss: tensor(0.3768)\n",
      "6242 Training Loss: tensor(0.3747)\n",
      "6243 Training Loss: tensor(0.3740)\n",
      "6244 Training Loss: tensor(0.3778)\n",
      "6245 Training Loss: tensor(0.3778)\n",
      "6246 Training Loss: tensor(0.3790)\n",
      "6247 Training Loss: tensor(0.3742)\n",
      "6248 Training Loss: tensor(0.3771)\n",
      "6249 Training Loss: tensor(0.3738)\n",
      "6250 Training Loss: tensor(0.3760)\n",
      "6251 Training Loss: tensor(0.3731)\n",
      "6252 Training Loss: tensor(0.3775)\n",
      "6253 Training Loss: tensor(0.3787)\n",
      "6254 Training Loss: tensor(0.3761)\n",
      "6255 Training Loss: tensor(0.3809)\n",
      "6256 Training Loss: tensor(0.3800)\n",
      "6257 Training Loss: tensor(0.3744)\n",
      "6258 Training Loss: tensor(0.3809)\n",
      "6259 Training Loss: tensor(0.3736)\n",
      "6260 Training Loss: tensor(0.3802)\n",
      "6261 Training Loss: tensor(0.3743)\n",
      "6262 Training Loss: tensor(0.3793)\n",
      "6263 Training Loss: tensor(0.3777)\n",
      "6264 Training Loss: tensor(0.3734)\n",
      "6265 Training Loss: tensor(0.3758)\n",
      "6266 Training Loss: tensor(0.3741)\n",
      "6267 Training Loss: tensor(0.3771)\n",
      "6268 Training Loss: tensor(0.3751)\n",
      "6269 Training Loss: tensor(0.3767)\n",
      "6270 Training Loss: tensor(0.3744)\n",
      "6271 Training Loss: tensor(0.3774)\n",
      "6272 Training Loss: tensor(0.3787)\n",
      "6273 Training Loss: tensor(0.3747)\n",
      "6274 Training Loss: tensor(0.3795)\n",
      "6275 Training Loss: tensor(0.3802)\n",
      "6276 Training Loss: tensor(0.3754)\n",
      "6277 Training Loss: tensor(0.3785)\n",
      "6278 Training Loss: tensor(0.3797)\n",
      "6279 Training Loss: tensor(0.3741)\n",
      "6280 Training Loss: tensor(0.3781)\n",
      "6281 Training Loss: tensor(0.3758)\n",
      "6282 Training Loss: tensor(0.3753)\n",
      "6283 Training Loss: tensor(0.3765)\n",
      "6284 Training Loss: tensor(0.3749)\n",
      "6285 Training Loss: tensor(0.3736)\n",
      "6286 Training Loss: tensor(0.3732)\n",
      "6287 Training Loss: tensor(0.3747)\n",
      "6288 Training Loss: tensor(0.3740)\n",
      "6289 Training Loss: tensor(0.3722)\n",
      "6290 Training Loss: tensor(0.3741)\n",
      "6291 Training Loss: tensor(0.3754)\n",
      "6292 Training Loss: tensor(0.3782)\n",
      "6293 Training Loss: tensor(0.3803)\n",
      "6294 Training Loss: tensor(0.3789)\n",
      "6295 Training Loss: tensor(0.3829)\n",
      "6296 Training Loss: tensor(0.3748)\n",
      "6297 Training Loss: tensor(0.3753)\n",
      "6298 Training Loss: tensor(0.3777)\n",
      "6299 Training Loss: tensor(0.3747)\n",
      "6300 Training Loss: tensor(0.3794)\n",
      "6301 Training Loss: tensor(0.3801)\n",
      "6302 Training Loss: tensor(0.3732)\n",
      "6303 Training Loss: tensor(0.3760)\n",
      "6304 Training Loss: tensor(0.3780)\n",
      "6305 Training Loss: tensor(0.3767)\n",
      "6306 Training Loss: tensor(0.3722)\n",
      "6307 Training Loss: tensor(0.3762)\n",
      "6308 Training Loss: tensor(0.3743)\n",
      "6309 Training Loss: tensor(0.3770)\n",
      "6310 Training Loss: tensor(0.3754)\n",
      "6311 Training Loss: tensor(0.3724)\n",
      "6312 Training Loss: tensor(0.3741)\n",
      "6313 Training Loss: tensor(0.3720)\n",
      "6314 Training Loss: tensor(0.3741)\n",
      "6315 Training Loss: tensor(0.3757)\n",
      "6316 Training Loss: tensor(0.3715)\n",
      "6317 Training Loss: tensor(0.3728)\n",
      "6318 Training Loss: tensor(0.3758)\n",
      "6319 Training Loss: tensor(0.3746)\n",
      "6320 Training Loss: tensor(0.3802)\n",
      "6321 Training Loss: tensor(0.3864)\n",
      "6322 Training Loss: tensor(0.3747)\n",
      "6323 Training Loss: tensor(0.3841)\n",
      "6324 Training Loss: tensor(0.3752)\n",
      "6325 Training Loss: tensor(0.3733)\n",
      "6326 Training Loss: tensor(0.3739)\n",
      "6327 Training Loss: tensor(0.3787)\n",
      "6328 Training Loss: tensor(0.3791)\n",
      "6329 Training Loss: tensor(0.3809)\n",
      "6330 Training Loss: tensor(0.3834)\n",
      "6331 Training Loss: tensor(0.3782)\n",
      "6332 Training Loss: tensor(0.3741)\n",
      "6333 Training Loss: tensor(0.3750)\n",
      "6334 Training Loss: tensor(0.3785)\n",
      "6335 Training Loss: tensor(0.3782)\n",
      "6336 Training Loss: tensor(0.3761)\n",
      "6337 Training Loss: tensor(0.3759)\n",
      "6338 Training Loss: tensor(0.3768)\n",
      "6339 Training Loss: tensor(0.3755)\n",
      "6340 Training Loss: tensor(0.3771)\n",
      "6341 Training Loss: tensor(0.3768)\n",
      "6342 Training Loss: tensor(0.3766)\n",
      "6343 Training Loss: tensor(0.3784)\n",
      "6344 Training Loss: tensor(0.3750)\n",
      "6345 Training Loss: tensor(0.3785)\n",
      "6346 Training Loss: tensor(0.3730)\n",
      "6347 Training Loss: tensor(0.3757)\n",
      "6348 Training Loss: tensor(0.3744)\n",
      "6349 Training Loss: tensor(0.3773)\n",
      "6350 Training Loss: tensor(0.3743)\n",
      "6351 Training Loss: tensor(0.3755)\n",
      "6352 Training Loss: tensor(0.3745)\n",
      "6353 Training Loss: tensor(0.3731)\n",
      "6354 Training Loss: tensor(0.3768)\n",
      "6355 Training Loss: tensor(0.3789)\n",
      "6356 Training Loss: tensor(0.3767)\n",
      "6357 Training Loss: tensor(0.3785)\n",
      "6358 Training Loss: tensor(0.3788)\n",
      "6359 Training Loss: tensor(0.3747)\n",
      "6360 Training Loss: tensor(0.3773)\n",
      "6361 Training Loss: tensor(0.3802)\n",
      "6362 Training Loss: tensor(0.3735)\n",
      "6363 Training Loss: tensor(0.3800)\n",
      "6364 Training Loss: tensor(0.3774)\n",
      "6365 Training Loss: tensor(0.3799)\n",
      "6366 Training Loss: tensor(0.3743)\n",
      "6367 Training Loss: tensor(0.3769)\n",
      "6368 Training Loss: tensor(0.3735)\n",
      "6369 Training Loss: tensor(0.3755)\n",
      "6370 Training Loss: tensor(0.3736)\n",
      "6371 Training Loss: tensor(0.3771)\n",
      "6372 Training Loss: tensor(0.3772)\n",
      "6373 Training Loss: tensor(0.3768)\n",
      "6374 Training Loss: tensor(0.3771)\n",
      "6375 Training Loss: tensor(0.3743)\n",
      "6376 Training Loss: tensor(0.3747)\n",
      "6377 Training Loss: tensor(0.3757)\n",
      "6378 Training Loss: tensor(0.3757)\n",
      "6379 Training Loss: tensor(0.3725)\n",
      "6380 Training Loss: tensor(0.3817)\n",
      "6381 Training Loss: tensor(0.3729)\n",
      "6382 Training Loss: tensor(0.3736)\n",
      "6383 Training Loss: tensor(0.3768)\n",
      "6384 Training Loss: tensor(0.3763)\n",
      "6385 Training Loss: tensor(0.3747)\n",
      "6386 Training Loss: tensor(0.3768)\n",
      "6387 Training Loss: tensor(0.3733)\n",
      "6388 Training Loss: tensor(0.3762)\n",
      "6389 Training Loss: tensor(0.3793)\n",
      "6390 Training Loss: tensor(0.3748)\n",
      "6391 Training Loss: tensor(0.3756)\n",
      "6392 Training Loss: tensor(0.3740)\n",
      "6393 Training Loss: tensor(0.3750)\n",
      "6394 Training Loss: tensor(0.3769)\n",
      "6395 Training Loss: tensor(0.3793)\n",
      "6396 Training Loss: tensor(0.3725)\n",
      "6397 Training Loss: tensor(0.3730)\n",
      "6398 Training Loss: tensor(0.3772)\n",
      "6399 Training Loss: tensor(0.3737)\n",
      "6400 Training Loss: tensor(0.3711)\n",
      "6401 Training Loss: tensor(0.3777)\n",
      "6402 Training Loss: tensor(0.3779)\n",
      "6403 Training Loss: tensor(0.3732)\n",
      "6404 Training Loss: tensor(0.3749)\n",
      "6405 Training Loss: tensor(0.3747)\n",
      "6406 Training Loss: tensor(0.3761)\n",
      "6407 Training Loss: tensor(0.3761)\n",
      "6408 Training Loss: tensor(0.3814)\n",
      "6409 Training Loss: tensor(0.3753)\n",
      "6410 Training Loss: tensor(0.3794)\n",
      "6411 Training Loss: tensor(0.3765)\n",
      "6412 Training Loss: tensor(0.3766)\n",
      "6413 Training Loss: tensor(0.3778)\n",
      "6414 Training Loss: tensor(0.3775)\n",
      "6415 Training Loss: tensor(0.3742)\n",
      "6416 Training Loss: tensor(0.3777)\n",
      "6417 Training Loss: tensor(0.3792)\n",
      "6418 Training Loss: tensor(0.3730)\n",
      "6419 Training Loss: tensor(0.3816)\n",
      "6420 Training Loss: tensor(0.3778)\n",
      "6421 Training Loss: tensor(0.3758)\n",
      "6422 Training Loss: tensor(0.3741)\n",
      "6423 Training Loss: tensor(0.3738)\n",
      "6424 Training Loss: tensor(0.3750)\n",
      "6425 Training Loss: tensor(0.3758)\n",
      "6426 Training Loss: tensor(0.3757)\n",
      "6427 Training Loss: tensor(0.3757)\n",
      "6428 Training Loss: tensor(0.3816)\n",
      "6429 Training Loss: tensor(0.3784)\n",
      "6430 Training Loss: tensor(0.3773)\n",
      "6431 Training Loss: tensor(0.3749)\n",
      "6432 Training Loss: tensor(0.3750)\n",
      "6433 Training Loss: tensor(0.3767)\n",
      "6434 Training Loss: tensor(0.3770)\n",
      "6435 Training Loss: tensor(0.3760)\n",
      "6436 Training Loss: tensor(0.3779)\n",
      "6437 Training Loss: tensor(0.3754)\n",
      "6438 Training Loss: tensor(0.3785)\n",
      "6439 Training Loss: tensor(0.3738)\n",
      "6440 Training Loss: tensor(0.3742)\n",
      "6441 Training Loss: tensor(0.3774)\n",
      "6442 Training Loss: tensor(0.3746)\n",
      "6443 Training Loss: tensor(0.3751)\n",
      "6444 Training Loss: tensor(0.3773)\n",
      "6445 Training Loss: tensor(0.3735)\n",
      "6446 Training Loss: tensor(0.3789)\n",
      "6447 Training Loss: tensor(0.3766)\n",
      "6448 Training Loss: tensor(0.3793)\n",
      "6449 Training Loss: tensor(0.3834)\n",
      "6450 Training Loss: tensor(0.3742)\n",
      "6451 Training Loss: tensor(0.3730)\n",
      "6452 Training Loss: tensor(0.3744)\n",
      "6453 Training Loss: tensor(0.3732)\n",
      "6454 Training Loss: tensor(0.3708)\n",
      "6455 Training Loss: tensor(0.3787)\n",
      "6456 Training Loss: tensor(0.3813)\n",
      "6457 Training Loss: tensor(0.3757)\n",
      "6458 Training Loss: tensor(0.3796)\n",
      "6459 Training Loss: tensor(0.3752)\n",
      "6460 Training Loss: tensor(0.3752)\n",
      "6461 Training Loss: tensor(0.3756)\n",
      "6462 Training Loss: tensor(0.3731)\n",
      "6463 Training Loss: tensor(0.3738)\n",
      "6464 Training Loss: tensor(0.3818)\n",
      "6465 Training Loss: tensor(0.3769)\n",
      "6466 Training Loss: tensor(0.3758)\n",
      "6467 Training Loss: tensor(0.3778)\n",
      "6468 Training Loss: tensor(0.3765)\n",
      "6469 Training Loss: tensor(0.3764)\n",
      "6470 Training Loss: tensor(0.3805)\n",
      "6471 Training Loss: tensor(0.3733)\n",
      "6472 Training Loss: tensor(0.3799)\n",
      "6473 Training Loss: tensor(0.3796)\n",
      "6474 Training Loss: tensor(0.3738)\n",
      "6475 Training Loss: tensor(0.3755)\n",
      "6476 Training Loss: tensor(0.3750)\n",
      "6477 Training Loss: tensor(0.3747)\n",
      "6478 Training Loss: tensor(0.3747)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6479 Training Loss: tensor(0.3797)\n",
      "6480 Training Loss: tensor(0.3786)\n",
      "6481 Training Loss: tensor(0.3742)\n",
      "6482 Training Loss: tensor(0.3759)\n",
      "6483 Training Loss: tensor(0.3787)\n",
      "6484 Training Loss: tensor(0.3737)\n",
      "6485 Training Loss: tensor(0.3765)\n",
      "6486 Training Loss: tensor(0.3747)\n",
      "6487 Training Loss: tensor(0.3771)\n",
      "6488 Training Loss: tensor(0.3753)\n",
      "6489 Training Loss: tensor(0.3745)\n",
      "6490 Training Loss: tensor(0.3750)\n",
      "6491 Training Loss: tensor(0.3731)\n",
      "6492 Training Loss: tensor(0.3749)\n",
      "6493 Training Loss: tensor(0.3785)\n",
      "6494 Training Loss: tensor(0.3771)\n",
      "6495 Training Loss: tensor(0.3716)\n",
      "6496 Training Loss: tensor(0.3822)\n",
      "6497 Training Loss: tensor(0.3768)\n",
      "6498 Training Loss: tensor(0.3769)\n",
      "6499 Training Loss: tensor(0.3815)\n",
      "6500 Training Loss: tensor(0.3742)\n",
      "6501 Training Loss: tensor(0.3725)\n",
      "6502 Training Loss: tensor(0.3754)\n",
      "6503 Training Loss: tensor(0.3756)\n",
      "6504 Training Loss: tensor(0.3804)\n",
      "6505 Training Loss: tensor(0.3744)\n",
      "6506 Training Loss: tensor(0.3768)\n",
      "6507 Training Loss: tensor(0.3762)\n",
      "6508 Training Loss: tensor(0.3753)\n",
      "6509 Training Loss: tensor(0.3716)\n",
      "6510 Training Loss: tensor(0.3733)\n",
      "6511 Training Loss: tensor(0.3741)\n",
      "6512 Training Loss: tensor(0.3771)\n",
      "6513 Training Loss: tensor(0.3774)\n",
      "6514 Training Loss: tensor(0.3746)\n",
      "6515 Training Loss: tensor(0.3769)\n",
      "6516 Training Loss: tensor(0.3777)\n",
      "6517 Training Loss: tensor(0.3767)\n",
      "6518 Training Loss: tensor(0.3750)\n",
      "6519 Training Loss: tensor(0.3761)\n",
      "6520 Training Loss: tensor(0.3746)\n",
      "6521 Training Loss: tensor(0.3746)\n",
      "6522 Training Loss: tensor(0.3782)\n",
      "6523 Training Loss: tensor(0.3735)\n",
      "6524 Training Loss: tensor(0.3762)\n",
      "6525 Training Loss: tensor(0.3761)\n",
      "6526 Training Loss: tensor(0.3735)\n",
      "6527 Training Loss: tensor(0.3788)\n",
      "6528 Training Loss: tensor(0.3796)\n",
      "6529 Training Loss: tensor(0.3731)\n",
      "6530 Training Loss: tensor(0.3745)\n",
      "6531 Training Loss: tensor(0.3759)\n",
      "6532 Training Loss: tensor(0.3760)\n",
      "6533 Training Loss: tensor(0.3808)\n",
      "6534 Training Loss: tensor(0.3804)\n",
      "6535 Training Loss: tensor(0.3759)\n",
      "6536 Training Loss: tensor(0.3752)\n",
      "6537 Training Loss: tensor(0.3731)\n",
      "6538 Training Loss: tensor(0.3763)\n",
      "6539 Training Loss: tensor(0.3741)\n",
      "6540 Training Loss: tensor(0.3774)\n",
      "6541 Training Loss: tensor(0.3754)\n",
      "6542 Training Loss: tensor(0.3763)\n",
      "6543 Training Loss: tensor(0.3799)\n",
      "6544 Training Loss: tensor(0.3770)\n",
      "6545 Training Loss: tensor(0.3820)\n",
      "6546 Training Loss: tensor(0.3777)\n",
      "6547 Training Loss: tensor(0.3757)\n",
      "6548 Training Loss: tensor(0.3814)\n",
      "6549 Training Loss: tensor(0.3770)\n",
      "6550 Training Loss: tensor(0.3782)\n",
      "6551 Training Loss: tensor(0.3775)\n",
      "6552 Training Loss: tensor(0.3770)\n",
      "6553 Training Loss: tensor(0.3731)\n",
      "6554 Training Loss: tensor(0.3740)\n",
      "6555 Training Loss: tensor(0.3790)\n",
      "6556 Training Loss: tensor(0.3758)\n",
      "6557 Training Loss: tensor(0.3757)\n",
      "6558 Training Loss: tensor(0.3758)\n",
      "6559 Training Loss: tensor(0.3810)\n",
      "6560 Training Loss: tensor(0.3756)\n",
      "6561 Training Loss: tensor(0.3757)\n",
      "6562 Training Loss: tensor(0.3766)\n",
      "6563 Training Loss: tensor(0.3789)\n",
      "6564 Training Loss: tensor(0.3764)\n",
      "6565 Training Loss: tensor(0.3736)\n",
      "6566 Training Loss: tensor(0.3788)\n",
      "6567 Training Loss: tensor(0.3803)\n",
      "6568 Training Loss: tensor(0.3758)\n",
      "6569 Training Loss: tensor(0.3733)\n",
      "6570 Training Loss: tensor(0.3751)\n",
      "6571 Training Loss: tensor(0.3807)\n",
      "6572 Training Loss: tensor(0.3749)\n",
      "6573 Training Loss: tensor(0.3746)\n",
      "6574 Training Loss: tensor(0.3729)\n",
      "6575 Training Loss: tensor(0.3783)\n",
      "6576 Training Loss: tensor(0.3752)\n",
      "6577 Training Loss: tensor(0.3735)\n",
      "6578 Training Loss: tensor(0.3776)\n",
      "6579 Training Loss: tensor(0.3741)\n",
      "6580 Training Loss: tensor(0.3790)\n",
      "6581 Training Loss: tensor(0.3809)\n",
      "6582 Training Loss: tensor(0.3805)\n",
      "6583 Training Loss: tensor(0.3737)\n",
      "6584 Training Loss: tensor(0.3742)\n",
      "6585 Training Loss: tensor(0.3768)\n",
      "6586 Training Loss: tensor(0.3764)\n",
      "6587 Training Loss: tensor(0.3726)\n",
      "6588 Training Loss: tensor(0.3800)\n",
      "6589 Training Loss: tensor(0.3733)\n",
      "6590 Training Loss: tensor(0.3749)\n",
      "6591 Training Loss: tensor(0.3757)\n",
      "6592 Training Loss: tensor(0.3715)\n",
      "6593 Training Loss: tensor(0.3744)\n",
      "6594 Training Loss: tensor(0.3794)\n",
      "6595 Training Loss: tensor(0.3786)\n",
      "6596 Training Loss: tensor(0.3765)\n",
      "6597 Training Loss: tensor(0.3797)\n",
      "6598 Training Loss: tensor(0.3739)\n",
      "6599 Training Loss: tensor(0.3751)\n",
      "6600 Training Loss: tensor(0.3771)\n",
      "6601 Training Loss: tensor(0.3825)\n",
      "6602 Training Loss: tensor(0.3767)\n",
      "6603 Training Loss: tensor(0.3741)\n",
      "6604 Training Loss: tensor(0.3743)\n",
      "6605 Training Loss: tensor(0.3825)\n",
      "6606 Training Loss: tensor(0.3764)\n",
      "6607 Training Loss: tensor(0.3794)\n",
      "6608 Training Loss: tensor(0.3810)\n",
      "6609 Training Loss: tensor(0.3725)\n",
      "6610 Training Loss: tensor(0.3752)\n",
      "6611 Training Loss: tensor(0.3731)\n",
      "6612 Training Loss: tensor(0.3794)\n",
      "6613 Training Loss: tensor(0.3766)\n",
      "6614 Training Loss: tensor(0.3759)\n",
      "6615 Training Loss: tensor(0.3732)\n",
      "6616 Training Loss: tensor(0.3749)\n",
      "6617 Training Loss: tensor(0.3734)\n",
      "6618 Training Loss: tensor(0.3727)\n",
      "6619 Training Loss: tensor(0.3785)\n",
      "6620 Training Loss: tensor(0.3766)\n",
      "6621 Training Loss: tensor(0.3764)\n",
      "6622 Training Loss: tensor(0.3759)\n",
      "6623 Training Loss: tensor(0.3768)\n",
      "6624 Training Loss: tensor(0.3801)\n",
      "6625 Training Loss: tensor(0.3767)\n",
      "6626 Training Loss: tensor(0.3734)\n",
      "6627 Training Loss: tensor(0.3793)\n",
      "6628 Training Loss: tensor(0.3758)\n",
      "6629 Training Loss: tensor(0.3698)\n",
      "6630 Training Loss: tensor(0.3758)\n",
      "6631 Training Loss: tensor(0.3738)\n",
      "6632 Training Loss: tensor(0.3743)\n",
      "6633 Training Loss: tensor(0.3717)\n",
      "6634 Training Loss: tensor(0.3746)\n",
      "6635 Training Loss: tensor(0.3762)\n",
      "6636 Training Loss: tensor(0.3766)\n",
      "6637 Training Loss: tensor(0.3814)\n",
      "6638 Training Loss: tensor(0.3798)\n",
      "6639 Training Loss: tensor(0.3795)\n",
      "6640 Training Loss: tensor(0.3762)\n",
      "6641 Training Loss: tensor(0.3723)\n",
      "6642 Training Loss: tensor(0.3747)\n",
      "6643 Training Loss: tensor(0.3785)\n",
      "6644 Training Loss: tensor(0.3774)\n",
      "6645 Training Loss: tensor(0.3736)\n",
      "6646 Training Loss: tensor(0.3735)\n",
      "6647 Training Loss: tensor(0.3769)\n",
      "6648 Training Loss: tensor(0.3754)\n",
      "6649 Training Loss: tensor(0.3747)\n",
      "6650 Training Loss: tensor(0.3730)\n",
      "6651 Training Loss: tensor(0.3744)\n",
      "6652 Training Loss: tensor(0.3743)\n",
      "6653 Training Loss: tensor(0.3750)\n",
      "6654 Training Loss: tensor(0.3751)\n",
      "6655 Training Loss: tensor(0.3753)\n",
      "6656 Training Loss: tensor(0.3789)\n",
      "6657 Training Loss: tensor(0.3743)\n",
      "6658 Training Loss: tensor(0.3720)\n",
      "6659 Training Loss: tensor(0.3766)\n",
      "6660 Training Loss: tensor(0.3760)\n",
      "6661 Training Loss: tensor(0.3764)\n",
      "6662 Training Loss: tensor(0.3791)\n",
      "6663 Training Loss: tensor(0.3851)\n",
      "6664 Training Loss: tensor(0.3725)\n",
      "6665 Training Loss: tensor(0.3766)\n",
      "6666 Training Loss: tensor(0.3764)\n",
      "6667 Training Loss: tensor(0.3768)\n",
      "6668 Training Loss: tensor(0.3763)\n",
      "6669 Training Loss: tensor(0.3736)\n",
      "6670 Training Loss: tensor(0.3789)\n",
      "6671 Training Loss: tensor(0.3722)\n",
      "6672 Training Loss: tensor(0.3751)\n",
      "6673 Training Loss: tensor(0.3748)\n",
      "6674 Training Loss: tensor(0.3749)\n",
      "6675 Training Loss: tensor(0.3780)\n",
      "6676 Training Loss: tensor(0.3765)\n",
      "6677 Training Loss: tensor(0.3782)\n",
      "6678 Training Loss: tensor(0.3771)\n",
      "6679 Training Loss: tensor(0.3747)\n",
      "6680 Training Loss: tensor(0.3750)\n",
      "6681 Training Loss: tensor(0.3755)\n",
      "6682 Training Loss: tensor(0.3764)\n",
      "6683 Training Loss: tensor(0.3744)\n",
      "6684 Training Loss: tensor(0.3773)\n",
      "6685 Training Loss: tensor(0.3773)\n",
      "6686 Training Loss: tensor(0.3787)\n",
      "6687 Training Loss: tensor(0.3755)\n",
      "6688 Training Loss: tensor(0.3755)\n",
      "6689 Training Loss: tensor(0.3765)\n",
      "6690 Training Loss: tensor(0.3744)\n",
      "6691 Training Loss: tensor(0.3761)\n",
      "6692 Training Loss: tensor(0.3730)\n",
      "6693 Training Loss: tensor(0.3745)\n",
      "6694 Training Loss: tensor(0.3777)\n",
      "6695 Training Loss: tensor(0.3809)\n",
      "6696 Training Loss: tensor(0.3746)\n",
      "6697 Training Loss: tensor(0.3796)\n",
      "6698 Training Loss: tensor(0.3756)\n",
      "6699 Training Loss: tensor(0.3772)\n",
      "6700 Training Loss: tensor(0.3737)\n",
      "6701 Training Loss: tensor(0.3778)\n",
      "6702 Training Loss: tensor(0.3723)\n",
      "6703 Training Loss: tensor(0.3755)\n",
      "6704 Training Loss: tensor(0.3744)\n",
      "6705 Training Loss: tensor(0.3769)\n",
      "6706 Training Loss: tensor(0.3792)\n",
      "6707 Training Loss: tensor(0.3780)\n",
      "6708 Training Loss: tensor(0.3768)\n",
      "6709 Training Loss: tensor(0.3775)\n",
      "6710 Training Loss: tensor(0.3748)\n",
      "6711 Training Loss: tensor(0.3764)\n",
      "6712 Training Loss: tensor(0.3796)\n",
      "6713 Training Loss: tensor(0.3773)\n",
      "6714 Training Loss: tensor(0.3800)\n",
      "6715 Training Loss: tensor(0.3766)\n",
      "6716 Training Loss: tensor(0.3768)\n",
      "6717 Training Loss: tensor(0.3733)\n",
      "6718 Training Loss: tensor(0.3766)\n",
      "6719 Training Loss: tensor(0.3726)\n",
      "6720 Training Loss: tensor(0.3738)\n",
      "6721 Training Loss: tensor(0.3773)\n",
      "6722 Training Loss: tensor(0.3754)\n",
      "6723 Training Loss: tensor(0.3768)\n",
      "6724 Training Loss: tensor(0.3795)\n",
      "6725 Training Loss: tensor(0.3727)\n",
      "6726 Training Loss: tensor(0.3762)\n",
      "6727 Training Loss: tensor(0.3751)\n",
      "6728 Training Loss: tensor(0.3786)\n",
      "6729 Training Loss: tensor(0.3760)\n",
      "6730 Training Loss: tensor(0.3714)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6731 Training Loss: tensor(0.3762)\n",
      "6732 Training Loss: tensor(0.3751)\n",
      "6733 Training Loss: tensor(0.3741)\n",
      "6734 Training Loss: tensor(0.3724)\n",
      "6735 Training Loss: tensor(0.3725)\n",
      "6736 Training Loss: tensor(0.3792)\n",
      "6737 Training Loss: tensor(0.3726)\n",
      "6738 Training Loss: tensor(0.3779)\n",
      "6739 Training Loss: tensor(0.3787)\n",
      "6740 Training Loss: tensor(0.3777)\n",
      "6741 Training Loss: tensor(0.3787)\n",
      "6742 Training Loss: tensor(0.3732)\n",
      "6743 Training Loss: tensor(0.3734)\n",
      "6744 Training Loss: tensor(0.3800)\n",
      "6745 Training Loss: tensor(0.3776)\n",
      "6746 Training Loss: tensor(0.3820)\n",
      "6747 Training Loss: tensor(0.3722)\n",
      "6748 Training Loss: tensor(0.3759)\n",
      "6749 Training Loss: tensor(0.3747)\n",
      "6750 Training Loss: tensor(0.3766)\n",
      "6751 Training Loss: tensor(0.3753)\n",
      "6752 Training Loss: tensor(0.3737)\n",
      "6753 Training Loss: tensor(0.3754)\n",
      "6754 Training Loss: tensor(0.3726)\n",
      "6755 Training Loss: tensor(0.3761)\n",
      "6756 Training Loss: tensor(0.3754)\n",
      "6757 Training Loss: tensor(0.3789)\n",
      "6758 Training Loss: tensor(0.3768)\n",
      "6759 Training Loss: tensor(0.3760)\n",
      "6760 Training Loss: tensor(0.3759)\n",
      "6761 Training Loss: tensor(0.3717)\n",
      "6762 Training Loss: tensor(0.3732)\n",
      "6763 Training Loss: tensor(0.3781)\n",
      "6764 Training Loss: tensor(0.3771)\n",
      "6765 Training Loss: tensor(0.3771)\n",
      "6766 Training Loss: tensor(0.3752)\n",
      "6767 Training Loss: tensor(0.3759)\n",
      "6768 Training Loss: tensor(0.3754)\n",
      "6769 Training Loss: tensor(0.3745)\n",
      "6770 Training Loss: tensor(0.3818)\n",
      "6771 Training Loss: tensor(0.3751)\n",
      "6772 Training Loss: tensor(0.3769)\n",
      "6773 Training Loss: tensor(0.3786)\n",
      "6774 Training Loss: tensor(0.3749)\n",
      "6775 Training Loss: tensor(0.3748)\n",
      "6776 Training Loss: tensor(0.3786)\n",
      "6777 Training Loss: tensor(0.3753)\n",
      "6778 Training Loss: tensor(0.3764)\n",
      "6779 Training Loss: tensor(0.3739)\n",
      "6780 Training Loss: tensor(0.3808)\n",
      "6781 Training Loss: tensor(0.3772)\n",
      "6782 Training Loss: tensor(0.3738)\n",
      "6783 Training Loss: tensor(0.3768)\n",
      "6784 Training Loss: tensor(0.3738)\n",
      "6785 Training Loss: tensor(0.3762)\n",
      "6786 Training Loss: tensor(0.3760)\n",
      "6787 Training Loss: tensor(0.3735)\n",
      "6788 Training Loss: tensor(0.3760)\n",
      "6789 Training Loss: tensor(0.3738)\n",
      "6790 Training Loss: tensor(0.3740)\n",
      "6791 Training Loss: tensor(0.3732)\n",
      "6792 Training Loss: tensor(0.3743)\n",
      "6793 Training Loss: tensor(0.3725)\n",
      "6794 Training Loss: tensor(0.3743)\n",
      "6795 Training Loss: tensor(0.3771)\n",
      "6796 Training Loss: tensor(0.3792)\n",
      "6797 Training Loss: tensor(0.3722)\n",
      "6798 Training Loss: tensor(0.3711)\n",
      "6799 Training Loss: tensor(0.3771)\n",
      "6800 Training Loss: tensor(0.3729)\n",
      "6801 Training Loss: tensor(0.3776)\n",
      "6802 Training Loss: tensor(0.3740)\n",
      "6803 Training Loss: tensor(0.3757)\n",
      "6804 Training Loss: tensor(0.3734)\n",
      "6805 Training Loss: tensor(0.3801)\n",
      "6806 Training Loss: tensor(0.3794)\n",
      "6807 Training Loss: tensor(0.3749)\n",
      "6808 Training Loss: tensor(0.3712)\n",
      "6809 Training Loss: tensor(0.3750)\n",
      "6810 Training Loss: tensor(0.3779)\n",
      "6811 Training Loss: tensor(0.3792)\n",
      "6812 Training Loss: tensor(0.3771)\n",
      "6813 Training Loss: tensor(0.3731)\n",
      "6814 Training Loss: tensor(0.3787)\n",
      "6815 Training Loss: tensor(0.3764)\n",
      "6816 Training Loss: tensor(0.3757)\n",
      "6817 Training Loss: tensor(0.3782)\n",
      "6818 Training Loss: tensor(0.3784)\n",
      "6819 Training Loss: tensor(0.3737)\n",
      "6820 Training Loss: tensor(0.3744)\n",
      "6821 Training Loss: tensor(0.3730)\n",
      "6822 Training Loss: tensor(0.3741)\n",
      "6823 Training Loss: tensor(0.3770)\n",
      "6824 Training Loss: tensor(0.3752)\n",
      "6825 Training Loss: tensor(0.3731)\n",
      "6826 Training Loss: tensor(0.3742)\n",
      "6827 Training Loss: tensor(0.3773)\n",
      "6828 Training Loss: tensor(0.3731)\n",
      "6829 Training Loss: tensor(0.3729)\n",
      "6830 Training Loss: tensor(0.3758)\n",
      "6831 Training Loss: tensor(0.3730)\n",
      "6832 Training Loss: tensor(0.3735)\n",
      "6833 Training Loss: tensor(0.3771)\n",
      "6834 Training Loss: tensor(0.3740)\n",
      "6835 Training Loss: tensor(0.3735)\n",
      "6836 Training Loss: tensor(0.3770)\n",
      "6837 Training Loss: tensor(0.3756)\n",
      "6838 Training Loss: tensor(0.3778)\n",
      "6839 Training Loss: tensor(0.3798)\n",
      "6840 Training Loss: tensor(0.3802)\n",
      "6841 Training Loss: tensor(0.3758)\n",
      "6842 Training Loss: tensor(0.3728)\n",
      "6843 Training Loss: tensor(0.3784)\n",
      "6844 Training Loss: tensor(0.3823)\n",
      "6845 Training Loss: tensor(0.3717)\n",
      "6846 Training Loss: tensor(0.3728)\n",
      "6847 Training Loss: tensor(0.3734)\n",
      "6848 Training Loss: tensor(0.3777)\n",
      "6849 Training Loss: tensor(0.3752)\n",
      "6850 Training Loss: tensor(0.3763)\n",
      "6851 Training Loss: tensor(0.3800)\n",
      "6852 Training Loss: tensor(0.3728)\n",
      "6853 Training Loss: tensor(0.3746)\n",
      "6854 Training Loss: tensor(0.3733)\n",
      "6855 Training Loss: tensor(0.3768)\n",
      "6856 Training Loss: tensor(0.3776)\n",
      "6857 Training Loss: tensor(0.3772)\n",
      "6858 Training Loss: tensor(0.3768)\n",
      "6859 Training Loss: tensor(0.3752)\n",
      "6860 Training Loss: tensor(0.3755)\n",
      "6861 Training Loss: tensor(0.3720)\n",
      "6862 Training Loss: tensor(0.3759)\n",
      "6863 Training Loss: tensor(0.3792)\n",
      "6864 Training Loss: tensor(0.3755)\n",
      "6865 Training Loss: tensor(0.3740)\n",
      "6866 Training Loss: tensor(0.3723)\n",
      "6867 Training Loss: tensor(0.3767)\n",
      "6868 Training Loss: tensor(0.3753)\n",
      "6869 Training Loss: tensor(0.3760)\n",
      "6870 Training Loss: tensor(0.3758)\n",
      "6871 Training Loss: tensor(0.3771)\n",
      "6872 Training Loss: tensor(0.3764)\n",
      "6873 Training Loss: tensor(0.3771)\n",
      "6874 Training Loss: tensor(0.3744)\n",
      "6875 Training Loss: tensor(0.3709)\n",
      "6876 Training Loss: tensor(0.3742)\n",
      "6877 Training Loss: tensor(0.3744)\n",
      "6878 Training Loss: tensor(0.3717)\n",
      "6879 Training Loss: tensor(0.3770)\n",
      "6880 Training Loss: tensor(0.3789)\n",
      "6881 Training Loss: tensor(0.3765)\n",
      "6882 Training Loss: tensor(0.3753)\n",
      "6883 Training Loss: tensor(0.3752)\n",
      "6884 Training Loss: tensor(0.3731)\n",
      "6885 Training Loss: tensor(0.3725)\n",
      "6886 Training Loss: tensor(0.3721)\n",
      "6887 Training Loss: tensor(0.3723)\n",
      "6888 Training Loss: tensor(0.3745)\n",
      "6889 Training Loss: tensor(0.3755)\n",
      "6890 Training Loss: tensor(0.3736)\n",
      "6891 Training Loss: tensor(0.3778)\n",
      "6892 Training Loss: tensor(0.3719)\n",
      "6893 Training Loss: tensor(0.3749)\n",
      "6894 Training Loss: tensor(0.3845)\n",
      "6895 Training Loss: tensor(0.3735)\n",
      "6896 Training Loss: tensor(0.3805)\n",
      "6897 Training Loss: tensor(0.3758)\n",
      "6898 Training Loss: tensor(0.3760)\n",
      "6899 Training Loss: tensor(0.3799)\n",
      "6900 Training Loss: tensor(0.3734)\n",
      "6901 Training Loss: tensor(0.3729)\n",
      "6902 Training Loss: tensor(0.3751)\n",
      "6903 Training Loss: tensor(0.3743)\n",
      "6904 Training Loss: tensor(0.3791)\n",
      "6905 Training Loss: tensor(0.3774)\n",
      "6906 Training Loss: tensor(0.3721)\n",
      "6907 Training Loss: tensor(0.3738)\n",
      "6908 Training Loss: tensor(0.3765)\n",
      "6909 Training Loss: tensor(0.3753)\n",
      "6910 Training Loss: tensor(0.3740)\n",
      "6911 Training Loss: tensor(0.3760)\n",
      "6912 Training Loss: tensor(0.3797)\n",
      "6913 Training Loss: tensor(0.3739)\n",
      "6914 Training Loss: tensor(0.3722)\n",
      "6915 Training Loss: tensor(0.3780)\n",
      "6916 Training Loss: tensor(0.3756)\n",
      "6917 Training Loss: tensor(0.3706)\n",
      "6918 Training Loss: tensor(0.3786)\n",
      "6919 Training Loss: tensor(0.3760)\n",
      "6920 Training Loss: tensor(0.3745)\n",
      "6921 Training Loss: tensor(0.3764)\n",
      "6922 Training Loss: tensor(0.3755)\n",
      "6923 Training Loss: tensor(0.3749)\n",
      "6924 Training Loss: tensor(0.3725)\n",
      "6925 Training Loss: tensor(0.3768)\n",
      "6926 Training Loss: tensor(0.3806)\n",
      "6927 Training Loss: tensor(0.3748)\n",
      "6928 Training Loss: tensor(0.3775)\n",
      "6929 Training Loss: tensor(0.3731)\n",
      "6930 Training Loss: tensor(0.3724)\n",
      "6931 Training Loss: tensor(0.3784)\n",
      "6932 Training Loss: tensor(0.3739)\n",
      "6933 Training Loss: tensor(0.3785)\n",
      "6934 Training Loss: tensor(0.3736)\n",
      "6935 Training Loss: tensor(0.3813)\n",
      "6936 Training Loss: tensor(0.3783)\n",
      "6937 Training Loss: tensor(0.3725)\n",
      "6938 Training Loss: tensor(0.3786)\n",
      "6939 Training Loss: tensor(0.3740)\n",
      "6940 Training Loss: tensor(0.3754)\n",
      "6941 Training Loss: tensor(0.3749)\n",
      "6942 Training Loss: tensor(0.3745)\n",
      "6943 Training Loss: tensor(0.3759)\n",
      "6944 Training Loss: tensor(0.3764)\n",
      "6945 Training Loss: tensor(0.3777)\n",
      "6946 Training Loss: tensor(0.3764)\n",
      "6947 Training Loss: tensor(0.3747)\n",
      "6948 Training Loss: tensor(0.3747)\n",
      "6949 Training Loss: tensor(0.3702)\n",
      "6950 Training Loss: tensor(0.3789)\n",
      "6951 Training Loss: tensor(0.3764)\n",
      "6952 Training Loss: tensor(0.3764)\n",
      "6953 Training Loss: tensor(0.3788)\n",
      "6954 Training Loss: tensor(0.3757)\n",
      "6955 Training Loss: tensor(0.3737)\n",
      "6956 Training Loss: tensor(0.3786)\n",
      "6957 Training Loss: tensor(0.3759)\n",
      "6958 Training Loss: tensor(0.3736)\n",
      "6959 Training Loss: tensor(0.3730)\n",
      "6960 Training Loss: tensor(0.3749)\n",
      "6961 Training Loss: tensor(0.3734)\n",
      "6962 Training Loss: tensor(0.3765)\n",
      "6963 Training Loss: tensor(0.3738)\n",
      "6964 Training Loss: tensor(0.3752)\n",
      "6965 Training Loss: tensor(0.3741)\n",
      "6966 Training Loss: tensor(0.3722)\n",
      "6967 Training Loss: tensor(0.3721)\n",
      "6968 Training Loss: tensor(0.3793)\n",
      "6969 Training Loss: tensor(0.3771)\n",
      "6970 Training Loss: tensor(0.3803)\n",
      "6971 Training Loss: tensor(0.3730)\n",
      "6972 Training Loss: tensor(0.3753)\n",
      "6973 Training Loss: tensor(0.3736)\n",
      "6974 Training Loss: tensor(0.3743)\n",
      "6975 Training Loss: tensor(0.3764)\n",
      "6976 Training Loss: tensor(0.3736)\n",
      "6977 Training Loss: tensor(0.3747)\n",
      "6978 Training Loss: tensor(0.3736)\n",
      "6979 Training Loss: tensor(0.3801)\n",
      "6980 Training Loss: tensor(0.3757)\n",
      "6981 Training Loss: tensor(0.3812)\n",
      "6982 Training Loss: tensor(0.3742)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6983 Training Loss: tensor(0.3743)\n",
      "6984 Training Loss: tensor(0.3712)\n",
      "6985 Training Loss: tensor(0.3708)\n",
      "6986 Training Loss: tensor(0.3738)\n",
      "6987 Training Loss: tensor(0.3764)\n",
      "6988 Training Loss: tensor(0.3754)\n",
      "6989 Training Loss: tensor(0.3745)\n",
      "6990 Training Loss: tensor(0.3721)\n",
      "6991 Training Loss: tensor(0.3751)\n",
      "6992 Training Loss: tensor(0.3740)\n",
      "6993 Training Loss: tensor(0.3769)\n",
      "6994 Training Loss: tensor(0.3747)\n",
      "6995 Training Loss: tensor(0.3753)\n",
      "6996 Training Loss: tensor(0.3747)\n",
      "6997 Training Loss: tensor(0.3838)\n",
      "6998 Training Loss: tensor(0.3724)\n",
      "6999 Training Loss: tensor(0.3738)\n",
      "7000 Training Loss: tensor(0.3749)\n",
      "7001 Training Loss: tensor(0.3756)\n",
      "7002 Training Loss: tensor(0.3766)\n",
      "7003 Training Loss: tensor(0.3776)\n",
      "7004 Training Loss: tensor(0.3758)\n",
      "7005 Training Loss: tensor(0.3792)\n",
      "7006 Training Loss: tensor(0.3776)\n",
      "7007 Training Loss: tensor(0.3740)\n",
      "7008 Training Loss: tensor(0.3791)\n",
      "7009 Training Loss: tensor(0.3747)\n",
      "7010 Training Loss: tensor(0.3735)\n",
      "7011 Training Loss: tensor(0.3757)\n",
      "7012 Training Loss: tensor(0.3762)\n",
      "7013 Training Loss: tensor(0.3777)\n",
      "7014 Training Loss: tensor(0.3746)\n",
      "7015 Training Loss: tensor(0.3760)\n",
      "7016 Training Loss: tensor(0.3738)\n",
      "7017 Training Loss: tensor(0.3724)\n",
      "7018 Training Loss: tensor(0.3796)\n",
      "7019 Training Loss: tensor(0.3734)\n",
      "7020 Training Loss: tensor(0.3778)\n",
      "7021 Training Loss: tensor(0.3794)\n",
      "7022 Training Loss: tensor(0.3771)\n",
      "7023 Training Loss: tensor(0.3772)\n",
      "7024 Training Loss: tensor(0.3761)\n",
      "7025 Training Loss: tensor(0.3733)\n",
      "7026 Training Loss: tensor(0.3747)\n",
      "7027 Training Loss: tensor(0.3731)\n",
      "7028 Training Loss: tensor(0.3749)\n",
      "7029 Training Loss: tensor(0.3737)\n",
      "7030 Training Loss: tensor(0.3745)\n",
      "7031 Training Loss: tensor(0.3766)\n",
      "7032 Training Loss: tensor(0.3754)\n",
      "7033 Training Loss: tensor(0.3807)\n",
      "7034 Training Loss: tensor(0.3737)\n",
      "7035 Training Loss: tensor(0.3767)\n",
      "7036 Training Loss: tensor(0.3762)\n",
      "7037 Training Loss: tensor(0.3724)\n",
      "7038 Training Loss: tensor(0.3805)\n",
      "7039 Training Loss: tensor(0.3716)\n",
      "7040 Training Loss: tensor(0.3745)\n",
      "7041 Training Loss: tensor(0.3695)\n",
      "7042 Training Loss: tensor(0.3775)\n",
      "7043 Training Loss: tensor(0.3711)\n",
      "7044 Training Loss: tensor(0.3720)\n",
      "7045 Training Loss: tensor(0.3816)\n",
      "7046 Training Loss: tensor(0.3748)\n",
      "7047 Training Loss: tensor(0.3706)\n",
      "7048 Training Loss: tensor(0.3809)\n",
      "7049 Training Loss: tensor(0.3716)\n",
      "7050 Training Loss: tensor(0.3727)\n",
      "7051 Training Loss: tensor(0.3739)\n",
      "7052 Training Loss: tensor(0.3732)\n",
      "7053 Training Loss: tensor(0.3758)\n",
      "7054 Training Loss: tensor(0.3798)\n",
      "7055 Training Loss: tensor(0.3766)\n",
      "7056 Training Loss: tensor(0.3766)\n",
      "7057 Training Loss: tensor(0.3728)\n",
      "7058 Training Loss: tensor(0.3756)\n",
      "7059 Training Loss: tensor(0.3727)\n",
      "7060 Training Loss: tensor(0.3760)\n",
      "7061 Training Loss: tensor(0.3775)\n",
      "7062 Training Loss: tensor(0.3736)\n",
      "7063 Training Loss: tensor(0.3753)\n",
      "7064 Training Loss: tensor(0.3729)\n",
      "7065 Training Loss: tensor(0.3721)\n",
      "7066 Training Loss: tensor(0.3756)\n",
      "7067 Training Loss: tensor(0.3738)\n",
      "7068 Training Loss: tensor(0.3767)\n",
      "7069 Training Loss: tensor(0.3718)\n",
      "7070 Training Loss: tensor(0.3786)\n",
      "7071 Training Loss: tensor(0.3756)\n",
      "7072 Training Loss: tensor(0.3708)\n",
      "7073 Training Loss: tensor(0.3726)\n",
      "7074 Training Loss: tensor(0.3733)\n",
      "7075 Training Loss: tensor(0.3744)\n",
      "7076 Training Loss: tensor(0.3740)\n",
      "7077 Training Loss: tensor(0.3731)\n",
      "7078 Training Loss: tensor(0.3778)\n",
      "7079 Training Loss: tensor(0.3783)\n",
      "7080 Training Loss: tensor(0.3737)\n",
      "7081 Training Loss: tensor(0.3759)\n",
      "7082 Training Loss: tensor(0.3743)\n",
      "7083 Training Loss: tensor(0.3738)\n",
      "7084 Training Loss: tensor(0.3776)\n",
      "7085 Training Loss: tensor(0.3718)\n",
      "7086 Training Loss: tensor(0.3733)\n",
      "7087 Training Loss: tensor(0.3815)\n",
      "7088 Training Loss: tensor(0.3775)\n",
      "7089 Training Loss: tensor(0.3731)\n",
      "7090 Training Loss: tensor(0.3818)\n",
      "7091 Training Loss: tensor(0.3788)\n",
      "7092 Training Loss: tensor(0.3755)\n",
      "7093 Training Loss: tensor(0.3711)\n",
      "7094 Training Loss: tensor(0.3725)\n",
      "7095 Training Loss: tensor(0.3729)\n",
      "7096 Training Loss: tensor(0.3748)\n",
      "7097 Training Loss: tensor(0.3746)\n",
      "7098 Training Loss: tensor(0.3765)\n",
      "7099 Training Loss: tensor(0.3735)\n",
      "7100 Training Loss: tensor(0.3720)\n",
      "7101 Training Loss: tensor(0.3752)\n",
      "7102 Training Loss: tensor(0.3752)\n",
      "7103 Training Loss: tensor(0.3739)\n",
      "7104 Training Loss: tensor(0.3777)\n",
      "7105 Training Loss: tensor(0.3730)\n",
      "7106 Training Loss: tensor(0.3753)\n",
      "7107 Training Loss: tensor(0.3773)\n",
      "7108 Training Loss: tensor(0.3801)\n",
      "7109 Training Loss: tensor(0.3751)\n",
      "7110 Training Loss: tensor(0.3783)\n",
      "7111 Training Loss: tensor(0.3806)\n",
      "7112 Training Loss: tensor(0.3739)\n",
      "7113 Training Loss: tensor(0.3741)\n",
      "7114 Training Loss: tensor(0.3757)\n",
      "7115 Training Loss: tensor(0.3742)\n",
      "7116 Training Loss: tensor(0.3765)\n",
      "7117 Training Loss: tensor(0.3742)\n",
      "7118 Training Loss: tensor(0.3742)\n",
      "7119 Training Loss: tensor(0.3788)\n",
      "7120 Training Loss: tensor(0.3746)\n",
      "7121 Training Loss: tensor(0.3718)\n",
      "7122 Training Loss: tensor(0.3715)\n",
      "7123 Training Loss: tensor(0.3754)\n",
      "7124 Training Loss: tensor(0.3768)\n",
      "7125 Training Loss: tensor(0.3752)\n",
      "7126 Training Loss: tensor(0.3742)\n",
      "7127 Training Loss: tensor(0.3740)\n",
      "7128 Training Loss: tensor(0.3761)\n",
      "7129 Training Loss: tensor(0.3742)\n",
      "7130 Training Loss: tensor(0.3810)\n",
      "7131 Training Loss: tensor(0.3714)\n",
      "7132 Training Loss: tensor(0.3712)\n",
      "7133 Training Loss: tensor(0.3809)\n",
      "7134 Training Loss: tensor(0.3711)\n",
      "7135 Training Loss: tensor(0.3712)\n",
      "7136 Training Loss: tensor(0.3734)\n",
      "7137 Training Loss: tensor(0.3746)\n",
      "7138 Training Loss: tensor(0.3794)\n",
      "7139 Training Loss: tensor(0.3749)\n",
      "7140 Training Loss: tensor(0.3748)\n",
      "7141 Training Loss: tensor(0.3811)\n",
      "7142 Training Loss: tensor(0.3800)\n",
      "7143 Training Loss: tensor(0.3728)\n",
      "7144 Training Loss: tensor(0.3734)\n",
      "7145 Training Loss: tensor(0.3733)\n",
      "7146 Training Loss: tensor(0.3776)\n",
      "7147 Training Loss: tensor(0.3764)\n",
      "7148 Training Loss: tensor(0.3761)\n",
      "7149 Training Loss: tensor(0.3724)\n",
      "7150 Training Loss: tensor(0.3756)\n",
      "7151 Training Loss: tensor(0.3758)\n",
      "7152 Training Loss: tensor(0.3740)\n",
      "7153 Training Loss: tensor(0.3740)\n",
      "7154 Training Loss: tensor(0.3744)\n",
      "7155 Training Loss: tensor(0.3736)\n",
      "7156 Training Loss: tensor(0.3759)\n",
      "7157 Training Loss: tensor(0.3748)\n",
      "7158 Training Loss: tensor(0.3748)\n",
      "7159 Training Loss: tensor(0.3756)\n",
      "7160 Training Loss: tensor(0.3731)\n",
      "7161 Training Loss: tensor(0.3777)\n",
      "7162 Training Loss: tensor(0.3762)\n",
      "7163 Training Loss: tensor(0.3771)\n",
      "7164 Training Loss: tensor(0.3722)\n",
      "7165 Training Loss: tensor(0.3716)\n",
      "7166 Training Loss: tensor(0.3744)\n",
      "7167 Training Loss: tensor(0.3735)\n",
      "7168 Training Loss: tensor(0.3753)\n",
      "7169 Training Loss: tensor(0.3770)\n",
      "7170 Training Loss: tensor(0.3774)\n",
      "7171 Training Loss: tensor(0.3741)\n",
      "7172 Training Loss: tensor(0.3746)\n",
      "7173 Training Loss: tensor(0.3764)\n",
      "7174 Training Loss: tensor(0.3749)\n",
      "7175 Training Loss: tensor(0.3740)\n",
      "7176 Training Loss: tensor(0.3758)\n",
      "7177 Training Loss: tensor(0.3738)\n",
      "7178 Training Loss: tensor(0.3754)\n",
      "7179 Training Loss: tensor(0.3724)\n",
      "7180 Training Loss: tensor(0.3787)\n",
      "7181 Training Loss: tensor(0.3726)\n",
      "7182 Training Loss: tensor(0.3715)\n",
      "7183 Training Loss: tensor(0.3734)\n",
      "7184 Training Loss: tensor(0.3750)\n",
      "7185 Training Loss: tensor(0.3737)\n",
      "7186 Training Loss: tensor(0.3717)\n",
      "7187 Training Loss: tensor(0.3762)\n",
      "7188 Training Loss: tensor(0.3724)\n",
      "7189 Training Loss: tensor(0.3710)\n",
      "7190 Training Loss: tensor(0.3728)\n",
      "7191 Training Loss: tensor(0.3746)\n",
      "7192 Training Loss: tensor(0.3771)\n",
      "7193 Training Loss: tensor(0.3748)\n",
      "7194 Training Loss: tensor(0.3795)\n",
      "7195 Training Loss: tensor(0.3715)\n",
      "7196 Training Loss: tensor(0.3774)\n",
      "7197 Training Loss: tensor(0.3725)\n",
      "7198 Training Loss: tensor(0.3735)\n",
      "7199 Training Loss: tensor(0.3745)\n",
      "7200 Training Loss: tensor(0.3771)\n",
      "7201 Training Loss: tensor(0.3775)\n",
      "7202 Training Loss: tensor(0.3742)\n",
      "7203 Training Loss: tensor(0.3760)\n",
      "7204 Training Loss: tensor(0.3776)\n",
      "7205 Training Loss: tensor(0.3737)\n",
      "7206 Training Loss: tensor(0.3720)\n",
      "7207 Training Loss: tensor(0.3743)\n",
      "7208 Training Loss: tensor(0.3754)\n",
      "7209 Training Loss: tensor(0.3724)\n",
      "7210 Training Loss: tensor(0.3754)\n",
      "7211 Training Loss: tensor(0.3787)\n",
      "7212 Training Loss: tensor(0.3780)\n",
      "7213 Training Loss: tensor(0.3753)\n",
      "7214 Training Loss: tensor(0.3740)\n",
      "7215 Training Loss: tensor(0.3735)\n",
      "7216 Training Loss: tensor(0.3732)\n",
      "7217 Training Loss: tensor(0.3746)\n",
      "7218 Training Loss: tensor(0.3751)\n",
      "7219 Training Loss: tensor(0.3751)\n",
      "7220 Training Loss: tensor(0.3715)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7221 Training Loss: tensor(0.3723)\n",
      "7222 Training Loss: tensor(0.3750)\n",
      "7223 Training Loss: tensor(0.3814)\n",
      "7224 Training Loss: tensor(0.3778)\n",
      "7225 Training Loss: tensor(0.3754)\n",
      "7226 Training Loss: tensor(0.3794)\n",
      "7227 Training Loss: tensor(0.3733)\n",
      "7228 Training Loss: tensor(0.3744)\n",
      "7229 Training Loss: tensor(0.3760)\n",
      "7230 Training Loss: tensor(0.3772)\n",
      "7231 Training Loss: tensor(0.3765)\n",
      "7232 Training Loss: tensor(0.3764)\n",
      "7233 Training Loss: tensor(0.3730)\n",
      "7234 Training Loss: tensor(0.3730)\n",
      "7235 Training Loss: tensor(0.3741)\n",
      "7236 Training Loss: tensor(0.3765)\n",
      "7237 Training Loss: tensor(0.3753)\n",
      "7238 Training Loss: tensor(0.3720)\n",
      "7239 Training Loss: tensor(0.3740)\n",
      "7240 Training Loss: tensor(0.3742)\n",
      "7241 Training Loss: tensor(0.3774)\n",
      "7242 Training Loss: tensor(0.3727)\n",
      "7243 Training Loss: tensor(0.3737)\n",
      "7244 Training Loss: tensor(0.3741)\n",
      "7245 Training Loss: tensor(0.3777)\n",
      "7246 Training Loss: tensor(0.3757)\n",
      "7247 Training Loss: tensor(0.3710)\n",
      "7248 Training Loss: tensor(0.3742)\n",
      "7249 Training Loss: tensor(0.3773)\n",
      "7250 Training Loss: tensor(0.3753)\n",
      "7251 Training Loss: tensor(0.3761)\n",
      "7252 Training Loss: tensor(0.3734)\n",
      "7253 Training Loss: tensor(0.3747)\n",
      "7254 Training Loss: tensor(0.3778)\n",
      "7255 Training Loss: tensor(0.3730)\n",
      "7256 Training Loss: tensor(0.3774)\n",
      "7257 Training Loss: tensor(0.3735)\n",
      "7258 Training Loss: tensor(0.3739)\n",
      "7259 Training Loss: tensor(0.3752)\n",
      "7260 Training Loss: tensor(0.3729)\n",
      "7261 Training Loss: tensor(0.3740)\n",
      "7262 Training Loss: tensor(0.3792)\n",
      "7263 Training Loss: tensor(0.3811)\n",
      "7264 Training Loss: tensor(0.3751)\n",
      "7265 Training Loss: tensor(0.3729)\n",
      "7266 Training Loss: tensor(0.3786)\n",
      "7267 Training Loss: tensor(0.3756)\n",
      "7268 Training Loss: tensor(0.3718)\n",
      "7269 Training Loss: tensor(0.3738)\n",
      "7270 Training Loss: tensor(0.3749)\n",
      "7271 Training Loss: tensor(0.3734)\n",
      "7272 Training Loss: tensor(0.3751)\n",
      "7273 Training Loss: tensor(0.3738)\n",
      "7274 Training Loss: tensor(0.3737)\n",
      "7275 Training Loss: tensor(0.3720)\n",
      "7276 Training Loss: tensor(0.3729)\n",
      "7277 Training Loss: tensor(0.3755)\n",
      "7278 Training Loss: tensor(0.3741)\n",
      "7279 Training Loss: tensor(0.3715)\n",
      "7280 Training Loss: tensor(0.3736)\n",
      "7281 Training Loss: tensor(0.3717)\n",
      "7282 Training Loss: tensor(0.3734)\n",
      "7283 Training Loss: tensor(0.3734)\n",
      "7284 Training Loss: tensor(0.3767)\n",
      "7285 Training Loss: tensor(0.3868)\n",
      "7286 Training Loss: tensor(0.3737)\n",
      "7287 Training Loss: tensor(0.3713)\n",
      "7288 Training Loss: tensor(0.3740)\n",
      "7289 Training Loss: tensor(0.3726)\n",
      "7290 Training Loss: tensor(0.3730)\n",
      "7291 Training Loss: tensor(0.3744)\n",
      "7292 Training Loss: tensor(0.3768)\n",
      "7293 Training Loss: tensor(0.3738)\n",
      "7294 Training Loss: tensor(0.3783)\n",
      "7295 Training Loss: tensor(0.3775)\n",
      "7296 Training Loss: tensor(0.3727)\n",
      "7297 Training Loss: tensor(0.3723)\n",
      "7298 Training Loss: tensor(0.3733)\n",
      "7299 Training Loss: tensor(0.3722)\n",
      "7300 Training Loss: tensor(0.3740)\n",
      "7301 Training Loss: tensor(0.3752)\n",
      "7302 Training Loss: tensor(0.3740)\n",
      "7303 Training Loss: tensor(0.3739)\n",
      "7304 Training Loss: tensor(0.3721)\n",
      "7305 Training Loss: tensor(0.3729)\n",
      "7306 Training Loss: tensor(0.3733)\n",
      "7307 Training Loss: tensor(0.3719)\n",
      "7308 Training Loss: tensor(0.3780)\n",
      "7309 Training Loss: tensor(0.3744)\n",
      "7310 Training Loss: tensor(0.3755)\n",
      "7311 Training Loss: tensor(0.3706)\n",
      "7312 Training Loss: tensor(0.3712)\n",
      "7313 Training Loss: tensor(0.3852)\n",
      "7314 Training Loss: tensor(0.3752)\n",
      "7315 Training Loss: tensor(0.3765)\n",
      "7316 Training Loss: tensor(0.3768)\n",
      "7317 Training Loss: tensor(0.3810)\n",
      "7318 Training Loss: tensor(0.3784)\n",
      "7319 Training Loss: tensor(0.3729)\n",
      "7320 Training Loss: tensor(0.3770)\n",
      "7321 Training Loss: tensor(0.3731)\n",
      "7322 Training Loss: tensor(0.3746)\n",
      "7323 Training Loss: tensor(0.3767)\n",
      "7324 Training Loss: tensor(0.3757)\n",
      "7325 Training Loss: tensor(0.3750)\n",
      "7326 Training Loss: tensor(0.3752)\n",
      "7327 Training Loss: tensor(0.3758)\n",
      "7328 Training Loss: tensor(0.3788)\n",
      "7329 Training Loss: tensor(0.3761)\n",
      "7330 Training Loss: tensor(0.3748)\n",
      "7331 Training Loss: tensor(0.3728)\n",
      "7332 Training Loss: tensor(0.3725)\n",
      "7333 Training Loss: tensor(0.3713)\n",
      "7334 Training Loss: tensor(0.3712)\n",
      "7335 Training Loss: tensor(0.3744)\n",
      "7336 Training Loss: tensor(0.3789)\n",
      "7337 Training Loss: tensor(0.3748)\n",
      "7338 Training Loss: tensor(0.3741)\n",
      "7339 Training Loss: tensor(0.3716)\n",
      "7340 Training Loss: tensor(0.3734)\n",
      "7341 Training Loss: tensor(0.3697)\n",
      "7342 Training Loss: tensor(0.3835)\n",
      "7343 Training Loss: tensor(0.3740)\n",
      "7344 Training Loss: tensor(0.3732)\n",
      "7345 Training Loss: tensor(0.3773)\n",
      "7346 Training Loss: tensor(0.3721)\n",
      "7347 Training Loss: tensor(0.3756)\n",
      "7348 Training Loss: tensor(0.3730)\n",
      "7349 Training Loss: tensor(0.3757)\n",
      "7350 Training Loss: tensor(0.3751)\n",
      "7351 Training Loss: tensor(0.3764)\n",
      "7352 Training Loss: tensor(0.3748)\n",
      "7353 Training Loss: tensor(0.3704)\n",
      "7354 Training Loss: tensor(0.3823)\n",
      "7355 Training Loss: tensor(0.3763)\n",
      "7356 Training Loss: tensor(0.3772)\n",
      "7357 Training Loss: tensor(0.3787)\n",
      "7358 Training Loss: tensor(0.3737)\n",
      "7359 Training Loss: tensor(0.3741)\n",
      "7360 Training Loss: tensor(0.3726)\n",
      "7361 Training Loss: tensor(0.3721)\n",
      "7362 Training Loss: tensor(0.3711)\n",
      "7363 Training Loss: tensor(0.3733)\n",
      "7364 Training Loss: tensor(0.3738)\n",
      "7365 Training Loss: tensor(0.3740)\n",
      "7366 Training Loss: tensor(0.3742)\n",
      "7367 Training Loss: tensor(0.3719)\n",
      "7368 Training Loss: tensor(0.3723)\n",
      "7369 Training Loss: tensor(0.3741)\n",
      "7370 Training Loss: tensor(0.3759)\n",
      "7371 Training Loss: tensor(0.3736)\n",
      "7372 Training Loss: tensor(0.3731)\n",
      "7373 Training Loss: tensor(0.3760)\n",
      "7374 Training Loss: tensor(0.3716)\n",
      "7375 Training Loss: tensor(0.3740)\n",
      "7376 Training Loss: tensor(0.3731)\n",
      "7377 Training Loss: tensor(0.3725)\n",
      "7378 Training Loss: tensor(0.3761)\n",
      "7379 Training Loss: tensor(0.3740)\n",
      "7380 Training Loss: tensor(0.3767)\n",
      "7381 Training Loss: tensor(0.3751)\n",
      "7382 Training Loss: tensor(0.3733)\n",
      "7383 Training Loss: tensor(0.3771)\n",
      "7384 Training Loss: tensor(0.3741)\n",
      "7385 Training Loss: tensor(0.3745)\n",
      "7386 Training Loss: tensor(0.3737)\n",
      "7387 Training Loss: tensor(0.3710)\n",
      "7388 Training Loss: tensor(0.3732)\n",
      "7389 Training Loss: tensor(0.3739)\n",
      "7390 Training Loss: tensor(0.3748)\n",
      "7391 Training Loss: tensor(0.3731)\n",
      "7392 Training Loss: tensor(0.3729)\n",
      "7393 Training Loss: tensor(0.3732)\n",
      "7394 Training Loss: tensor(0.3760)\n",
      "7395 Training Loss: tensor(0.3721)\n",
      "7396 Training Loss: tensor(0.3721)\n",
      "7397 Training Loss: tensor(0.3716)\n",
      "7398 Training Loss: tensor(0.3713)\n",
      "7399 Training Loss: tensor(0.3757)\n",
      "7400 Training Loss: tensor(0.3711)\n",
      "7401 Training Loss: tensor(0.3725)\n",
      "7402 Training Loss: tensor(0.3757)\n",
      "7403 Training Loss: tensor(0.3752)\n",
      "7404 Training Loss: tensor(0.3767)\n",
      "7405 Training Loss: tensor(0.3742)\n",
      "7406 Training Loss: tensor(0.3723)\n",
      "7407 Training Loss: tensor(0.3794)\n",
      "7408 Training Loss: tensor(0.3716)\n",
      "7409 Training Loss: tensor(0.3757)\n",
      "7410 Training Loss: tensor(0.3737)\n",
      "7411 Training Loss: tensor(0.3748)\n",
      "7412 Training Loss: tensor(0.3832)\n",
      "7413 Training Loss: tensor(0.3722)\n",
      "7414 Training Loss: tensor(0.3762)\n",
      "7415 Training Loss: tensor(0.3787)\n",
      "7416 Training Loss: tensor(0.3721)\n",
      "7417 Training Loss: tensor(0.3741)\n",
      "7418 Training Loss: tensor(0.3715)\n",
      "7419 Training Loss: tensor(0.3788)\n",
      "7420 Training Loss: tensor(0.3736)\n",
      "7421 Training Loss: tensor(0.3766)\n",
      "7422 Training Loss: tensor(0.3793)\n",
      "7423 Training Loss: tensor(0.3759)\n",
      "7424 Training Loss: tensor(0.3733)\n",
      "7425 Training Loss: tensor(0.3782)\n",
      "7426 Training Loss: tensor(0.3753)\n",
      "7427 Training Loss: tensor(0.3710)\n",
      "7428 Training Loss: tensor(0.3767)\n",
      "7429 Training Loss: tensor(0.3735)\n",
      "7430 Training Loss: tensor(0.3746)\n",
      "7431 Training Loss: tensor(0.3733)\n",
      "7432 Training Loss: tensor(0.3734)\n",
      "7433 Training Loss: tensor(0.3720)\n",
      "7434 Training Loss: tensor(0.3723)\n",
      "7435 Training Loss: tensor(0.3717)\n",
      "7436 Training Loss: tensor(0.3752)\n",
      "7437 Training Loss: tensor(0.3774)\n",
      "7438 Training Loss: tensor(0.3739)\n",
      "7439 Training Loss: tensor(0.3726)\n",
      "7440 Training Loss: tensor(0.3699)\n",
      "7441 Training Loss: tensor(0.3796)\n",
      "7442 Training Loss: tensor(0.3728)\n",
      "7443 Training Loss: tensor(0.3731)\n",
      "7444 Training Loss: tensor(0.3782)\n",
      "7445 Training Loss: tensor(0.3774)\n",
      "7446 Training Loss: tensor(0.3739)\n",
      "7447 Training Loss: tensor(0.3736)\n",
      "7448 Training Loss: tensor(0.3718)\n",
      "7449 Training Loss: tensor(0.3755)\n",
      "7450 Training Loss: tensor(0.3723)\n",
      "7451 Training Loss: tensor(0.3768)\n",
      "7452 Training Loss: tensor(0.3739)\n",
      "7453 Training Loss: tensor(0.3748)\n",
      "7454 Training Loss: tensor(0.3753)\n",
      "7455 Training Loss: tensor(0.3720)\n",
      "7456 Training Loss: tensor(0.3722)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7457 Training Loss: tensor(0.3710)\n",
      "7458 Training Loss: tensor(0.3713)\n",
      "7459 Training Loss: tensor(0.3730)\n",
      "7460 Training Loss: tensor(0.3779)\n",
      "7461 Training Loss: tensor(0.3731)\n",
      "7462 Training Loss: tensor(0.3769)\n",
      "7463 Training Loss: tensor(0.3730)\n",
      "7464 Training Loss: tensor(0.3749)\n",
      "7465 Training Loss: tensor(0.3735)\n",
      "7466 Training Loss: tensor(0.3732)\n",
      "7467 Training Loss: tensor(0.3732)\n",
      "7468 Training Loss: tensor(0.3741)\n",
      "7469 Training Loss: tensor(0.3755)\n",
      "7470 Training Loss: tensor(0.3746)\n",
      "7471 Training Loss: tensor(0.3778)\n",
      "7472 Training Loss: tensor(0.3778)\n",
      "7473 Training Loss: tensor(0.3715)\n",
      "7474 Training Loss: tensor(0.3729)\n",
      "7475 Training Loss: tensor(0.3744)\n",
      "7476 Training Loss: tensor(0.3758)\n",
      "7477 Training Loss: tensor(0.3730)\n",
      "7478 Training Loss: tensor(0.3767)\n",
      "7479 Training Loss: tensor(0.3750)\n",
      "7480 Training Loss: tensor(0.3738)\n",
      "7481 Training Loss: tensor(0.3775)\n",
      "7482 Training Loss: tensor(0.3747)\n",
      "7483 Training Loss: tensor(0.3737)\n",
      "7484 Training Loss: tensor(0.3738)\n",
      "7485 Training Loss: tensor(0.3758)\n",
      "7486 Training Loss: tensor(0.3717)\n",
      "7487 Training Loss: tensor(0.3743)\n",
      "7488 Training Loss: tensor(0.3793)\n",
      "7489 Training Loss: tensor(0.3742)\n",
      "7490 Training Loss: tensor(0.3733)\n",
      "7491 Training Loss: tensor(0.3726)\n",
      "7492 Training Loss: tensor(0.3750)\n",
      "7493 Training Loss: tensor(0.3728)\n",
      "7494 Training Loss: tensor(0.3728)\n",
      "7495 Training Loss: tensor(0.3725)\n",
      "7496 Training Loss: tensor(0.3743)\n",
      "7497 Training Loss: tensor(0.3718)\n",
      "7498 Training Loss: tensor(0.3737)\n",
      "7499 Training Loss: tensor(0.3716)\n",
      "7500 Training Loss: tensor(0.3753)\n",
      "7501 Training Loss: tensor(0.3733)\n",
      "7502 Training Loss: tensor(0.3753)\n",
      "7503 Training Loss: tensor(0.3712)\n",
      "7504 Training Loss: tensor(0.3767)\n",
      "7505 Training Loss: tensor(0.3739)\n",
      "7506 Training Loss: tensor(0.3757)\n",
      "7507 Training Loss: tensor(0.3797)\n",
      "7508 Training Loss: tensor(0.3749)\n",
      "7509 Training Loss: tensor(0.3756)\n",
      "7510 Training Loss: tensor(0.3760)\n",
      "7511 Training Loss: tensor(0.3749)\n",
      "7512 Training Loss: tensor(0.3751)\n",
      "7513 Training Loss: tensor(0.3751)\n",
      "7514 Training Loss: tensor(0.3742)\n",
      "7515 Training Loss: tensor(0.3730)\n",
      "7516 Training Loss: tensor(0.3735)\n",
      "7517 Training Loss: tensor(0.3766)\n",
      "7518 Training Loss: tensor(0.3702)\n",
      "7519 Training Loss: tensor(0.3792)\n",
      "7520 Training Loss: tensor(0.3746)\n",
      "7521 Training Loss: tensor(0.3725)\n",
      "7522 Training Loss: tensor(0.3749)\n",
      "7523 Training Loss: tensor(0.3740)\n",
      "7524 Training Loss: tensor(0.3722)\n",
      "7525 Training Loss: tensor(0.3755)\n",
      "7526 Training Loss: tensor(0.3789)\n",
      "7527 Training Loss: tensor(0.3750)\n",
      "7528 Training Loss: tensor(0.3754)\n",
      "7529 Training Loss: tensor(0.3724)\n",
      "7530 Training Loss: tensor(0.3811)\n",
      "7531 Training Loss: tensor(0.3753)\n",
      "7532 Training Loss: tensor(0.3791)\n",
      "7533 Training Loss: tensor(0.3739)\n",
      "7534 Training Loss: tensor(0.3743)\n",
      "7535 Training Loss: tensor(0.3746)\n",
      "7536 Training Loss: tensor(0.3743)\n",
      "7537 Training Loss: tensor(0.3729)\n",
      "7538 Training Loss: tensor(0.3750)\n",
      "7539 Training Loss: tensor(0.3751)\n",
      "7540 Training Loss: tensor(0.3741)\n",
      "7541 Training Loss: tensor(0.3749)\n",
      "7542 Training Loss: tensor(0.3746)\n",
      "7543 Training Loss: tensor(0.3736)\n",
      "7544 Training Loss: tensor(0.3719)\n",
      "7545 Training Loss: tensor(0.3766)\n",
      "7546 Training Loss: tensor(0.3732)\n",
      "7547 Training Loss: tensor(0.3701)\n",
      "7548 Training Loss: tensor(0.3756)\n",
      "7549 Training Loss: tensor(0.3710)\n",
      "7550 Training Loss: tensor(0.3734)\n",
      "7551 Training Loss: tensor(0.3706)\n",
      "7552 Training Loss: tensor(0.3721)\n",
      "7553 Training Loss: tensor(0.3728)\n",
      "7554 Training Loss: tensor(0.3723)\n",
      "7555 Training Loss: tensor(0.3774)\n",
      "7556 Training Loss: tensor(0.3726)\n",
      "7557 Training Loss: tensor(0.3699)\n",
      "7558 Training Loss: tensor(0.3768)\n",
      "7559 Training Loss: tensor(0.3750)\n",
      "7560 Training Loss: tensor(0.3709)\n",
      "7561 Training Loss: tensor(0.3734)\n",
      "7562 Training Loss: tensor(0.3701)\n",
      "7563 Training Loss: tensor(0.3745)\n",
      "7564 Training Loss: tensor(0.3754)\n",
      "7565 Training Loss: tensor(0.3750)\n",
      "7566 Training Loss: tensor(0.3748)\n",
      "7567 Training Loss: tensor(0.3719)\n",
      "7568 Training Loss: tensor(0.3771)\n",
      "7569 Training Loss: tensor(0.3742)\n",
      "7570 Training Loss: tensor(0.3736)\n",
      "7571 Training Loss: tensor(0.3694)\n",
      "7572 Training Loss: tensor(0.3736)\n",
      "7573 Training Loss: tensor(0.3748)\n",
      "7574 Training Loss: tensor(0.3728)\n",
      "7575 Training Loss: tensor(0.3716)\n",
      "7576 Training Loss: tensor(0.3729)\n",
      "7577 Training Loss: tensor(0.3725)\n",
      "7578 Training Loss: tensor(0.3745)\n",
      "7579 Training Loss: tensor(0.3714)\n",
      "7580 Training Loss: tensor(0.3760)\n",
      "7581 Training Loss: tensor(0.3825)\n",
      "7582 Training Loss: tensor(0.3819)\n",
      "7583 Training Loss: tensor(0.3706)\n",
      "7584 Training Loss: tensor(0.3766)\n",
      "7585 Training Loss: tensor(0.3733)\n",
      "7586 Training Loss: tensor(0.3722)\n",
      "7587 Training Loss: tensor(0.3723)\n",
      "7588 Training Loss: tensor(0.3788)\n",
      "7589 Training Loss: tensor(0.3747)\n",
      "7590 Training Loss: tensor(0.3717)\n",
      "7591 Training Loss: tensor(0.3717)\n",
      "7592 Training Loss: tensor(0.3759)\n",
      "7593 Training Loss: tensor(0.3734)\n",
      "7594 Training Loss: tensor(0.3721)\n",
      "7595 Training Loss: tensor(0.3757)\n",
      "7596 Training Loss: tensor(0.3741)\n",
      "7597 Training Loss: tensor(0.3772)\n",
      "7598 Training Loss: tensor(0.3726)\n",
      "7599 Training Loss: tensor(0.3773)\n",
      "7600 Training Loss: tensor(0.3768)\n",
      "7601 Training Loss: tensor(0.3774)\n",
      "7602 Training Loss: tensor(0.3732)\n",
      "7603 Training Loss: tensor(0.3773)\n",
      "7604 Training Loss: tensor(0.3712)\n",
      "7605 Training Loss: tensor(0.3714)\n",
      "7606 Training Loss: tensor(0.3776)\n",
      "7607 Training Loss: tensor(0.3808)\n",
      "7608 Training Loss: tensor(0.3753)\n",
      "7609 Training Loss: tensor(0.3727)\n",
      "7610 Training Loss: tensor(0.3737)\n",
      "7611 Training Loss: tensor(0.3726)\n",
      "7612 Training Loss: tensor(0.3739)\n",
      "7613 Training Loss: tensor(0.3794)\n",
      "7614 Training Loss: tensor(0.3721)\n",
      "7615 Training Loss: tensor(0.3773)\n",
      "7616 Training Loss: tensor(0.3754)\n",
      "7617 Training Loss: tensor(0.3752)\n",
      "7618 Training Loss: tensor(0.3701)\n",
      "7619 Training Loss: tensor(0.3721)\n",
      "7620 Training Loss: tensor(0.3718)\n",
      "7621 Training Loss: tensor(0.3740)\n",
      "7622 Training Loss: tensor(0.3750)\n",
      "7623 Training Loss: tensor(0.3709)\n",
      "7624 Training Loss: tensor(0.3725)\n",
      "7625 Training Loss: tensor(0.3724)\n",
      "7626 Training Loss: tensor(0.3767)\n",
      "7627 Training Loss: tensor(0.3695)\n",
      "7628 Training Loss: tensor(0.3762)\n",
      "7629 Training Loss: tensor(0.3723)\n",
      "7630 Training Loss: tensor(0.3756)\n",
      "7631 Training Loss: tensor(0.3752)\n",
      "7632 Training Loss: tensor(0.3723)\n",
      "7633 Training Loss: tensor(0.3762)\n",
      "7634 Training Loss: tensor(0.3748)\n",
      "7635 Training Loss: tensor(0.3747)\n",
      "7636 Training Loss: tensor(0.3725)\n",
      "7637 Training Loss: tensor(0.3753)\n",
      "7638 Training Loss: tensor(0.3770)\n",
      "7639 Training Loss: tensor(0.3733)\n",
      "7640 Training Loss: tensor(0.3708)\n",
      "7641 Training Loss: tensor(0.3726)\n",
      "7642 Training Loss: tensor(0.3760)\n",
      "7643 Training Loss: tensor(0.3764)\n",
      "7644 Training Loss: tensor(0.3762)\n",
      "7645 Training Loss: tensor(0.3737)\n",
      "7646 Training Loss: tensor(0.3726)\n",
      "7647 Training Loss: tensor(0.3718)\n",
      "7648 Training Loss: tensor(0.3742)\n",
      "7649 Training Loss: tensor(0.3736)\n",
      "7650 Training Loss: tensor(0.3762)\n",
      "7651 Training Loss: tensor(0.3722)\n",
      "7652 Training Loss: tensor(0.3733)\n",
      "7653 Training Loss: tensor(0.3767)\n",
      "7654 Training Loss: tensor(0.3728)\n",
      "7655 Training Loss: tensor(0.3735)\n",
      "7656 Training Loss: tensor(0.3731)\n",
      "7657 Training Loss: tensor(0.3772)\n",
      "7658 Training Loss: tensor(0.3739)\n",
      "7659 Training Loss: tensor(0.3762)\n",
      "7660 Training Loss: tensor(0.3707)\n",
      "7661 Training Loss: tensor(0.3729)\n",
      "7662 Training Loss: tensor(0.3732)\n",
      "7663 Training Loss: tensor(0.3753)\n",
      "7664 Training Loss: tensor(0.3769)\n",
      "7665 Training Loss: tensor(0.3736)\n",
      "7666 Training Loss: tensor(0.3748)\n",
      "7667 Training Loss: tensor(0.3695)\n",
      "7668 Training Loss: tensor(0.3717)\n",
      "7669 Training Loss: tensor(0.3734)\n",
      "7670 Training Loss: tensor(0.3766)\n",
      "7671 Training Loss: tensor(0.3713)\n",
      "7672 Training Loss: tensor(0.3777)\n",
      "7673 Training Loss: tensor(0.3730)\n",
      "7674 Training Loss: tensor(0.3706)\n",
      "7675 Training Loss: tensor(0.3691)\n",
      "7676 Training Loss: tensor(0.3737)\n",
      "7677 Training Loss: tensor(0.3770)\n",
      "7678 Training Loss: tensor(0.3708)\n",
      "7679 Training Loss: tensor(0.3739)\n",
      "7680 Training Loss: tensor(0.3762)\n",
      "7681 Training Loss: tensor(0.3753)\n",
      "7682 Training Loss: tensor(0.3745)\n",
      "7683 Training Loss: tensor(0.3721)\n",
      "7684 Training Loss: tensor(0.3797)\n",
      "7685 Training Loss: tensor(0.3743)\n",
      "7686 Training Loss: tensor(0.3733)\n",
      "7687 Training Loss: tensor(0.3748)\n",
      "7688 Training Loss: tensor(0.3723)\n",
      "7689 Training Loss: tensor(0.3744)\n",
      "7690 Training Loss: tensor(0.3751)\n",
      "7691 Training Loss: tensor(0.3734)\n",
      "7692 Training Loss: tensor(0.3753)\n",
      "7693 Training Loss: tensor(0.3758)\n",
      "7694 Training Loss: tensor(0.3714)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7695 Training Loss: tensor(0.3743)\n",
      "7696 Training Loss: tensor(0.3718)\n",
      "7697 Training Loss: tensor(0.3713)\n",
      "7698 Training Loss: tensor(0.3742)\n",
      "7699 Training Loss: tensor(0.3713)\n",
      "7700 Training Loss: tensor(0.3733)\n",
      "7701 Training Loss: tensor(0.3716)\n",
      "7702 Training Loss: tensor(0.3752)\n",
      "7703 Training Loss: tensor(0.3729)\n",
      "7704 Training Loss: tensor(0.3704)\n",
      "7705 Training Loss: tensor(0.3742)\n",
      "7706 Training Loss: tensor(0.3721)\n",
      "7707 Training Loss: tensor(0.3759)\n",
      "7708 Training Loss: tensor(0.3767)\n",
      "7709 Training Loss: tensor(0.3724)\n",
      "7710 Training Loss: tensor(0.3731)\n",
      "7711 Training Loss: tensor(0.3732)\n",
      "7712 Training Loss: tensor(0.3794)\n",
      "7713 Training Loss: tensor(0.3746)\n",
      "7714 Training Loss: tensor(0.3808)\n",
      "7715 Training Loss: tensor(0.3739)\n",
      "7716 Training Loss: tensor(0.3724)\n",
      "7717 Training Loss: tensor(0.3722)\n",
      "7718 Training Loss: tensor(0.3757)\n",
      "7719 Training Loss: tensor(0.3727)\n",
      "7720 Training Loss: tensor(0.3735)\n",
      "7721 Training Loss: tensor(0.3743)\n",
      "7722 Training Loss: tensor(0.3789)\n",
      "7723 Training Loss: tensor(0.3782)\n",
      "7724 Training Loss: tensor(0.3752)\n",
      "7725 Training Loss: tensor(0.3722)\n",
      "7726 Training Loss: tensor(0.3765)\n",
      "7727 Training Loss: tensor(0.3768)\n",
      "7728 Training Loss: tensor(0.3775)\n",
      "7729 Training Loss: tensor(0.3717)\n",
      "7730 Training Loss: tensor(0.3783)\n",
      "7731 Training Loss: tensor(0.3732)\n",
      "7732 Training Loss: tensor(0.3742)\n",
      "7733 Training Loss: tensor(0.3749)\n",
      "7734 Training Loss: tensor(0.3724)\n",
      "7735 Training Loss: tensor(0.3759)\n",
      "7736 Training Loss: tensor(0.3715)\n",
      "7737 Training Loss: tensor(0.3735)\n",
      "7738 Training Loss: tensor(0.3786)\n",
      "7739 Training Loss: tensor(0.3764)\n",
      "7740 Training Loss: tensor(0.3749)\n",
      "7741 Training Loss: tensor(0.3743)\n",
      "7742 Training Loss: tensor(0.3717)\n",
      "7743 Training Loss: tensor(0.3757)\n",
      "7744 Training Loss: tensor(0.3750)\n",
      "7745 Training Loss: tensor(0.3729)\n",
      "7746 Training Loss: tensor(0.3822)\n",
      "7747 Training Loss: tensor(0.3731)\n",
      "7748 Training Loss: tensor(0.3730)\n",
      "7749 Training Loss: tensor(0.3735)\n",
      "7750 Training Loss: tensor(0.3746)\n",
      "7751 Training Loss: tensor(0.3720)\n",
      "7752 Training Loss: tensor(0.3787)\n",
      "7753 Training Loss: tensor(0.3710)\n",
      "7754 Training Loss: tensor(0.3745)\n",
      "7755 Training Loss: tensor(0.3729)\n",
      "7756 Training Loss: tensor(0.3772)\n",
      "7757 Training Loss: tensor(0.3785)\n",
      "7758 Training Loss: tensor(0.3792)\n",
      "7759 Training Loss: tensor(0.3722)\n",
      "7760 Training Loss: tensor(0.3720)\n",
      "7761 Training Loss: tensor(0.3732)\n",
      "7762 Training Loss: tensor(0.3728)\n",
      "7763 Training Loss: tensor(0.3717)\n",
      "7764 Training Loss: tensor(0.3746)\n",
      "7765 Training Loss: tensor(0.3709)\n",
      "7766 Training Loss: tensor(0.3714)\n",
      "7767 Training Loss: tensor(0.3783)\n",
      "7768 Training Loss: tensor(0.3739)\n",
      "7769 Training Loss: tensor(0.3766)\n",
      "7770 Training Loss: tensor(0.3768)\n",
      "7771 Training Loss: tensor(0.3754)\n",
      "7772 Training Loss: tensor(0.3726)\n",
      "7773 Training Loss: tensor(0.3755)\n",
      "7774 Training Loss: tensor(0.3704)\n",
      "7775 Training Loss: tensor(0.3734)\n",
      "7776 Training Loss: tensor(0.3696)\n",
      "7777 Training Loss: tensor(0.3705)\n",
      "7778 Training Loss: tensor(0.3806)\n",
      "7779 Training Loss: tensor(0.3787)\n",
      "7780 Training Loss: tensor(0.3733)\n",
      "7781 Training Loss: tensor(0.3801)\n",
      "7782 Training Loss: tensor(0.3732)\n",
      "7783 Training Loss: tensor(0.3722)\n",
      "7784 Training Loss: tensor(0.3707)\n",
      "7785 Training Loss: tensor(0.3725)\n",
      "7786 Training Loss: tensor(0.3705)\n",
      "7787 Training Loss: tensor(0.3739)\n",
      "7788 Training Loss: tensor(0.3772)\n",
      "7789 Training Loss: tensor(0.3735)\n",
      "7790 Training Loss: tensor(0.3749)\n",
      "7791 Training Loss: tensor(0.3718)\n",
      "7792 Training Loss: tensor(0.3743)\n",
      "7793 Training Loss: tensor(0.3687)\n",
      "7794 Training Loss: tensor(0.3779)\n",
      "7795 Training Loss: tensor(0.3756)\n",
      "7796 Training Loss: tensor(0.3744)\n",
      "7797 Training Loss: tensor(0.3776)\n",
      "7798 Training Loss: tensor(0.3735)\n",
      "7799 Training Loss: tensor(0.3734)\n",
      "7800 Training Loss: tensor(0.3703)\n",
      "7801 Training Loss: tensor(0.3743)\n",
      "7802 Training Loss: tensor(0.3764)\n",
      "7803 Training Loss: tensor(0.3709)\n",
      "7804 Training Loss: tensor(0.3778)\n",
      "7805 Training Loss: tensor(0.3719)\n",
      "7806 Training Loss: tensor(0.3719)\n",
      "7807 Training Loss: tensor(0.3714)\n",
      "7808 Training Loss: tensor(0.3721)\n",
      "7809 Training Loss: tensor(0.3729)\n",
      "7810 Training Loss: tensor(0.3752)\n",
      "7811 Training Loss: tensor(0.3775)\n",
      "7812 Training Loss: tensor(0.3750)\n",
      "7813 Training Loss: tensor(0.3743)\n",
      "7814 Training Loss: tensor(0.3752)\n",
      "7815 Training Loss: tensor(0.3719)\n",
      "7816 Training Loss: tensor(0.3713)\n",
      "7817 Training Loss: tensor(0.3731)\n",
      "7818 Training Loss: tensor(0.3705)\n",
      "7819 Training Loss: tensor(0.3741)\n",
      "7820 Training Loss: tensor(0.3751)\n",
      "7821 Training Loss: tensor(0.3717)\n",
      "7822 Training Loss: tensor(0.3772)\n",
      "7823 Training Loss: tensor(0.3758)\n",
      "7824 Training Loss: tensor(0.3777)\n",
      "7825 Training Loss: tensor(0.3766)\n",
      "7826 Training Loss: tensor(0.3726)\n",
      "7827 Training Loss: tensor(0.3747)\n",
      "7828 Training Loss: tensor(0.3723)\n",
      "7829 Training Loss: tensor(0.3713)\n",
      "7830 Training Loss: tensor(0.3734)\n",
      "7831 Training Loss: tensor(0.3791)\n",
      "7832 Training Loss: tensor(0.3790)\n",
      "7833 Training Loss: tensor(0.3741)\n",
      "7834 Training Loss: tensor(0.3780)\n",
      "7835 Training Loss: tensor(0.3755)\n",
      "7836 Training Loss: tensor(0.3754)\n",
      "7837 Training Loss: tensor(0.3735)\n",
      "7838 Training Loss: tensor(0.3756)\n",
      "7839 Training Loss: tensor(0.3750)\n",
      "7840 Training Loss: tensor(0.3729)\n",
      "7841 Training Loss: tensor(0.3728)\n",
      "7842 Training Loss: tensor(0.3735)\n",
      "7843 Training Loss: tensor(0.3733)\n",
      "7844 Training Loss: tensor(0.3755)\n",
      "7845 Training Loss: tensor(0.3756)\n",
      "7846 Training Loss: tensor(0.3724)\n",
      "7847 Training Loss: tensor(0.3764)\n",
      "7848 Training Loss: tensor(0.3736)\n",
      "7849 Training Loss: tensor(0.3734)\n",
      "7850 Training Loss: tensor(0.3747)\n",
      "7851 Training Loss: tensor(0.3729)\n",
      "7852 Training Loss: tensor(0.3713)\n",
      "7853 Training Loss: tensor(0.3705)\n",
      "7854 Training Loss: tensor(0.3734)\n",
      "7855 Training Loss: tensor(0.3696)\n",
      "7856 Training Loss: tensor(0.3742)\n",
      "7857 Training Loss: tensor(0.3743)\n",
      "7858 Training Loss: tensor(0.3720)\n",
      "7859 Training Loss: tensor(0.3733)\n",
      "7860 Training Loss: tensor(0.3729)\n",
      "7861 Training Loss: tensor(0.3714)\n",
      "7862 Training Loss: tensor(0.3705)\n",
      "7863 Training Loss: tensor(0.3723)\n",
      "7864 Training Loss: tensor(0.3713)\n",
      "7865 Training Loss: tensor(0.3702)\n",
      "7866 Training Loss: tensor(0.3717)\n",
      "7867 Training Loss: tensor(0.3722)\n",
      "7868 Training Loss: tensor(0.3714)\n",
      "7869 Training Loss: tensor(0.3779)\n",
      "7870 Training Loss: tensor(0.3724)\n",
      "7871 Training Loss: tensor(0.3710)\n",
      "7872 Training Loss: tensor(0.3757)\n",
      "7873 Training Loss: tensor(0.3716)\n",
      "7874 Training Loss: tensor(0.3748)\n",
      "7875 Training Loss: tensor(0.3726)\n",
      "7876 Training Loss: tensor(0.3836)\n",
      "7877 Training Loss: tensor(0.3765)\n",
      "7878 Training Loss: tensor(0.3779)\n",
      "7879 Training Loss: tensor(0.3719)\n",
      "7880 Training Loss: tensor(0.3728)\n",
      "7881 Training Loss: tensor(0.3754)\n",
      "7882 Training Loss: tensor(0.3727)\n",
      "7883 Training Loss: tensor(0.3731)\n",
      "7884 Training Loss: tensor(0.3741)\n",
      "7885 Training Loss: tensor(0.3750)\n",
      "7886 Training Loss: tensor(0.3727)\n",
      "7887 Training Loss: tensor(0.3719)\n",
      "7888 Training Loss: tensor(0.3744)\n",
      "7889 Training Loss: tensor(0.3747)\n",
      "7890 Training Loss: tensor(0.3714)\n",
      "7891 Training Loss: tensor(0.3722)\n",
      "7892 Training Loss: tensor(0.3703)\n",
      "7893 Training Loss: tensor(0.3732)\n",
      "7894 Training Loss: tensor(0.3767)\n",
      "7895 Training Loss: tensor(0.3702)\n",
      "7896 Training Loss: tensor(0.3793)\n",
      "7897 Training Loss: tensor(0.3710)\n",
      "7898 Training Loss: tensor(0.3730)\n",
      "7899 Training Loss: tensor(0.3719)\n",
      "7900 Training Loss: tensor(0.3745)\n",
      "7901 Training Loss: tensor(0.3748)\n",
      "7902 Training Loss: tensor(0.3732)\n",
      "7903 Training Loss: tensor(0.3729)\n",
      "7904 Training Loss: tensor(0.3748)\n",
      "7905 Training Loss: tensor(0.3715)\n",
      "7906 Training Loss: tensor(0.3733)\n",
      "7907 Training Loss: tensor(0.3702)\n",
      "7908 Training Loss: tensor(0.3712)\n",
      "7909 Training Loss: tensor(0.3738)\n",
      "7910 Training Loss: tensor(0.3772)\n",
      "7911 Training Loss: tensor(0.3762)\n",
      "7912 Training Loss: tensor(0.3735)\n",
      "7913 Training Loss: tensor(0.3768)\n",
      "7914 Training Loss: tensor(0.3755)\n",
      "7915 Training Loss: tensor(0.3740)\n",
      "7916 Training Loss: tensor(0.3785)\n",
      "7917 Training Loss: tensor(0.3796)\n",
      "7918 Training Loss: tensor(0.3780)\n",
      "7919 Training Loss: tensor(0.3732)\n",
      "7920 Training Loss: tensor(0.3735)\n",
      "7921 Training Loss: tensor(0.3725)\n",
      "7922 Training Loss: tensor(0.3774)\n",
      "7923 Training Loss: tensor(0.3782)\n",
      "7924 Training Loss: tensor(0.3764)\n",
      "7925 Training Loss: tensor(0.3760)\n",
      "7926 Training Loss: tensor(0.3745)\n",
      "7927 Training Loss: tensor(0.3750)\n",
      "7928 Training Loss: tensor(0.3750)\n",
      "7929 Training Loss: tensor(0.3761)\n",
      "7930 Training Loss: tensor(0.3743)\n",
      "7931 Training Loss: tensor(0.3728)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7932 Training Loss: tensor(0.3715)\n",
      "7933 Training Loss: tensor(0.3733)\n",
      "7934 Training Loss: tensor(0.3726)\n",
      "7935 Training Loss: tensor(0.3729)\n",
      "7936 Training Loss: tensor(0.3789)\n",
      "7937 Training Loss: tensor(0.3760)\n",
      "7938 Training Loss: tensor(0.3741)\n",
      "7939 Training Loss: tensor(0.3701)\n",
      "7940 Training Loss: tensor(0.3763)\n",
      "7941 Training Loss: tensor(0.3759)\n",
      "7942 Training Loss: tensor(0.3790)\n",
      "7943 Training Loss: tensor(0.3714)\n",
      "7944 Training Loss: tensor(0.3748)\n",
      "7945 Training Loss: tensor(0.3749)\n",
      "7946 Training Loss: tensor(0.3761)\n",
      "7947 Training Loss: tensor(0.3733)\n",
      "7948 Training Loss: tensor(0.3711)\n",
      "7949 Training Loss: tensor(0.3814)\n",
      "7950 Training Loss: tensor(0.3754)\n",
      "7951 Training Loss: tensor(0.3737)\n",
      "7952 Training Loss: tensor(0.3731)\n",
      "7953 Training Loss: tensor(0.3729)\n",
      "7954 Training Loss: tensor(0.3763)\n",
      "7955 Training Loss: tensor(0.3726)\n",
      "7956 Training Loss: tensor(0.3728)\n",
      "7957 Training Loss: tensor(0.3783)\n",
      "7958 Training Loss: tensor(0.3760)\n",
      "7959 Training Loss: tensor(0.3752)\n",
      "7960 Training Loss: tensor(0.3728)\n",
      "7961 Training Loss: tensor(0.3738)\n",
      "7962 Training Loss: tensor(0.3717)\n",
      "7963 Training Loss: tensor(0.3752)\n",
      "7964 Training Loss: tensor(0.3716)\n",
      "7965 Training Loss: tensor(0.3729)\n",
      "7966 Training Loss: tensor(0.3741)\n",
      "7967 Training Loss: tensor(0.3720)\n",
      "7968 Training Loss: tensor(0.3733)\n",
      "7969 Training Loss: tensor(0.3761)\n",
      "7970 Training Loss: tensor(0.3751)\n",
      "7971 Training Loss: tensor(0.3792)\n",
      "7972 Training Loss: tensor(0.3716)\n",
      "7973 Training Loss: tensor(0.3715)\n",
      "7974 Training Loss: tensor(0.3759)\n",
      "7975 Training Loss: tensor(0.3745)\n",
      "7976 Training Loss: tensor(0.3722)\n",
      "7977 Training Loss: tensor(0.3693)\n",
      "7978 Training Loss: tensor(0.3768)\n",
      "7979 Training Loss: tensor(0.3709)\n",
      "7980 Training Loss: tensor(0.3720)\n",
      "7981 Training Loss: tensor(0.3738)\n",
      "7982 Training Loss: tensor(0.3708)\n",
      "7983 Training Loss: tensor(0.3728)\n",
      "7984 Training Loss: tensor(0.3734)\n",
      "7985 Training Loss: tensor(0.3731)\n",
      "7986 Training Loss: tensor(0.3728)\n",
      "7987 Training Loss: tensor(0.3762)\n",
      "7988 Training Loss: tensor(0.3747)\n",
      "7989 Training Loss: tensor(0.3685)\n",
      "7990 Training Loss: tensor(0.3774)\n",
      "7991 Training Loss: tensor(0.3722)\n",
      "7992 Training Loss: tensor(0.3752)\n",
      "7993 Training Loss: tensor(0.3736)\n",
      "7994 Training Loss: tensor(0.3738)\n",
      "7995 Training Loss: tensor(0.3756)\n",
      "7996 Training Loss: tensor(0.3759)\n",
      "7997 Training Loss: tensor(0.3779)\n",
      "7998 Training Loss: tensor(0.3764)\n",
      "7999 Training Loss: tensor(0.3747)\n",
      "8000 Training Loss: tensor(0.3779)\n",
      "8001 Training Loss: tensor(0.3754)\n",
      "8002 Training Loss: tensor(0.3767)\n",
      "8003 Training Loss: tensor(0.3741)\n",
      "8004 Training Loss: tensor(0.3713)\n",
      "8005 Training Loss: tensor(0.3719)\n",
      "8006 Training Loss: tensor(0.3741)\n",
      "8007 Training Loss: tensor(0.3720)\n",
      "8008 Training Loss: tensor(0.3762)\n",
      "8009 Training Loss: tensor(0.3743)\n",
      "8010 Training Loss: tensor(0.3745)\n",
      "8011 Training Loss: tensor(0.3740)\n",
      "8012 Training Loss: tensor(0.3748)\n",
      "8013 Training Loss: tensor(0.3732)\n",
      "8014 Training Loss: tensor(0.3728)\n",
      "8015 Training Loss: tensor(0.3715)\n",
      "8016 Training Loss: tensor(0.3772)\n",
      "8017 Training Loss: tensor(0.3736)\n",
      "8018 Training Loss: tensor(0.3777)\n",
      "8019 Training Loss: tensor(0.3685)\n",
      "8020 Training Loss: tensor(0.3707)\n",
      "8021 Training Loss: tensor(0.3767)\n",
      "8022 Training Loss: tensor(0.3707)\n",
      "8023 Training Loss: tensor(0.3782)\n",
      "8024 Training Loss: tensor(0.3730)\n",
      "8025 Training Loss: tensor(0.3781)\n",
      "8026 Training Loss: tensor(0.3706)\n",
      "8027 Training Loss: tensor(0.3777)\n",
      "8028 Training Loss: tensor(0.3720)\n",
      "8029 Training Loss: tensor(0.3716)\n",
      "8030 Training Loss: tensor(0.3743)\n",
      "8031 Training Loss: tensor(0.3749)\n",
      "8032 Training Loss: tensor(0.3721)\n",
      "8033 Training Loss: tensor(0.3742)\n",
      "8034 Training Loss: tensor(0.3766)\n",
      "8035 Training Loss: tensor(0.3732)\n",
      "8036 Training Loss: tensor(0.3723)\n",
      "8037 Training Loss: tensor(0.3727)\n",
      "8038 Training Loss: tensor(0.3730)\n",
      "8039 Training Loss: tensor(0.3760)\n",
      "8040 Training Loss: tensor(0.3754)\n",
      "8041 Training Loss: tensor(0.3708)\n",
      "8042 Training Loss: tensor(0.3697)\n",
      "8043 Training Loss: tensor(0.3750)\n",
      "8044 Training Loss: tensor(0.3710)\n",
      "8045 Training Loss: tensor(0.3734)\n",
      "8046 Training Loss: tensor(0.3728)\n",
      "8047 Training Loss: tensor(0.3751)\n",
      "8048 Training Loss: tensor(0.3765)\n",
      "8049 Training Loss: tensor(0.3718)\n",
      "8050 Training Loss: tensor(0.3739)\n",
      "8051 Training Loss: tensor(0.3691)\n",
      "8052 Training Loss: tensor(0.3705)\n",
      "8053 Training Loss: tensor(0.3769)\n",
      "8054 Training Loss: tensor(0.3769)\n",
      "8055 Training Loss: tensor(0.3760)\n",
      "8056 Training Loss: tensor(0.3696)\n",
      "8057 Training Loss: tensor(0.3759)\n",
      "8058 Training Loss: tensor(0.3706)\n",
      "8059 Training Loss: tensor(0.3774)\n",
      "8060 Training Loss: tensor(0.3751)\n",
      "8061 Training Loss: tensor(0.3758)\n",
      "8062 Training Loss: tensor(0.3763)\n",
      "8063 Training Loss: tensor(0.3726)\n",
      "8064 Training Loss: tensor(0.3697)\n",
      "8065 Training Loss: tensor(0.3757)\n",
      "8066 Training Loss: tensor(0.3761)\n",
      "8067 Training Loss: tensor(0.3746)\n",
      "8068 Training Loss: tensor(0.3719)\n",
      "8069 Training Loss: tensor(0.3712)\n",
      "8070 Training Loss: tensor(0.3740)\n",
      "8071 Training Loss: tensor(0.3729)\n",
      "8072 Training Loss: tensor(0.3766)\n",
      "8073 Training Loss: tensor(0.3694)\n",
      "8074 Training Loss: tensor(0.3740)\n",
      "8075 Training Loss: tensor(0.3748)\n",
      "8076 Training Loss: tensor(0.3740)\n",
      "8077 Training Loss: tensor(0.3722)\n",
      "8078 Training Loss: tensor(0.3750)\n",
      "8079 Training Loss: tensor(0.3721)\n",
      "8080 Training Loss: tensor(0.3697)\n",
      "8081 Training Loss: tensor(0.3762)\n",
      "8082 Training Loss: tensor(0.3690)\n",
      "8083 Training Loss: tensor(0.3767)\n",
      "8084 Training Loss: tensor(0.3763)\n",
      "8085 Training Loss: tensor(0.3805)\n",
      "8086 Training Loss: tensor(0.3705)\n",
      "8087 Training Loss: tensor(0.3762)\n",
      "8088 Training Loss: tensor(0.3686)\n",
      "8089 Training Loss: tensor(0.3742)\n",
      "8090 Training Loss: tensor(0.3724)\n",
      "8091 Training Loss: tensor(0.3706)\n",
      "8092 Training Loss: tensor(0.3768)\n",
      "8093 Training Loss: tensor(0.3748)\n",
      "8094 Training Loss: tensor(0.3709)\n",
      "8095 Training Loss: tensor(0.3716)\n",
      "8096 Training Loss: tensor(0.3761)\n",
      "8097 Training Loss: tensor(0.3717)\n",
      "8098 Training Loss: tensor(0.3706)\n",
      "8099 Training Loss: tensor(0.3731)\n",
      "8100 Training Loss: tensor(0.3736)\n",
      "8101 Training Loss: tensor(0.3756)\n",
      "8102 Training Loss: tensor(0.3723)\n",
      "8103 Training Loss: tensor(0.3754)\n",
      "8104 Training Loss: tensor(0.3731)\n",
      "8105 Training Loss: tensor(0.3708)\n",
      "8106 Training Loss: tensor(0.3744)\n",
      "8107 Training Loss: tensor(0.3754)\n",
      "8108 Training Loss: tensor(0.3737)\n",
      "8109 Training Loss: tensor(0.3761)\n",
      "8110 Training Loss: tensor(0.3744)\n",
      "8111 Training Loss: tensor(0.3731)\n",
      "8112 Training Loss: tensor(0.3704)\n",
      "8113 Training Loss: tensor(0.3694)\n",
      "8114 Training Loss: tensor(0.3754)\n",
      "8115 Training Loss: tensor(0.3753)\n",
      "8116 Training Loss: tensor(0.3698)\n",
      "8117 Training Loss: tensor(0.3793)\n",
      "8118 Training Loss: tensor(0.3773)\n",
      "8119 Training Loss: tensor(0.3724)\n",
      "8120 Training Loss: tensor(0.3709)\n",
      "8121 Training Loss: tensor(0.3802)\n",
      "8122 Training Loss: tensor(0.3741)\n",
      "8123 Training Loss: tensor(0.3725)\n",
      "8124 Training Loss: tensor(0.3742)\n",
      "8125 Training Loss: tensor(0.3711)\n",
      "8126 Training Loss: tensor(0.3738)\n",
      "8127 Training Loss: tensor(0.3712)\n",
      "8128 Training Loss: tensor(0.3721)\n",
      "8129 Training Loss: tensor(0.3761)\n",
      "8130 Training Loss: tensor(0.3730)\n",
      "8131 Training Loss: tensor(0.3701)\n",
      "8132 Training Loss: tensor(0.3728)\n",
      "8133 Training Loss: tensor(0.3700)\n",
      "8134 Training Loss: tensor(0.3740)\n",
      "8135 Training Loss: tensor(0.3712)\n",
      "8136 Training Loss: tensor(0.3712)\n",
      "8137 Training Loss: tensor(0.3709)\n",
      "8138 Training Loss: tensor(0.3743)\n",
      "8139 Training Loss: tensor(0.3730)\n",
      "8140 Training Loss: tensor(0.3743)\n",
      "8141 Training Loss: tensor(0.3773)\n",
      "8142 Training Loss: tensor(0.3810)\n",
      "8143 Training Loss: tensor(0.3688)\n",
      "8144 Training Loss: tensor(0.3750)\n",
      "8145 Training Loss: tensor(0.3739)\n",
      "8146 Training Loss: tensor(0.3731)\n",
      "8147 Training Loss: tensor(0.3802)\n",
      "8148 Training Loss: tensor(0.3787)\n",
      "8149 Training Loss: tensor(0.3698)\n",
      "8150 Training Loss: tensor(0.3771)\n",
      "8151 Training Loss: tensor(0.3763)\n",
      "8152 Training Loss: tensor(0.3732)\n",
      "8153 Training Loss: tensor(0.3727)\n",
      "8154 Training Loss: tensor(0.3735)\n",
      "8155 Training Loss: tensor(0.3707)\n",
      "8156 Training Loss: tensor(0.3736)\n",
      "8157 Training Loss: tensor(0.3743)\n",
      "8158 Training Loss: tensor(0.3722)\n",
      "8159 Training Loss: tensor(0.3739)\n",
      "8160 Training Loss: tensor(0.3711)\n",
      "8161 Training Loss: tensor(0.3725)\n",
      "8162 Training Loss: tensor(0.3719)\n",
      "8163 Training Loss: tensor(0.3709)\n",
      "8164 Training Loss: tensor(0.3737)\n",
      "8165 Training Loss: tensor(0.3713)\n",
      "8166 Training Loss: tensor(0.3709)\n",
      "8167 Training Loss: tensor(0.3754)\n",
      "8168 Training Loss: tensor(0.3769)\n",
      "8169 Training Loss: tensor(0.3737)\n",
      "8170 Training Loss: tensor(0.3729)\n",
      "8171 Training Loss: tensor(0.3752)\n",
      "8172 Training Loss: tensor(0.3762)\n",
      "8173 Training Loss: tensor(0.3704)\n",
      "8174 Training Loss: tensor(0.3717)\n",
      "8175 Training Loss: tensor(0.3697)\n",
      "8176 Training Loss: tensor(0.3717)\n",
      "8177 Training Loss: tensor(0.3688)\n",
      "8178 Training Loss: tensor(0.3702)\n",
      "8179 Training Loss: tensor(0.3736)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8180 Training Loss: tensor(0.3773)\n",
      "8181 Training Loss: tensor(0.3801)\n",
      "8182 Training Loss: tensor(0.3728)\n",
      "8183 Training Loss: tensor(0.3747)\n",
      "8184 Training Loss: tensor(0.3728)\n",
      "8185 Training Loss: tensor(0.3725)\n",
      "8186 Training Loss: tensor(0.3703)\n",
      "8187 Training Loss: tensor(0.3724)\n",
      "8188 Training Loss: tensor(0.3712)\n",
      "8189 Training Loss: tensor(0.3724)\n",
      "8190 Training Loss: tensor(0.3726)\n",
      "8191 Training Loss: tensor(0.3767)\n",
      "8192 Training Loss: tensor(0.3791)\n",
      "8193 Training Loss: tensor(0.3746)\n",
      "8194 Training Loss: tensor(0.3720)\n",
      "8195 Training Loss: tensor(0.3734)\n",
      "8196 Training Loss: tensor(0.3708)\n",
      "8197 Training Loss: tensor(0.3739)\n",
      "8198 Training Loss: tensor(0.3706)\n",
      "8199 Training Loss: tensor(0.3743)\n",
      "8200 Training Loss: tensor(0.3776)\n",
      "8201 Training Loss: tensor(0.3714)\n",
      "8202 Training Loss: tensor(0.3699)\n",
      "8203 Training Loss: tensor(0.3762)\n",
      "8204 Training Loss: tensor(0.3706)\n",
      "8205 Training Loss: tensor(0.3742)\n",
      "8206 Training Loss: tensor(0.3709)\n",
      "8207 Training Loss: tensor(0.3749)\n",
      "8208 Training Loss: tensor(0.3759)\n",
      "8209 Training Loss: tensor(0.3780)\n",
      "8210 Training Loss: tensor(0.3708)\n",
      "8211 Training Loss: tensor(0.3749)\n",
      "8212 Training Loss: tensor(0.3730)\n",
      "8213 Training Loss: tensor(0.3775)\n",
      "8214 Training Loss: tensor(0.3725)\n",
      "8215 Training Loss: tensor(0.3718)\n",
      "8216 Training Loss: tensor(0.3716)\n",
      "8217 Training Loss: tensor(0.3790)\n",
      "8218 Training Loss: tensor(0.3748)\n",
      "8219 Training Loss: tensor(0.3756)\n",
      "8220 Training Loss: tensor(0.3731)\n",
      "8221 Training Loss: tensor(0.3767)\n",
      "8222 Training Loss: tensor(0.3730)\n",
      "8223 Training Loss: tensor(0.3761)\n",
      "8224 Training Loss: tensor(0.3699)\n",
      "8225 Training Loss: tensor(0.3718)\n",
      "8226 Training Loss: tensor(0.3777)\n",
      "8227 Training Loss: tensor(0.3773)\n",
      "8228 Training Loss: tensor(0.3757)\n",
      "8229 Training Loss: tensor(0.3779)\n",
      "8230 Training Loss: tensor(0.3740)\n",
      "8231 Training Loss: tensor(0.3779)\n",
      "8232 Training Loss: tensor(0.3777)\n",
      "8233 Training Loss: tensor(0.3710)\n",
      "8234 Training Loss: tensor(0.3727)\n",
      "8235 Training Loss: tensor(0.3752)\n",
      "8236 Training Loss: tensor(0.3745)\n",
      "8237 Training Loss: tensor(0.3748)\n",
      "8238 Training Loss: tensor(0.3707)\n",
      "8239 Training Loss: tensor(0.3753)\n",
      "8240 Training Loss: tensor(0.3714)\n",
      "8241 Training Loss: tensor(0.3740)\n",
      "8242 Training Loss: tensor(0.3720)\n",
      "8243 Training Loss: tensor(0.3702)\n",
      "8244 Training Loss: tensor(0.3705)\n",
      "8245 Training Loss: tensor(0.3750)\n",
      "8246 Training Loss: tensor(0.3703)\n",
      "8247 Training Loss: tensor(0.3765)\n",
      "8248 Training Loss: tensor(0.3701)\n",
      "8249 Training Loss: tensor(0.3690)\n",
      "8250 Training Loss: tensor(0.3715)\n",
      "8251 Training Loss: tensor(0.3745)\n",
      "8252 Training Loss: tensor(0.3710)\n",
      "8253 Training Loss: tensor(0.3712)\n",
      "8254 Training Loss: tensor(0.3704)\n",
      "8255 Training Loss: tensor(0.3780)\n",
      "8256 Training Loss: tensor(0.3775)\n",
      "8257 Training Loss: tensor(0.3724)\n",
      "8258 Training Loss: tensor(0.3674)\n",
      "8259 Training Loss: tensor(0.3761)\n",
      "8260 Training Loss: tensor(0.3744)\n",
      "8261 Training Loss: tensor(0.3713)\n",
      "8262 Training Loss: tensor(0.3732)\n",
      "8263 Training Loss: tensor(0.3735)\n",
      "8264 Training Loss: tensor(0.3721)\n",
      "8265 Training Loss: tensor(0.3755)\n",
      "8266 Training Loss: tensor(0.3697)\n",
      "8267 Training Loss: tensor(0.3730)\n",
      "8268 Training Loss: tensor(0.3733)\n",
      "8269 Training Loss: tensor(0.3751)\n",
      "8270 Training Loss: tensor(0.3721)\n",
      "8271 Training Loss: tensor(0.3708)\n",
      "8272 Training Loss: tensor(0.3713)\n",
      "8273 Training Loss: tensor(0.3727)\n",
      "8274 Training Loss: tensor(0.3737)\n",
      "8275 Training Loss: tensor(0.3746)\n",
      "8276 Training Loss: tensor(0.3703)\n",
      "8277 Training Loss: tensor(0.3707)\n",
      "8278 Training Loss: tensor(0.3686)\n",
      "8279 Training Loss: tensor(0.3775)\n",
      "8280 Training Loss: tensor(0.3705)\n",
      "8281 Training Loss: tensor(0.3673)\n",
      "8282 Training Loss: tensor(0.3768)\n",
      "8283 Training Loss: tensor(0.3776)\n",
      "8284 Training Loss: tensor(0.3698)\n",
      "8285 Training Loss: tensor(0.3765)\n",
      "8286 Training Loss: tensor(0.3696)\n",
      "8287 Training Loss: tensor(0.3804)\n",
      "8288 Training Loss: tensor(0.3808)\n",
      "8289 Training Loss: tensor(0.3707)\n",
      "8290 Training Loss: tensor(0.3770)\n",
      "8291 Training Loss: tensor(0.3753)\n",
      "8292 Training Loss: tensor(0.3703)\n",
      "8293 Training Loss: tensor(0.3728)\n",
      "8294 Training Loss: tensor(0.3755)\n",
      "8295 Training Loss: tensor(0.3730)\n",
      "8296 Training Loss: tensor(0.3777)\n",
      "8297 Training Loss: tensor(0.3746)\n",
      "8298 Training Loss: tensor(0.3789)\n",
      "8299 Training Loss: tensor(0.3716)\n",
      "8300 Training Loss: tensor(0.3706)\n",
      "8301 Training Loss: tensor(0.3756)\n",
      "8302 Training Loss: tensor(0.3730)\n",
      "8303 Training Loss: tensor(0.3714)\n",
      "8304 Training Loss: tensor(0.3751)\n",
      "8305 Training Loss: tensor(0.3753)\n",
      "8306 Training Loss: tensor(0.3716)\n",
      "8307 Training Loss: tensor(0.3737)\n",
      "8308 Training Loss: tensor(0.3728)\n",
      "8309 Training Loss: tensor(0.3767)\n",
      "8310 Training Loss: tensor(0.3731)\n",
      "8311 Training Loss: tensor(0.3697)\n",
      "8312 Training Loss: tensor(0.3721)\n",
      "8313 Training Loss: tensor(0.3714)\n",
      "8314 Training Loss: tensor(0.3754)\n",
      "8315 Training Loss: tensor(0.3706)\n",
      "8316 Training Loss: tensor(0.3711)\n",
      "8317 Training Loss: tensor(0.3764)\n",
      "8318 Training Loss: tensor(0.3728)\n",
      "8319 Training Loss: tensor(0.3702)\n",
      "8320 Training Loss: tensor(0.3684)\n",
      "8321 Training Loss: tensor(0.3696)\n",
      "8322 Training Loss: tensor(0.3752)\n",
      "8323 Training Loss: tensor(0.3749)\n",
      "8324 Training Loss: tensor(0.3717)\n",
      "8325 Training Loss: tensor(0.3737)\n",
      "8326 Training Loss: tensor(0.3707)\n",
      "8327 Training Loss: tensor(0.3748)\n",
      "8328 Training Loss: tensor(0.3725)\n",
      "8329 Training Loss: tensor(0.3696)\n",
      "8330 Training Loss: tensor(0.3767)\n",
      "8331 Training Loss: tensor(0.3710)\n",
      "8332 Training Loss: tensor(0.3770)\n",
      "8333 Training Loss: tensor(0.3690)\n",
      "8334 Training Loss: tensor(0.3770)\n",
      "8335 Training Loss: tensor(0.3724)\n",
      "8336 Training Loss: tensor(0.3694)\n",
      "8337 Training Loss: tensor(0.3727)\n",
      "8338 Training Loss: tensor(0.3773)\n",
      "8339 Training Loss: tensor(0.3716)\n",
      "8340 Training Loss: tensor(0.3731)\n",
      "8341 Training Loss: tensor(0.3767)\n",
      "8342 Training Loss: tensor(0.3747)\n",
      "8343 Training Loss: tensor(0.3739)\n",
      "8344 Training Loss: tensor(0.3735)\n",
      "8345 Training Loss: tensor(0.3736)\n",
      "8346 Training Loss: tensor(0.3753)\n",
      "8347 Training Loss: tensor(0.3735)\n",
      "8348 Training Loss: tensor(0.3725)\n",
      "8349 Training Loss: tensor(0.3747)\n",
      "8350 Training Loss: tensor(0.3717)\n",
      "8351 Training Loss: tensor(0.3783)\n",
      "8352 Training Loss: tensor(0.3725)\n",
      "8353 Training Loss: tensor(0.3760)\n",
      "8354 Training Loss: tensor(0.3774)\n",
      "8355 Training Loss: tensor(0.3758)\n",
      "8356 Training Loss: tensor(0.3722)\n",
      "8357 Training Loss: tensor(0.3726)\n",
      "8358 Training Loss: tensor(0.3694)\n",
      "8359 Training Loss: tensor(0.3748)\n",
      "8360 Training Loss: tensor(0.3714)\n",
      "8361 Training Loss: tensor(0.3707)\n",
      "8362 Training Loss: tensor(0.3694)\n",
      "8363 Training Loss: tensor(0.3707)\n",
      "8364 Training Loss: tensor(0.3703)\n",
      "8365 Training Loss: tensor(0.3754)\n",
      "8366 Training Loss: tensor(0.3693)\n",
      "8367 Training Loss: tensor(0.3702)\n",
      "8368 Training Loss: tensor(0.3731)\n",
      "8369 Training Loss: tensor(0.3802)\n",
      "8370 Training Loss: tensor(0.3679)\n",
      "8371 Training Loss: tensor(0.3771)\n",
      "8372 Training Loss: tensor(0.3761)\n",
      "8373 Training Loss: tensor(0.3862)\n",
      "8374 Training Loss: tensor(0.3728)\n",
      "8375 Training Loss: tensor(0.3766)\n",
      "8376 Training Loss: tensor(0.3728)\n",
      "8377 Training Loss: tensor(0.3739)\n",
      "8378 Training Loss: tensor(0.3709)\n",
      "8379 Training Loss: tensor(0.3733)\n",
      "8380 Training Loss: tensor(0.3716)\n",
      "8381 Training Loss: tensor(0.3745)\n",
      "8382 Training Loss: tensor(0.3748)\n",
      "8383 Training Loss: tensor(0.3767)\n",
      "8384 Training Loss: tensor(0.3703)\n",
      "8385 Training Loss: tensor(0.3713)\n",
      "8386 Training Loss: tensor(0.3718)\n",
      "8387 Training Loss: tensor(0.3713)\n",
      "8388 Training Loss: tensor(0.3754)\n",
      "8389 Training Loss: tensor(0.3721)\n",
      "8390 Training Loss: tensor(0.3736)\n",
      "8391 Training Loss: tensor(0.3708)\n",
      "8392 Training Loss: tensor(0.3701)\n",
      "8393 Training Loss: tensor(0.3757)\n",
      "8394 Training Loss: tensor(0.3677)\n",
      "8395 Training Loss: tensor(0.3720)\n",
      "8396 Training Loss: tensor(0.3722)\n",
      "8397 Training Loss: tensor(0.3703)\n",
      "8398 Training Loss: tensor(0.3713)\n",
      "8399 Training Loss: tensor(0.3734)\n",
      "8400 Training Loss: tensor(0.3701)\n",
      "8401 Training Loss: tensor(0.3704)\n",
      "8402 Training Loss: tensor(0.3713)\n",
      "8403 Training Loss: tensor(0.3720)\n",
      "8404 Training Loss: tensor(0.3765)\n",
      "8405 Training Loss: tensor(0.3684)\n",
      "8406 Training Loss: tensor(0.3756)\n",
      "8407 Training Loss: tensor(0.3739)\n",
      "8408 Training Loss: tensor(0.3737)\n",
      "8409 Training Loss: tensor(0.3735)\n",
      "8410 Training Loss: tensor(0.3712)\n",
      "8411 Training Loss: tensor(0.3715)\n",
      "8412 Training Loss: tensor(0.3759)\n",
      "8413 Training Loss: tensor(0.3701)\n",
      "8414 Training Loss: tensor(0.3747)\n",
      "8415 Training Loss: tensor(0.3699)\n",
      "8416 Training Loss: tensor(0.3724)\n",
      "8417 Training Loss: tensor(0.3726)\n",
      "8418 Training Loss: tensor(0.3742)\n",
      "8419 Training Loss: tensor(0.3712)\n",
      "8420 Training Loss: tensor(0.3732)\n",
      "8421 Training Loss: tensor(0.3760)\n",
      "8422 Training Loss: tensor(0.3756)\n",
      "8423 Training Loss: tensor(0.3748)\n",
      "8424 Training Loss: tensor(0.3710)\n",
      "8425 Training Loss: tensor(0.3719)\n",
      "8426 Training Loss: tensor(0.3699)\n",
      "8427 Training Loss: tensor(0.3715)\n",
      "8428 Training Loss: tensor(0.3732)\n",
      "8429 Training Loss: tensor(0.3725)\n",
      "8430 Training Loss: tensor(0.3706)\n",
      "8431 Training Loss: tensor(0.3755)\n",
      "8432 Training Loss: tensor(0.3772)\n",
      "8433 Training Loss: tensor(0.3715)\n",
      "8434 Training Loss: tensor(0.3693)\n",
      "8435 Training Loss: tensor(0.3716)\n",
      "8436 Training Loss: tensor(0.3761)\n",
      "8437 Training Loss: tensor(0.3749)\n",
      "8438 Training Loss: tensor(0.3778)\n",
      "8439 Training Loss: tensor(0.3747)\n",
      "8440 Training Loss: tensor(0.3718)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8441 Training Loss: tensor(0.3712)\n",
      "8442 Training Loss: tensor(0.3737)\n",
      "8443 Training Loss: tensor(0.3701)\n",
      "8444 Training Loss: tensor(0.3734)\n",
      "8445 Training Loss: tensor(0.3736)\n",
      "8446 Training Loss: tensor(0.3749)\n",
      "8447 Training Loss: tensor(0.3750)\n",
      "8448 Training Loss: tensor(0.3741)\n",
      "8449 Training Loss: tensor(0.3758)\n",
      "8450 Training Loss: tensor(0.3719)\n",
      "8451 Training Loss: tensor(0.3690)\n",
      "8452 Training Loss: tensor(0.3693)\n",
      "8453 Training Loss: tensor(0.3718)\n",
      "8454 Training Loss: tensor(0.3723)\n",
      "8455 Training Loss: tensor(0.3687)\n",
      "8456 Training Loss: tensor(0.3687)\n",
      "8457 Training Loss: tensor(0.3706)\n",
      "8458 Training Loss: tensor(0.3708)\n",
      "8459 Training Loss: tensor(0.3675)\n",
      "8460 Training Loss: tensor(0.3729)\n",
      "8461 Training Loss: tensor(0.3718)\n",
      "8462 Training Loss: tensor(0.3683)\n",
      "8463 Training Loss: tensor(0.3744)\n",
      "8464 Training Loss: tensor(0.3737)\n",
      "8465 Training Loss: tensor(0.3713)\n",
      "8466 Training Loss: tensor(0.3775)\n",
      "8467 Training Loss: tensor(0.3729)\n",
      "8468 Training Loss: tensor(0.3682)\n",
      "8469 Training Loss: tensor(0.3719)\n",
      "8470 Training Loss: tensor(0.3695)\n",
      "8471 Training Loss: tensor(0.3722)\n",
      "8472 Training Loss: tensor(0.3734)\n",
      "8473 Training Loss: tensor(0.3713)\n",
      "8474 Training Loss: tensor(0.3697)\n",
      "8475 Training Loss: tensor(0.3714)\n",
      "8476 Training Loss: tensor(0.3748)\n",
      "8477 Training Loss: tensor(0.3746)\n",
      "8478 Training Loss: tensor(0.3702)\n",
      "8479 Training Loss: tensor(0.3703)\n",
      "8480 Training Loss: tensor(0.3768)\n",
      "8481 Training Loss: tensor(0.3752)\n",
      "8482 Training Loss: tensor(0.3684)\n",
      "8483 Training Loss: tensor(0.3734)\n",
      "8484 Training Loss: tensor(0.3704)\n",
      "8485 Training Loss: tensor(0.3736)\n",
      "8486 Training Loss: tensor(0.3693)\n",
      "8487 Training Loss: tensor(0.3716)\n",
      "8488 Training Loss: tensor(0.3742)\n",
      "8489 Training Loss: tensor(0.3715)\n",
      "8490 Training Loss: tensor(0.3724)\n",
      "8491 Training Loss: tensor(0.3703)\n",
      "8492 Training Loss: tensor(0.3719)\n",
      "8493 Training Loss: tensor(0.3746)\n",
      "8494 Training Loss: tensor(0.3792)\n",
      "8495 Training Loss: tensor(0.3721)\n",
      "8496 Training Loss: tensor(0.3760)\n",
      "8497 Training Loss: tensor(0.3711)\n",
      "8498 Training Loss: tensor(0.3697)\n",
      "8499 Training Loss: tensor(0.3723)\n",
      "8500 Training Loss: tensor(0.3734)\n",
      "8501 Training Loss: tensor(0.3730)\n",
      "8502 Training Loss: tensor(0.3691)\n",
      "8503 Training Loss: tensor(0.3709)\n",
      "8504 Training Loss: tensor(0.3763)\n",
      "8505 Training Loss: tensor(0.3720)\n",
      "8506 Training Loss: tensor(0.3728)\n",
      "8507 Training Loss: tensor(0.3737)\n",
      "8508 Training Loss: tensor(0.3719)\n",
      "8509 Training Loss: tensor(0.3750)\n",
      "8510 Training Loss: tensor(0.3732)\n",
      "8511 Training Loss: tensor(0.3717)\n",
      "8512 Training Loss: tensor(0.3750)\n",
      "8513 Training Loss: tensor(0.3740)\n",
      "8514 Training Loss: tensor(0.3751)\n",
      "8515 Training Loss: tensor(0.3737)\n",
      "8516 Training Loss: tensor(0.3705)\n",
      "8517 Training Loss: tensor(0.3745)\n",
      "8518 Training Loss: tensor(0.3701)\n",
      "8519 Training Loss: tensor(0.3743)\n",
      "8520 Training Loss: tensor(0.3710)\n",
      "8521 Training Loss: tensor(0.3711)\n",
      "8522 Training Loss: tensor(0.3757)\n",
      "8523 Training Loss: tensor(0.3698)\n",
      "8524 Training Loss: tensor(0.3725)\n",
      "8525 Training Loss: tensor(0.3752)\n",
      "8526 Training Loss: tensor(0.3766)\n",
      "8527 Training Loss: tensor(0.3691)\n",
      "8528 Training Loss: tensor(0.3747)\n",
      "8529 Training Loss: tensor(0.3735)\n",
      "8530 Training Loss: tensor(0.3733)\n",
      "8531 Training Loss: tensor(0.3725)\n",
      "8532 Training Loss: tensor(0.3748)\n",
      "8533 Training Loss: tensor(0.3733)\n",
      "8534 Training Loss: tensor(0.3715)\n",
      "8535 Training Loss: tensor(0.3746)\n",
      "8536 Training Loss: tensor(0.3703)\n",
      "8537 Training Loss: tensor(0.3770)\n",
      "8538 Training Loss: tensor(0.3753)\n",
      "8539 Training Loss: tensor(0.3724)\n",
      "8540 Training Loss: tensor(0.3721)\n",
      "8541 Training Loss: tensor(0.3755)\n",
      "8542 Training Loss: tensor(0.3707)\n",
      "8543 Training Loss: tensor(0.3691)\n",
      "8544 Training Loss: tensor(0.3696)\n",
      "8545 Training Loss: tensor(0.3726)\n",
      "8546 Training Loss: tensor(0.3692)\n",
      "8547 Training Loss: tensor(0.3705)\n",
      "8548 Training Loss: tensor(0.3699)\n",
      "8549 Training Loss: tensor(0.3767)\n",
      "8550 Training Loss: tensor(0.3693)\n",
      "8551 Training Loss: tensor(0.3780)\n",
      "8552 Training Loss: tensor(0.3714)\n",
      "8553 Training Loss: tensor(0.3716)\n",
      "8554 Training Loss: tensor(0.3706)\n",
      "8555 Training Loss: tensor(0.3744)\n",
      "8556 Training Loss: tensor(0.3716)\n",
      "8557 Training Loss: tensor(0.3803)\n",
      "8558 Training Loss: tensor(0.3722)\n",
      "8559 Training Loss: tensor(0.3738)\n",
      "8560 Training Loss: tensor(0.3706)\n",
      "8561 Training Loss: tensor(0.3717)\n",
      "8562 Training Loss: tensor(0.3719)\n",
      "8563 Training Loss: tensor(0.3767)\n",
      "8564 Training Loss: tensor(0.3724)\n",
      "8565 Training Loss: tensor(0.3726)\n",
      "8566 Training Loss: tensor(0.3729)\n",
      "8567 Training Loss: tensor(0.3787)\n",
      "8568 Training Loss: tensor(0.3769)\n",
      "8569 Training Loss: tensor(0.3734)\n",
      "8570 Training Loss: tensor(0.3780)\n",
      "8571 Training Loss: tensor(0.3723)\n",
      "8572 Training Loss: tensor(0.3754)\n",
      "8573 Training Loss: tensor(0.3703)\n",
      "8574 Training Loss: tensor(0.3726)\n",
      "8575 Training Loss: tensor(0.3700)\n",
      "8576 Training Loss: tensor(0.3734)\n",
      "8577 Training Loss: tensor(0.3744)\n",
      "8578 Training Loss: tensor(0.3749)\n",
      "8579 Training Loss: tensor(0.3705)\n",
      "8580 Training Loss: tensor(0.3714)\n",
      "8581 Training Loss: tensor(0.3713)\n",
      "8582 Training Loss: tensor(0.3780)\n",
      "8583 Training Loss: tensor(0.3718)\n",
      "8584 Training Loss: tensor(0.3716)\n",
      "8585 Training Loss: tensor(0.3761)\n",
      "8586 Training Loss: tensor(0.3693)\n",
      "8587 Training Loss: tensor(0.3689)\n",
      "8588 Training Loss: tensor(0.3689)\n",
      "8589 Training Loss: tensor(0.3785)\n",
      "8590 Training Loss: tensor(0.3706)\n",
      "8591 Training Loss: tensor(0.3746)\n",
      "8592 Training Loss: tensor(0.3770)\n",
      "8593 Training Loss: tensor(0.3746)\n",
      "8594 Training Loss: tensor(0.3700)\n",
      "8595 Training Loss: tensor(0.3766)\n",
      "8596 Training Loss: tensor(0.3740)\n",
      "8597 Training Loss: tensor(0.3724)\n",
      "8598 Training Loss: tensor(0.3710)\n",
      "8599 Training Loss: tensor(0.3711)\n",
      "8600 Training Loss: tensor(0.3701)\n",
      "8601 Training Loss: tensor(0.3703)\n",
      "8602 Training Loss: tensor(0.3721)\n",
      "8603 Training Loss: tensor(0.3735)\n",
      "8604 Training Loss: tensor(0.3703)\n",
      "8605 Training Loss: tensor(0.3745)\n",
      "8606 Training Loss: tensor(0.3701)\n",
      "8607 Training Loss: tensor(0.3720)\n",
      "8608 Training Loss: tensor(0.3744)\n",
      "8609 Training Loss: tensor(0.3717)\n",
      "8610 Training Loss: tensor(0.3727)\n",
      "8611 Training Loss: tensor(0.3743)\n",
      "8612 Training Loss: tensor(0.3719)\n",
      "8613 Training Loss: tensor(0.3737)\n",
      "8614 Training Loss: tensor(0.3702)\n",
      "8615 Training Loss: tensor(0.3745)\n",
      "8616 Training Loss: tensor(0.3757)\n",
      "8617 Training Loss: tensor(0.3708)\n",
      "8618 Training Loss: tensor(0.3696)\n",
      "8619 Training Loss: tensor(0.3725)\n",
      "8620 Training Loss: tensor(0.3708)\n",
      "8621 Training Loss: tensor(0.3736)\n",
      "8622 Training Loss: tensor(0.3744)\n",
      "8623 Training Loss: tensor(0.3760)\n",
      "8624 Training Loss: tensor(0.3714)\n",
      "8625 Training Loss: tensor(0.3710)\n",
      "8626 Training Loss: tensor(0.3789)\n",
      "8627 Training Loss: tensor(0.3681)\n",
      "8628 Training Loss: tensor(0.3705)\n",
      "8629 Training Loss: tensor(0.3715)\n",
      "8630 Training Loss: tensor(0.3755)\n",
      "8631 Training Loss: tensor(0.3729)\n",
      "8632 Training Loss: tensor(0.3739)\n",
      "8633 Training Loss: tensor(0.3708)\n",
      "8634 Training Loss: tensor(0.3707)\n",
      "8635 Training Loss: tensor(0.3767)\n",
      "8636 Training Loss: tensor(0.3715)\n",
      "8637 Training Loss: tensor(0.3709)\n",
      "8638 Training Loss: tensor(0.3716)\n",
      "8639 Training Loss: tensor(0.3750)\n",
      "8640 Training Loss: tensor(0.3758)\n",
      "8641 Training Loss: tensor(0.3712)\n",
      "8642 Training Loss: tensor(0.3701)\n",
      "8643 Training Loss: tensor(0.3717)\n",
      "8644 Training Loss: tensor(0.3754)\n",
      "8645 Training Loss: tensor(0.3711)\n",
      "8646 Training Loss: tensor(0.3688)\n",
      "8647 Training Loss: tensor(0.3737)\n",
      "8648 Training Loss: tensor(0.3709)\n",
      "8649 Training Loss: tensor(0.3692)\n",
      "8650 Training Loss: tensor(0.3753)\n",
      "8651 Training Loss: tensor(0.3775)\n",
      "8652 Training Loss: tensor(0.3740)\n",
      "8653 Training Loss: tensor(0.3748)\n",
      "8654 Training Loss: tensor(0.3718)\n",
      "8655 Training Loss: tensor(0.3706)\n",
      "8656 Training Loss: tensor(0.3735)\n",
      "8657 Training Loss: tensor(0.3794)\n",
      "8658 Training Loss: tensor(0.3707)\n",
      "8659 Training Loss: tensor(0.3719)\n",
      "8660 Training Loss: tensor(0.3746)\n",
      "8661 Training Loss: tensor(0.3710)\n",
      "8662 Training Loss: tensor(0.3712)\n",
      "8663 Training Loss: tensor(0.3691)\n",
      "8664 Training Loss: tensor(0.3726)\n",
      "8665 Training Loss: tensor(0.3708)\n",
      "8666 Training Loss: tensor(0.3705)\n",
      "8667 Training Loss: tensor(0.3729)\n",
      "8668 Training Loss: tensor(0.3734)\n",
      "8669 Training Loss: tensor(0.3728)\n",
      "8670 Training Loss: tensor(0.3754)\n",
      "8671 Training Loss: tensor(0.3714)\n",
      "8672 Training Loss: tensor(0.3703)\n",
      "8673 Training Loss: tensor(0.3754)\n",
      "8674 Training Loss: tensor(0.3706)\n",
      "8675 Training Loss: tensor(0.3730)\n",
      "8676 Training Loss: tensor(0.3693)\n",
      "8677 Training Loss: tensor(0.3779)\n",
      "8678 Training Loss: tensor(0.3755)\n",
      "8679 Training Loss: tensor(0.3744)\n",
      "8680 Training Loss: tensor(0.3714)\n",
      "8681 Training Loss: tensor(0.3721)\n",
      "8682 Training Loss: tensor(0.3694)\n",
      "8683 Training Loss: tensor(0.3704)\n",
      "8684 Training Loss: tensor(0.3690)\n",
      "8685 Training Loss: tensor(0.3792)\n",
      "8686 Training Loss: tensor(0.3714)\n",
      "8687 Training Loss: tensor(0.3748)\n",
      "8688 Training Loss: tensor(0.3703)\n",
      "8689 Training Loss: tensor(0.3717)\n",
      "8690 Training Loss: tensor(0.3706)\n",
      "8691 Training Loss: tensor(0.3740)\n",
      "8692 Training Loss: tensor(0.3733)\n",
      "8693 Training Loss: tensor(0.3690)\n",
      "8694 Training Loss: tensor(0.3731)\n",
      "8695 Training Loss: tensor(0.3730)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8696 Training Loss: tensor(0.3698)\n",
      "8697 Training Loss: tensor(0.3706)\n",
      "8698 Training Loss: tensor(0.3683)\n",
      "8699 Training Loss: tensor(0.3751)\n",
      "8700 Training Loss: tensor(0.3743)\n",
      "8701 Training Loss: tensor(0.3711)\n",
      "8702 Training Loss: tensor(0.3775)\n",
      "8703 Training Loss: tensor(0.3729)\n",
      "8704 Training Loss: tensor(0.3783)\n",
      "8705 Training Loss: tensor(0.3684)\n",
      "8706 Training Loss: tensor(0.3729)\n",
      "8707 Training Loss: tensor(0.3721)\n",
      "8708 Training Loss: tensor(0.3758)\n",
      "8709 Training Loss: tensor(0.3694)\n",
      "8710 Training Loss: tensor(0.3757)\n",
      "8711 Training Loss: tensor(0.3744)\n",
      "8712 Training Loss: tensor(0.3724)\n",
      "8713 Training Loss: tensor(0.3729)\n",
      "8714 Training Loss: tensor(0.3725)\n",
      "8715 Training Loss: tensor(0.3746)\n",
      "8716 Training Loss: tensor(0.3734)\n",
      "8717 Training Loss: tensor(0.3698)\n",
      "8718 Training Loss: tensor(0.3700)\n",
      "8719 Training Loss: tensor(0.3748)\n",
      "8720 Training Loss: tensor(0.3750)\n",
      "8721 Training Loss: tensor(0.3712)\n",
      "8722 Training Loss: tensor(0.3736)\n",
      "8723 Training Loss: tensor(0.3724)\n",
      "8724 Training Loss: tensor(0.3746)\n",
      "8725 Training Loss: tensor(0.3731)\n",
      "8726 Training Loss: tensor(0.3704)\n",
      "8727 Training Loss: tensor(0.3735)\n",
      "8728 Training Loss: tensor(0.3738)\n",
      "8729 Training Loss: tensor(0.3750)\n",
      "8730 Training Loss: tensor(0.3719)\n",
      "8731 Training Loss: tensor(0.3720)\n",
      "8732 Training Loss: tensor(0.3695)\n",
      "8733 Training Loss: tensor(0.3731)\n",
      "8734 Training Loss: tensor(0.3729)\n",
      "8735 Training Loss: tensor(0.3755)\n",
      "8736 Training Loss: tensor(0.3691)\n",
      "8737 Training Loss: tensor(0.3773)\n",
      "8738 Training Loss: tensor(0.3735)\n",
      "8739 Training Loss: tensor(0.3716)\n",
      "8740 Training Loss: tensor(0.3743)\n",
      "8741 Training Loss: tensor(0.3732)\n",
      "8742 Training Loss: tensor(0.3739)\n",
      "8743 Training Loss: tensor(0.3707)\n",
      "8744 Training Loss: tensor(0.3737)\n",
      "8745 Training Loss: tensor(0.3730)\n",
      "8746 Training Loss: tensor(0.3744)\n",
      "8747 Training Loss: tensor(0.3687)\n",
      "8748 Training Loss: tensor(0.3687)\n",
      "8749 Training Loss: tensor(0.3732)\n",
      "8750 Training Loss: tensor(0.3702)\n",
      "8751 Training Loss: tensor(0.3710)\n",
      "8752 Training Loss: tensor(0.3738)\n",
      "8753 Training Loss: tensor(0.3727)\n",
      "8754 Training Loss: tensor(0.3744)\n",
      "8755 Training Loss: tensor(0.3752)\n",
      "8756 Training Loss: tensor(0.3709)\n",
      "8757 Training Loss: tensor(0.3683)\n",
      "8758 Training Loss: tensor(0.3755)\n",
      "8759 Training Loss: tensor(0.3771)\n",
      "8760 Training Loss: tensor(0.3758)\n",
      "8761 Training Loss: tensor(0.3793)\n",
      "8762 Training Loss: tensor(0.3717)\n",
      "8763 Training Loss: tensor(0.3696)\n",
      "8764 Training Loss: tensor(0.3722)\n",
      "8765 Training Loss: tensor(0.3711)\n",
      "8766 Training Loss: tensor(0.3750)\n",
      "8767 Training Loss: tensor(0.3728)\n",
      "8768 Training Loss: tensor(0.3701)\n",
      "8769 Training Loss: tensor(0.3748)\n",
      "8770 Training Loss: tensor(0.3713)\n",
      "8771 Training Loss: tensor(0.3750)\n",
      "8772 Training Loss: tensor(0.3728)\n",
      "8773 Training Loss: tensor(0.3726)\n",
      "8774 Training Loss: tensor(0.3707)\n",
      "8775 Training Loss: tensor(0.3708)\n",
      "8776 Training Loss: tensor(0.3713)\n",
      "8777 Training Loss: tensor(0.3748)\n",
      "8778 Training Loss: tensor(0.3726)\n",
      "8779 Training Loss: tensor(0.3780)\n",
      "8780 Training Loss: tensor(0.3684)\n",
      "8781 Training Loss: tensor(0.3687)\n",
      "8782 Training Loss: tensor(0.3774)\n",
      "8783 Training Loss: tensor(0.3688)\n",
      "8784 Training Loss: tensor(0.3745)\n",
      "8785 Training Loss: tensor(0.3720)\n",
      "8786 Training Loss: tensor(0.3694)\n",
      "8787 Training Loss: tensor(0.3722)\n",
      "8788 Training Loss: tensor(0.3706)\n",
      "8789 Training Loss: tensor(0.3736)\n",
      "8790 Training Loss: tensor(0.3747)\n",
      "8791 Training Loss: tensor(0.3678)\n",
      "8792 Training Loss: tensor(0.3830)\n",
      "8793 Training Loss: tensor(0.3717)\n",
      "8794 Training Loss: tensor(0.3734)\n",
      "8795 Training Loss: tensor(0.3712)\n",
      "8796 Training Loss: tensor(0.3709)\n",
      "8797 Training Loss: tensor(0.3720)\n",
      "8798 Training Loss: tensor(0.3761)\n",
      "8799 Training Loss: tensor(0.3682)\n",
      "8800 Training Loss: tensor(0.3754)\n",
      "8801 Training Loss: tensor(0.3734)\n",
      "8802 Training Loss: tensor(0.3735)\n",
      "8803 Training Loss: tensor(0.3831)\n",
      "8804 Training Loss: tensor(0.3690)\n",
      "8805 Training Loss: tensor(0.3698)\n",
      "8806 Training Loss: tensor(0.3707)\n",
      "8807 Training Loss: tensor(0.3717)\n",
      "8808 Training Loss: tensor(0.3709)\n",
      "8809 Training Loss: tensor(0.3734)\n",
      "8810 Training Loss: tensor(0.3723)\n",
      "8811 Training Loss: tensor(0.3701)\n",
      "8812 Training Loss: tensor(0.3703)\n",
      "8813 Training Loss: tensor(0.3722)\n",
      "8814 Training Loss: tensor(0.3705)\n",
      "8815 Training Loss: tensor(0.3689)\n",
      "8816 Training Loss: tensor(0.3708)\n",
      "8817 Training Loss: tensor(0.3691)\n",
      "8818 Training Loss: tensor(0.3699)\n",
      "8819 Training Loss: tensor(0.3733)\n",
      "8820 Training Loss: tensor(0.3706)\n",
      "8821 Training Loss: tensor(0.3752)\n",
      "8822 Training Loss: tensor(0.3675)\n",
      "8823 Training Loss: tensor(0.3711)\n",
      "8824 Training Loss: tensor(0.3684)\n",
      "8825 Training Loss: tensor(0.3714)\n",
      "8826 Training Loss: tensor(0.3716)\n",
      "8827 Training Loss: tensor(0.3767)\n",
      "8828 Training Loss: tensor(0.3768)\n",
      "8829 Training Loss: tensor(0.3713)\n",
      "8830 Training Loss: tensor(0.3712)\n",
      "8831 Training Loss: tensor(0.3662)\n",
      "8832 Training Loss: tensor(0.3698)\n",
      "8833 Training Loss: tensor(0.3715)\n",
      "8834 Training Loss: tensor(0.3760)\n",
      "8835 Training Loss: tensor(0.3730)\n",
      "8836 Training Loss: tensor(0.3718)\n",
      "8837 Training Loss: tensor(0.3716)\n",
      "8838 Training Loss: tensor(0.3712)\n",
      "8839 Training Loss: tensor(0.3758)\n",
      "8840 Training Loss: tensor(0.3781)\n",
      "8841 Training Loss: tensor(0.3810)\n",
      "8842 Training Loss: tensor(0.3683)\n",
      "8843 Training Loss: tensor(0.3711)\n",
      "8844 Training Loss: tensor(0.3715)\n",
      "8845 Training Loss: tensor(0.3757)\n",
      "8846 Training Loss: tensor(0.3704)\n",
      "8847 Training Loss: tensor(0.3695)\n",
      "8848 Training Loss: tensor(0.3769)\n",
      "8849 Training Loss: tensor(0.3720)\n",
      "8850 Training Loss: tensor(0.3712)\n",
      "8851 Training Loss: tensor(0.3738)\n",
      "8852 Training Loss: tensor(0.3716)\n",
      "8853 Training Loss: tensor(0.3695)\n",
      "8854 Training Loss: tensor(0.3743)\n",
      "8855 Training Loss: tensor(0.3705)\n",
      "8856 Training Loss: tensor(0.3712)\n",
      "8857 Training Loss: tensor(0.3720)\n",
      "8858 Training Loss: tensor(0.3718)\n",
      "8859 Training Loss: tensor(0.3726)\n",
      "8860 Training Loss: tensor(0.3710)\n",
      "8861 Training Loss: tensor(0.3739)\n",
      "8862 Training Loss: tensor(0.3708)\n",
      "8863 Training Loss: tensor(0.3739)\n",
      "8864 Training Loss: tensor(0.3738)\n",
      "8865 Training Loss: tensor(0.3706)\n",
      "8866 Training Loss: tensor(0.3831)\n",
      "8867 Training Loss: tensor(0.3734)\n",
      "8868 Training Loss: tensor(0.3687)\n",
      "8869 Training Loss: tensor(0.3719)\n",
      "8870 Training Loss: tensor(0.3723)\n",
      "8871 Training Loss: tensor(0.3694)\n",
      "8872 Training Loss: tensor(0.3692)\n",
      "8873 Training Loss: tensor(0.3730)\n",
      "8874 Training Loss: tensor(0.3686)\n",
      "8875 Training Loss: tensor(0.3722)\n",
      "8876 Training Loss: tensor(0.3684)\n",
      "8877 Training Loss: tensor(0.3720)\n",
      "8878 Training Loss: tensor(0.3733)\n",
      "8879 Training Loss: tensor(0.3706)\n",
      "8880 Training Loss: tensor(0.3718)\n",
      "8881 Training Loss: tensor(0.3729)\n",
      "8882 Training Loss: tensor(0.3710)\n",
      "8883 Training Loss: tensor(0.3751)\n",
      "8884 Training Loss: tensor(0.3735)\n",
      "8885 Training Loss: tensor(0.3685)\n",
      "8886 Training Loss: tensor(0.3745)\n",
      "8887 Training Loss: tensor(0.3718)\n",
      "8888 Training Loss: tensor(0.3704)\n",
      "8889 Training Loss: tensor(0.3708)\n",
      "8890 Training Loss: tensor(0.3686)\n",
      "8891 Training Loss: tensor(0.3699)\n",
      "8892 Training Loss: tensor(0.3688)\n",
      "8893 Training Loss: tensor(0.3740)\n",
      "8894 Training Loss: tensor(0.3700)\n",
      "8895 Training Loss: tensor(0.3697)\n",
      "8896 Training Loss: tensor(0.3691)\n",
      "8897 Training Loss: tensor(0.3778)\n",
      "8898 Training Loss: tensor(0.3675)\n",
      "8899 Training Loss: tensor(0.3700)\n",
      "8900 Training Loss: tensor(0.3712)\n",
      "8901 Training Loss: tensor(0.3660)\n",
      "8902 Training Loss: tensor(0.3749)\n",
      "8903 Training Loss: tensor(0.3665)\n",
      "8904 Training Loss: tensor(0.3761)\n",
      "8905 Training Loss: tensor(0.3697)\n",
      "8906 Training Loss: tensor(0.3730)\n",
      "8907 Training Loss: tensor(0.3733)\n",
      "8908 Training Loss: tensor(0.3747)\n",
      "8909 Training Loss: tensor(0.3734)\n",
      "8910 Training Loss: tensor(0.3767)\n",
      "8911 Training Loss: tensor(0.3705)\n",
      "8912 Training Loss: tensor(0.3804)\n",
      "8913 Training Loss: tensor(0.3732)\n",
      "8914 Training Loss: tensor(0.3710)\n",
      "8915 Training Loss: tensor(0.3732)\n",
      "8916 Training Loss: tensor(0.3728)\n",
      "8917 Training Loss: tensor(0.3763)\n",
      "8918 Training Loss: tensor(0.3709)\n",
      "8919 Training Loss: tensor(0.3730)\n",
      "8920 Training Loss: tensor(0.3709)\n",
      "8921 Training Loss: tensor(0.3721)\n",
      "8922 Training Loss: tensor(0.3740)\n",
      "8923 Training Loss: tensor(0.3738)\n",
      "8924 Training Loss: tensor(0.3725)\n",
      "8925 Training Loss: tensor(0.3712)\n",
      "8926 Training Loss: tensor(0.3706)\n",
      "8927 Training Loss: tensor(0.3728)\n",
      "8928 Training Loss: tensor(0.3706)\n",
      "8929 Training Loss: tensor(0.3731)\n",
      "8930 Training Loss: tensor(0.3686)\n",
      "8931 Training Loss: tensor(0.3673)\n",
      "8932 Training Loss: tensor(0.3746)\n",
      "8933 Training Loss: tensor(0.3731)\n",
      "8934 Training Loss: tensor(0.3695)\n",
      "8935 Training Loss: tensor(0.3682)\n",
      "8936 Training Loss: tensor(0.3696)\n",
      "8937 Training Loss: tensor(0.3691)\n",
      "8938 Training Loss: tensor(0.3755)\n",
      "8939 Training Loss: tensor(0.3742)\n",
      "8940 Training Loss: tensor(0.3776)\n",
      "8941 Training Loss: tensor(0.3693)\n",
      "8942 Training Loss: tensor(0.3750)\n",
      "8943 Training Loss: tensor(0.3690)\n",
      "8944 Training Loss: tensor(0.3757)\n",
      "8945 Training Loss: tensor(0.3748)\n",
      "8946 Training Loss: tensor(0.3705)\n",
      "8947 Training Loss: tensor(0.3703)\n",
      "8948 Training Loss: tensor(0.3703)\n",
      "8949 Training Loss: tensor(0.3714)\n",
      "8950 Training Loss: tensor(0.3707)\n",
      "8951 Training Loss: tensor(0.3699)\n",
      "8952 Training Loss: tensor(0.3709)\n",
      "8953 Training Loss: tensor(0.3679)\n",
      "8954 Training Loss: tensor(0.3732)\n",
      "8955 Training Loss: tensor(0.3758)\n",
      "8956 Training Loss: tensor(0.3715)\n",
      "8957 Training Loss: tensor(0.3714)\n",
      "8958 Training Loss: tensor(0.3726)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8959 Training Loss: tensor(0.3707)\n",
      "8960 Training Loss: tensor(0.3684)\n",
      "8961 Training Loss: tensor(0.3697)\n",
      "8962 Training Loss: tensor(0.3716)\n",
      "8963 Training Loss: tensor(0.3715)\n",
      "8964 Training Loss: tensor(0.3725)\n",
      "8965 Training Loss: tensor(0.3685)\n",
      "8966 Training Loss: tensor(0.3715)\n",
      "8967 Training Loss: tensor(0.3730)\n",
      "8968 Training Loss: tensor(0.3697)\n",
      "8969 Training Loss: tensor(0.3719)\n",
      "8970 Training Loss: tensor(0.3685)\n",
      "8971 Training Loss: tensor(0.3748)\n",
      "8972 Training Loss: tensor(0.3725)\n",
      "8973 Training Loss: tensor(0.3714)\n",
      "8974 Training Loss: tensor(0.3739)\n",
      "8975 Training Loss: tensor(0.3706)\n",
      "8976 Training Loss: tensor(0.3728)\n",
      "8977 Training Loss: tensor(0.3728)\n",
      "8978 Training Loss: tensor(0.3747)\n",
      "8979 Training Loss: tensor(0.3725)\n",
      "8980 Training Loss: tensor(0.3740)\n",
      "8981 Training Loss: tensor(0.3710)\n",
      "8982 Training Loss: tensor(0.3691)\n",
      "8983 Training Loss: tensor(0.3703)\n",
      "8984 Training Loss: tensor(0.3736)\n",
      "8985 Training Loss: tensor(0.3737)\n",
      "8986 Training Loss: tensor(0.3721)\n",
      "8987 Training Loss: tensor(0.3721)\n",
      "8988 Training Loss: tensor(0.3739)\n",
      "8989 Training Loss: tensor(0.3695)\n",
      "8990 Training Loss: tensor(0.3703)\n",
      "8991 Training Loss: tensor(0.3746)\n",
      "8992 Training Loss: tensor(0.3714)\n",
      "8993 Training Loss: tensor(0.3713)\n",
      "8994 Training Loss: tensor(0.3778)\n",
      "8995 Training Loss: tensor(0.3732)\n",
      "8996 Training Loss: tensor(0.3739)\n",
      "8997 Training Loss: tensor(0.3701)\n",
      "8998 Training Loss: tensor(0.3706)\n",
      "8999 Training Loss: tensor(0.3712)\n",
      "9000 Training Loss: tensor(0.3713)\n",
      "9001 Training Loss: tensor(0.3723)\n",
      "9002 Training Loss: tensor(0.3760)\n",
      "9003 Training Loss: tensor(0.3739)\n",
      "9004 Training Loss: tensor(0.3701)\n",
      "9005 Training Loss: tensor(0.3679)\n",
      "9006 Training Loss: tensor(0.3685)\n",
      "9007 Training Loss: tensor(0.3726)\n",
      "9008 Training Loss: tensor(0.3744)\n",
      "9009 Training Loss: tensor(0.3759)\n",
      "9010 Training Loss: tensor(0.3710)\n",
      "9011 Training Loss: tensor(0.3717)\n",
      "9012 Training Loss: tensor(0.3704)\n",
      "9013 Training Loss: tensor(0.3733)\n",
      "9014 Training Loss: tensor(0.3766)\n",
      "9015 Training Loss: tensor(0.3720)\n",
      "9016 Training Loss: tensor(0.3770)\n",
      "9017 Training Loss: tensor(0.3695)\n",
      "9018 Training Loss: tensor(0.3690)\n",
      "9019 Training Loss: tensor(0.3707)\n",
      "9020 Training Loss: tensor(0.3736)\n",
      "9021 Training Loss: tensor(0.3681)\n",
      "9022 Training Loss: tensor(0.3710)\n",
      "9023 Training Loss: tensor(0.3708)\n",
      "9024 Training Loss: tensor(0.3705)\n",
      "9025 Training Loss: tensor(0.3734)\n",
      "9026 Training Loss: tensor(0.3703)\n",
      "9027 Training Loss: tensor(0.3688)\n",
      "9028 Training Loss: tensor(0.3698)\n",
      "9029 Training Loss: tensor(0.3780)\n",
      "9030 Training Loss: tensor(0.3737)\n",
      "9031 Training Loss: tensor(0.3730)\n",
      "9032 Training Loss: tensor(0.3711)\n",
      "9033 Training Loss: tensor(0.3671)\n",
      "9034 Training Loss: tensor(0.3726)\n",
      "9035 Training Loss: tensor(0.3716)\n",
      "9036 Training Loss: tensor(0.3722)\n",
      "9037 Training Loss: tensor(0.3713)\n",
      "9038 Training Loss: tensor(0.3701)\n",
      "9039 Training Loss: tensor(0.3697)\n",
      "9040 Training Loss: tensor(0.3714)\n",
      "9041 Training Loss: tensor(0.3714)\n",
      "9042 Training Loss: tensor(0.3695)\n",
      "9043 Training Loss: tensor(0.3708)\n",
      "9044 Training Loss: tensor(0.3684)\n",
      "9045 Training Loss: tensor(0.3724)\n",
      "9046 Training Loss: tensor(0.3754)\n",
      "9047 Training Loss: tensor(0.3723)\n",
      "9048 Training Loss: tensor(0.3702)\n",
      "9049 Training Loss: tensor(0.3720)\n",
      "9050 Training Loss: tensor(0.3733)\n",
      "9051 Training Loss: tensor(0.3736)\n",
      "9052 Training Loss: tensor(0.3687)\n",
      "9053 Training Loss: tensor(0.3694)\n",
      "9054 Training Loss: tensor(0.3728)\n",
      "9055 Training Loss: tensor(0.3778)\n",
      "9056 Training Loss: tensor(0.3662)\n",
      "9057 Training Loss: tensor(0.3721)\n",
      "9058 Training Loss: tensor(0.3722)\n",
      "9059 Training Loss: tensor(0.3705)\n",
      "9060 Training Loss: tensor(0.3766)\n",
      "9061 Training Loss: tensor(0.3722)\n",
      "9062 Training Loss: tensor(0.3721)\n",
      "9063 Training Loss: tensor(0.3763)\n",
      "9064 Training Loss: tensor(0.3729)\n",
      "9065 Training Loss: tensor(0.3752)\n",
      "9066 Training Loss: tensor(0.3742)\n",
      "9067 Training Loss: tensor(0.3723)\n",
      "9068 Training Loss: tensor(0.3720)\n",
      "9069 Training Loss: tensor(0.3712)\n",
      "9070 Training Loss: tensor(0.3694)\n",
      "9071 Training Loss: tensor(0.3709)\n",
      "9072 Training Loss: tensor(0.3761)\n",
      "9073 Training Loss: tensor(0.3724)\n",
      "9074 Training Loss: tensor(0.3693)\n",
      "9075 Training Loss: tensor(0.3711)\n",
      "9076 Training Loss: tensor(0.3713)\n",
      "9077 Training Loss: tensor(0.3708)\n",
      "9078 Training Loss: tensor(0.3739)\n",
      "9079 Training Loss: tensor(0.3833)\n",
      "9080 Training Loss: tensor(0.3729)\n",
      "9081 Training Loss: tensor(0.3743)\n",
      "9082 Training Loss: tensor(0.3765)\n",
      "9083 Training Loss: tensor(0.3709)\n",
      "9084 Training Loss: tensor(0.3730)\n",
      "9085 Training Loss: tensor(0.3718)\n",
      "9086 Training Loss: tensor(0.3719)\n",
      "9087 Training Loss: tensor(0.3711)\n",
      "9088 Training Loss: tensor(0.3698)\n",
      "9089 Training Loss: tensor(0.3699)\n",
      "9090 Training Loss: tensor(0.3690)\n",
      "9091 Training Loss: tensor(0.3731)\n",
      "9092 Training Loss: tensor(0.3716)\n",
      "9093 Training Loss: tensor(0.3696)\n",
      "9094 Training Loss: tensor(0.3718)\n",
      "9095 Training Loss: tensor(0.3688)\n",
      "9096 Training Loss: tensor(0.3786)\n",
      "9097 Training Loss: tensor(0.3687)\n",
      "9098 Training Loss: tensor(0.3756)\n",
      "9099 Training Loss: tensor(0.3706)\n",
      "9100 Training Loss: tensor(0.3787)\n",
      "9101 Training Loss: tensor(0.3714)\n",
      "9102 Training Loss: tensor(0.3724)\n",
      "9103 Training Loss: tensor(0.3702)\n",
      "9104 Training Loss: tensor(0.3720)\n",
      "9105 Training Loss: tensor(0.3738)\n",
      "9106 Training Loss: tensor(0.3706)\n",
      "9107 Training Loss: tensor(0.3706)\n",
      "9108 Training Loss: tensor(0.3757)\n",
      "9109 Training Loss: tensor(0.3700)\n",
      "9110 Training Loss: tensor(0.3702)\n",
      "9111 Training Loss: tensor(0.3725)\n",
      "9112 Training Loss: tensor(0.3683)\n",
      "9113 Training Loss: tensor(0.3717)\n",
      "9114 Training Loss: tensor(0.3685)\n",
      "9115 Training Loss: tensor(0.3695)\n",
      "9116 Training Loss: tensor(0.3739)\n",
      "9117 Training Loss: tensor(0.3697)\n",
      "9118 Training Loss: tensor(0.3751)\n",
      "9119 Training Loss: tensor(0.3706)\n",
      "9120 Training Loss: tensor(0.3675)\n",
      "9121 Training Loss: tensor(0.3701)\n",
      "9122 Training Loss: tensor(0.3754)\n",
      "9123 Training Loss: tensor(0.3714)\n",
      "9124 Training Loss: tensor(0.3767)\n",
      "9125 Training Loss: tensor(0.3711)\n",
      "9126 Training Loss: tensor(0.3697)\n",
      "9127 Training Loss: tensor(0.3703)\n",
      "9128 Training Loss: tensor(0.3737)\n",
      "9129 Training Loss: tensor(0.3694)\n",
      "9130 Training Loss: tensor(0.3742)\n",
      "9131 Training Loss: tensor(0.3729)\n",
      "9132 Training Loss: tensor(0.3677)\n",
      "9133 Training Loss: tensor(0.3742)\n",
      "9134 Training Loss: tensor(0.3696)\n",
      "9135 Training Loss: tensor(0.3745)\n",
      "9136 Training Loss: tensor(0.3723)\n",
      "9137 Training Loss: tensor(0.3836)\n",
      "9138 Training Loss: tensor(0.3737)\n",
      "9139 Training Loss: tensor(0.3729)\n",
      "9140 Training Loss: tensor(0.3769)\n",
      "9141 Training Loss: tensor(0.3684)\n",
      "9142 Training Loss: tensor(0.3699)\n",
      "9143 Training Loss: tensor(0.3709)\n",
      "9144 Training Loss: tensor(0.3734)\n",
      "9145 Training Loss: tensor(0.3729)\n",
      "9146 Training Loss: tensor(0.3731)\n",
      "9147 Training Loss: tensor(0.3725)\n",
      "9148 Training Loss: tensor(0.3736)\n",
      "9149 Training Loss: tensor(0.3698)\n",
      "9150 Training Loss: tensor(0.3715)\n",
      "9151 Training Loss: tensor(0.3711)\n",
      "9152 Training Loss: tensor(0.3693)\n",
      "9153 Training Loss: tensor(0.3759)\n",
      "9154 Training Loss: tensor(0.3719)\n",
      "9155 Training Loss: tensor(0.3759)\n",
      "9156 Training Loss: tensor(0.3758)\n",
      "9157 Training Loss: tensor(0.3712)\n",
      "9158 Training Loss: tensor(0.3706)\n",
      "9159 Training Loss: tensor(0.3720)\n",
      "9160 Training Loss: tensor(0.3717)\n",
      "9161 Training Loss: tensor(0.3716)\n",
      "9162 Training Loss: tensor(0.3685)\n",
      "9163 Training Loss: tensor(0.3713)\n",
      "9164 Training Loss: tensor(0.3763)\n",
      "9165 Training Loss: tensor(0.3715)\n",
      "9166 Training Loss: tensor(0.3701)\n",
      "9167 Training Loss: tensor(0.3709)\n",
      "9168 Training Loss: tensor(0.3689)\n",
      "9169 Training Loss: tensor(0.3734)\n",
      "9170 Training Loss: tensor(0.3738)\n",
      "9171 Training Loss: tensor(0.3723)\n",
      "9172 Training Loss: tensor(0.3695)\n",
      "9173 Training Loss: tensor(0.3675)\n",
      "9174 Training Loss: tensor(0.3736)\n",
      "9175 Training Loss: tensor(0.3761)\n",
      "9176 Training Loss: tensor(0.3709)\n",
      "9177 Training Loss: tensor(0.3705)\n",
      "9178 Training Loss: tensor(0.3717)\n",
      "9179 Training Loss: tensor(0.3763)\n",
      "9180 Training Loss: tensor(0.3762)\n",
      "9181 Training Loss: tensor(0.3755)\n",
      "9182 Training Loss: tensor(0.3708)\n",
      "9183 Training Loss: tensor(0.3716)\n",
      "9184 Training Loss: tensor(0.3792)\n",
      "9185 Training Loss: tensor(0.3702)\n",
      "9186 Training Loss: tensor(0.3738)\n",
      "9187 Training Loss: tensor(0.3716)\n",
      "9188 Training Loss: tensor(0.3740)\n",
      "9189 Training Loss: tensor(0.3729)\n",
      "9190 Training Loss: tensor(0.3711)\n",
      "9191 Training Loss: tensor(0.3736)\n",
      "9192 Training Loss: tensor(0.3736)\n",
      "9193 Training Loss: tensor(0.3714)\n",
      "9194 Training Loss: tensor(0.3698)\n",
      "9195 Training Loss: tensor(0.3726)\n",
      "9196 Training Loss: tensor(0.3684)\n",
      "9197 Training Loss: tensor(0.3732)\n",
      "9198 Training Loss: tensor(0.3740)\n",
      "9199 Training Loss: tensor(0.3722)\n",
      "9200 Training Loss: tensor(0.3747)\n",
      "9201 Training Loss: tensor(0.3743)\n",
      "9202 Training Loss: tensor(0.3726)\n",
      "9203 Training Loss: tensor(0.3686)\n",
      "9204 Training Loss: tensor(0.3712)\n",
      "9205 Training Loss: tensor(0.3690)\n",
      "9206 Training Loss: tensor(0.3705)\n",
      "9207 Training Loss: tensor(0.3687)\n",
      "9208 Training Loss: tensor(0.3718)\n",
      "9209 Training Loss: tensor(0.3708)\n",
      "9210 Training Loss: tensor(0.3678)\n",
      "9211 Training Loss: tensor(0.3687)\n",
      "9212 Training Loss: tensor(0.3759)\n",
      "9213 Training Loss: tensor(0.3702)\n",
      "9214 Training Loss: tensor(0.3698)\n",
      "9215 Training Loss: tensor(0.3710)\n",
      "9216 Training Loss: tensor(0.3690)\n",
      "9217 Training Loss: tensor(0.3747)\n",
      "9218 Training Loss: tensor(0.3701)\n",
      "9219 Training Loss: tensor(0.3701)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9220 Training Loss: tensor(0.3694)\n",
      "9221 Training Loss: tensor(0.3714)\n",
      "9222 Training Loss: tensor(0.3724)\n",
      "9223 Training Loss: tensor(0.3719)\n",
      "9224 Training Loss: tensor(0.3771)\n",
      "9225 Training Loss: tensor(0.3719)\n",
      "9226 Training Loss: tensor(0.3732)\n",
      "9227 Training Loss: tensor(0.3703)\n",
      "9228 Training Loss: tensor(0.3756)\n",
      "9229 Training Loss: tensor(0.3741)\n",
      "9230 Training Loss: tensor(0.3742)\n",
      "9231 Training Loss: tensor(0.3746)\n",
      "9232 Training Loss: tensor(0.3718)\n",
      "9233 Training Loss: tensor(0.3708)\n",
      "9234 Training Loss: tensor(0.3693)\n",
      "9235 Training Loss: tensor(0.3789)\n",
      "9236 Training Loss: tensor(0.3684)\n",
      "9237 Training Loss: tensor(0.3695)\n",
      "9238 Training Loss: tensor(0.3745)\n",
      "9239 Training Loss: tensor(0.3726)\n",
      "9240 Training Loss: tensor(0.3704)\n",
      "9241 Training Loss: tensor(0.3706)\n",
      "9242 Training Loss: tensor(0.3734)\n",
      "9243 Training Loss: tensor(0.3705)\n",
      "9244 Training Loss: tensor(0.3719)\n",
      "9245 Training Loss: tensor(0.3734)\n",
      "9246 Training Loss: tensor(0.3739)\n",
      "9247 Training Loss: tensor(0.3715)\n",
      "9248 Training Loss: tensor(0.3698)\n",
      "9249 Training Loss: tensor(0.3708)\n",
      "9250 Training Loss: tensor(0.3711)\n",
      "9251 Training Loss: tensor(0.3750)\n",
      "9252 Training Loss: tensor(0.3672)\n",
      "9253 Training Loss: tensor(0.3718)\n",
      "9254 Training Loss: tensor(0.3726)\n",
      "9255 Training Loss: tensor(0.3760)\n",
      "9256 Training Loss: tensor(0.3688)\n",
      "9257 Training Loss: tensor(0.3715)\n",
      "9258 Training Loss: tensor(0.3699)\n",
      "9259 Training Loss: tensor(0.3705)\n",
      "9260 Training Loss: tensor(0.3703)\n",
      "9261 Training Loss: tensor(0.3703)\n",
      "9262 Training Loss: tensor(0.3688)\n",
      "9263 Training Loss: tensor(0.3674)\n",
      "9264 Training Loss: tensor(0.3744)\n",
      "9265 Training Loss: tensor(0.3798)\n",
      "9266 Training Loss: tensor(0.3728)\n",
      "9267 Training Loss: tensor(0.3701)\n",
      "9268 Training Loss: tensor(0.3694)\n",
      "9269 Training Loss: tensor(0.3716)\n",
      "9270 Training Loss: tensor(0.3688)\n",
      "9271 Training Loss: tensor(0.3694)\n",
      "9272 Training Loss: tensor(0.3751)\n",
      "9273 Training Loss: tensor(0.3703)\n",
      "9274 Training Loss: tensor(0.3687)\n",
      "9275 Training Loss: tensor(0.3700)\n",
      "9276 Training Loss: tensor(0.3746)\n",
      "9277 Training Loss: tensor(0.3734)\n",
      "9278 Training Loss: tensor(0.3704)\n",
      "9279 Training Loss: tensor(0.3753)\n",
      "9280 Training Loss: tensor(0.3744)\n",
      "9281 Training Loss: tensor(0.3742)\n",
      "9282 Training Loss: tensor(0.3757)\n",
      "9283 Training Loss: tensor(0.3710)\n",
      "9284 Training Loss: tensor(0.3719)\n",
      "9285 Training Loss: tensor(0.3684)\n",
      "9286 Training Loss: tensor(0.3714)\n",
      "9287 Training Loss: tensor(0.3712)\n",
      "9288 Training Loss: tensor(0.3700)\n",
      "9289 Training Loss: tensor(0.3701)\n",
      "9290 Training Loss: tensor(0.3705)\n",
      "9291 Training Loss: tensor(0.3742)\n",
      "9292 Training Loss: tensor(0.3695)\n",
      "9293 Training Loss: tensor(0.3728)\n",
      "9294 Training Loss: tensor(0.3687)\n",
      "9295 Training Loss: tensor(0.3704)\n",
      "9296 Training Loss: tensor(0.3722)\n",
      "9297 Training Loss: tensor(0.3718)\n",
      "9298 Training Loss: tensor(0.3719)\n",
      "9299 Training Loss: tensor(0.3662)\n",
      "9300 Training Loss: tensor(0.3729)\n",
      "9301 Training Loss: tensor(0.3678)\n",
      "9302 Training Loss: tensor(0.3719)\n",
      "9303 Training Loss: tensor(0.3675)\n",
      "9304 Training Loss: tensor(0.3728)\n",
      "9305 Training Loss: tensor(0.3715)\n",
      "9306 Training Loss: tensor(0.3761)\n",
      "9307 Training Loss: tensor(0.3680)\n",
      "9308 Training Loss: tensor(0.3813)\n",
      "9309 Training Loss: tensor(0.3687)\n",
      "9310 Training Loss: tensor(0.3729)\n",
      "9311 Training Loss: tensor(0.3720)\n",
      "9312 Training Loss: tensor(0.3691)\n",
      "9313 Training Loss: tensor(0.3717)\n",
      "9314 Training Loss: tensor(0.3693)\n",
      "9315 Training Loss: tensor(0.3727)\n",
      "9316 Training Loss: tensor(0.3711)\n",
      "9317 Training Loss: tensor(0.3688)\n",
      "9318 Training Loss: tensor(0.3725)\n",
      "9319 Training Loss: tensor(0.3723)\n",
      "9320 Training Loss: tensor(0.3713)\n",
      "9321 Training Loss: tensor(0.3736)\n",
      "9322 Training Loss: tensor(0.3726)\n",
      "9323 Training Loss: tensor(0.3710)\n",
      "9324 Training Loss: tensor(0.3691)\n",
      "9325 Training Loss: tensor(0.3691)\n",
      "9326 Training Loss: tensor(0.3706)\n",
      "9327 Training Loss: tensor(0.3729)\n",
      "9328 Training Loss: tensor(0.3721)\n",
      "9329 Training Loss: tensor(0.3685)\n",
      "9330 Training Loss: tensor(0.3707)\n",
      "9331 Training Loss: tensor(0.3705)\n",
      "9332 Training Loss: tensor(0.3745)\n",
      "9333 Training Loss: tensor(0.3727)\n",
      "9334 Training Loss: tensor(0.3725)\n",
      "9335 Training Loss: tensor(0.3709)\n",
      "9336 Training Loss: tensor(0.3759)\n",
      "9337 Training Loss: tensor(0.3751)\n",
      "9338 Training Loss: tensor(0.3737)\n",
      "9339 Training Loss: tensor(0.3694)\n",
      "9340 Training Loss: tensor(0.3706)\n",
      "9341 Training Loss: tensor(0.3715)\n",
      "9342 Training Loss: tensor(0.3715)\n",
      "9343 Training Loss: tensor(0.3710)\n",
      "9344 Training Loss: tensor(0.3705)\n",
      "9345 Training Loss: tensor(0.3701)\n",
      "9346 Training Loss: tensor(0.3680)\n",
      "9347 Training Loss: tensor(0.3696)\n",
      "9348 Training Loss: tensor(0.3727)\n",
      "9349 Training Loss: tensor(0.3711)\n",
      "9350 Training Loss: tensor(0.3747)\n",
      "9351 Training Loss: tensor(0.3677)\n",
      "9352 Training Loss: tensor(0.3701)\n",
      "9353 Training Loss: tensor(0.3739)\n",
      "9354 Training Loss: tensor(0.3707)\n",
      "9355 Training Loss: tensor(0.3703)\n",
      "9356 Training Loss: tensor(0.3690)\n",
      "9357 Training Loss: tensor(0.3687)\n",
      "9358 Training Loss: tensor(0.3676)\n",
      "9359 Training Loss: tensor(0.3683)\n",
      "9360 Training Loss: tensor(0.3674)\n",
      "9361 Training Loss: tensor(0.3733)\n",
      "9362 Training Loss: tensor(0.3784)\n",
      "9363 Training Loss: tensor(0.3746)\n",
      "9364 Training Loss: tensor(0.3675)\n",
      "9365 Training Loss: tensor(0.3791)\n",
      "9366 Training Loss: tensor(0.3700)\n",
      "9367 Training Loss: tensor(0.3684)\n",
      "9368 Training Loss: tensor(0.3697)\n",
      "9369 Training Loss: tensor(0.3722)\n",
      "9370 Training Loss: tensor(0.3692)\n",
      "9371 Training Loss: tensor(0.3784)\n",
      "9372 Training Loss: tensor(0.3705)\n",
      "9373 Training Loss: tensor(0.3703)\n",
      "9374 Training Loss: tensor(0.3678)\n",
      "9375 Training Loss: tensor(0.3696)\n",
      "9376 Training Loss: tensor(0.3748)\n",
      "9377 Training Loss: tensor(0.3730)\n",
      "9378 Training Loss: tensor(0.3684)\n",
      "9379 Training Loss: tensor(0.3732)\n",
      "9380 Training Loss: tensor(0.3688)\n",
      "9381 Training Loss: tensor(0.3749)\n",
      "9382 Training Loss: tensor(0.3709)\n",
      "9383 Training Loss: tensor(0.3747)\n",
      "9384 Training Loss: tensor(0.3735)\n",
      "9385 Training Loss: tensor(0.3704)\n",
      "9386 Training Loss: tensor(0.3716)\n",
      "9387 Training Loss: tensor(0.3714)\n",
      "9388 Training Loss: tensor(0.3800)\n",
      "9389 Training Loss: tensor(0.3750)\n",
      "9390 Training Loss: tensor(0.3691)\n",
      "9391 Training Loss: tensor(0.3694)\n",
      "9392 Training Loss: tensor(0.3725)\n",
      "9393 Training Loss: tensor(0.3682)\n",
      "9394 Training Loss: tensor(0.3674)\n",
      "9395 Training Loss: tensor(0.3709)\n",
      "9396 Training Loss: tensor(0.3706)\n",
      "9397 Training Loss: tensor(0.3729)\n",
      "9398 Training Loss: tensor(0.3747)\n",
      "9399 Training Loss: tensor(0.3697)\n",
      "9400 Training Loss: tensor(0.3687)\n",
      "9401 Training Loss: tensor(0.3762)\n",
      "9402 Training Loss: tensor(0.3712)\n",
      "9403 Training Loss: tensor(0.3698)\n",
      "9404 Training Loss: tensor(0.3681)\n",
      "9405 Training Loss: tensor(0.3773)\n",
      "9406 Training Loss: tensor(0.3665)\n",
      "9407 Training Loss: tensor(0.3742)\n",
      "9408 Training Loss: tensor(0.3674)\n",
      "9409 Training Loss: tensor(0.3678)\n",
      "9410 Training Loss: tensor(0.3730)\n",
      "9411 Training Loss: tensor(0.3697)\n",
      "9412 Training Loss: tensor(0.3729)\n",
      "9413 Training Loss: tensor(0.3745)\n",
      "9414 Training Loss: tensor(0.3721)\n",
      "9415 Training Loss: tensor(0.3730)\n",
      "9416 Training Loss: tensor(0.3725)\n",
      "9417 Training Loss: tensor(0.3728)\n",
      "9418 Training Loss: tensor(0.3709)\n",
      "9419 Training Loss: tensor(0.3705)\n",
      "9420 Training Loss: tensor(0.3683)\n",
      "9421 Training Loss: tensor(0.3696)\n",
      "9422 Training Loss: tensor(0.3720)\n",
      "9423 Training Loss: tensor(0.3670)\n",
      "9424 Training Loss: tensor(0.3690)\n",
      "9425 Training Loss: tensor(0.3752)\n",
      "9426 Training Loss: tensor(0.3693)\n",
      "9427 Training Loss: tensor(0.3675)\n",
      "9428 Training Loss: tensor(0.3726)\n",
      "9429 Training Loss: tensor(0.3726)\n",
      "9430 Training Loss: tensor(0.3721)\n",
      "9431 Training Loss: tensor(0.3716)\n",
      "9432 Training Loss: tensor(0.3693)\n",
      "9433 Training Loss: tensor(0.3717)\n",
      "9434 Training Loss: tensor(0.3742)\n",
      "9435 Training Loss: tensor(0.3759)\n",
      "9436 Training Loss: tensor(0.3707)\n",
      "9437 Training Loss: tensor(0.3682)\n",
      "9438 Training Loss: tensor(0.3706)\n",
      "9439 Training Loss: tensor(0.3689)\n",
      "9440 Training Loss: tensor(0.3687)\n",
      "9441 Training Loss: tensor(0.3727)\n",
      "9442 Training Loss: tensor(0.3717)\n",
      "9443 Training Loss: tensor(0.3697)\n",
      "9444 Training Loss: tensor(0.3703)\n",
      "9445 Training Loss: tensor(0.3713)\n",
      "9446 Training Loss: tensor(0.3693)\n",
      "9447 Training Loss: tensor(0.3709)\n",
      "9448 Training Loss: tensor(0.3681)\n",
      "9449 Training Loss: tensor(0.3732)\n",
      "9450 Training Loss: tensor(0.3744)\n",
      "9451 Training Loss: tensor(0.3730)\n",
      "9452 Training Loss: tensor(0.3698)\n",
      "9453 Training Loss: tensor(0.3695)\n",
      "9454 Training Loss: tensor(0.3715)\n",
      "9455 Training Loss: tensor(0.3739)\n",
      "9456 Training Loss: tensor(0.3690)\n",
      "9457 Training Loss: tensor(0.3690)\n",
      "9458 Training Loss: tensor(0.3707)\n",
      "9459 Training Loss: tensor(0.3717)\n",
      "9460 Training Loss: tensor(0.3755)\n",
      "9461 Training Loss: tensor(0.3725)\n",
      "9462 Training Loss: tensor(0.3759)\n",
      "9463 Training Loss: tensor(0.3721)\n",
      "9464 Training Loss: tensor(0.3714)\n",
      "9465 Training Loss: tensor(0.3715)\n",
      "9466 Training Loss: tensor(0.3681)\n",
      "9467 Training Loss: tensor(0.3748)\n",
      "9468 Training Loss: tensor(0.3726)\n",
      "9469 Training Loss: tensor(0.3774)\n",
      "9470 Training Loss: tensor(0.3685)\n",
      "9471 Training Loss: tensor(0.3705)\n",
      "9472 Training Loss: tensor(0.3712)\n",
      "9473 Training Loss: tensor(0.3692)\n",
      "9474 Training Loss: tensor(0.3681)\n",
      "9475 Training Loss: tensor(0.3704)\n",
      "9476 Training Loss: tensor(0.3739)\n",
      "9477 Training Loss: tensor(0.3715)\n",
      "9478 Training Loss: tensor(0.3679)\n",
      "9479 Training Loss: tensor(0.3668)\n",
      "9480 Training Loss: tensor(0.3740)\n",
      "9481 Training Loss: tensor(0.3711)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9482 Training Loss: tensor(0.3710)\n",
      "9483 Training Loss: tensor(0.3689)\n",
      "9484 Training Loss: tensor(0.3733)\n",
      "9485 Training Loss: tensor(0.3718)\n",
      "9486 Training Loss: tensor(0.3714)\n",
      "9487 Training Loss: tensor(0.3698)\n",
      "9488 Training Loss: tensor(0.3699)\n",
      "9489 Training Loss: tensor(0.3694)\n",
      "9490 Training Loss: tensor(0.3698)\n",
      "9491 Training Loss: tensor(0.3738)\n",
      "9492 Training Loss: tensor(0.3715)\n",
      "9493 Training Loss: tensor(0.3746)\n",
      "9494 Training Loss: tensor(0.3716)\n",
      "9495 Training Loss: tensor(0.3712)\n",
      "9496 Training Loss: tensor(0.3686)\n",
      "9497 Training Loss: tensor(0.3708)\n",
      "9498 Training Loss: tensor(0.3680)\n",
      "9499 Training Loss: tensor(0.3723)\n",
      "9500 Training Loss: tensor(0.3719)\n",
      "9501 Training Loss: tensor(0.3678)\n",
      "9502 Training Loss: tensor(0.3714)\n",
      "9503 Training Loss: tensor(0.3701)\n",
      "9504 Training Loss: tensor(0.3746)\n",
      "9505 Training Loss: tensor(0.3686)\n",
      "9506 Training Loss: tensor(0.3765)\n",
      "9507 Training Loss: tensor(0.3696)\n",
      "9508 Training Loss: tensor(0.3729)\n",
      "9509 Training Loss: tensor(0.3750)\n",
      "9510 Training Loss: tensor(0.3717)\n",
      "9511 Training Loss: tensor(0.3681)\n",
      "9512 Training Loss: tensor(0.3669)\n",
      "9513 Training Loss: tensor(0.3676)\n",
      "9514 Training Loss: tensor(0.3702)\n",
      "9515 Training Loss: tensor(0.3706)\n",
      "9516 Training Loss: tensor(0.3746)\n",
      "9517 Training Loss: tensor(0.3706)\n",
      "9518 Training Loss: tensor(0.3760)\n",
      "9519 Training Loss: tensor(0.3695)\n",
      "9520 Training Loss: tensor(0.3712)\n",
      "9521 Training Loss: tensor(0.3678)\n",
      "9522 Training Loss: tensor(0.3738)\n",
      "9523 Training Loss: tensor(0.3721)\n",
      "9524 Training Loss: tensor(0.3714)\n",
      "9525 Training Loss: tensor(0.3707)\n",
      "9526 Training Loss: tensor(0.3769)\n",
      "9527 Training Loss: tensor(0.3689)\n",
      "9528 Training Loss: tensor(0.3747)\n",
      "9529 Training Loss: tensor(0.3692)\n",
      "9530 Training Loss: tensor(0.3700)\n",
      "9531 Training Loss: tensor(0.3744)\n",
      "9532 Training Loss: tensor(0.3734)\n",
      "9533 Training Loss: tensor(0.3739)\n",
      "9534 Training Loss: tensor(0.3716)\n",
      "9535 Training Loss: tensor(0.3740)\n",
      "9536 Training Loss: tensor(0.3706)\n",
      "9537 Training Loss: tensor(0.3669)\n",
      "9538 Training Loss: tensor(0.3744)\n",
      "9539 Training Loss: tensor(0.3703)\n",
      "9540 Training Loss: tensor(0.3691)\n",
      "9541 Training Loss: tensor(0.3708)\n",
      "9542 Training Loss: tensor(0.3739)\n",
      "9543 Training Loss: tensor(0.3690)\n",
      "9544 Training Loss: tensor(0.3676)\n",
      "9545 Training Loss: tensor(0.3702)\n",
      "9546 Training Loss: tensor(0.3718)\n",
      "9547 Training Loss: tensor(0.3690)\n",
      "9548 Training Loss: tensor(0.3713)\n",
      "9549 Training Loss: tensor(0.3741)\n",
      "9550 Training Loss: tensor(0.3703)\n",
      "9551 Training Loss: tensor(0.3695)\n",
      "9552 Training Loss: tensor(0.3742)\n",
      "9553 Training Loss: tensor(0.3775)\n",
      "9554 Training Loss: tensor(0.3685)\n",
      "9555 Training Loss: tensor(0.3672)\n",
      "9556 Training Loss: tensor(0.3716)\n",
      "9557 Training Loss: tensor(0.3717)\n",
      "9558 Training Loss: tensor(0.3704)\n",
      "9559 Training Loss: tensor(0.3701)\n",
      "9560 Training Loss: tensor(0.3671)\n",
      "9561 Training Loss: tensor(0.3676)\n",
      "9562 Training Loss: tensor(0.3705)\n",
      "9563 Training Loss: tensor(0.3731)\n",
      "9564 Training Loss: tensor(0.3680)\n",
      "9565 Training Loss: tensor(0.3718)\n",
      "9566 Training Loss: tensor(0.3705)\n",
      "9567 Training Loss: tensor(0.3707)\n",
      "9568 Training Loss: tensor(0.3688)\n",
      "9569 Training Loss: tensor(0.3705)\n",
      "9570 Training Loss: tensor(0.3771)\n",
      "9571 Training Loss: tensor(0.3690)\n",
      "9572 Training Loss: tensor(0.3707)\n",
      "9573 Training Loss: tensor(0.3697)\n",
      "9574 Training Loss: tensor(0.3760)\n",
      "9575 Training Loss: tensor(0.3673)\n",
      "9576 Training Loss: tensor(0.3765)\n",
      "9577 Training Loss: tensor(0.3682)\n",
      "9578 Training Loss: tensor(0.3680)\n",
      "9579 Training Loss: tensor(0.3689)\n",
      "9580 Training Loss: tensor(0.3707)\n",
      "9581 Training Loss: tensor(0.3682)\n",
      "9582 Training Loss: tensor(0.3704)\n",
      "9583 Training Loss: tensor(0.3719)\n",
      "9584 Training Loss: tensor(0.3731)\n",
      "9585 Training Loss: tensor(0.3703)\n",
      "9586 Training Loss: tensor(0.3708)\n",
      "9587 Training Loss: tensor(0.3704)\n",
      "9588 Training Loss: tensor(0.3708)\n",
      "9589 Training Loss: tensor(0.3703)\n",
      "9590 Training Loss: tensor(0.3728)\n",
      "9591 Training Loss: tensor(0.3697)\n",
      "9592 Training Loss: tensor(0.3716)\n",
      "9593 Training Loss: tensor(0.3688)\n",
      "9594 Training Loss: tensor(0.3720)\n",
      "9595 Training Loss: tensor(0.3707)\n",
      "9596 Training Loss: tensor(0.3740)\n",
      "9597 Training Loss: tensor(0.3719)\n",
      "9598 Training Loss: tensor(0.3680)\n",
      "9599 Training Loss: tensor(0.3714)\n",
      "9600 Training Loss: tensor(0.3725)\n",
      "9601 Training Loss: tensor(0.3709)\n",
      "9602 Training Loss: tensor(0.3722)\n",
      "9603 Training Loss: tensor(0.3713)\n",
      "9604 Training Loss: tensor(0.3730)\n",
      "9605 Training Loss: tensor(0.3708)\n",
      "9606 Training Loss: tensor(0.3719)\n",
      "9607 Training Loss: tensor(0.3684)\n",
      "9608 Training Loss: tensor(0.3741)\n",
      "9609 Training Loss: tensor(0.3692)\n",
      "9610 Training Loss: tensor(0.3739)\n",
      "9611 Training Loss: tensor(0.3714)\n",
      "9612 Training Loss: tensor(0.3713)\n",
      "9613 Training Loss: tensor(0.3666)\n",
      "9614 Training Loss: tensor(0.3665)\n",
      "9615 Training Loss: tensor(0.3698)\n",
      "9616 Training Loss: tensor(0.3671)\n",
      "9617 Training Loss: tensor(0.3703)\n",
      "9618 Training Loss: tensor(0.3698)\n",
      "9619 Training Loss: tensor(0.3656)\n",
      "9620 Training Loss: tensor(0.3758)\n",
      "9621 Training Loss: tensor(0.3726)\n",
      "9622 Training Loss: tensor(0.3671)\n",
      "9623 Training Loss: tensor(0.3736)\n",
      "9624 Training Loss: tensor(0.3674)\n",
      "9625 Training Loss: tensor(0.3782)\n",
      "9626 Training Loss: tensor(0.3688)\n",
      "9627 Training Loss: tensor(0.3696)\n",
      "9628 Training Loss: tensor(0.3731)\n",
      "9629 Training Loss: tensor(0.3712)\n",
      "9630 Training Loss: tensor(0.3663)\n",
      "9631 Training Loss: tensor(0.3722)\n",
      "9632 Training Loss: tensor(0.3771)\n",
      "9633 Training Loss: tensor(0.3681)\n",
      "9634 Training Loss: tensor(0.3677)\n",
      "9635 Training Loss: tensor(0.3756)\n",
      "9636 Training Loss: tensor(0.3697)\n",
      "9637 Training Loss: tensor(0.3736)\n",
      "9638 Training Loss: tensor(0.3677)\n",
      "9639 Training Loss: tensor(0.3705)\n",
      "9640 Training Loss: tensor(0.3728)\n",
      "9641 Training Loss: tensor(0.3692)\n",
      "9642 Training Loss: tensor(0.3721)\n",
      "9643 Training Loss: tensor(0.3710)\n",
      "9644 Training Loss: tensor(0.3746)\n",
      "9645 Training Loss: tensor(0.3718)\n",
      "9646 Training Loss: tensor(0.3750)\n",
      "9647 Training Loss: tensor(0.3715)\n",
      "9648 Training Loss: tensor(0.3690)\n",
      "9649 Training Loss: tensor(0.3692)\n",
      "9650 Training Loss: tensor(0.3734)\n",
      "9651 Training Loss: tensor(0.3694)\n",
      "9652 Training Loss: tensor(0.3742)\n",
      "9653 Training Loss: tensor(0.3693)\n",
      "9654 Training Loss: tensor(0.3702)\n",
      "9655 Training Loss: tensor(0.3720)\n",
      "9656 Training Loss: tensor(0.3697)\n",
      "9657 Training Loss: tensor(0.3761)\n",
      "9658 Training Loss: tensor(0.3704)\n",
      "9659 Training Loss: tensor(0.3703)\n",
      "9660 Training Loss: tensor(0.3768)\n",
      "9661 Training Loss: tensor(0.3758)\n",
      "9662 Training Loss: tensor(0.3714)\n",
      "9663 Training Loss: tensor(0.3730)\n",
      "9664 Training Loss: tensor(0.3714)\n",
      "9665 Training Loss: tensor(0.3727)\n",
      "9666 Training Loss: tensor(0.3770)\n",
      "9667 Training Loss: tensor(0.3709)\n",
      "9668 Training Loss: tensor(0.3693)\n",
      "9669 Training Loss: tensor(0.3681)\n",
      "9670 Training Loss: tensor(0.3692)\n",
      "9671 Training Loss: tensor(0.3728)\n",
      "9672 Training Loss: tensor(0.3744)\n",
      "9673 Training Loss: tensor(0.3697)\n",
      "9674 Training Loss: tensor(0.3669)\n",
      "9675 Training Loss: tensor(0.3677)\n",
      "9676 Training Loss: tensor(0.3687)\n",
      "9677 Training Loss: tensor(0.3725)\n",
      "9678 Training Loss: tensor(0.3707)\n",
      "9679 Training Loss: tensor(0.3701)\n",
      "9680 Training Loss: tensor(0.3721)\n",
      "9681 Training Loss: tensor(0.3669)\n",
      "9682 Training Loss: tensor(0.3709)\n",
      "9683 Training Loss: tensor(0.3712)\n",
      "9684 Training Loss: tensor(0.3789)\n",
      "9685 Training Loss: tensor(0.3800)\n",
      "9686 Training Loss: tensor(0.3717)\n",
      "9687 Training Loss: tensor(0.3742)\n",
      "9688 Training Loss: tensor(0.3724)\n",
      "9689 Training Loss: tensor(0.3723)\n",
      "9690 Training Loss: tensor(0.3695)\n",
      "9691 Training Loss: tensor(0.3695)\n",
      "9692 Training Loss: tensor(0.3730)\n",
      "9693 Training Loss: tensor(0.3721)\n",
      "9694 Training Loss: tensor(0.3736)\n",
      "9695 Training Loss: tensor(0.3717)\n",
      "9696 Training Loss: tensor(0.3719)\n",
      "9697 Training Loss: tensor(0.3732)\n",
      "9698 Training Loss: tensor(0.3703)\n",
      "9699 Training Loss: tensor(0.3713)\n",
      "9700 Training Loss: tensor(0.3730)\n",
      "9701 Training Loss: tensor(0.3693)\n",
      "9702 Training Loss: tensor(0.3707)\n",
      "9703 Training Loss: tensor(0.3690)\n",
      "9704 Training Loss: tensor(0.3724)\n",
      "9705 Training Loss: tensor(0.3724)\n",
      "9706 Training Loss: tensor(0.3727)\n",
      "9707 Training Loss: tensor(0.3693)\n",
      "9708 Training Loss: tensor(0.3753)\n",
      "9709 Training Loss: tensor(0.3704)\n",
      "9710 Training Loss: tensor(0.3717)\n",
      "9711 Training Loss: tensor(0.3727)\n",
      "9712 Training Loss: tensor(0.3708)\n",
      "9713 Training Loss: tensor(0.3679)\n",
      "9714 Training Loss: tensor(0.3697)\n",
      "9715 Training Loss: tensor(0.3731)\n",
      "9716 Training Loss: tensor(0.3724)\n",
      "9717 Training Loss: tensor(0.3701)\n",
      "9718 Training Loss: tensor(0.3686)\n",
      "9719 Training Loss: tensor(0.3708)\n",
      "9720 Training Loss: tensor(0.3721)\n",
      "9721 Training Loss: tensor(0.3702)\n",
      "9722 Training Loss: tensor(0.3751)\n",
      "9723 Training Loss: tensor(0.3699)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9724 Training Loss: tensor(0.3723)\n",
      "9725 Training Loss: tensor(0.3768)\n",
      "9726 Training Loss: tensor(0.3761)\n",
      "9727 Training Loss: tensor(0.3750)\n",
      "9728 Training Loss: tensor(0.3704)\n",
      "9729 Training Loss: tensor(0.3744)\n",
      "9730 Training Loss: tensor(0.3706)\n",
      "9731 Training Loss: tensor(0.3700)\n",
      "9732 Training Loss: tensor(0.3706)\n",
      "9733 Training Loss: tensor(0.3706)\n",
      "9734 Training Loss: tensor(0.3735)\n",
      "9735 Training Loss: tensor(0.3698)\n",
      "9736 Training Loss: tensor(0.3701)\n",
      "9737 Training Loss: tensor(0.3698)\n",
      "9738 Training Loss: tensor(0.3701)\n",
      "9739 Training Loss: tensor(0.3710)\n",
      "9740 Training Loss: tensor(0.3763)\n",
      "9741 Training Loss: tensor(0.3704)\n",
      "9742 Training Loss: tensor(0.3691)\n",
      "9743 Training Loss: tensor(0.3712)\n",
      "9744 Training Loss: tensor(0.3685)\n",
      "9745 Training Loss: tensor(0.3695)\n",
      "9746 Training Loss: tensor(0.3676)\n",
      "9747 Training Loss: tensor(0.3696)\n",
      "9748 Training Loss: tensor(0.3722)\n",
      "9749 Training Loss: tensor(0.3701)\n",
      "9750 Training Loss: tensor(0.3674)\n",
      "9751 Training Loss: tensor(0.3759)\n",
      "9752 Training Loss: tensor(0.3687)\n",
      "9753 Training Loss: tensor(0.3705)\n",
      "9754 Training Loss: tensor(0.3702)\n",
      "9755 Training Loss: tensor(0.3649)\n",
      "9756 Training Loss: tensor(0.3685)\n",
      "9757 Training Loss: tensor(0.3688)\n",
      "9758 Training Loss: tensor(0.3706)\n",
      "9759 Training Loss: tensor(0.3715)\n",
      "9760 Training Loss: tensor(0.3699)\n",
      "9761 Training Loss: tensor(0.3701)\n",
      "9762 Training Loss: tensor(0.3726)\n",
      "9763 Training Loss: tensor(0.3673)\n",
      "9764 Training Loss: tensor(0.3719)\n",
      "9765 Training Loss: tensor(0.3794)\n",
      "9766 Training Loss: tensor(0.3717)\n",
      "9767 Training Loss: tensor(0.3728)\n",
      "9768 Training Loss: tensor(0.3712)\n",
      "9769 Training Loss: tensor(0.3684)\n",
      "9770 Training Loss: tensor(0.3699)\n",
      "9771 Training Loss: tensor(0.3691)\n",
      "9772 Training Loss: tensor(0.3719)\n",
      "9773 Training Loss: tensor(0.3700)\n",
      "9774 Training Loss: tensor(0.3693)\n",
      "9775 Training Loss: tensor(0.3695)\n",
      "9776 Training Loss: tensor(0.3703)\n",
      "9777 Training Loss: tensor(0.3673)\n",
      "9778 Training Loss: tensor(0.3719)\n",
      "9779 Training Loss: tensor(0.3715)\n",
      "9780 Training Loss: tensor(0.3694)\n",
      "9781 Training Loss: tensor(0.3705)\n",
      "9782 Training Loss: tensor(0.3738)\n",
      "9783 Training Loss: tensor(0.3748)\n",
      "9784 Training Loss: tensor(0.3732)\n",
      "9785 Training Loss: tensor(0.3782)\n",
      "9786 Training Loss: tensor(0.3763)\n",
      "9787 Training Loss: tensor(0.3735)\n",
      "9788 Training Loss: tensor(0.3740)\n",
      "9789 Training Loss: tensor(0.3678)\n",
      "9790 Training Loss: tensor(0.3705)\n",
      "9791 Training Loss: tensor(0.3707)\n",
      "9792 Training Loss: tensor(0.3691)\n",
      "9793 Training Loss: tensor(0.3721)\n",
      "9794 Training Loss: tensor(0.3695)\n",
      "9795 Training Loss: tensor(0.3744)\n",
      "9796 Training Loss: tensor(0.3702)\n",
      "9797 Training Loss: tensor(0.3751)\n",
      "9798 Training Loss: tensor(0.3728)\n",
      "9799 Training Loss: tensor(0.3695)\n",
      "9800 Training Loss: tensor(0.3707)\n",
      "9801 Training Loss: tensor(0.3698)\n",
      "9802 Training Loss: tensor(0.3713)\n",
      "9803 Training Loss: tensor(0.3736)\n",
      "9804 Training Loss: tensor(0.3728)\n",
      "9805 Training Loss: tensor(0.3698)\n",
      "9806 Training Loss: tensor(0.3681)\n",
      "9807 Training Loss: tensor(0.3683)\n",
      "9808 Training Loss: tensor(0.3682)\n",
      "9809 Training Loss: tensor(0.3765)\n",
      "9810 Training Loss: tensor(0.3699)\n",
      "9811 Training Loss: tensor(0.3721)\n",
      "9812 Training Loss: tensor(0.3706)\n",
      "9813 Training Loss: tensor(0.3717)\n",
      "9814 Training Loss: tensor(0.3764)\n",
      "9815 Training Loss: tensor(0.3719)\n",
      "9816 Training Loss: tensor(0.3755)\n",
      "9817 Training Loss: tensor(0.3714)\n",
      "9818 Training Loss: tensor(0.3730)\n",
      "9819 Training Loss: tensor(0.3711)\n",
      "9820 Training Loss: tensor(0.3700)\n",
      "9821 Training Loss: tensor(0.3700)\n",
      "9822 Training Loss: tensor(0.3696)\n",
      "9823 Training Loss: tensor(0.3675)\n",
      "9824 Training Loss: tensor(0.3693)\n",
      "9825 Training Loss: tensor(0.3677)\n",
      "9826 Training Loss: tensor(0.3709)\n",
      "9827 Training Loss: tensor(0.3705)\n",
      "9828 Training Loss: tensor(0.3699)\n",
      "9829 Training Loss: tensor(0.3713)\n",
      "9830 Training Loss: tensor(0.3699)\n",
      "9831 Training Loss: tensor(0.3679)\n",
      "9832 Training Loss: tensor(0.3685)\n",
      "9833 Training Loss: tensor(0.3692)\n",
      "9834 Training Loss: tensor(0.3719)\n",
      "9835 Training Loss: tensor(0.3688)\n",
      "9836 Training Loss: tensor(0.3725)\n",
      "9837 Training Loss: tensor(0.3702)\n",
      "9838 Training Loss: tensor(0.3703)\n",
      "9839 Training Loss: tensor(0.3715)\n",
      "9840 Training Loss: tensor(0.3763)\n",
      "9841 Training Loss: tensor(0.3731)\n",
      "9842 Training Loss: tensor(0.3702)\n",
      "9843 Training Loss: tensor(0.3737)\n",
      "9844 Training Loss: tensor(0.3728)\n",
      "9845 Training Loss: tensor(0.3737)\n",
      "9846 Training Loss: tensor(0.3750)\n",
      "9847 Training Loss: tensor(0.3686)\n",
      "9848 Training Loss: tensor(0.3692)\n",
      "9849 Training Loss: tensor(0.3731)\n",
      "9850 Training Loss: tensor(0.3706)\n",
      "9851 Training Loss: tensor(0.3705)\n",
      "9852 Training Loss: tensor(0.3739)\n",
      "9853 Training Loss: tensor(0.3713)\n",
      "9854 Training Loss: tensor(0.3701)\n",
      "9855 Training Loss: tensor(0.3711)\n",
      "9856 Training Loss: tensor(0.3689)\n",
      "9857 Training Loss: tensor(0.3692)\n",
      "9858 Training Loss: tensor(0.3766)\n",
      "9859 Training Loss: tensor(0.3701)\n",
      "9860 Training Loss: tensor(0.3692)\n",
      "9861 Training Loss: tensor(0.3693)\n",
      "9862 Training Loss: tensor(0.3700)\n",
      "9863 Training Loss: tensor(0.3700)\n",
      "9864 Training Loss: tensor(0.3775)\n",
      "9865 Training Loss: tensor(0.3725)\n",
      "9866 Training Loss: tensor(0.3716)\n",
      "9867 Training Loss: tensor(0.3707)\n",
      "9868 Training Loss: tensor(0.3705)\n",
      "9869 Training Loss: tensor(0.3719)\n",
      "9870 Training Loss: tensor(0.3720)\n",
      "9871 Training Loss: tensor(0.3671)\n",
      "9872 Training Loss: tensor(0.3712)\n",
      "9873 Training Loss: tensor(0.3762)\n",
      "9874 Training Loss: tensor(0.3673)\n",
      "9875 Training Loss: tensor(0.3722)\n",
      "9876 Training Loss: tensor(0.3728)\n",
      "9877 Training Loss: tensor(0.3696)\n",
      "9878 Training Loss: tensor(0.3721)\n",
      "9879 Training Loss: tensor(0.3669)\n",
      "9880 Training Loss: tensor(0.3698)\n",
      "9881 Training Loss: tensor(0.3707)\n",
      "9882 Training Loss: tensor(0.3764)\n",
      "9883 Training Loss: tensor(0.3690)\n",
      "9884 Training Loss: tensor(0.3686)\n",
      "9885 Training Loss: tensor(0.3691)\n",
      "9886 Training Loss: tensor(0.3751)\n",
      "9887 Training Loss: tensor(0.3717)\n",
      "9888 Training Loss: tensor(0.3739)\n",
      "9889 Training Loss: tensor(0.3704)\n",
      "9890 Training Loss: tensor(0.3686)\n",
      "9891 Training Loss: tensor(0.3694)\n",
      "9892 Training Loss: tensor(0.3676)\n",
      "9893 Training Loss: tensor(0.3693)\n",
      "9894 Training Loss: tensor(0.3708)\n",
      "9895 Training Loss: tensor(0.3691)\n",
      "9896 Training Loss: tensor(0.3689)\n",
      "9897 Training Loss: tensor(0.3751)\n",
      "9898 Training Loss: tensor(0.3689)\n",
      "9899 Training Loss: tensor(0.3689)\n",
      "9900 Training Loss: tensor(0.3733)\n",
      "9901 Training Loss: tensor(0.3712)\n",
      "9902 Training Loss: tensor(0.3657)\n",
      "9903 Training Loss: tensor(0.3717)\n",
      "9904 Training Loss: tensor(0.3703)\n",
      "9905 Training Loss: tensor(0.3697)\n",
      "9906 Training Loss: tensor(0.3681)\n",
      "9907 Training Loss: tensor(0.3700)\n",
      "9908 Training Loss: tensor(0.3699)\n",
      "9909 Training Loss: tensor(0.3747)\n",
      "9910 Training Loss: tensor(0.3706)\n",
      "9911 Training Loss: tensor(0.3707)\n",
      "9912 Training Loss: tensor(0.3683)\n",
      "9913 Training Loss: tensor(0.3656)\n",
      "9914 Training Loss: tensor(0.3675)\n",
      "9915 Training Loss: tensor(0.3677)\n",
      "9916 Training Loss: tensor(0.3773)\n",
      "9917 Training Loss: tensor(0.3670)\n",
      "9918 Training Loss: tensor(0.3680)\n",
      "9919 Training Loss: tensor(0.3730)\n",
      "9920 Training Loss: tensor(0.3721)\n",
      "9921 Training Loss: tensor(0.3691)\n",
      "9922 Training Loss: tensor(0.3696)\n",
      "9923 Training Loss: tensor(0.3666)\n",
      "9924 Training Loss: tensor(0.3730)\n",
      "9925 Training Loss: tensor(0.3724)\n",
      "9926 Training Loss: tensor(0.3665)\n",
      "9927 Training Loss: tensor(0.3682)\n",
      "9928 Training Loss: tensor(0.3682)\n",
      "9929 Training Loss: tensor(0.3685)\n",
      "9930 Training Loss: tensor(0.3672)\n",
      "9931 Training Loss: tensor(0.3724)\n",
      "9932 Training Loss: tensor(0.3688)\n",
      "9933 Training Loss: tensor(0.3675)\n",
      "9934 Training Loss: tensor(0.3688)\n",
      "9935 Training Loss: tensor(0.3672)\n",
      "9936 Training Loss: tensor(0.3659)\n",
      "9937 Training Loss: tensor(0.3700)\n",
      "9938 Training Loss: tensor(0.3687)\n",
      "9939 Training Loss: tensor(0.3687)\n",
      "9940 Training Loss: tensor(0.3672)\n",
      "9941 Training Loss: tensor(0.3677)\n",
      "9942 Training Loss: tensor(0.3699)\n",
      "9943 Training Loss: tensor(0.3776)\n",
      "9944 Training Loss: tensor(0.3687)\n",
      "9945 Training Loss: tensor(0.3667)\n",
      "9946 Training Loss: tensor(0.3721)\n",
      "9947 Training Loss: tensor(0.3798)\n",
      "9948 Training Loss: tensor(0.3738)\n",
      "9949 Training Loss: tensor(0.3711)\n",
      "9950 Training Loss: tensor(0.3695)\n",
      "9951 Training Loss: tensor(0.3668)\n",
      "9952 Training Loss: tensor(0.3709)\n",
      "9953 Training Loss: tensor(0.3713)\n",
      "9954 Training Loss: tensor(0.3700)\n",
      "9955 Training Loss: tensor(0.3688)\n",
      "9956 Training Loss: tensor(0.3695)\n",
      "9957 Training Loss: tensor(0.3699)\n",
      "9958 Training Loss: tensor(0.3696)\n",
      "9959 Training Loss: tensor(0.3707)\n",
      "9960 Training Loss: tensor(0.3677)\n",
      "9961 Training Loss: tensor(0.3678)\n",
      "9962 Training Loss: tensor(0.3696)\n",
      "9963 Training Loss: tensor(0.3716)\n",
      "9964 Training Loss: tensor(0.3737)\n",
      "9965 Training Loss: tensor(0.3690)\n",
      "9966 Training Loss: tensor(0.3698)\n",
      "9967 Training Loss: tensor(0.3680)\n",
      "9968 Training Loss: tensor(0.3715)\n",
      "9969 Training Loss: tensor(0.3725)\n",
      "9970 Training Loss: tensor(0.3709)\n",
      "9971 Training Loss: tensor(0.3684)\n",
      "9972 Training Loss: tensor(0.3686)\n",
      "9973 Training Loss: tensor(0.3735)\n",
      "9974 Training Loss: tensor(0.3747)\n",
      "9975 Training Loss: tensor(0.3694)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9976 Training Loss: tensor(0.3688)\n",
      "9977 Training Loss: tensor(0.3692)\n",
      "9978 Training Loss: tensor(0.3693)\n",
      "9979 Training Loss: tensor(0.3726)\n",
      "9980 Training Loss: tensor(0.3726)\n",
      "9981 Training Loss: tensor(0.3697)\n",
      "9982 Training Loss: tensor(0.3706)\n",
      "9983 Training Loss: tensor(0.3743)\n",
      "9984 Training Loss: tensor(0.3678)\n",
      "9985 Training Loss: tensor(0.3666)\n",
      "9986 Training Loss: tensor(0.3721)\n",
      "9987 Training Loss: tensor(0.3698)\n",
      "9988 Training Loss: tensor(0.3744)\n",
      "9989 Training Loss: tensor(0.3661)\n",
      "9990 Training Loss: tensor(0.3681)\n",
      "9991 Training Loss: tensor(0.3739)\n",
      "9992 Training Loss: tensor(0.3686)\n",
      "9993 Training Loss: tensor(0.3703)\n",
      "9994 Training Loss: tensor(0.3706)\n",
      "9995 Training Loss: tensor(0.3676)\n",
      "9996 Training Loss: tensor(0.3687)\n",
      "9997 Training Loss: tensor(0.3688)\n",
      "9998 Training Loss: tensor(0.3739)\n",
      "9999 Training Loss: tensor(0.3683)\n",
      "10000 Training Loss: tensor(0.3680)\n",
      "10001 Training Loss: tensor(0.3686)\n",
      "10002 Training Loss: tensor(0.3661)\n",
      "10003 Training Loss: tensor(0.3670)\n",
      "10004 Training Loss: tensor(0.3697)\n",
      "10005 Training Loss: tensor(0.3705)\n",
      "10006 Training Loss: tensor(0.3666)\n",
      "10007 Training Loss: tensor(0.3711)\n",
      "10008 Training Loss: tensor(0.3705)\n",
      "10009 Training Loss: tensor(0.3716)\n",
      "10010 Training Loss: tensor(0.3703)\n",
      "10011 Training Loss: tensor(0.3657)\n",
      "10012 Training Loss: tensor(0.3800)\n",
      "10013 Training Loss: tensor(0.3733)\n",
      "10014 Training Loss: tensor(0.3763)\n",
      "10015 Training Loss: tensor(0.3689)\n",
      "10016 Training Loss: tensor(0.3675)\n",
      "10017 Training Loss: tensor(0.3695)\n",
      "10018 Training Loss: tensor(0.3697)\n",
      "10019 Training Loss: tensor(0.3729)\n",
      "10020 Training Loss: tensor(0.3691)\n",
      "10021 Training Loss: tensor(0.3680)\n",
      "10022 Training Loss: tensor(0.3719)\n",
      "10023 Training Loss: tensor(0.3664)\n",
      "10024 Training Loss: tensor(0.3727)\n",
      "10025 Training Loss: tensor(0.3688)\n",
      "10026 Training Loss: tensor(0.3711)\n",
      "10027 Training Loss: tensor(0.3690)\n",
      "10028 Training Loss: tensor(0.3653)\n",
      "10029 Training Loss: tensor(0.3677)\n",
      "10030 Training Loss: tensor(0.3705)\n",
      "10031 Training Loss: tensor(0.3682)\n",
      "10032 Training Loss: tensor(0.3692)\n",
      "10033 Training Loss: tensor(0.3674)\n",
      "10034 Training Loss: tensor(0.3690)\n",
      "10035 Training Loss: tensor(0.3703)\n",
      "10036 Training Loss: tensor(0.3667)\n",
      "10037 Training Loss: tensor(0.3709)\n",
      "10038 Training Loss: tensor(0.3725)\n",
      "10039 Training Loss: tensor(0.3718)\n",
      "10040 Training Loss: tensor(0.3663)\n",
      "10041 Training Loss: tensor(0.3667)\n",
      "10042 Training Loss: tensor(0.3721)\n",
      "10043 Training Loss: tensor(0.3734)\n",
      "10044 Training Loss: tensor(0.3677)\n",
      "10045 Training Loss: tensor(0.3726)\n",
      "10046 Training Loss: tensor(0.3694)\n",
      "10047 Training Loss: tensor(0.3685)\n",
      "10048 Training Loss: tensor(0.3682)\n",
      "10049 Training Loss: tensor(0.3680)\n",
      "10050 Training Loss: tensor(0.3671)\n",
      "10051 Training Loss: tensor(0.3700)\n",
      "10052 Training Loss: tensor(0.3717)\n",
      "10053 Training Loss: tensor(0.3676)\n",
      "10054 Training Loss: tensor(0.3700)\n",
      "10055 Training Loss: tensor(0.3742)\n",
      "10056 Training Loss: tensor(0.3692)\n",
      "10057 Training Loss: tensor(0.3687)\n",
      "10058 Training Loss: tensor(0.3717)\n",
      "10059 Training Loss: tensor(0.3719)\n",
      "10060 Training Loss: tensor(0.3703)\n",
      "10061 Training Loss: tensor(0.3707)\n",
      "10062 Training Loss: tensor(0.3709)\n",
      "10063 Training Loss: tensor(0.3689)\n",
      "10064 Training Loss: tensor(0.3705)\n",
      "10065 Training Loss: tensor(0.3682)\n",
      "10066 Training Loss: tensor(0.3668)\n",
      "10067 Training Loss: tensor(0.3739)\n",
      "10068 Training Loss: tensor(0.3730)\n",
      "10069 Training Loss: tensor(0.3692)\n",
      "10070 Training Loss: tensor(0.3692)\n",
      "10071 Training Loss: tensor(0.3785)\n",
      "10072 Training Loss: tensor(0.3722)\n",
      "10073 Training Loss: tensor(0.3676)\n",
      "10074 Training Loss: tensor(0.3769)\n",
      "10075 Training Loss: tensor(0.3716)\n",
      "10076 Training Loss: tensor(0.3698)\n",
      "10077 Training Loss: tensor(0.3690)\n",
      "10078 Training Loss: tensor(0.3674)\n",
      "10079 Training Loss: tensor(0.3696)\n",
      "10080 Training Loss: tensor(0.3719)\n",
      "10081 Training Loss: tensor(0.3711)\n",
      "10082 Training Loss: tensor(0.3709)\n",
      "10083 Training Loss: tensor(0.3736)\n",
      "10084 Training Loss: tensor(0.3719)\n",
      "10085 Training Loss: tensor(0.3697)\n",
      "10086 Training Loss: tensor(0.3737)\n",
      "10087 Training Loss: tensor(0.3731)\n",
      "10088 Training Loss: tensor(0.3680)\n",
      "10089 Training Loss: tensor(0.3707)\n",
      "10090 Training Loss: tensor(0.3669)\n",
      "10091 Training Loss: tensor(0.3677)\n",
      "10092 Training Loss: tensor(0.3718)\n",
      "10093 Training Loss: tensor(0.3704)\n",
      "10094 Training Loss: tensor(0.3694)\n",
      "10095 Training Loss: tensor(0.3758)\n",
      "10096 Training Loss: tensor(0.3685)\n",
      "10097 Training Loss: tensor(0.3664)\n",
      "10098 Training Loss: tensor(0.3697)\n",
      "10099 Training Loss: tensor(0.3656)\n",
      "10100 Training Loss: tensor(0.3687)\n",
      "10101 Training Loss: tensor(0.3700)\n",
      "10102 Training Loss: tensor(0.3657)\n",
      "10103 Training Loss: tensor(0.3696)\n",
      "10104 Training Loss: tensor(0.3695)\n",
      "10105 Training Loss: tensor(0.3645)\n",
      "10106 Training Loss: tensor(0.3719)\n",
      "10107 Training Loss: tensor(0.3684)\n",
      "10108 Training Loss: tensor(0.3664)\n",
      "10109 Training Loss: tensor(0.3668)\n",
      "10110 Training Loss: tensor(0.3667)\n",
      "10111 Training Loss: tensor(0.3687)\n",
      "10112 Training Loss: tensor(0.3652)\n",
      "10113 Training Loss: tensor(0.3765)\n",
      "10114 Training Loss: tensor(0.3738)\n",
      "10115 Training Loss: tensor(0.3680)\n",
      "10116 Training Loss: tensor(0.3660)\n",
      "10117 Training Loss: tensor(0.3713)\n",
      "10118 Training Loss: tensor(0.3765)\n",
      "10119 Training Loss: tensor(0.3682)\n",
      "10120 Training Loss: tensor(0.3679)\n",
      "10121 Training Loss: tensor(0.3691)\n",
      "10122 Training Loss: tensor(0.3778)\n",
      "10123 Training Loss: tensor(0.3772)\n",
      "10124 Training Loss: tensor(0.3747)\n",
      "10125 Training Loss: tensor(0.3737)\n",
      "10126 Training Loss: tensor(0.3687)\n",
      "10127 Training Loss: tensor(0.3729)\n",
      "10128 Training Loss: tensor(0.3697)\n",
      "10129 Training Loss: tensor(0.3707)\n",
      "10130 Training Loss: tensor(0.3705)\n",
      "10131 Training Loss: tensor(0.3707)\n",
      "10132 Training Loss: tensor(0.3694)\n",
      "10133 Training Loss: tensor(0.3710)\n",
      "10134 Training Loss: tensor(0.3768)\n",
      "10135 Training Loss: tensor(0.3722)\n",
      "10136 Training Loss: tensor(0.3704)\n",
      "10137 Training Loss: tensor(0.3673)\n",
      "10138 Training Loss: tensor(0.3669)\n",
      "10139 Training Loss: tensor(0.3728)\n",
      "10140 Training Loss: tensor(0.3728)\n",
      "10141 Training Loss: tensor(0.3720)\n",
      "10142 Training Loss: tensor(0.3704)\n",
      "10143 Training Loss: tensor(0.3689)\n",
      "10144 Training Loss: tensor(0.3734)\n",
      "10145 Training Loss: tensor(0.3683)\n",
      "10146 Training Loss: tensor(0.3682)\n",
      "10147 Training Loss: tensor(0.3696)\n",
      "10148 Training Loss: tensor(0.3700)\n",
      "10149 Training Loss: tensor(0.3661)\n",
      "10150 Training Loss: tensor(0.3736)\n",
      "10151 Training Loss: tensor(0.3695)\n",
      "10152 Training Loss: tensor(0.3653)\n",
      "10153 Training Loss: tensor(0.3705)\n",
      "10154 Training Loss: tensor(0.3744)\n",
      "10155 Training Loss: tensor(0.3680)\n",
      "10156 Training Loss: tensor(0.3817)\n",
      "10157 Training Loss: tensor(0.3664)\n",
      "10158 Training Loss: tensor(0.3694)\n",
      "10159 Training Loss: tensor(0.3659)\n",
      "10160 Training Loss: tensor(0.3735)\n",
      "10161 Training Loss: tensor(0.3684)\n",
      "10162 Training Loss: tensor(0.3683)\n",
      "10163 Training Loss: tensor(0.3680)\n",
      "10164 Training Loss: tensor(0.3677)\n",
      "10165 Training Loss: tensor(0.3710)\n",
      "10166 Training Loss: tensor(0.3685)\n",
      "10167 Training Loss: tensor(0.3687)\n",
      "10168 Training Loss: tensor(0.3701)\n",
      "10169 Training Loss: tensor(0.3718)\n",
      "10170 Training Loss: tensor(0.3709)\n",
      "10171 Training Loss: tensor(0.3705)\n",
      "10172 Training Loss: tensor(0.3655)\n",
      "10173 Training Loss: tensor(0.3694)\n",
      "10174 Training Loss: tensor(0.3737)\n",
      "10175 Training Loss: tensor(0.3650)\n",
      "10176 Training Loss: tensor(0.3666)\n",
      "10177 Training Loss: tensor(0.3686)\n",
      "10178 Training Loss: tensor(0.3726)\n",
      "10179 Training Loss: tensor(0.3651)\n",
      "10180 Training Loss: tensor(0.3714)\n",
      "10181 Training Loss: tensor(0.3698)\n",
      "10182 Training Loss: tensor(0.3760)\n",
      "10183 Training Loss: tensor(0.3680)\n",
      "10184 Training Loss: tensor(0.3715)\n",
      "10185 Training Loss: tensor(0.3698)\n",
      "10186 Training Loss: tensor(0.3672)\n",
      "10187 Training Loss: tensor(0.3689)\n",
      "10188 Training Loss: tensor(0.3653)\n",
      "10189 Training Loss: tensor(0.3694)\n",
      "10190 Training Loss: tensor(0.3669)\n",
      "10191 Training Loss: tensor(0.3685)\n",
      "10192 Training Loss: tensor(0.3703)\n",
      "10193 Training Loss: tensor(0.3693)\n",
      "10194 Training Loss: tensor(0.3700)\n",
      "10195 Training Loss: tensor(0.3688)\n",
      "10196 Training Loss: tensor(0.3719)\n",
      "10197 Training Loss: tensor(0.3713)\n",
      "10198 Training Loss: tensor(0.3721)\n",
      "10199 Training Loss: tensor(0.3709)\n",
      "10200 Training Loss: tensor(0.3706)\n",
      "10201 Training Loss: tensor(0.3706)\n",
      "10202 Training Loss: tensor(0.3667)\n",
      "10203 Training Loss: tensor(0.3667)\n",
      "10204 Training Loss: tensor(0.3698)\n",
      "10205 Training Loss: tensor(0.3699)\n",
      "10206 Training Loss: tensor(0.3715)\n",
      "10207 Training Loss: tensor(0.3668)\n",
      "10208 Training Loss: tensor(0.3695)\n",
      "10209 Training Loss: tensor(0.3735)\n",
      "10210 Training Loss: tensor(0.3715)\n",
      "10211 Training Loss: tensor(0.3704)\n",
      "10212 Training Loss: tensor(0.3703)\n",
      "10213 Training Loss: tensor(0.3696)\n",
      "10214 Training Loss: tensor(0.3724)\n",
      "10215 Training Loss: tensor(0.3684)\n",
      "10216 Training Loss: tensor(0.3675)\n",
      "10217 Training Loss: tensor(0.3691)\n",
      "10218 Training Loss: tensor(0.3675)\n",
      "10219 Training Loss: tensor(0.3678)\n",
      "10220 Training Loss: tensor(0.3723)\n",
      "10221 Training Loss: tensor(0.3712)\n",
      "10222 Training Loss: tensor(0.3657)\n",
      "10223 Training Loss: tensor(0.3674)\n",
      "10224 Training Loss: tensor(0.3661)\n",
      "10225 Training Loss: tensor(0.3690)\n",
      "10226 Training Loss: tensor(0.3779)\n",
      "10227 Training Loss: tensor(0.3692)\n",
      "10228 Training Loss: tensor(0.3702)\n",
      "10229 Training Loss: tensor(0.3666)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10230 Training Loss: tensor(0.3710)\n",
      "10231 Training Loss: tensor(0.3692)\n",
      "10232 Training Loss: tensor(0.3735)\n",
      "10233 Training Loss: tensor(0.3720)\n",
      "10234 Training Loss: tensor(0.3712)\n",
      "10235 Training Loss: tensor(0.3728)\n",
      "10236 Training Loss: tensor(0.3694)\n",
      "10237 Training Loss: tensor(0.3683)\n",
      "10238 Training Loss: tensor(0.3671)\n",
      "10239 Training Loss: tensor(0.3707)\n",
      "10240 Training Loss: tensor(0.3696)\n",
      "10241 Training Loss: tensor(0.3711)\n",
      "10242 Training Loss: tensor(0.3666)\n",
      "10243 Training Loss: tensor(0.3703)\n",
      "10244 Training Loss: tensor(0.3676)\n",
      "10245 Training Loss: tensor(0.3753)\n",
      "10246 Training Loss: tensor(0.3729)\n",
      "10247 Training Loss: tensor(0.3737)\n",
      "10248 Training Loss: tensor(0.3667)\n",
      "10249 Training Loss: tensor(0.3690)\n",
      "10250 Training Loss: tensor(0.3678)\n",
      "10251 Training Loss: tensor(0.3688)\n",
      "10252 Training Loss: tensor(0.3667)\n",
      "10253 Training Loss: tensor(0.3712)\n",
      "10254 Training Loss: tensor(0.3696)\n",
      "10255 Training Loss: tensor(0.3693)\n",
      "10256 Training Loss: tensor(0.3676)\n",
      "10257 Training Loss: tensor(0.3648)\n",
      "10258 Training Loss: tensor(0.3667)\n",
      "10259 Training Loss: tensor(0.3713)\n",
      "10260 Training Loss: tensor(0.3726)\n",
      "10261 Training Loss: tensor(0.3737)\n",
      "10262 Training Loss: tensor(0.3684)\n",
      "10263 Training Loss: tensor(0.3676)\n",
      "10264 Training Loss: tensor(0.3691)\n",
      "10265 Training Loss: tensor(0.3695)\n",
      "10266 Training Loss: tensor(0.3677)\n",
      "10267 Training Loss: tensor(0.3692)\n",
      "10268 Training Loss: tensor(0.3705)\n",
      "10269 Training Loss: tensor(0.3695)\n",
      "10270 Training Loss: tensor(0.3679)\n",
      "10271 Training Loss: tensor(0.3666)\n",
      "10272 Training Loss: tensor(0.3669)\n",
      "10273 Training Loss: tensor(0.3732)\n",
      "10274 Training Loss: tensor(0.3734)\n",
      "10275 Training Loss: tensor(0.3723)\n",
      "10276 Training Loss: tensor(0.3707)\n",
      "10277 Training Loss: tensor(0.3694)\n",
      "10278 Training Loss: tensor(0.3692)\n",
      "10279 Training Loss: tensor(0.3746)\n",
      "10280 Training Loss: tensor(0.3696)\n",
      "10281 Training Loss: tensor(0.3706)\n",
      "10282 Training Loss: tensor(0.3732)\n",
      "10283 Training Loss: tensor(0.3669)\n",
      "10284 Training Loss: tensor(0.3758)\n",
      "10285 Training Loss: tensor(0.3751)\n",
      "10286 Training Loss: tensor(0.3747)\n",
      "10287 Training Loss: tensor(0.3687)\n",
      "10288 Training Loss: tensor(0.3724)\n",
      "10289 Training Loss: tensor(0.3679)\n",
      "10290 Training Loss: tensor(0.3662)\n",
      "10291 Training Loss: tensor(0.3683)\n",
      "10292 Training Loss: tensor(0.3688)\n",
      "10293 Training Loss: tensor(0.3691)\n",
      "10294 Training Loss: tensor(0.3672)\n",
      "10295 Training Loss: tensor(0.3700)\n",
      "10296 Training Loss: tensor(0.3697)\n",
      "10297 Training Loss: tensor(0.3701)\n",
      "10298 Training Loss: tensor(0.3673)\n",
      "10299 Training Loss: tensor(0.3772)\n",
      "10300 Training Loss: tensor(0.3713)\n",
      "10301 Training Loss: tensor(0.3656)\n",
      "10302 Training Loss: tensor(0.3703)\n",
      "10303 Training Loss: tensor(0.3677)\n",
      "10304 Training Loss: tensor(0.3653)\n",
      "10305 Training Loss: tensor(0.3695)\n",
      "10306 Training Loss: tensor(0.3764)\n",
      "10307 Training Loss: tensor(0.3672)\n",
      "10308 Training Loss: tensor(0.3722)\n",
      "10309 Training Loss: tensor(0.3703)\n",
      "10310 Training Loss: tensor(0.3657)\n",
      "10311 Training Loss: tensor(0.3706)\n",
      "10312 Training Loss: tensor(0.3699)\n",
      "10313 Training Loss: tensor(0.3695)\n",
      "10314 Training Loss: tensor(0.3679)\n",
      "10315 Training Loss: tensor(0.3703)\n",
      "10316 Training Loss: tensor(0.3700)\n",
      "10317 Training Loss: tensor(0.3684)\n",
      "10318 Training Loss: tensor(0.3734)\n",
      "10319 Training Loss: tensor(0.3665)\n",
      "10320 Training Loss: tensor(0.3725)\n",
      "10321 Training Loss: tensor(0.3683)\n",
      "10322 Training Loss: tensor(0.3663)\n",
      "10323 Training Loss: tensor(0.3756)\n",
      "10324 Training Loss: tensor(0.3687)\n",
      "10325 Training Loss: tensor(0.3699)\n",
      "10326 Training Loss: tensor(0.3694)\n",
      "10327 Training Loss: tensor(0.3711)\n",
      "10328 Training Loss: tensor(0.3665)\n",
      "10329 Training Loss: tensor(0.3668)\n",
      "10330 Training Loss: tensor(0.3769)\n",
      "10331 Training Loss: tensor(0.3676)\n",
      "10332 Training Loss: tensor(0.3658)\n",
      "10333 Training Loss: tensor(0.3719)\n",
      "10334 Training Loss: tensor(0.3659)\n",
      "10335 Training Loss: tensor(0.3706)\n",
      "10336 Training Loss: tensor(0.3668)\n",
      "10337 Training Loss: tensor(0.3819)\n",
      "10338 Training Loss: tensor(0.3716)\n",
      "10339 Training Loss: tensor(0.3690)\n",
      "10340 Training Loss: tensor(0.3704)\n",
      "10341 Training Loss: tensor(0.3697)\n",
      "10342 Training Loss: tensor(0.3671)\n",
      "10343 Training Loss: tensor(0.3698)\n",
      "10344 Training Loss: tensor(0.3694)\n",
      "10345 Training Loss: tensor(0.3673)\n",
      "10346 Training Loss: tensor(0.3710)\n",
      "10347 Training Loss: tensor(0.3687)\n",
      "10348 Training Loss: tensor(0.3684)\n",
      "10349 Training Loss: tensor(0.3665)\n",
      "10350 Training Loss: tensor(0.3721)\n",
      "10351 Training Loss: tensor(0.3739)\n",
      "10352 Training Loss: tensor(0.3722)\n",
      "10353 Training Loss: tensor(0.3693)\n",
      "10354 Training Loss: tensor(0.3685)\n",
      "10355 Training Loss: tensor(0.3678)\n",
      "10356 Training Loss: tensor(0.3687)\n",
      "10357 Training Loss: tensor(0.3676)\n",
      "10358 Training Loss: tensor(0.3676)\n",
      "10359 Training Loss: tensor(0.3688)\n",
      "10360 Training Loss: tensor(0.3664)\n",
      "10361 Training Loss: tensor(0.3721)\n",
      "10362 Training Loss: tensor(0.3687)\n",
      "10363 Training Loss: tensor(0.3656)\n",
      "10364 Training Loss: tensor(0.3678)\n",
      "10365 Training Loss: tensor(0.3683)\n",
      "10366 Training Loss: tensor(0.3707)\n",
      "10367 Training Loss: tensor(0.3704)\n",
      "10368 Training Loss: tensor(0.3736)\n",
      "10369 Training Loss: tensor(0.3690)\n",
      "10370 Training Loss: tensor(0.3737)\n",
      "10371 Training Loss: tensor(0.3689)\n",
      "10372 Training Loss: tensor(0.3703)\n",
      "10373 Training Loss: tensor(0.3663)\n",
      "10374 Training Loss: tensor(0.3707)\n",
      "10375 Training Loss: tensor(0.3700)\n",
      "10376 Training Loss: tensor(0.3718)\n",
      "10377 Training Loss: tensor(0.3718)\n",
      "10378 Training Loss: tensor(0.3698)\n",
      "10379 Training Loss: tensor(0.3668)\n",
      "10380 Training Loss: tensor(0.3712)\n",
      "10381 Training Loss: tensor(0.3693)\n",
      "10382 Training Loss: tensor(0.3686)\n",
      "10383 Training Loss: tensor(0.3784)\n",
      "10384 Training Loss: tensor(0.3700)\n",
      "10385 Training Loss: tensor(0.3707)\n",
      "10386 Training Loss: tensor(0.3680)\n",
      "10387 Training Loss: tensor(0.3664)\n",
      "10388 Training Loss: tensor(0.3700)\n",
      "10389 Training Loss: tensor(0.3671)\n",
      "10390 Training Loss: tensor(0.3719)\n",
      "10391 Training Loss: tensor(0.3648)\n",
      "10392 Training Loss: tensor(0.3699)\n",
      "10393 Training Loss: tensor(0.3695)\n",
      "10394 Training Loss: tensor(0.3677)\n",
      "10395 Training Loss: tensor(0.3692)\n",
      "10396 Training Loss: tensor(0.3667)\n",
      "10397 Training Loss: tensor(0.3676)\n",
      "10398 Training Loss: tensor(0.3709)\n",
      "10399 Training Loss: tensor(0.3745)\n",
      "10400 Training Loss: tensor(0.3682)\n",
      "10401 Training Loss: tensor(0.3709)\n",
      "10402 Training Loss: tensor(0.3675)\n",
      "10403 Training Loss: tensor(0.3665)\n",
      "10404 Training Loss: tensor(0.3704)\n",
      "10405 Training Loss: tensor(0.3680)\n",
      "10406 Training Loss: tensor(0.3690)\n",
      "10407 Training Loss: tensor(0.3675)\n",
      "10408 Training Loss: tensor(0.3712)\n",
      "10409 Training Loss: tensor(0.3677)\n",
      "10410 Training Loss: tensor(0.3709)\n",
      "10411 Training Loss: tensor(0.3693)\n",
      "10412 Training Loss: tensor(0.3675)\n",
      "10413 Training Loss: tensor(0.3739)\n",
      "10414 Training Loss: tensor(0.3688)\n",
      "10415 Training Loss: tensor(0.3691)\n",
      "10416 Training Loss: tensor(0.3743)\n",
      "10417 Training Loss: tensor(0.3663)\n",
      "10418 Training Loss: tensor(0.3645)\n",
      "10419 Training Loss: tensor(0.3725)\n",
      "10420 Training Loss: tensor(0.3782)\n",
      "10421 Training Loss: tensor(0.3671)\n",
      "10422 Training Loss: tensor(0.3679)\n",
      "10423 Training Loss: tensor(0.3702)\n",
      "10424 Training Loss: tensor(0.3694)\n",
      "10425 Training Loss: tensor(0.3708)\n",
      "10426 Training Loss: tensor(0.3677)\n",
      "10427 Training Loss: tensor(0.3720)\n",
      "10428 Training Loss: tensor(0.3689)\n",
      "10429 Training Loss: tensor(0.3688)\n",
      "10430 Training Loss: tensor(0.3731)\n",
      "10431 Training Loss: tensor(0.3702)\n",
      "10432 Training Loss: tensor(0.3689)\n",
      "10433 Training Loss: tensor(0.3719)\n",
      "10434 Training Loss: tensor(0.3696)\n",
      "10435 Training Loss: tensor(0.3683)\n",
      "10436 Training Loss: tensor(0.3661)\n",
      "10437 Training Loss: tensor(0.3672)\n",
      "10438 Training Loss: tensor(0.3670)\n",
      "10439 Training Loss: tensor(0.3654)\n",
      "10440 Training Loss: tensor(0.3715)\n",
      "10441 Training Loss: tensor(0.3688)\n",
      "10442 Training Loss: tensor(0.3654)\n",
      "10443 Training Loss: tensor(0.3708)\n",
      "10444 Training Loss: tensor(0.3732)\n",
      "10445 Training Loss: tensor(0.3687)\n",
      "10446 Training Loss: tensor(0.3732)\n",
      "10447 Training Loss: tensor(0.3691)\n",
      "10448 Training Loss: tensor(0.3686)\n",
      "10449 Training Loss: tensor(0.3685)\n",
      "10450 Training Loss: tensor(0.3702)\n",
      "10451 Training Loss: tensor(0.3750)\n",
      "10452 Training Loss: tensor(0.3681)\n",
      "10453 Training Loss: tensor(0.3678)\n",
      "10454 Training Loss: tensor(0.3696)\n",
      "10455 Training Loss: tensor(0.3678)\n",
      "10456 Training Loss: tensor(0.3728)\n",
      "10457 Training Loss: tensor(0.3692)\n",
      "10458 Training Loss: tensor(0.3684)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10459 Training Loss: tensor(0.3680)\n",
      "10460 Training Loss: tensor(0.3714)\n",
      "10461 Training Loss: tensor(0.3700)\n",
      "10462 Training Loss: tensor(0.3712)\n",
      "10463 Training Loss: tensor(0.3703)\n",
      "10464 Training Loss: tensor(0.3688)\n",
      "10465 Training Loss: tensor(0.3650)\n",
      "10466 Training Loss: tensor(0.3678)\n",
      "10467 Training Loss: tensor(0.3710)\n",
      "10468 Training Loss: tensor(0.3670)\n",
      "10469 Training Loss: tensor(0.3678)\n",
      "10470 Training Loss: tensor(0.3687)\n",
      "10471 Training Loss: tensor(0.3688)\n",
      "10472 Training Loss: tensor(0.3655)\n",
      "10473 Training Loss: tensor(0.3686)\n",
      "10474 Training Loss: tensor(0.3674)\n",
      "10475 Training Loss: tensor(0.3707)\n",
      "10476 Training Loss: tensor(0.3673)\n",
      "10477 Training Loss: tensor(0.3727)\n",
      "10478 Training Loss: tensor(0.3691)\n",
      "10479 Training Loss: tensor(0.3682)\n",
      "10480 Training Loss: tensor(0.3722)\n",
      "10481 Training Loss: tensor(0.3722)\n",
      "10482 Training Loss: tensor(0.3684)\n",
      "10483 Training Loss: tensor(0.3684)\n",
      "10484 Training Loss: tensor(0.3663)\n",
      "10485 Training Loss: tensor(0.3681)\n",
      "10486 Training Loss: tensor(0.3676)\n",
      "10487 Training Loss: tensor(0.3714)\n",
      "10488 Training Loss: tensor(0.3685)\n",
      "10489 Training Loss: tensor(0.3689)\n",
      "10490 Training Loss: tensor(0.3674)\n",
      "10491 Training Loss: tensor(0.3679)\n",
      "10492 Training Loss: tensor(0.3704)\n",
      "10493 Training Loss: tensor(0.3688)\n",
      "10494 Training Loss: tensor(0.3670)\n",
      "10495 Training Loss: tensor(0.3677)\n",
      "10496 Training Loss: tensor(0.3663)\n",
      "10497 Training Loss: tensor(0.3714)\n",
      "10498 Training Loss: tensor(0.3683)\n",
      "10499 Training Loss: tensor(0.3674)\n",
      "10500 Training Loss: tensor(0.3686)\n",
      "10501 Training Loss: tensor(0.3681)\n",
      "10502 Training Loss: tensor(0.3695)\n",
      "10503 Training Loss: tensor(0.3683)\n",
      "10504 Training Loss: tensor(0.3687)\n",
      "10505 Training Loss: tensor(0.3697)\n",
      "10506 Training Loss: tensor(0.3674)\n",
      "10507 Training Loss: tensor(0.3687)\n",
      "10508 Training Loss: tensor(0.3649)\n",
      "10509 Training Loss: tensor(0.3667)\n",
      "10510 Training Loss: tensor(0.3697)\n",
      "10511 Training Loss: tensor(0.3668)\n",
      "10512 Training Loss: tensor(0.3767)\n",
      "10513 Training Loss: tensor(0.3674)\n",
      "10514 Training Loss: tensor(0.3717)\n",
      "10515 Training Loss: tensor(0.3702)\n",
      "10516 Training Loss: tensor(0.3720)\n",
      "10517 Training Loss: tensor(0.3657)\n",
      "10518 Training Loss: tensor(0.3693)\n",
      "10519 Training Loss: tensor(0.3730)\n",
      "10520 Training Loss: tensor(0.3687)\n",
      "10521 Training Loss: tensor(0.3739)\n",
      "10522 Training Loss: tensor(0.3715)\n",
      "10523 Training Loss: tensor(0.3678)\n",
      "10524 Training Loss: tensor(0.3698)\n",
      "10525 Training Loss: tensor(0.3655)\n",
      "10526 Training Loss: tensor(0.3689)\n",
      "10527 Training Loss: tensor(0.3726)\n",
      "10528 Training Loss: tensor(0.3709)\n",
      "10529 Training Loss: tensor(0.3671)\n",
      "10530 Training Loss: tensor(0.3682)\n",
      "10531 Training Loss: tensor(0.3654)\n",
      "10532 Training Loss: tensor(0.3688)\n",
      "10533 Training Loss: tensor(0.3677)\n",
      "10534 Training Loss: tensor(0.3704)\n",
      "10535 Training Loss: tensor(0.3695)\n",
      "10536 Training Loss: tensor(0.3737)\n",
      "10537 Training Loss: tensor(0.3678)\n",
      "10538 Training Loss: tensor(0.3706)\n",
      "10539 Training Loss: tensor(0.3702)\n",
      "10540 Training Loss: tensor(0.3667)\n",
      "10541 Training Loss: tensor(0.3688)\n",
      "10542 Training Loss: tensor(0.3687)\n",
      "10543 Training Loss: tensor(0.3763)\n",
      "10544 Training Loss: tensor(0.3702)\n",
      "10545 Training Loss: tensor(0.3686)\n",
      "10546 Training Loss: tensor(0.3679)\n",
      "10547 Training Loss: tensor(0.3722)\n",
      "10548 Training Loss: tensor(0.3654)\n",
      "10549 Training Loss: tensor(0.3658)\n",
      "10550 Training Loss: tensor(0.3675)\n",
      "10551 Training Loss: tensor(0.3743)\n",
      "10552 Training Loss: tensor(0.3657)\n",
      "10553 Training Loss: tensor(0.3715)\n",
      "10554 Training Loss: tensor(0.3726)\n",
      "10555 Training Loss: tensor(0.3711)\n",
      "10556 Training Loss: tensor(0.3705)\n",
      "10557 Training Loss: tensor(0.3698)\n",
      "10558 Training Loss: tensor(0.3708)\n",
      "10559 Training Loss: tensor(0.3669)\n",
      "10560 Training Loss: tensor(0.3661)\n",
      "10561 Training Loss: tensor(0.3697)\n",
      "10562 Training Loss: tensor(0.3717)\n",
      "10563 Training Loss: tensor(0.3675)\n",
      "10564 Training Loss: tensor(0.3776)\n",
      "10565 Training Loss: tensor(0.3663)\n",
      "10566 Training Loss: tensor(0.3654)\n",
      "10567 Training Loss: tensor(0.3694)\n",
      "10568 Training Loss: tensor(0.3706)\n",
      "10569 Training Loss: tensor(0.3696)\n",
      "10570 Training Loss: tensor(0.3753)\n",
      "10571 Training Loss: tensor(0.3697)\n",
      "10572 Training Loss: tensor(0.3688)\n",
      "10573 Training Loss: tensor(0.3683)\n",
      "10574 Training Loss: tensor(0.3718)\n",
      "10575 Training Loss: tensor(0.3701)\n",
      "10576 Training Loss: tensor(0.3679)\n",
      "10577 Training Loss: tensor(0.3693)\n",
      "10578 Training Loss: tensor(0.3685)\n",
      "10579 Training Loss: tensor(0.3677)\n",
      "10580 Training Loss: tensor(0.3681)\n",
      "10581 Training Loss: tensor(0.3763)\n",
      "10582 Training Loss: tensor(0.3665)\n",
      "10583 Training Loss: tensor(0.3731)\n",
      "10584 Training Loss: tensor(0.3663)\n",
      "10585 Training Loss: tensor(0.3668)\n",
      "10586 Training Loss: tensor(0.3703)\n",
      "10587 Training Loss: tensor(0.3690)\n",
      "10588 Training Loss: tensor(0.3675)\n",
      "10589 Training Loss: tensor(0.3716)\n",
      "10590 Training Loss: tensor(0.3660)\n",
      "10591 Training Loss: tensor(0.3697)\n",
      "10592 Training Loss: tensor(0.3660)\n",
      "10593 Training Loss: tensor(0.3672)\n",
      "10594 Training Loss: tensor(0.3687)\n",
      "10595 Training Loss: tensor(0.3680)\n",
      "10596 Training Loss: tensor(0.3717)\n",
      "10597 Training Loss: tensor(0.3639)\n",
      "10598 Training Loss: tensor(0.3783)\n",
      "10599 Training Loss: tensor(0.3725)\n",
      "10600 Training Loss: tensor(0.3743)\n",
      "10601 Training Loss: tensor(0.3653)\n",
      "10602 Training Loss: tensor(0.3739)\n",
      "10603 Training Loss: tensor(0.3697)\n",
      "10604 Training Loss: tensor(0.3690)\n",
      "10605 Training Loss: tensor(0.3685)\n",
      "10606 Training Loss: tensor(0.3685)\n",
      "10607 Training Loss: tensor(0.3658)\n",
      "10608 Training Loss: tensor(0.3679)\n",
      "10609 Training Loss: tensor(0.3685)\n",
      "10610 Training Loss: tensor(0.3676)\n",
      "10611 Training Loss: tensor(0.3685)\n",
      "10612 Training Loss: tensor(0.3724)\n",
      "10613 Training Loss: tensor(0.3699)\n",
      "10614 Training Loss: tensor(0.3711)\n",
      "10615 Training Loss: tensor(0.3695)\n",
      "10616 Training Loss: tensor(0.3686)\n",
      "10617 Training Loss: tensor(0.3714)\n",
      "10618 Training Loss: tensor(0.3704)\n",
      "10619 Training Loss: tensor(0.3712)\n",
      "10620 Training Loss: tensor(0.3688)\n",
      "10621 Training Loss: tensor(0.3672)\n",
      "10622 Training Loss: tensor(0.3678)\n",
      "10623 Training Loss: tensor(0.3681)\n",
      "10624 Training Loss: tensor(0.3669)\n",
      "10625 Training Loss: tensor(0.3654)\n",
      "10626 Training Loss: tensor(0.3713)\n",
      "10627 Training Loss: tensor(0.3686)\n",
      "10628 Training Loss: tensor(0.3681)\n",
      "10629 Training Loss: tensor(0.3666)\n",
      "10630 Training Loss: tensor(0.3729)\n",
      "10631 Training Loss: tensor(0.3747)\n",
      "10632 Training Loss: tensor(0.3655)\n",
      "10633 Training Loss: tensor(0.3666)\n",
      "10634 Training Loss: tensor(0.3670)\n",
      "10635 Training Loss: tensor(0.3679)\n",
      "10636 Training Loss: tensor(0.3680)\n",
      "10637 Training Loss: tensor(0.3703)\n",
      "10638 Training Loss: tensor(0.3694)\n",
      "10639 Training Loss: tensor(0.3674)\n",
      "10640 Training Loss: tensor(0.3659)\n",
      "10641 Training Loss: tensor(0.3643)\n",
      "10642 Training Loss: tensor(0.3713)\n",
      "10643 Training Loss: tensor(0.3707)\n",
      "10644 Training Loss: tensor(0.3723)\n",
      "10645 Training Loss: tensor(0.3639)\n",
      "10646 Training Loss: tensor(0.3676)\n",
      "10647 Training Loss: tensor(0.3688)\n",
      "10648 Training Loss: tensor(0.3712)\n",
      "10649 Training Loss: tensor(0.3670)\n",
      "10650 Training Loss: tensor(0.3709)\n",
      "10651 Training Loss: tensor(0.3679)\n",
      "10652 Training Loss: tensor(0.3690)\n",
      "10653 Training Loss: tensor(0.3651)\n",
      "10654 Training Loss: tensor(0.3676)\n",
      "10655 Training Loss: tensor(0.3684)\n",
      "10656 Training Loss: tensor(0.3668)\n",
      "10657 Training Loss: tensor(0.3700)\n",
      "10658 Training Loss: tensor(0.3679)\n",
      "10659 Training Loss: tensor(0.3710)\n",
      "10660 Training Loss: tensor(0.3683)\n",
      "10661 Training Loss: tensor(0.3660)\n",
      "10662 Training Loss: tensor(0.3715)\n",
      "10663 Training Loss: tensor(0.3709)\n",
      "10664 Training Loss: tensor(0.3746)\n",
      "10665 Training Loss: tensor(0.3666)\n",
      "10666 Training Loss: tensor(0.3686)\n",
      "10667 Training Loss: tensor(0.3699)\n",
      "10668 Training Loss: tensor(0.3731)\n",
      "10669 Training Loss: tensor(0.3735)\n",
      "10670 Training Loss: tensor(0.3687)\n",
      "10671 Training Loss: tensor(0.3682)\n",
      "10672 Training Loss: tensor(0.3701)\n",
      "10673 Training Loss: tensor(0.3719)\n",
      "10674 Training Loss: tensor(0.3690)\n",
      "10675 Training Loss: tensor(0.3685)\n",
      "10676 Training Loss: tensor(0.3689)\n",
      "10677 Training Loss: tensor(0.3686)\n",
      "10678 Training Loss: tensor(0.3663)\n",
      "10679 Training Loss: tensor(0.3686)\n",
      "10680 Training Loss: tensor(0.3663)\n",
      "10681 Training Loss: tensor(0.3668)\n",
      "10682 Training Loss: tensor(0.3658)\n",
      "10683 Training Loss: tensor(0.3692)\n",
      "10684 Training Loss: tensor(0.3693)\n",
      "10685 Training Loss: tensor(0.3674)\n",
      "10686 Training Loss: tensor(0.3718)\n",
      "10687 Training Loss: tensor(0.3688)\n",
      "10688 Training Loss: tensor(0.3736)\n",
      "10689 Training Loss: tensor(0.3673)\n",
      "10690 Training Loss: tensor(0.3692)\n",
      "10691 Training Loss: tensor(0.3697)\n",
      "10692 Training Loss: tensor(0.3702)\n",
      "10693 Training Loss: tensor(0.3636)\n",
      "10694 Training Loss: tensor(0.3678)\n",
      "10695 Training Loss: tensor(0.3681)\n",
      "10696 Training Loss: tensor(0.3667)\n",
      "10697 Training Loss: tensor(0.3702)\n",
      "10698 Training Loss: tensor(0.3673)\n",
      "10699 Training Loss: tensor(0.3716)\n",
      "10700 Training Loss: tensor(0.3738)\n",
      "10701 Training Loss: tensor(0.3730)\n",
      "10702 Training Loss: tensor(0.3644)\n",
      "10703 Training Loss: tensor(0.3689)\n",
      "10704 Training Loss: tensor(0.3661)\n",
      "10705 Training Loss: tensor(0.3715)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10706 Training Loss: tensor(0.3709)\n",
      "10707 Training Loss: tensor(0.3668)\n",
      "10708 Training Loss: tensor(0.3752)\n",
      "10709 Training Loss: tensor(0.3670)\n",
      "10710 Training Loss: tensor(0.3707)\n",
      "10711 Training Loss: tensor(0.3758)\n",
      "10712 Training Loss: tensor(0.3673)\n",
      "10713 Training Loss: tensor(0.3691)\n",
      "10714 Training Loss: tensor(0.3688)\n",
      "10715 Training Loss: tensor(0.3682)\n",
      "10716 Training Loss: tensor(0.3688)\n",
      "10717 Training Loss: tensor(0.3698)\n",
      "10718 Training Loss: tensor(0.3680)\n",
      "10719 Training Loss: tensor(0.3719)\n",
      "10720 Training Loss: tensor(0.3750)\n",
      "10721 Training Loss: tensor(0.3726)\n",
      "10722 Training Loss: tensor(0.3715)\n",
      "10723 Training Loss: tensor(0.3717)\n",
      "10724 Training Loss: tensor(0.3670)\n",
      "10725 Training Loss: tensor(0.3659)\n",
      "10726 Training Loss: tensor(0.3673)\n",
      "10727 Training Loss: tensor(0.3678)\n",
      "10728 Training Loss: tensor(0.3650)\n",
      "10729 Training Loss: tensor(0.3712)\n",
      "10730 Training Loss: tensor(0.3709)\n",
      "10731 Training Loss: tensor(0.3665)\n",
      "10732 Training Loss: tensor(0.3737)\n",
      "10733 Training Loss: tensor(0.3706)\n",
      "10734 Training Loss: tensor(0.3752)\n",
      "10735 Training Loss: tensor(0.3734)\n",
      "10736 Training Loss: tensor(0.3691)\n",
      "10737 Training Loss: tensor(0.3674)\n",
      "10738 Training Loss: tensor(0.3676)\n",
      "10739 Training Loss: tensor(0.3720)\n",
      "10740 Training Loss: tensor(0.3719)\n",
      "10741 Training Loss: tensor(0.3676)\n",
      "10742 Training Loss: tensor(0.3707)\n",
      "10743 Training Loss: tensor(0.3673)\n",
      "10744 Training Loss: tensor(0.3679)\n",
      "10745 Training Loss: tensor(0.3684)\n",
      "10746 Training Loss: tensor(0.3678)\n",
      "10747 Training Loss: tensor(0.3664)\n",
      "10748 Training Loss: tensor(0.3681)\n",
      "10749 Training Loss: tensor(0.3657)\n",
      "10750 Training Loss: tensor(0.3667)\n",
      "10751 Training Loss: tensor(0.3676)\n",
      "10752 Training Loss: tensor(0.3687)\n",
      "10753 Training Loss: tensor(0.3705)\n",
      "10754 Training Loss: tensor(0.3664)\n",
      "10755 Training Loss: tensor(0.3717)\n",
      "10756 Training Loss: tensor(0.3665)\n",
      "10757 Training Loss: tensor(0.3668)\n",
      "10758 Training Loss: tensor(0.3702)\n",
      "10759 Training Loss: tensor(0.3685)\n",
      "10760 Training Loss: tensor(0.3693)\n",
      "10761 Training Loss: tensor(0.3656)\n",
      "10762 Training Loss: tensor(0.3672)\n",
      "10763 Training Loss: tensor(0.3779)\n",
      "10764 Training Loss: tensor(0.3710)\n",
      "10765 Training Loss: tensor(0.3701)\n",
      "10766 Training Loss: tensor(0.3755)\n",
      "10767 Training Loss: tensor(0.3652)\n",
      "10768 Training Loss: tensor(0.3652)\n",
      "10769 Training Loss: tensor(0.3693)\n",
      "10770 Training Loss: tensor(0.3657)\n",
      "10771 Training Loss: tensor(0.3672)\n",
      "10772 Training Loss: tensor(0.3676)\n",
      "10773 Training Loss: tensor(0.3660)\n",
      "10774 Training Loss: tensor(0.3671)\n",
      "10775 Training Loss: tensor(0.3692)\n",
      "10776 Training Loss: tensor(0.3754)\n",
      "10777 Training Loss: tensor(0.3679)\n",
      "10778 Training Loss: tensor(0.3699)\n",
      "10779 Training Loss: tensor(0.3686)\n",
      "10780 Training Loss: tensor(0.3687)\n",
      "10781 Training Loss: tensor(0.3746)\n",
      "10782 Training Loss: tensor(0.3648)\n",
      "10783 Training Loss: tensor(0.3694)\n",
      "10784 Training Loss: tensor(0.3721)\n",
      "10785 Training Loss: tensor(0.3673)\n",
      "10786 Training Loss: tensor(0.3691)\n",
      "10787 Training Loss: tensor(0.3648)\n",
      "10788 Training Loss: tensor(0.3710)\n",
      "10789 Training Loss: tensor(0.3668)\n",
      "10790 Training Loss: tensor(0.3655)\n",
      "10791 Training Loss: tensor(0.3747)\n",
      "10792 Training Loss: tensor(0.3676)\n",
      "10793 Training Loss: tensor(0.3651)\n",
      "10794 Training Loss: tensor(0.3734)\n",
      "10795 Training Loss: tensor(0.3688)\n",
      "10796 Training Loss: tensor(0.3722)\n",
      "10797 Training Loss: tensor(0.3678)\n",
      "10798 Training Loss: tensor(0.3642)\n",
      "10799 Training Loss: tensor(0.3659)\n",
      "10800 Training Loss: tensor(0.3716)\n",
      "10801 Training Loss: tensor(0.3789)\n",
      "10802 Training Loss: tensor(0.3678)\n",
      "10803 Training Loss: tensor(0.3690)\n",
      "10804 Training Loss: tensor(0.3686)\n",
      "10805 Training Loss: tensor(0.3706)\n",
      "10806 Training Loss: tensor(0.3690)\n",
      "10807 Training Loss: tensor(0.3671)\n",
      "10808 Training Loss: tensor(0.3699)\n",
      "10809 Training Loss: tensor(0.3735)\n",
      "10810 Training Loss: tensor(0.3718)\n",
      "10811 Training Loss: tensor(0.3681)\n",
      "10812 Training Loss: tensor(0.3669)\n",
      "10813 Training Loss: tensor(0.3650)\n",
      "10814 Training Loss: tensor(0.3705)\n",
      "10815 Training Loss: tensor(0.3682)\n",
      "10816 Training Loss: tensor(0.3703)\n",
      "10817 Training Loss: tensor(0.3731)\n",
      "10818 Training Loss: tensor(0.3657)\n",
      "10819 Training Loss: tensor(0.3659)\n",
      "10820 Training Loss: tensor(0.3692)\n",
      "10821 Training Loss: tensor(0.3669)\n",
      "10822 Training Loss: tensor(0.3657)\n",
      "10823 Training Loss: tensor(0.3701)\n",
      "10824 Training Loss: tensor(0.3713)\n",
      "10825 Training Loss: tensor(0.3763)\n",
      "10826 Training Loss: tensor(0.3632)\n",
      "10827 Training Loss: tensor(0.3691)\n",
      "10828 Training Loss: tensor(0.3708)\n",
      "10829 Training Loss: tensor(0.3687)\n",
      "10830 Training Loss: tensor(0.3694)\n",
      "10831 Training Loss: tensor(0.3686)\n",
      "10832 Training Loss: tensor(0.3695)\n",
      "10833 Training Loss: tensor(0.3677)\n",
      "10834 Training Loss: tensor(0.3672)\n",
      "10835 Training Loss: tensor(0.3662)\n",
      "10836 Training Loss: tensor(0.3687)\n",
      "10837 Training Loss: tensor(0.3700)\n",
      "10838 Training Loss: tensor(0.3704)\n",
      "10839 Training Loss: tensor(0.3734)\n",
      "10840 Training Loss: tensor(0.3653)\n",
      "10841 Training Loss: tensor(0.3700)\n",
      "10842 Training Loss: tensor(0.3689)\n",
      "10843 Training Loss: tensor(0.3663)\n",
      "10844 Training Loss: tensor(0.3693)\n",
      "10845 Training Loss: tensor(0.3650)\n",
      "10846 Training Loss: tensor(0.3708)\n",
      "10847 Training Loss: tensor(0.3692)\n",
      "10848 Training Loss: tensor(0.3668)\n",
      "10849 Training Loss: tensor(0.3686)\n",
      "10850 Training Loss: tensor(0.3653)\n",
      "10851 Training Loss: tensor(0.3701)\n",
      "10852 Training Loss: tensor(0.3681)\n",
      "10853 Training Loss: tensor(0.3686)\n",
      "10854 Training Loss: tensor(0.3646)\n",
      "10855 Training Loss: tensor(0.3661)\n",
      "10856 Training Loss: tensor(0.3687)\n",
      "10857 Training Loss: tensor(0.3676)\n",
      "10858 Training Loss: tensor(0.3743)\n",
      "10859 Training Loss: tensor(0.3649)\n",
      "10860 Training Loss: tensor(0.3688)\n",
      "10861 Training Loss: tensor(0.3681)\n",
      "10862 Training Loss: tensor(0.3672)\n",
      "10863 Training Loss: tensor(0.3704)\n",
      "10864 Training Loss: tensor(0.3667)\n",
      "10865 Training Loss: tensor(0.3700)\n",
      "10866 Training Loss: tensor(0.3726)\n",
      "10867 Training Loss: tensor(0.3672)\n",
      "10868 Training Loss: tensor(0.3674)\n",
      "10869 Training Loss: tensor(0.3699)\n",
      "10870 Training Loss: tensor(0.3658)\n",
      "10871 Training Loss: tensor(0.3690)\n",
      "10872 Training Loss: tensor(0.3727)\n",
      "10873 Training Loss: tensor(0.3722)\n",
      "10874 Training Loss: tensor(0.3671)\n",
      "10875 Training Loss: tensor(0.3719)\n",
      "10876 Training Loss: tensor(0.3637)\n",
      "10877 Training Loss: tensor(0.3661)\n",
      "10878 Training Loss: tensor(0.3666)\n",
      "10879 Training Loss: tensor(0.3654)\n",
      "10880 Training Loss: tensor(0.3659)\n",
      "10881 Training Loss: tensor(0.3659)\n",
      "10882 Training Loss: tensor(0.3655)\n",
      "10883 Training Loss: tensor(0.3683)\n",
      "10884 Training Loss: tensor(0.3658)\n",
      "10885 Training Loss: tensor(0.3669)\n",
      "10886 Training Loss: tensor(0.3651)\n",
      "10887 Training Loss: tensor(0.3709)\n",
      "10888 Training Loss: tensor(0.3748)\n",
      "10889 Training Loss: tensor(0.3667)\n",
      "10890 Training Loss: tensor(0.3655)\n",
      "10891 Training Loss: tensor(0.3677)\n",
      "10892 Training Loss: tensor(0.3690)\n",
      "10893 Training Loss: tensor(0.3711)\n",
      "10894 Training Loss: tensor(0.3682)\n",
      "10895 Training Loss: tensor(0.3708)\n",
      "10896 Training Loss: tensor(0.3662)\n",
      "10897 Training Loss: tensor(0.3736)\n",
      "10898 Training Loss: tensor(0.3652)\n",
      "10899 Training Loss: tensor(0.3679)\n",
      "10900 Training Loss: tensor(0.3683)\n",
      "10901 Training Loss: tensor(0.3658)\n",
      "10902 Training Loss: tensor(0.3730)\n",
      "10903 Training Loss: tensor(0.3723)\n",
      "10904 Training Loss: tensor(0.3673)\n",
      "10905 Training Loss: tensor(0.3716)\n",
      "10906 Training Loss: tensor(0.3665)\n",
      "10907 Training Loss: tensor(0.3714)\n",
      "10908 Training Loss: tensor(0.3681)\n",
      "10909 Training Loss: tensor(0.3757)\n",
      "10910 Training Loss: tensor(0.3696)\n",
      "10911 Training Loss: tensor(0.3668)\n",
      "10912 Training Loss: tensor(0.3690)\n",
      "10913 Training Loss: tensor(0.3703)\n",
      "10914 Training Loss: tensor(0.3680)\n",
      "10915 Training Loss: tensor(0.3664)\n",
      "10916 Training Loss: tensor(0.3694)\n",
      "10917 Training Loss: tensor(0.3706)\n",
      "10918 Training Loss: tensor(0.3677)\n",
      "10919 Training Loss: tensor(0.3683)\n",
      "10920 Training Loss: tensor(0.3663)\n",
      "10921 Training Loss: tensor(0.3665)\n",
      "10922 Training Loss: tensor(0.3671)\n",
      "10923 Training Loss: tensor(0.3691)\n",
      "10924 Training Loss: tensor(0.3737)\n",
      "10925 Training Loss: tensor(0.3648)\n",
      "10926 Training Loss: tensor(0.3683)\n",
      "10927 Training Loss: tensor(0.3707)\n",
      "10928 Training Loss: tensor(0.3741)\n",
      "10929 Training Loss: tensor(0.3715)\n",
      "10930 Training Loss: tensor(0.3662)\n",
      "10931 Training Loss: tensor(0.3659)\n",
      "10932 Training Loss: tensor(0.3672)\n",
      "10933 Training Loss: tensor(0.3687)\n",
      "10934 Training Loss: tensor(0.3653)\n",
      "10935 Training Loss: tensor(0.3665)\n",
      "10936 Training Loss: tensor(0.3671)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10937 Training Loss: tensor(0.3663)\n",
      "10938 Training Loss: tensor(0.3645)\n",
      "10939 Training Loss: tensor(0.3673)\n",
      "10940 Training Loss: tensor(0.3680)\n",
      "10941 Training Loss: tensor(0.3731)\n",
      "10942 Training Loss: tensor(0.3654)\n",
      "10943 Training Loss: tensor(0.3679)\n",
      "10944 Training Loss: tensor(0.3709)\n",
      "10945 Training Loss: tensor(0.3741)\n",
      "10946 Training Loss: tensor(0.3709)\n",
      "10947 Training Loss: tensor(0.3754)\n",
      "10948 Training Loss: tensor(0.3662)\n",
      "10949 Training Loss: tensor(0.3684)\n",
      "10950 Training Loss: tensor(0.3698)\n",
      "10951 Training Loss: tensor(0.3673)\n",
      "10952 Training Loss: tensor(0.3718)\n",
      "10953 Training Loss: tensor(0.3711)\n",
      "10954 Training Loss: tensor(0.3709)\n",
      "10955 Training Loss: tensor(0.3678)\n",
      "10956 Training Loss: tensor(0.3696)\n",
      "10957 Training Loss: tensor(0.3686)\n",
      "10958 Training Loss: tensor(0.3663)\n",
      "10959 Training Loss: tensor(0.3681)\n",
      "10960 Training Loss: tensor(0.3679)\n",
      "10961 Training Loss: tensor(0.3669)\n",
      "10962 Training Loss: tensor(0.3685)\n",
      "10963 Training Loss: tensor(0.3700)\n",
      "10964 Training Loss: tensor(0.3680)\n",
      "10965 Training Loss: tensor(0.3649)\n",
      "10966 Training Loss: tensor(0.3717)\n",
      "10967 Training Loss: tensor(0.3664)\n",
      "10968 Training Loss: tensor(0.3675)\n",
      "10969 Training Loss: tensor(0.3697)\n",
      "10970 Training Loss: tensor(0.3683)\n",
      "10971 Training Loss: tensor(0.3704)\n",
      "10972 Training Loss: tensor(0.3717)\n",
      "10973 Training Loss: tensor(0.3702)\n",
      "10974 Training Loss: tensor(0.3746)\n",
      "10975 Training Loss: tensor(0.3699)\n",
      "10976 Training Loss: tensor(0.3648)\n",
      "10977 Training Loss: tensor(0.3691)\n",
      "10978 Training Loss: tensor(0.3712)\n",
      "10979 Training Loss: tensor(0.3669)\n",
      "10980 Training Loss: tensor(0.3663)\n",
      "10981 Training Loss: tensor(0.3666)\n",
      "10982 Training Loss: tensor(0.3683)\n",
      "10983 Training Loss: tensor(0.3694)\n",
      "10984 Training Loss: tensor(0.3643)\n",
      "10985 Training Loss: tensor(0.3653)\n",
      "10986 Training Loss: tensor(0.3674)\n",
      "10987 Training Loss: tensor(0.3704)\n",
      "10988 Training Loss: tensor(0.3630)\n",
      "10989 Training Loss: tensor(0.3754)\n",
      "10990 Training Loss: tensor(0.3715)\n",
      "10991 Training Loss: tensor(0.3726)\n",
      "10992 Training Loss: tensor(0.3696)\n",
      "10993 Training Loss: tensor(0.3666)\n",
      "10994 Training Loss: tensor(0.3703)\n",
      "10995 Training Loss: tensor(0.3676)\n",
      "10996 Training Loss: tensor(0.3679)\n",
      "10997 Training Loss: tensor(0.3664)\n",
      "10998 Training Loss: tensor(0.3680)\n",
      "10999 Training Loss: tensor(0.3648)\n",
      "11000 Training Loss: tensor(0.3655)\n",
      "11001 Training Loss: tensor(0.3701)\n",
      "11002 Training Loss: tensor(0.3691)\n",
      "11003 Training Loss: tensor(0.3637)\n",
      "11004 Training Loss: tensor(0.3669)\n",
      "11005 Training Loss: tensor(0.3674)\n",
      "11006 Training Loss: tensor(0.3702)\n",
      "11007 Training Loss: tensor(0.3707)\n",
      "11008 Training Loss: tensor(0.3677)\n",
      "11009 Training Loss: tensor(0.3670)\n",
      "11010 Training Loss: tensor(0.3765)\n",
      "11011 Training Loss: tensor(0.3652)\n",
      "11012 Training Loss: tensor(0.3708)\n",
      "11013 Training Loss: tensor(0.3687)\n",
      "11014 Training Loss: tensor(0.3694)\n",
      "11015 Training Loss: tensor(0.3645)\n",
      "11016 Training Loss: tensor(0.3690)\n",
      "11017 Training Loss: tensor(0.3642)\n",
      "11018 Training Loss: tensor(0.3661)\n",
      "11019 Training Loss: tensor(0.3650)\n",
      "11020 Training Loss: tensor(0.3671)\n",
      "11021 Training Loss: tensor(0.3707)\n",
      "11022 Training Loss: tensor(0.3664)\n",
      "11023 Training Loss: tensor(0.3709)\n",
      "11024 Training Loss: tensor(0.3682)\n",
      "11025 Training Loss: tensor(0.3672)\n",
      "11026 Training Loss: tensor(0.3669)\n",
      "11027 Training Loss: tensor(0.3643)\n",
      "11028 Training Loss: tensor(0.3671)\n",
      "11029 Training Loss: tensor(0.3684)\n",
      "11030 Training Loss: tensor(0.3661)\n",
      "11031 Training Loss: tensor(0.3690)\n",
      "11032 Training Loss: tensor(0.3701)\n",
      "11033 Training Loss: tensor(0.3661)\n",
      "11034 Training Loss: tensor(0.3678)\n",
      "11035 Training Loss: tensor(0.3664)\n",
      "11036 Training Loss: tensor(0.3647)\n",
      "11037 Training Loss: tensor(0.3697)\n",
      "11038 Training Loss: tensor(0.3734)\n",
      "11039 Training Loss: tensor(0.3691)\n",
      "11040 Training Loss: tensor(0.3681)\n",
      "11041 Training Loss: tensor(0.3658)\n",
      "11042 Training Loss: tensor(0.3655)\n",
      "11043 Training Loss: tensor(0.3698)\n",
      "11044 Training Loss: tensor(0.3662)\n",
      "11045 Training Loss: tensor(0.3668)\n",
      "11046 Training Loss: tensor(0.3683)\n",
      "11047 Training Loss: tensor(0.3686)\n",
      "11048 Training Loss: tensor(0.3685)\n",
      "11049 Training Loss: tensor(0.3746)\n",
      "11050 Training Loss: tensor(0.3647)\n",
      "11051 Training Loss: tensor(0.3659)\n",
      "11052 Training Loss: tensor(0.3670)\n",
      "11053 Training Loss: tensor(0.3690)\n",
      "11054 Training Loss: tensor(0.3644)\n",
      "11055 Training Loss: tensor(0.3672)\n",
      "11056 Training Loss: tensor(0.3656)\n",
      "11057 Training Loss: tensor(0.3717)\n",
      "11058 Training Loss: tensor(0.3673)\n",
      "11059 Training Loss: tensor(0.3754)\n",
      "11060 Training Loss: tensor(0.3658)\n",
      "11061 Training Loss: tensor(0.3648)\n",
      "11062 Training Loss: tensor(0.3648)\n",
      "11063 Training Loss: tensor(0.3649)\n",
      "11064 Training Loss: tensor(0.3629)\n",
      "11065 Training Loss: tensor(0.3667)\n",
      "11066 Training Loss: tensor(0.3730)\n",
      "11067 Training Loss: tensor(0.3717)\n",
      "11068 Training Loss: tensor(0.3685)\n",
      "11069 Training Loss: tensor(0.3648)\n",
      "11070 Training Loss: tensor(0.3636)\n",
      "11071 Training Loss: tensor(0.3684)\n",
      "11072 Training Loss: tensor(0.3658)\n",
      "11073 Training Loss: tensor(0.3659)\n",
      "11074 Training Loss: tensor(0.3659)\n",
      "11075 Training Loss: tensor(0.3664)\n",
      "11076 Training Loss: tensor(0.3635)\n",
      "11077 Training Loss: tensor(0.3770)\n",
      "11078 Training Loss: tensor(0.3627)\n",
      "11079 Training Loss: tensor(0.3720)\n",
      "11080 Training Loss: tensor(0.3683)\n",
      "11081 Training Loss: tensor(0.3677)\n",
      "11082 Training Loss: tensor(0.3690)\n",
      "11083 Training Loss: tensor(0.3682)\n",
      "11084 Training Loss: tensor(0.3677)\n",
      "11085 Training Loss: tensor(0.3679)\n",
      "11086 Training Loss: tensor(0.3702)\n",
      "11087 Training Loss: tensor(0.3665)\n",
      "11088 Training Loss: tensor(0.3730)\n",
      "11089 Training Loss: tensor(0.3643)\n",
      "11090 Training Loss: tensor(0.3674)\n",
      "11091 Training Loss: tensor(0.3664)\n",
      "11092 Training Loss: tensor(0.3697)\n",
      "11093 Training Loss: tensor(0.3692)\n",
      "11094 Training Loss: tensor(0.3665)\n",
      "11095 Training Loss: tensor(0.3660)\n",
      "11096 Training Loss: tensor(0.3661)\n",
      "11097 Training Loss: tensor(0.3760)\n",
      "11098 Training Loss: tensor(0.3697)\n",
      "11099 Training Loss: tensor(0.3681)\n",
      "11100 Training Loss: tensor(0.3714)\n",
      "11101 Training Loss: tensor(0.3713)\n",
      "11102 Training Loss: tensor(0.3671)\n",
      "11103 Training Loss: tensor(0.3648)\n",
      "11104 Training Loss: tensor(0.3667)\n",
      "11105 Training Loss: tensor(0.3661)\n",
      "11106 Training Loss: tensor(0.3677)\n",
      "11107 Training Loss: tensor(0.3712)\n",
      "11108 Training Loss: tensor(0.3669)\n",
      "11109 Training Loss: tensor(0.3655)\n",
      "11110 Training Loss: tensor(0.3727)\n",
      "11111 Training Loss: tensor(0.3663)\n",
      "11112 Training Loss: tensor(0.3682)\n",
      "11113 Training Loss: tensor(0.3758)\n",
      "11114 Training Loss: tensor(0.3660)\n",
      "11115 Training Loss: tensor(0.3696)\n",
      "11116 Training Loss: tensor(0.3681)\n",
      "11117 Training Loss: tensor(0.3699)\n",
      "11118 Training Loss: tensor(0.3685)\n",
      "11119 Training Loss: tensor(0.3685)\n",
      "11120 Training Loss: tensor(0.3665)\n",
      "11121 Training Loss: tensor(0.3700)\n",
      "11122 Training Loss: tensor(0.3700)\n",
      "11123 Training Loss: tensor(0.3664)\n",
      "11124 Training Loss: tensor(0.3739)\n",
      "11125 Training Loss: tensor(0.3694)\n",
      "11126 Training Loss: tensor(0.3714)\n",
      "11127 Training Loss: tensor(0.3676)\n",
      "11128 Training Loss: tensor(0.3678)\n",
      "11129 Training Loss: tensor(0.3679)\n",
      "11130 Training Loss: tensor(0.3664)\n",
      "11131 Training Loss: tensor(0.3717)\n",
      "11132 Training Loss: tensor(0.3702)\n",
      "11133 Training Loss: tensor(0.3695)\n",
      "11134 Training Loss: tensor(0.3665)\n",
      "11135 Training Loss: tensor(0.3713)\n",
      "11136 Training Loss: tensor(0.3738)\n",
      "11137 Training Loss: tensor(0.3664)\n",
      "11138 Training Loss: tensor(0.3663)\n",
      "11139 Training Loss: tensor(0.3723)\n",
      "11140 Training Loss: tensor(0.3692)\n",
      "11141 Training Loss: tensor(0.3668)\n",
      "11142 Training Loss: tensor(0.3664)\n",
      "11143 Training Loss: tensor(0.3673)\n",
      "11144 Training Loss: tensor(0.3706)\n",
      "11145 Training Loss: tensor(0.3677)\n",
      "11146 Training Loss: tensor(0.3716)\n",
      "11147 Training Loss: tensor(0.3668)\n",
      "11148 Training Loss: tensor(0.3688)\n",
      "11149 Training Loss: tensor(0.3694)\n",
      "11150 Training Loss: tensor(0.3658)\n",
      "11151 Training Loss: tensor(0.3648)\n",
      "11152 Training Loss: tensor(0.3635)\n",
      "11153 Training Loss: tensor(0.3678)\n",
      "11154 Training Loss: tensor(0.3709)\n",
      "11155 Training Loss: tensor(0.3641)\n",
      "11156 Training Loss: tensor(0.3703)\n",
      "11157 Training Loss: tensor(0.3737)\n",
      "11158 Training Loss: tensor(0.3705)\n",
      "11159 Training Loss: tensor(0.3675)\n",
      "11160 Training Loss: tensor(0.3732)\n",
      "11161 Training Loss: tensor(0.3689)\n",
      "11162 Training Loss: tensor(0.3696)\n",
      "11163 Training Loss: tensor(0.3662)\n",
      "11164 Training Loss: tensor(0.3717)\n",
      "11165 Training Loss: tensor(0.3672)\n",
      "11166 Training Loss: tensor(0.3695)\n",
      "11167 Training Loss: tensor(0.3730)\n",
      "11168 Training Loss: tensor(0.3693)\n",
      "11169 Training Loss: tensor(0.3676)\n",
      "11170 Training Loss: tensor(0.3677)\n",
      "11171 Training Loss: tensor(0.3654)\n",
      "11172 Training Loss: tensor(0.3667)\n",
      "11173 Training Loss: tensor(0.3677)\n",
      "11174 Training Loss: tensor(0.3686)\n",
      "11175 Training Loss: tensor(0.3660)\n",
      "11176 Training Loss: tensor(0.3660)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11177 Training Loss: tensor(0.3700)\n",
      "11178 Training Loss: tensor(0.3670)\n",
      "11179 Training Loss: tensor(0.3706)\n",
      "11180 Training Loss: tensor(0.3662)\n",
      "11181 Training Loss: tensor(0.3675)\n",
      "11182 Training Loss: tensor(0.3679)\n",
      "11183 Training Loss: tensor(0.3652)\n",
      "11184 Training Loss: tensor(0.3677)\n",
      "11185 Training Loss: tensor(0.3719)\n",
      "11186 Training Loss: tensor(0.3682)\n",
      "11187 Training Loss: tensor(0.3675)\n",
      "11188 Training Loss: tensor(0.3687)\n",
      "11189 Training Loss: tensor(0.3670)\n",
      "11190 Training Loss: tensor(0.3678)\n",
      "11191 Training Loss: tensor(0.3670)\n",
      "11192 Training Loss: tensor(0.3674)\n",
      "11193 Training Loss: tensor(0.3690)\n",
      "11194 Training Loss: tensor(0.3668)\n",
      "11195 Training Loss: tensor(0.3666)\n",
      "11196 Training Loss: tensor(0.3677)\n",
      "11197 Training Loss: tensor(0.3699)\n",
      "11198 Training Loss: tensor(0.3657)\n",
      "11199 Training Loss: tensor(0.3659)\n",
      "11200 Training Loss: tensor(0.3690)\n",
      "11201 Training Loss: tensor(0.3645)\n",
      "11202 Training Loss: tensor(0.3679)\n",
      "11203 Training Loss: tensor(0.3649)\n",
      "11204 Training Loss: tensor(0.3701)\n",
      "11205 Training Loss: tensor(0.3676)\n",
      "11206 Training Loss: tensor(0.3658)\n",
      "11207 Training Loss: tensor(0.3657)\n",
      "11208 Training Loss: tensor(0.3654)\n",
      "11209 Training Loss: tensor(0.3687)\n",
      "11210 Training Loss: tensor(0.3737)\n",
      "11211 Training Loss: tensor(0.3654)\n",
      "11212 Training Loss: tensor(0.3684)\n",
      "11213 Training Loss: tensor(0.3659)\n",
      "11214 Training Loss: tensor(0.3706)\n",
      "11215 Training Loss: tensor(0.3683)\n",
      "11216 Training Loss: tensor(0.3668)\n",
      "11217 Training Loss: tensor(0.3657)\n",
      "11218 Training Loss: tensor(0.3658)\n",
      "11219 Training Loss: tensor(0.3646)\n",
      "11220 Training Loss: tensor(0.3661)\n",
      "11221 Training Loss: tensor(0.3649)\n",
      "11222 Training Loss: tensor(0.3656)\n",
      "11223 Training Loss: tensor(0.3705)\n",
      "11224 Training Loss: tensor(0.3675)\n",
      "11225 Training Loss: tensor(0.3668)\n",
      "11226 Training Loss: tensor(0.3657)\n",
      "11227 Training Loss: tensor(0.3656)\n",
      "11228 Training Loss: tensor(0.3646)\n",
      "11229 Training Loss: tensor(0.3654)\n",
      "11230 Training Loss: tensor(0.3650)\n",
      "11231 Training Loss: tensor(0.3664)\n",
      "11232 Training Loss: tensor(0.3706)\n",
      "11233 Training Loss: tensor(0.3674)\n",
      "11234 Training Loss: tensor(0.3647)\n",
      "11235 Training Loss: tensor(0.3665)\n",
      "11236 Training Loss: tensor(0.3680)\n",
      "11237 Training Loss: tensor(0.3666)\n",
      "11238 Training Loss: tensor(0.3667)\n",
      "11239 Training Loss: tensor(0.3706)\n",
      "11240 Training Loss: tensor(0.3710)\n",
      "11241 Training Loss: tensor(0.3670)\n",
      "11242 Training Loss: tensor(0.3656)\n",
      "11243 Training Loss: tensor(0.3660)\n",
      "11244 Training Loss: tensor(0.3677)\n",
      "11245 Training Loss: tensor(0.3686)\n",
      "11246 Training Loss: tensor(0.3654)\n",
      "11247 Training Loss: tensor(0.3649)\n",
      "11248 Training Loss: tensor(0.3710)\n",
      "11249 Training Loss: tensor(0.3624)\n",
      "11250 Training Loss: tensor(0.3644)\n",
      "11251 Training Loss: tensor(0.3647)\n",
      "11252 Training Loss: tensor(0.3633)\n",
      "11253 Training Loss: tensor(0.3666)\n",
      "11254 Training Loss: tensor(0.3641)\n",
      "11255 Training Loss: tensor(0.3677)\n",
      "11256 Training Loss: tensor(0.3740)\n",
      "11257 Training Loss: tensor(0.3702)\n",
      "11258 Training Loss: tensor(0.3706)\n",
      "11259 Training Loss: tensor(0.3683)\n",
      "11260 Training Loss: tensor(0.3700)\n",
      "11261 Training Loss: tensor(0.3658)\n",
      "11262 Training Loss: tensor(0.3678)\n",
      "11263 Training Loss: tensor(0.3696)\n",
      "11264 Training Loss: tensor(0.3702)\n",
      "11265 Training Loss: tensor(0.3727)\n",
      "11266 Training Loss: tensor(0.3738)\n",
      "11267 Training Loss: tensor(0.3671)\n",
      "11268 Training Loss: tensor(0.3714)\n",
      "11269 Training Loss: tensor(0.3659)\n",
      "11270 Training Loss: tensor(0.3755)\n",
      "11271 Training Loss: tensor(0.3676)\n",
      "11272 Training Loss: tensor(0.3673)\n",
      "11273 Training Loss: tensor(0.3709)\n",
      "11274 Training Loss: tensor(0.3715)\n",
      "11275 Training Loss: tensor(0.3693)\n",
      "11276 Training Loss: tensor(0.3701)\n",
      "11277 Training Loss: tensor(0.3685)\n",
      "11278 Training Loss: tensor(0.3701)\n",
      "11279 Training Loss: tensor(0.3701)\n",
      "11280 Training Loss: tensor(0.3734)\n",
      "11281 Training Loss: tensor(0.3675)\n",
      "11282 Training Loss: tensor(0.3674)\n",
      "11283 Training Loss: tensor(0.3717)\n",
      "11284 Training Loss: tensor(0.3746)\n",
      "11285 Training Loss: tensor(0.3677)\n",
      "11286 Training Loss: tensor(0.3716)\n",
      "11287 Training Loss: tensor(0.3678)\n",
      "11288 Training Loss: tensor(0.3677)\n",
      "11289 Training Loss: tensor(0.3659)\n",
      "11290 Training Loss: tensor(0.3684)\n",
      "11291 Training Loss: tensor(0.3663)\n",
      "11292 Training Loss: tensor(0.3685)\n",
      "11293 Training Loss: tensor(0.3663)\n",
      "11294 Training Loss: tensor(0.3633)\n",
      "11295 Training Loss: tensor(0.3700)\n",
      "11296 Training Loss: tensor(0.3648)\n",
      "11297 Training Loss: tensor(0.3700)\n",
      "11298 Training Loss: tensor(0.3688)\n",
      "11299 Training Loss: tensor(0.3642)\n",
      "11300 Training Loss: tensor(0.3662)\n",
      "11301 Training Loss: tensor(0.3694)\n",
      "11302 Training Loss: tensor(0.3637)\n",
      "11303 Training Loss: tensor(0.3648)\n",
      "11304 Training Loss: tensor(0.3640)\n",
      "11305 Training Loss: tensor(0.3679)\n",
      "11306 Training Loss: tensor(0.3637)\n",
      "11307 Training Loss: tensor(0.3645)\n",
      "11308 Training Loss: tensor(0.3642)\n",
      "11309 Training Loss: tensor(0.3634)\n",
      "11310 Training Loss: tensor(0.3757)\n",
      "11311 Training Loss: tensor(0.3647)\n",
      "11312 Training Loss: tensor(0.3686)\n",
      "11313 Training Loss: tensor(0.3696)\n",
      "11314 Training Loss: tensor(0.3694)\n",
      "11315 Training Loss: tensor(0.3689)\n",
      "11316 Training Loss: tensor(0.3692)\n",
      "11317 Training Loss: tensor(0.3692)\n",
      "11318 Training Loss: tensor(0.3686)\n",
      "11319 Training Loss: tensor(0.3639)\n",
      "11320 Training Loss: tensor(0.3643)\n",
      "11321 Training Loss: tensor(0.3635)\n",
      "11322 Training Loss: tensor(0.3666)\n",
      "11323 Training Loss: tensor(0.3644)\n",
      "11324 Training Loss: tensor(0.3642)\n",
      "11325 Training Loss: tensor(0.3690)\n",
      "11326 Training Loss: tensor(0.3667)\n",
      "11327 Training Loss: tensor(0.3761)\n",
      "11328 Training Loss: tensor(0.3670)\n",
      "11329 Training Loss: tensor(0.3679)\n",
      "11330 Training Loss: tensor(0.3656)\n",
      "11331 Training Loss: tensor(0.3669)\n",
      "11332 Training Loss: tensor(0.3626)\n",
      "11333 Training Loss: tensor(0.3666)\n",
      "11334 Training Loss: tensor(0.3658)\n",
      "11335 Training Loss: tensor(0.3642)\n",
      "11336 Training Loss: tensor(0.3662)\n",
      "11337 Training Loss: tensor(0.3683)\n",
      "11338 Training Loss: tensor(0.3652)\n",
      "11339 Training Loss: tensor(0.3639)\n",
      "11340 Training Loss: tensor(0.3672)\n",
      "11341 Training Loss: tensor(0.3671)\n",
      "11342 Training Loss: tensor(0.3675)\n",
      "11343 Training Loss: tensor(0.3637)\n",
      "11344 Training Loss: tensor(0.3675)\n",
      "11345 Training Loss: tensor(0.3666)\n",
      "11346 Training Loss: tensor(0.3676)\n",
      "11347 Training Loss: tensor(0.3649)\n",
      "11348 Training Loss: tensor(0.3690)\n",
      "11349 Training Loss: tensor(0.3646)\n",
      "11350 Training Loss: tensor(0.3632)\n",
      "11351 Training Loss: tensor(0.3729)\n",
      "11352 Training Loss: tensor(0.3666)\n",
      "11353 Training Loss: tensor(0.3684)\n",
      "11354 Training Loss: tensor(0.3693)\n",
      "11355 Training Loss: tensor(0.3668)\n",
      "11356 Training Loss: tensor(0.3684)\n",
      "11357 Training Loss: tensor(0.3671)\n",
      "11358 Training Loss: tensor(0.3685)\n",
      "11359 Training Loss: tensor(0.3729)\n",
      "11360 Training Loss: tensor(0.3682)\n",
      "11361 Training Loss: tensor(0.3683)\n",
      "11362 Training Loss: tensor(0.3647)\n",
      "11363 Training Loss: tensor(0.3702)\n",
      "11364 Training Loss: tensor(0.3654)\n",
      "11365 Training Loss: tensor(0.3680)\n",
      "11366 Training Loss: tensor(0.3655)\n",
      "11367 Training Loss: tensor(0.3682)\n",
      "11368 Training Loss: tensor(0.3668)\n",
      "11369 Training Loss: tensor(0.3682)\n",
      "11370 Training Loss: tensor(0.3687)\n",
      "11371 Training Loss: tensor(0.3692)\n",
      "11372 Training Loss: tensor(0.3705)\n",
      "11373 Training Loss: tensor(0.3720)\n",
      "11374 Training Loss: tensor(0.3658)\n",
      "11375 Training Loss: tensor(0.3656)\n",
      "11376 Training Loss: tensor(0.3633)\n",
      "11377 Training Loss: tensor(0.3632)\n",
      "11378 Training Loss: tensor(0.3713)\n",
      "11379 Training Loss: tensor(0.3644)\n",
      "11380 Training Loss: tensor(0.3754)\n",
      "11381 Training Loss: tensor(0.3710)\n",
      "11382 Training Loss: tensor(0.3657)\n",
      "11383 Training Loss: tensor(0.3674)\n",
      "11384 Training Loss: tensor(0.3675)\n",
      "11385 Training Loss: tensor(0.3647)\n",
      "11386 Training Loss: tensor(0.3669)\n",
      "11387 Training Loss: tensor(0.3672)\n",
      "11388 Training Loss: tensor(0.3672)\n",
      "11389 Training Loss: tensor(0.3710)\n",
      "11390 Training Loss: tensor(0.3699)\n",
      "11391 Training Loss: tensor(0.3749)\n",
      "11392 Training Loss: tensor(0.3679)\n",
      "11393 Training Loss: tensor(0.3683)\n",
      "11394 Training Loss: tensor(0.3667)\n",
      "11395 Training Loss: tensor(0.3687)\n",
      "11396 Training Loss: tensor(0.3703)\n",
      "11397 Training Loss: tensor(0.3646)\n",
      "11398 Training Loss: tensor(0.3664)\n",
      "11399 Training Loss: tensor(0.3687)\n",
      "11400 Training Loss: tensor(0.3677)\n",
      "11401 Training Loss: tensor(0.3645)\n",
      "11402 Training Loss: tensor(0.3649)\n",
      "11403 Training Loss: tensor(0.3666)\n",
      "11404 Training Loss: tensor(0.3693)\n",
      "11405 Training Loss: tensor(0.3637)\n",
      "11406 Training Loss: tensor(0.3670)\n",
      "11407 Training Loss: tensor(0.3630)\n",
      "11408 Training Loss: tensor(0.3649)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11409 Training Loss: tensor(0.3656)\n",
      "11410 Training Loss: tensor(0.3684)\n",
      "11411 Training Loss: tensor(0.3690)\n",
      "11412 Training Loss: tensor(0.3670)\n",
      "11413 Training Loss: tensor(0.3678)\n",
      "11414 Training Loss: tensor(0.3617)\n",
      "11415 Training Loss: tensor(0.3647)\n",
      "11416 Training Loss: tensor(0.3641)\n",
      "11417 Training Loss: tensor(0.3792)\n",
      "11418 Training Loss: tensor(0.3636)\n",
      "11419 Training Loss: tensor(0.3683)\n",
      "11420 Training Loss: tensor(0.3718)\n",
      "11421 Training Loss: tensor(0.3779)\n",
      "11422 Training Loss: tensor(0.3656)\n",
      "11423 Training Loss: tensor(0.3646)\n",
      "11424 Training Loss: tensor(0.3664)\n",
      "11425 Training Loss: tensor(0.3649)\n",
      "11426 Training Loss: tensor(0.3682)\n",
      "11427 Training Loss: tensor(0.3643)\n",
      "11428 Training Loss: tensor(0.3655)\n",
      "11429 Training Loss: tensor(0.3668)\n",
      "11430 Training Loss: tensor(0.3658)\n",
      "11431 Training Loss: tensor(0.3714)\n",
      "11432 Training Loss: tensor(0.3638)\n",
      "11433 Training Loss: tensor(0.3675)\n",
      "11434 Training Loss: tensor(0.3653)\n",
      "11435 Training Loss: tensor(0.3653)\n",
      "11436 Training Loss: tensor(0.3686)\n",
      "11437 Training Loss: tensor(0.3668)\n",
      "11438 Training Loss: tensor(0.3653)\n",
      "11439 Training Loss: tensor(0.3686)\n",
      "11440 Training Loss: tensor(0.3692)\n",
      "11441 Training Loss: tensor(0.3650)\n",
      "11442 Training Loss: tensor(0.3658)\n",
      "11443 Training Loss: tensor(0.3693)\n",
      "11444 Training Loss: tensor(0.3631)\n",
      "11445 Training Loss: tensor(0.3779)\n",
      "11446 Training Loss: tensor(0.3644)\n",
      "11447 Training Loss: tensor(0.3652)\n",
      "11448 Training Loss: tensor(0.3635)\n",
      "11449 Training Loss: tensor(0.3641)\n",
      "11450 Training Loss: tensor(0.3683)\n",
      "11451 Training Loss: tensor(0.3674)\n",
      "11452 Training Loss: tensor(0.3661)\n",
      "11453 Training Loss: tensor(0.3666)\n",
      "11454 Training Loss: tensor(0.3638)\n",
      "11455 Training Loss: tensor(0.3640)\n",
      "11456 Training Loss: tensor(0.3662)\n",
      "11457 Training Loss: tensor(0.3672)\n",
      "11458 Training Loss: tensor(0.3648)\n",
      "11459 Training Loss: tensor(0.3701)\n",
      "11460 Training Loss: tensor(0.3643)\n",
      "11461 Training Loss: tensor(0.3671)\n",
      "11462 Training Loss: tensor(0.3746)\n",
      "11463 Training Loss: tensor(0.3693)\n",
      "11464 Training Loss: tensor(0.3677)\n",
      "11465 Training Loss: tensor(0.3697)\n",
      "11466 Training Loss: tensor(0.3701)\n",
      "11467 Training Loss: tensor(0.3652)\n",
      "11468 Training Loss: tensor(0.3694)\n",
      "11469 Training Loss: tensor(0.3677)\n",
      "11470 Training Loss: tensor(0.3675)\n",
      "11471 Training Loss: tensor(0.3653)\n",
      "11472 Training Loss: tensor(0.3649)\n",
      "11473 Training Loss: tensor(0.3703)\n",
      "11474 Training Loss: tensor(0.3706)\n",
      "11475 Training Loss: tensor(0.3733)\n",
      "11476 Training Loss: tensor(0.3651)\n",
      "11477 Training Loss: tensor(0.3659)\n",
      "11478 Training Loss: tensor(0.3665)\n",
      "11479 Training Loss: tensor(0.3679)\n",
      "11480 Training Loss: tensor(0.3637)\n",
      "11481 Training Loss: tensor(0.3694)\n",
      "11482 Training Loss: tensor(0.3672)\n",
      "11483 Training Loss: tensor(0.3659)\n",
      "11484 Training Loss: tensor(0.3662)\n",
      "11485 Training Loss: tensor(0.3681)\n",
      "11486 Training Loss: tensor(0.3686)\n",
      "11487 Training Loss: tensor(0.3649)\n",
      "11488 Training Loss: tensor(0.3671)\n",
      "11489 Training Loss: tensor(0.3703)\n",
      "11490 Training Loss: tensor(0.3698)\n",
      "11491 Training Loss: tensor(0.3642)\n",
      "11492 Training Loss: tensor(0.3674)\n",
      "11493 Training Loss: tensor(0.3647)\n",
      "11494 Training Loss: tensor(0.3711)\n",
      "11495 Training Loss: tensor(0.3714)\n",
      "11496 Training Loss: tensor(0.3671)\n",
      "11497 Training Loss: tensor(0.3709)\n",
      "11498 Training Loss: tensor(0.3648)\n",
      "11499 Training Loss: tensor(0.3683)\n",
      "11500 Training Loss: tensor(0.3689)\n",
      "11501 Training Loss: tensor(0.3670)\n",
      "11502 Training Loss: tensor(0.3644)\n",
      "11503 Training Loss: tensor(0.3671)\n",
      "11504 Training Loss: tensor(0.3664)\n",
      "11505 Training Loss: tensor(0.3665)\n",
      "11506 Training Loss: tensor(0.3666)\n",
      "11507 Training Loss: tensor(0.3686)\n",
      "11508 Training Loss: tensor(0.3637)\n",
      "11509 Training Loss: tensor(0.3671)\n",
      "11510 Training Loss: tensor(0.3674)\n",
      "11511 Training Loss: tensor(0.3672)\n",
      "11512 Training Loss: tensor(0.3679)\n",
      "11513 Training Loss: tensor(0.3675)\n",
      "11514 Training Loss: tensor(0.3697)\n",
      "11515 Training Loss: tensor(0.3695)\n",
      "11516 Training Loss: tensor(0.3676)\n",
      "11517 Training Loss: tensor(0.3684)\n",
      "11518 Training Loss: tensor(0.3719)\n",
      "11519 Training Loss: tensor(0.3648)\n",
      "11520 Training Loss: tensor(0.3701)\n",
      "11521 Training Loss: tensor(0.3695)\n",
      "11522 Training Loss: tensor(0.3626)\n",
      "11523 Training Loss: tensor(0.3670)\n",
      "11524 Training Loss: tensor(0.3707)\n",
      "11525 Training Loss: tensor(0.3690)\n",
      "11526 Training Loss: tensor(0.3639)\n",
      "11527 Training Loss: tensor(0.3641)\n",
      "11528 Training Loss: tensor(0.3713)\n",
      "11529 Training Loss: tensor(0.3671)\n",
      "11530 Training Loss: tensor(0.3641)\n",
      "11531 Training Loss: tensor(0.3720)\n",
      "11532 Training Loss: tensor(0.3668)\n",
      "11533 Training Loss: tensor(0.3761)\n",
      "11534 Training Loss: tensor(0.3665)\n",
      "11535 Training Loss: tensor(0.3673)\n",
      "11536 Training Loss: tensor(0.3667)\n",
      "11537 Training Loss: tensor(0.3675)\n",
      "11538 Training Loss: tensor(0.3680)\n",
      "11539 Training Loss: tensor(0.3681)\n",
      "11540 Training Loss: tensor(0.3681)\n",
      "11541 Training Loss: tensor(0.3660)\n",
      "11542 Training Loss: tensor(0.3673)\n",
      "11543 Training Loss: tensor(0.3662)\n",
      "11544 Training Loss: tensor(0.3662)\n",
      "11545 Training Loss: tensor(0.3674)\n",
      "11546 Training Loss: tensor(0.3652)\n",
      "11547 Training Loss: tensor(0.3643)\n",
      "11548 Training Loss: tensor(0.3667)\n",
      "11549 Training Loss: tensor(0.3638)\n",
      "11550 Training Loss: tensor(0.3670)\n",
      "11551 Training Loss: tensor(0.3675)\n",
      "11552 Training Loss: tensor(0.3754)\n",
      "11553 Training Loss: tensor(0.3677)\n",
      "11554 Training Loss: tensor(0.3630)\n",
      "11555 Training Loss: tensor(0.3639)\n",
      "11556 Training Loss: tensor(0.3660)\n",
      "11557 Training Loss: tensor(0.3674)\n",
      "11558 Training Loss: tensor(0.3646)\n",
      "11559 Training Loss: tensor(0.3658)\n",
      "11560 Training Loss: tensor(0.3733)\n",
      "11561 Training Loss: tensor(0.3682)\n",
      "11562 Training Loss: tensor(0.3697)\n",
      "11563 Training Loss: tensor(0.3686)\n",
      "11564 Training Loss: tensor(0.3652)\n",
      "11565 Training Loss: tensor(0.3678)\n",
      "11566 Training Loss: tensor(0.3665)\n",
      "11567 Training Loss: tensor(0.3664)\n",
      "11568 Training Loss: tensor(0.3753)\n",
      "11569 Training Loss: tensor(0.3705)\n",
      "11570 Training Loss: tensor(0.3644)\n",
      "11571 Training Loss: tensor(0.3652)\n",
      "11572 Training Loss: tensor(0.3675)\n",
      "11573 Training Loss: tensor(0.3663)\n",
      "11574 Training Loss: tensor(0.3671)\n",
      "11575 Training Loss: tensor(0.3659)\n",
      "11576 Training Loss: tensor(0.3674)\n",
      "11577 Training Loss: tensor(0.3646)\n",
      "11578 Training Loss: tensor(0.3661)\n",
      "11579 Training Loss: tensor(0.3648)\n",
      "11580 Training Loss: tensor(0.3676)\n",
      "11581 Training Loss: tensor(0.3636)\n",
      "11582 Training Loss: tensor(0.3655)\n",
      "11583 Training Loss: tensor(0.3665)\n",
      "11584 Training Loss: tensor(0.3647)\n",
      "11585 Training Loss: tensor(0.3672)\n",
      "11586 Training Loss: tensor(0.3661)\n",
      "11587 Training Loss: tensor(0.3669)\n",
      "11588 Training Loss: tensor(0.3632)\n",
      "11589 Training Loss: tensor(0.3703)\n",
      "11590 Training Loss: tensor(0.3682)\n",
      "11591 Training Loss: tensor(0.3670)\n",
      "11592 Training Loss: tensor(0.3657)\n",
      "11593 Training Loss: tensor(0.3645)\n",
      "11594 Training Loss: tensor(0.3645)\n",
      "11595 Training Loss: tensor(0.3708)\n",
      "11596 Training Loss: tensor(0.3638)\n",
      "11597 Training Loss: tensor(0.3675)\n",
      "11598 Training Loss: tensor(0.3683)\n",
      "11599 Training Loss: tensor(0.3704)\n",
      "11600 Training Loss: tensor(0.3658)\n",
      "11601 Training Loss: tensor(0.3669)\n",
      "11602 Training Loss: tensor(0.3651)\n",
      "11603 Training Loss: tensor(0.3676)\n",
      "11604 Training Loss: tensor(0.3672)\n",
      "11605 Training Loss: tensor(0.3670)\n",
      "11606 Training Loss: tensor(0.3644)\n",
      "11607 Training Loss: tensor(0.3630)\n",
      "11608 Training Loss: tensor(0.3662)\n",
      "11609 Training Loss: tensor(0.3707)\n",
      "11610 Training Loss: tensor(0.3736)\n",
      "11611 Training Loss: tensor(0.3637)\n",
      "11612 Training Loss: tensor(0.3656)\n",
      "11613 Training Loss: tensor(0.3670)\n",
      "11614 Training Loss: tensor(0.3700)\n",
      "11615 Training Loss: tensor(0.3663)\n",
      "11616 Training Loss: tensor(0.3656)\n",
      "11617 Training Loss: tensor(0.3646)\n",
      "11618 Training Loss: tensor(0.3685)\n",
      "11619 Training Loss: tensor(0.3626)\n",
      "11620 Training Loss: tensor(0.3672)\n",
      "11621 Training Loss: tensor(0.3659)\n",
      "11622 Training Loss: tensor(0.3661)\n",
      "11623 Training Loss: tensor(0.3651)\n",
      "11624 Training Loss: tensor(0.3743)\n",
      "11625 Training Loss: tensor(0.3778)\n",
      "11626 Training Loss: tensor(0.3675)\n",
      "11627 Training Loss: tensor(0.3720)\n",
      "11628 Training Loss: tensor(0.3701)\n",
      "11629 Training Loss: tensor(0.3653)\n",
      "11630 Training Loss: tensor(0.3654)\n",
      "11631 Training Loss: tensor(0.3661)\n",
      "11632 Training Loss: tensor(0.3650)\n",
      "11633 Training Loss: tensor(0.3649)\n",
      "11634 Training Loss: tensor(0.3644)\n",
      "11635 Training Loss: tensor(0.3669)\n",
      "11636 Training Loss: tensor(0.3674)\n",
      "11637 Training Loss: tensor(0.3689)\n",
      "11638 Training Loss: tensor(0.3668)\n",
      "11639 Training Loss: tensor(0.3697)\n",
      "11640 Training Loss: tensor(0.3702)\n",
      "11641 Training Loss: tensor(0.3691)\n",
      "11642 Training Loss: tensor(0.3657)\n",
      "11643 Training Loss: tensor(0.3653)\n",
      "11644 Training Loss: tensor(0.3667)\n",
      "11645 Training Loss: tensor(0.3677)\n",
      "11646 Training Loss: tensor(0.3667)\n",
      "11647 Training Loss: tensor(0.3672)\n",
      "11648 Training Loss: tensor(0.3669)\n",
      "11649 Training Loss: tensor(0.3748)\n",
      "11650 Training Loss: tensor(0.3650)\n",
      "11651 Training Loss: tensor(0.3710)\n",
      "11652 Training Loss: tensor(0.3716)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11653 Training Loss: tensor(0.3669)\n",
      "11654 Training Loss: tensor(0.3681)\n",
      "11655 Training Loss: tensor(0.3644)\n",
      "11656 Training Loss: tensor(0.3659)\n",
      "11657 Training Loss: tensor(0.3635)\n",
      "11658 Training Loss: tensor(0.3650)\n",
      "11659 Training Loss: tensor(0.3660)\n",
      "11660 Training Loss: tensor(0.3680)\n",
      "11661 Training Loss: tensor(0.3703)\n",
      "11662 Training Loss: tensor(0.3694)\n",
      "11663 Training Loss: tensor(0.3650)\n",
      "11664 Training Loss: tensor(0.3647)\n",
      "11665 Training Loss: tensor(0.3684)\n",
      "11666 Training Loss: tensor(0.3669)\n",
      "11667 Training Loss: tensor(0.3652)\n",
      "11668 Training Loss: tensor(0.3691)\n",
      "11669 Training Loss: tensor(0.3686)\n",
      "11670 Training Loss: tensor(0.3632)\n",
      "11671 Training Loss: tensor(0.3657)\n",
      "11672 Training Loss: tensor(0.3658)\n",
      "11673 Training Loss: tensor(0.3658)\n",
      "11674 Training Loss: tensor(0.3650)\n",
      "11675 Training Loss: tensor(0.3656)\n",
      "11676 Training Loss: tensor(0.3644)\n",
      "11677 Training Loss: tensor(0.3633)\n",
      "11678 Training Loss: tensor(0.3675)\n",
      "11679 Training Loss: tensor(0.3740)\n",
      "11680 Training Loss: tensor(0.3692)\n",
      "11681 Training Loss: tensor(0.3642)\n",
      "11682 Training Loss: tensor(0.3626)\n",
      "11683 Training Loss: tensor(0.3655)\n",
      "11684 Training Loss: tensor(0.3652)\n",
      "11685 Training Loss: tensor(0.3670)\n",
      "11686 Training Loss: tensor(0.3647)\n",
      "11687 Training Loss: tensor(0.3666)\n",
      "11688 Training Loss: tensor(0.3679)\n",
      "11689 Training Loss: tensor(0.3649)\n",
      "11690 Training Loss: tensor(0.3717)\n",
      "11691 Training Loss: tensor(0.3647)\n",
      "11692 Training Loss: tensor(0.3643)\n",
      "11693 Training Loss: tensor(0.3635)\n",
      "11694 Training Loss: tensor(0.3628)\n",
      "11695 Training Loss: tensor(0.3670)\n",
      "11696 Training Loss: tensor(0.3649)\n",
      "11697 Training Loss: tensor(0.3653)\n",
      "11698 Training Loss: tensor(0.3694)\n",
      "11699 Training Loss: tensor(0.3681)\n",
      "11700 Training Loss: tensor(0.3650)\n",
      "11701 Training Loss: tensor(0.3619)\n",
      "11702 Training Loss: tensor(0.3641)\n",
      "11703 Training Loss: tensor(0.3690)\n",
      "11704 Training Loss: tensor(0.3655)\n",
      "11705 Training Loss: tensor(0.3641)\n",
      "11706 Training Loss: tensor(0.3638)\n",
      "11707 Training Loss: tensor(0.3617)\n",
      "11708 Training Loss: tensor(0.3637)\n",
      "11709 Training Loss: tensor(0.3655)\n",
      "11710 Training Loss: tensor(0.3708)\n",
      "11711 Training Loss: tensor(0.3689)\n",
      "11712 Training Loss: tensor(0.3644)\n",
      "11713 Training Loss: tensor(0.3664)\n",
      "11714 Training Loss: tensor(0.3675)\n",
      "11715 Training Loss: tensor(0.3654)\n",
      "11716 Training Loss: tensor(0.3663)\n",
      "11717 Training Loss: tensor(0.3679)\n",
      "11718 Training Loss: tensor(0.3638)\n",
      "11719 Training Loss: tensor(0.3651)\n",
      "11720 Training Loss: tensor(0.3647)\n",
      "11721 Training Loss: tensor(0.3727)\n",
      "11722 Training Loss: tensor(0.3633)\n",
      "11723 Training Loss: tensor(0.3672)\n",
      "11724 Training Loss: tensor(0.3656)\n",
      "11725 Training Loss: tensor(0.3653)\n",
      "11726 Training Loss: tensor(0.3624)\n",
      "11727 Training Loss: tensor(0.3681)\n",
      "11728 Training Loss: tensor(0.3650)\n",
      "11729 Training Loss: tensor(0.3685)\n",
      "11730 Training Loss: tensor(0.3682)\n",
      "11731 Training Loss: tensor(0.3712)\n",
      "11732 Training Loss: tensor(0.3661)\n",
      "11733 Training Loss: tensor(0.3644)\n",
      "11734 Training Loss: tensor(0.3669)\n",
      "11735 Training Loss: tensor(0.3684)\n",
      "11736 Training Loss: tensor(0.3639)\n",
      "11737 Training Loss: tensor(0.3688)\n",
      "11738 Training Loss: tensor(0.3717)\n",
      "11739 Training Loss: tensor(0.3650)\n",
      "11740 Training Loss: tensor(0.3686)\n",
      "11741 Training Loss: tensor(0.3694)\n",
      "11742 Training Loss: tensor(0.3684)\n",
      "11743 Training Loss: tensor(0.3685)\n",
      "11744 Training Loss: tensor(0.3638)\n",
      "11745 Training Loss: tensor(0.3655)\n",
      "11746 Training Loss: tensor(0.3712)\n",
      "11747 Training Loss: tensor(0.3660)\n",
      "11748 Training Loss: tensor(0.3674)\n",
      "11749 Training Loss: tensor(0.3650)\n",
      "11750 Training Loss: tensor(0.3680)\n",
      "11751 Training Loss: tensor(0.3675)\n",
      "11752 Training Loss: tensor(0.3670)\n",
      "11753 Training Loss: tensor(0.3636)\n",
      "11754 Training Loss: tensor(0.3699)\n",
      "11755 Training Loss: tensor(0.3663)\n",
      "11756 Training Loss: tensor(0.3639)\n",
      "11757 Training Loss: tensor(0.3633)\n",
      "11758 Training Loss: tensor(0.3648)\n",
      "11759 Training Loss: tensor(0.3662)\n",
      "11760 Training Loss: tensor(0.3628)\n",
      "11761 Training Loss: tensor(0.3658)\n",
      "11762 Training Loss: tensor(0.3694)\n",
      "11763 Training Loss: tensor(0.3638)\n",
      "11764 Training Loss: tensor(0.3681)\n",
      "11765 Training Loss: tensor(0.3628)\n",
      "11766 Training Loss: tensor(0.3698)\n",
      "11767 Training Loss: tensor(0.3655)\n",
      "11768 Training Loss: tensor(0.3683)\n",
      "11769 Training Loss: tensor(0.3640)\n",
      "11770 Training Loss: tensor(0.3616)\n",
      "11771 Training Loss: tensor(0.3684)\n",
      "11772 Training Loss: tensor(0.3615)\n",
      "11773 Training Loss: tensor(0.3690)\n",
      "11774 Training Loss: tensor(0.3642)\n",
      "11775 Training Loss: tensor(0.3628)\n",
      "11776 Training Loss: tensor(0.3664)\n",
      "11777 Training Loss: tensor(0.3681)\n",
      "11778 Training Loss: tensor(0.3636)\n",
      "11779 Training Loss: tensor(0.3652)\n",
      "11780 Training Loss: tensor(0.3635)\n",
      "11781 Training Loss: tensor(0.3624)\n",
      "11782 Training Loss: tensor(0.3636)\n",
      "11783 Training Loss: tensor(0.3654)\n",
      "11784 Training Loss: tensor(0.3636)\n",
      "11785 Training Loss: tensor(0.3655)\n",
      "11786 Training Loss: tensor(0.3652)\n",
      "11787 Training Loss: tensor(0.3612)\n",
      "11788 Training Loss: tensor(0.3727)\n",
      "11789 Training Loss: tensor(0.3658)\n",
      "11790 Training Loss: tensor(0.3647)\n",
      "11791 Training Loss: tensor(0.3693)\n",
      "11792 Training Loss: tensor(0.3701)\n",
      "11793 Training Loss: tensor(0.3710)\n",
      "11794 Training Loss: tensor(0.3725)\n",
      "11795 Training Loss: tensor(0.3650)\n",
      "11796 Training Loss: tensor(0.3662)\n",
      "11797 Training Loss: tensor(0.3665)\n",
      "11798 Training Loss: tensor(0.3648)\n",
      "11799 Training Loss: tensor(0.3666)\n",
      "11800 Training Loss: tensor(0.3631)\n",
      "11801 Training Loss: tensor(0.3646)\n",
      "11802 Training Loss: tensor(0.3635)\n",
      "11803 Training Loss: tensor(0.3660)\n",
      "11804 Training Loss: tensor(0.3685)\n",
      "11805 Training Loss: tensor(0.3683)\n",
      "11806 Training Loss: tensor(0.3667)\n",
      "11807 Training Loss: tensor(0.3725)\n",
      "11808 Training Loss: tensor(0.3658)\n",
      "11809 Training Loss: tensor(0.3702)\n",
      "11810 Training Loss: tensor(0.3676)\n",
      "11811 Training Loss: tensor(0.3657)\n",
      "11812 Training Loss: tensor(0.3648)\n",
      "11813 Training Loss: tensor(0.3680)\n",
      "11814 Training Loss: tensor(0.3668)\n",
      "11815 Training Loss: tensor(0.3658)\n",
      "11816 Training Loss: tensor(0.3662)\n",
      "11817 Training Loss: tensor(0.3685)\n",
      "11818 Training Loss: tensor(0.3676)\n",
      "11819 Training Loss: tensor(0.3668)\n",
      "11820 Training Loss: tensor(0.3675)\n",
      "11821 Training Loss: tensor(0.3642)\n",
      "11822 Training Loss: tensor(0.3647)\n",
      "11823 Training Loss: tensor(0.3709)\n",
      "11824 Training Loss: tensor(0.3640)\n",
      "11825 Training Loss: tensor(0.3683)\n",
      "11826 Training Loss: tensor(0.3649)\n",
      "11827 Training Loss: tensor(0.3671)\n",
      "11828 Training Loss: tensor(0.3691)\n",
      "11829 Training Loss: tensor(0.3630)\n",
      "11830 Training Loss: tensor(0.3645)\n",
      "11831 Training Loss: tensor(0.3690)\n",
      "11832 Training Loss: tensor(0.3744)\n",
      "11833 Training Loss: tensor(0.3690)\n",
      "11834 Training Loss: tensor(0.3681)\n",
      "11835 Training Loss: tensor(0.3666)\n",
      "11836 Training Loss: tensor(0.3693)\n",
      "11837 Training Loss: tensor(0.3667)\n",
      "11838 Training Loss: tensor(0.3699)\n",
      "11839 Training Loss: tensor(0.3676)\n",
      "11840 Training Loss: tensor(0.3672)\n",
      "11841 Training Loss: tensor(0.3690)\n",
      "11842 Training Loss: tensor(0.3638)\n",
      "11843 Training Loss: tensor(0.3691)\n",
      "11844 Training Loss: tensor(0.3675)\n",
      "11845 Training Loss: tensor(0.3665)\n",
      "11846 Training Loss: tensor(0.3697)\n",
      "11847 Training Loss: tensor(0.3652)\n",
      "11848 Training Loss: tensor(0.3657)\n",
      "11849 Training Loss: tensor(0.3673)\n",
      "11850 Training Loss: tensor(0.3646)\n",
      "11851 Training Loss: tensor(0.3631)\n",
      "11852 Training Loss: tensor(0.3633)\n",
      "11853 Training Loss: tensor(0.3672)\n",
      "11854 Training Loss: tensor(0.3705)\n",
      "11855 Training Loss: tensor(0.3630)\n",
      "11856 Training Loss: tensor(0.3631)\n",
      "11857 Training Loss: tensor(0.3659)\n",
      "11858 Training Loss: tensor(0.3613)\n",
      "11859 Training Loss: tensor(0.3612)\n",
      "11860 Training Loss: tensor(0.3618)\n",
      "11861 Training Loss: tensor(0.3785)\n",
      "11862 Training Loss: tensor(0.3678)\n",
      "11863 Training Loss: tensor(0.3713)\n",
      "11864 Training Loss: tensor(0.3643)\n",
      "11865 Training Loss: tensor(0.3667)\n",
      "11866 Training Loss: tensor(0.3678)\n",
      "11867 Training Loss: tensor(0.3709)\n",
      "11868 Training Loss: tensor(0.3734)\n",
      "11869 Training Loss: tensor(0.3674)\n",
      "11870 Training Loss: tensor(0.3652)\n",
      "11871 Training Loss: tensor(0.3640)\n",
      "11872 Training Loss: tensor(0.3648)\n",
      "11873 Training Loss: tensor(0.3629)\n",
      "11874 Training Loss: tensor(0.3634)\n",
      "11875 Training Loss: tensor(0.3647)\n",
      "11876 Training Loss: tensor(0.3698)\n",
      "11877 Training Loss: tensor(0.3644)\n",
      "11878 Training Loss: tensor(0.3660)\n",
      "11879 Training Loss: tensor(0.3673)\n",
      "11880 Training Loss: tensor(0.3670)\n",
      "11881 Training Loss: tensor(0.3644)\n",
      "11882 Training Loss: tensor(0.3685)\n",
      "11883 Training Loss: tensor(0.3643)\n",
      "11884 Training Loss: tensor(0.3639)\n",
      "11885 Training Loss: tensor(0.3655)\n",
      "11886 Training Loss: tensor(0.3671)\n",
      "11887 Training Loss: tensor(0.3683)\n",
      "11888 Training Loss: tensor(0.3640)\n",
      "11889 Training Loss: tensor(0.3690)\n",
      "11890 Training Loss: tensor(0.3656)\n",
      "11891 Training Loss: tensor(0.3680)\n",
      "11892 Training Loss: tensor(0.3629)\n",
      "11893 Training Loss: tensor(0.3649)\n",
      "11894 Training Loss: tensor(0.3622)\n",
      "11895 Training Loss: tensor(0.3682)\n",
      "11896 Training Loss: tensor(0.3697)\n",
      "11897 Training Loss: tensor(0.3639)\n",
      "11898 Training Loss: tensor(0.3626)\n",
      "11899 Training Loss: tensor(0.3705)\n",
      "11900 Training Loss: tensor(0.3650)\n",
      "11901 Training Loss: tensor(0.3636)\n",
      "11902 Training Loss: tensor(0.3634)\n",
      "11903 Training Loss: tensor(0.3709)\n",
      "11904 Training Loss: tensor(0.3634)\n",
      "11905 Training Loss: tensor(0.3697)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11906 Training Loss: tensor(0.3722)\n",
      "11907 Training Loss: tensor(0.3704)\n",
      "11908 Training Loss: tensor(0.3643)\n",
      "11909 Training Loss: tensor(0.3652)\n",
      "11910 Training Loss: tensor(0.3668)\n",
      "11911 Training Loss: tensor(0.3623)\n",
      "11912 Training Loss: tensor(0.3647)\n",
      "11913 Training Loss: tensor(0.3626)\n",
      "11914 Training Loss: tensor(0.3714)\n",
      "11915 Training Loss: tensor(0.3669)\n",
      "11916 Training Loss: tensor(0.3638)\n",
      "11917 Training Loss: tensor(0.3668)\n",
      "11918 Training Loss: tensor(0.3653)\n",
      "11919 Training Loss: tensor(0.3672)\n",
      "11920 Training Loss: tensor(0.3672)\n",
      "11921 Training Loss: tensor(0.3639)\n",
      "11922 Training Loss: tensor(0.3710)\n",
      "11923 Training Loss: tensor(0.3659)\n",
      "11924 Training Loss: tensor(0.3629)\n",
      "11925 Training Loss: tensor(0.3659)\n",
      "11926 Training Loss: tensor(0.3642)\n",
      "11927 Training Loss: tensor(0.3654)\n",
      "11928 Training Loss: tensor(0.3658)\n",
      "11929 Training Loss: tensor(0.3659)\n",
      "11930 Training Loss: tensor(0.3637)\n",
      "11931 Training Loss: tensor(0.3634)\n",
      "11932 Training Loss: tensor(0.3648)\n",
      "11933 Training Loss: tensor(0.3660)\n",
      "11934 Training Loss: tensor(0.3672)\n",
      "11935 Training Loss: tensor(0.3625)\n",
      "11936 Training Loss: tensor(0.3687)\n",
      "11937 Training Loss: tensor(0.3711)\n",
      "11938 Training Loss: tensor(0.3643)\n",
      "11939 Training Loss: tensor(0.3617)\n",
      "11940 Training Loss: tensor(0.3677)\n",
      "11941 Training Loss: tensor(0.3749)\n",
      "11942 Training Loss: tensor(0.3702)\n",
      "11943 Training Loss: tensor(0.3665)\n",
      "11944 Training Loss: tensor(0.3701)\n",
      "11945 Training Loss: tensor(0.3651)\n",
      "11946 Training Loss: tensor(0.3667)\n",
      "11947 Training Loss: tensor(0.3658)\n",
      "11948 Training Loss: tensor(0.3659)\n",
      "11949 Training Loss: tensor(0.3667)\n",
      "11950 Training Loss: tensor(0.3656)\n",
      "11951 Training Loss: tensor(0.3671)\n",
      "11952 Training Loss: tensor(0.3657)\n",
      "11953 Training Loss: tensor(0.3673)\n",
      "11954 Training Loss: tensor(0.3635)\n",
      "11955 Training Loss: tensor(0.3655)\n",
      "11956 Training Loss: tensor(0.3630)\n",
      "11957 Training Loss: tensor(0.3644)\n",
      "11958 Training Loss: tensor(0.3645)\n",
      "11959 Training Loss: tensor(0.3638)\n",
      "11960 Training Loss: tensor(0.3665)\n",
      "11961 Training Loss: tensor(0.3653)\n",
      "11962 Training Loss: tensor(0.3687)\n",
      "11963 Training Loss: tensor(0.3714)\n",
      "11964 Training Loss: tensor(0.3758)\n",
      "11965 Training Loss: tensor(0.3679)\n",
      "11966 Training Loss: tensor(0.3752)\n",
      "11967 Training Loss: tensor(0.3691)\n",
      "11968 Training Loss: tensor(0.3675)\n",
      "11969 Training Loss: tensor(0.3682)\n",
      "11970 Training Loss: tensor(0.3668)\n",
      "11971 Training Loss: tensor(0.3658)\n",
      "11972 Training Loss: tensor(0.3661)\n",
      "11973 Training Loss: tensor(0.3669)\n",
      "11974 Training Loss: tensor(0.3665)\n",
      "11975 Training Loss: tensor(0.3693)\n",
      "11976 Training Loss: tensor(0.3670)\n",
      "11977 Training Loss: tensor(0.3695)\n",
      "11978 Training Loss: tensor(0.3662)\n",
      "11979 Training Loss: tensor(0.3672)\n",
      "11980 Training Loss: tensor(0.3636)\n",
      "11981 Training Loss: tensor(0.3663)\n",
      "11982 Training Loss: tensor(0.3666)\n",
      "11983 Training Loss: tensor(0.3658)\n",
      "11984 Training Loss: tensor(0.3681)\n",
      "11985 Training Loss: tensor(0.3663)\n",
      "11986 Training Loss: tensor(0.3642)\n",
      "11987 Training Loss: tensor(0.3668)\n",
      "11988 Training Loss: tensor(0.3653)\n",
      "11989 Training Loss: tensor(0.3641)\n",
      "11990 Training Loss: tensor(0.3658)\n",
      "11991 Training Loss: tensor(0.3649)\n",
      "11992 Training Loss: tensor(0.3653)\n",
      "11993 Training Loss: tensor(0.3660)\n",
      "11994 Training Loss: tensor(0.3622)\n",
      "11995 Training Loss: tensor(0.3666)\n",
      "11996 Training Loss: tensor(0.3684)\n",
      "11997 Training Loss: tensor(0.3633)\n",
      "11998 Training Loss: tensor(0.3666)\n",
      "11999 Training Loss: tensor(0.3685)\n",
      "12000 Training Loss: tensor(0.3640)\n",
      "12001 Training Loss: tensor(0.3684)\n",
      "12002 Training Loss: tensor(0.3655)\n",
      "12003 Training Loss: tensor(0.3719)\n",
      "12004 Training Loss: tensor(0.3639)\n",
      "12005 Training Loss: tensor(0.3668)\n",
      "12006 Training Loss: tensor(0.3640)\n",
      "12007 Training Loss: tensor(0.3653)\n",
      "12008 Training Loss: tensor(0.3666)\n",
      "12009 Training Loss: tensor(0.3639)\n",
      "12010 Training Loss: tensor(0.3693)\n",
      "12011 Training Loss: tensor(0.3693)\n",
      "12012 Training Loss: tensor(0.3714)\n",
      "12013 Training Loss: tensor(0.3653)\n",
      "12014 Training Loss: tensor(0.3669)\n",
      "12015 Training Loss: tensor(0.3658)\n",
      "12016 Training Loss: tensor(0.3640)\n",
      "12017 Training Loss: tensor(0.3666)\n",
      "12018 Training Loss: tensor(0.3681)\n",
      "12019 Training Loss: tensor(0.3709)\n",
      "12020 Training Loss: tensor(0.3650)\n",
      "12021 Training Loss: tensor(0.3641)\n",
      "12022 Training Loss: tensor(0.3660)\n",
      "12023 Training Loss: tensor(0.3652)\n",
      "12024 Training Loss: tensor(0.3664)\n",
      "12025 Training Loss: tensor(0.3655)\n",
      "12026 Training Loss: tensor(0.3625)\n",
      "12027 Training Loss: tensor(0.3724)\n",
      "12028 Training Loss: tensor(0.3651)\n",
      "12029 Training Loss: tensor(0.3661)\n",
      "12030 Training Loss: tensor(0.3628)\n",
      "12031 Training Loss: tensor(0.3657)\n",
      "12032 Training Loss: tensor(0.3653)\n",
      "12033 Training Loss: tensor(0.3645)\n",
      "12034 Training Loss: tensor(0.3678)\n",
      "12035 Training Loss: tensor(0.3664)\n",
      "12036 Training Loss: tensor(0.3654)\n",
      "12037 Training Loss: tensor(0.3655)\n",
      "12038 Training Loss: tensor(0.3700)\n",
      "12039 Training Loss: tensor(0.3665)\n",
      "12040 Training Loss: tensor(0.3651)\n",
      "12041 Training Loss: tensor(0.3636)\n",
      "12042 Training Loss: tensor(0.3626)\n",
      "12043 Training Loss: tensor(0.3634)\n",
      "12044 Training Loss: tensor(0.3639)\n",
      "12045 Training Loss: tensor(0.3624)\n",
      "12046 Training Loss: tensor(0.3630)\n",
      "12047 Training Loss: tensor(0.3713)\n",
      "12048 Training Loss: tensor(0.3630)\n",
      "12049 Training Loss: tensor(0.3695)\n",
      "12050 Training Loss: tensor(0.3633)\n",
      "12051 Training Loss: tensor(0.3629)\n",
      "12052 Training Loss: tensor(0.3645)\n",
      "12053 Training Loss: tensor(0.3633)\n",
      "12054 Training Loss: tensor(0.3662)\n",
      "12055 Training Loss: tensor(0.3677)\n",
      "12056 Training Loss: tensor(0.3628)\n",
      "12057 Training Loss: tensor(0.3677)\n",
      "12058 Training Loss: tensor(0.3662)\n",
      "12059 Training Loss: tensor(0.3631)\n",
      "12060 Training Loss: tensor(0.3627)\n",
      "12061 Training Loss: tensor(0.3651)\n",
      "12062 Training Loss: tensor(0.3655)\n",
      "12063 Training Loss: tensor(0.3648)\n",
      "12064 Training Loss: tensor(0.3631)\n",
      "12065 Training Loss: tensor(0.3648)\n",
      "12066 Training Loss: tensor(0.3638)\n",
      "12067 Training Loss: tensor(0.3654)\n",
      "12068 Training Loss: tensor(0.3646)\n",
      "12069 Training Loss: tensor(0.3651)\n",
      "12070 Training Loss: tensor(0.3643)\n",
      "12071 Training Loss: tensor(0.3676)\n",
      "12072 Training Loss: tensor(0.3664)\n",
      "12073 Training Loss: tensor(0.3658)\n",
      "12074 Training Loss: tensor(0.3704)\n",
      "12075 Training Loss: tensor(0.3685)\n",
      "12076 Training Loss: tensor(0.3682)\n",
      "12077 Training Loss: tensor(0.3695)\n",
      "12078 Training Loss: tensor(0.3682)\n",
      "12079 Training Loss: tensor(0.3626)\n",
      "12080 Training Loss: tensor(0.3671)\n",
      "12081 Training Loss: tensor(0.3697)\n",
      "12082 Training Loss: tensor(0.3658)\n",
      "12083 Training Loss: tensor(0.3641)\n",
      "12084 Training Loss: tensor(0.3663)\n",
      "12085 Training Loss: tensor(0.3694)\n",
      "12086 Training Loss: tensor(0.3650)\n",
      "12087 Training Loss: tensor(0.3647)\n",
      "12088 Training Loss: tensor(0.3652)\n",
      "12089 Training Loss: tensor(0.3643)\n",
      "12090 Training Loss: tensor(0.3662)\n",
      "12091 Training Loss: tensor(0.3677)\n",
      "12092 Training Loss: tensor(0.3650)\n",
      "12093 Training Loss: tensor(0.3647)\n",
      "12094 Training Loss: tensor(0.3634)\n",
      "12095 Training Loss: tensor(0.3690)\n",
      "12096 Training Loss: tensor(0.3662)\n",
      "12097 Training Loss: tensor(0.3698)\n",
      "12098 Training Loss: tensor(0.3650)\n",
      "12099 Training Loss: tensor(0.3689)\n",
      "12100 Training Loss: tensor(0.3650)\n",
      "12101 Training Loss: tensor(0.3650)\n",
      "12102 Training Loss: tensor(0.3631)\n",
      "12103 Training Loss: tensor(0.3653)\n",
      "12104 Training Loss: tensor(0.3668)\n",
      "12105 Training Loss: tensor(0.3608)\n",
      "12106 Training Loss: tensor(0.3662)\n",
      "12107 Training Loss: tensor(0.3634)\n",
      "12108 Training Loss: tensor(0.3663)\n",
      "12109 Training Loss: tensor(0.3611)\n",
      "12110 Training Loss: tensor(0.3646)\n",
      "12111 Training Loss: tensor(0.3666)\n",
      "12112 Training Loss: tensor(0.3683)\n",
      "12113 Training Loss: tensor(0.3707)\n",
      "12114 Training Loss: tensor(0.3693)\n",
      "12115 Training Loss: tensor(0.3640)\n",
      "12116 Training Loss: tensor(0.3666)\n",
      "12117 Training Loss: tensor(0.3661)\n",
      "12118 Training Loss: tensor(0.3651)\n",
      "12119 Training Loss: tensor(0.3647)\n",
      "12120 Training Loss: tensor(0.3680)\n",
      "12121 Training Loss: tensor(0.3664)\n",
      "12122 Training Loss: tensor(0.3645)\n",
      "12123 Training Loss: tensor(0.3697)\n",
      "12124 Training Loss: tensor(0.3641)\n",
      "12125 Training Loss: tensor(0.3646)\n",
      "12126 Training Loss: tensor(0.3665)\n",
      "12127 Training Loss: tensor(0.3692)\n",
      "12128 Training Loss: tensor(0.3642)\n",
      "12129 Training Loss: tensor(0.3637)\n",
      "12130 Training Loss: tensor(0.3664)\n",
      "12131 Training Loss: tensor(0.3657)\n",
      "12132 Training Loss: tensor(0.3667)\n",
      "12133 Training Loss: tensor(0.3671)\n",
      "12134 Training Loss: tensor(0.3635)\n",
      "12135 Training Loss: tensor(0.3699)\n",
      "12136 Training Loss: tensor(0.3653)\n",
      "12137 Training Loss: tensor(0.3670)\n",
      "12138 Training Loss: tensor(0.3657)\n",
      "12139 Training Loss: tensor(0.3707)\n",
      "12140 Training Loss: tensor(0.3646)\n",
      "12141 Training Loss: tensor(0.3668)\n",
      "12142 Training Loss: tensor(0.3642)\n",
      "12143 Training Loss: tensor(0.3652)\n",
      "12144 Training Loss: tensor(0.3662)\n",
      "12145 Training Loss: tensor(0.3648)\n",
      "12146 Training Loss: tensor(0.3682)\n",
      "12147 Training Loss: tensor(0.3637)\n",
      "12148 Training Loss: tensor(0.3664)\n",
      "12149 Training Loss: tensor(0.3649)\n",
      "12150 Training Loss: tensor(0.3644)\n",
      "12151 Training Loss: tensor(0.3648)\n",
      "12152 Training Loss: tensor(0.3676)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12153 Training Loss: tensor(0.3653)\n",
      "12154 Training Loss: tensor(0.3653)\n",
      "12155 Training Loss: tensor(0.3636)\n",
      "12156 Training Loss: tensor(0.3630)\n",
      "12157 Training Loss: tensor(0.3673)\n",
      "12158 Training Loss: tensor(0.3695)\n",
      "12159 Training Loss: tensor(0.3633)\n",
      "12160 Training Loss: tensor(0.3669)\n",
      "12161 Training Loss: tensor(0.3656)\n",
      "12162 Training Loss: tensor(0.3759)\n",
      "12163 Training Loss: tensor(0.3633)\n",
      "12164 Training Loss: tensor(0.3660)\n",
      "12165 Training Loss: tensor(0.3625)\n",
      "12166 Training Loss: tensor(0.3644)\n",
      "12167 Training Loss: tensor(0.3658)\n",
      "12168 Training Loss: tensor(0.3640)\n",
      "12169 Training Loss: tensor(0.3661)\n",
      "12170 Training Loss: tensor(0.3620)\n",
      "12171 Training Loss: tensor(0.3699)\n",
      "12172 Training Loss: tensor(0.3657)\n",
      "12173 Training Loss: tensor(0.3615)\n",
      "12174 Training Loss: tensor(0.3635)\n",
      "12175 Training Loss: tensor(0.3718)\n",
      "12176 Training Loss: tensor(0.3622)\n",
      "12177 Training Loss: tensor(0.3648)\n",
      "12178 Training Loss: tensor(0.3619)\n",
      "12179 Training Loss: tensor(0.3645)\n",
      "12180 Training Loss: tensor(0.3639)\n",
      "12181 Training Loss: tensor(0.3616)\n",
      "12182 Training Loss: tensor(0.3713)\n",
      "12183 Training Loss: tensor(0.3640)\n",
      "12184 Training Loss: tensor(0.3658)\n",
      "12185 Training Loss: tensor(0.3653)\n",
      "12186 Training Loss: tensor(0.3647)\n",
      "12187 Training Loss: tensor(0.3670)\n",
      "12188 Training Loss: tensor(0.3653)\n",
      "12189 Training Loss: tensor(0.3633)\n",
      "12190 Training Loss: tensor(0.3654)\n",
      "12191 Training Loss: tensor(0.3619)\n",
      "12192 Training Loss: tensor(0.3644)\n",
      "12193 Training Loss: tensor(0.3631)\n",
      "12194 Training Loss: tensor(0.3653)\n",
      "12195 Training Loss: tensor(0.3654)\n",
      "12196 Training Loss: tensor(0.3666)\n",
      "12197 Training Loss: tensor(0.3604)\n",
      "12198 Training Loss: tensor(0.3650)\n",
      "12199 Training Loss: tensor(0.3651)\n",
      "12200 Training Loss: tensor(0.3618)\n",
      "12201 Training Loss: tensor(0.3617)\n",
      "12202 Training Loss: tensor(0.3634)\n",
      "12203 Training Loss: tensor(0.3655)\n",
      "12204 Training Loss: tensor(0.3643)\n",
      "12205 Training Loss: tensor(0.3604)\n",
      "12206 Training Loss: tensor(0.3615)\n",
      "12207 Training Loss: tensor(0.3618)\n",
      "12208 Training Loss: tensor(0.3693)\n",
      "12209 Training Loss: tensor(0.3664)\n",
      "12210 Training Loss: tensor(0.3678)\n",
      "12211 Training Loss: tensor(0.3670)\n",
      "12212 Training Loss: tensor(0.3638)\n",
      "12213 Training Loss: tensor(0.3649)\n",
      "12214 Training Loss: tensor(0.3696)\n",
      "12215 Training Loss: tensor(0.3635)\n",
      "12216 Training Loss: tensor(0.3654)\n",
      "12217 Training Loss: tensor(0.3615)\n",
      "12218 Training Loss: tensor(0.3619)\n",
      "12219 Training Loss: tensor(0.3663)\n",
      "12220 Training Loss: tensor(0.3646)\n",
      "12221 Training Loss: tensor(0.3633)\n",
      "12222 Training Loss: tensor(0.3648)\n",
      "12223 Training Loss: tensor(0.3638)\n",
      "12224 Training Loss: tensor(0.3617)\n",
      "12225 Training Loss: tensor(0.3648)\n",
      "12226 Training Loss: tensor(0.3646)\n",
      "12227 Training Loss: tensor(0.3644)\n",
      "12228 Training Loss: tensor(0.3675)\n",
      "12229 Training Loss: tensor(0.3675)\n",
      "12230 Training Loss: tensor(0.3632)\n",
      "12231 Training Loss: tensor(0.3670)\n",
      "12232 Training Loss: tensor(0.3644)\n",
      "12233 Training Loss: tensor(0.3698)\n",
      "12234 Training Loss: tensor(0.3669)\n",
      "12235 Training Loss: tensor(0.3669)\n",
      "12236 Training Loss: tensor(0.3618)\n",
      "12237 Training Loss: tensor(0.3642)\n",
      "12238 Training Loss: tensor(0.3631)\n",
      "12239 Training Loss: tensor(0.3623)\n",
      "12240 Training Loss: tensor(0.3624)\n",
      "12241 Training Loss: tensor(0.3627)\n",
      "12242 Training Loss: tensor(0.3686)\n",
      "12243 Training Loss: tensor(0.3628)\n",
      "12244 Training Loss: tensor(0.3708)\n",
      "12245 Training Loss: tensor(0.3624)\n",
      "12246 Training Loss: tensor(0.3624)\n",
      "12247 Training Loss: tensor(0.3616)\n",
      "12248 Training Loss: tensor(0.3639)\n",
      "12249 Training Loss: tensor(0.3653)\n",
      "12250 Training Loss: tensor(0.3650)\n",
      "12251 Training Loss: tensor(0.3692)\n",
      "12252 Training Loss: tensor(0.3679)\n",
      "12253 Training Loss: tensor(0.3672)\n",
      "12254 Training Loss: tensor(0.3613)\n",
      "12255 Training Loss: tensor(0.3656)\n",
      "12256 Training Loss: tensor(0.3618)\n",
      "12257 Training Loss: tensor(0.3664)\n",
      "12258 Training Loss: tensor(0.3686)\n",
      "12259 Training Loss: tensor(0.3729)\n",
      "12260 Training Loss: tensor(0.3659)\n",
      "12261 Training Loss: tensor(0.3648)\n",
      "12262 Training Loss: tensor(0.3704)\n",
      "12263 Training Loss: tensor(0.3658)\n",
      "12264 Training Loss: tensor(0.3634)\n",
      "12265 Training Loss: tensor(0.3665)\n",
      "12266 Training Loss: tensor(0.3647)\n",
      "12267 Training Loss: tensor(0.3685)\n",
      "12268 Training Loss: tensor(0.3680)\n",
      "12269 Training Loss: tensor(0.3652)\n",
      "12270 Training Loss: tensor(0.3650)\n",
      "12271 Training Loss: tensor(0.3646)\n",
      "12272 Training Loss: tensor(0.3642)\n",
      "12273 Training Loss: tensor(0.3659)\n",
      "12274 Training Loss: tensor(0.3644)\n",
      "12275 Training Loss: tensor(0.3647)\n",
      "12276 Training Loss: tensor(0.3646)\n",
      "12277 Training Loss: tensor(0.3635)\n",
      "12278 Training Loss: tensor(0.3694)\n",
      "12279 Training Loss: tensor(0.3645)\n",
      "12280 Training Loss: tensor(0.3668)\n",
      "12281 Training Loss: tensor(0.3681)\n",
      "12282 Training Loss: tensor(0.3641)\n",
      "12283 Training Loss: tensor(0.3636)\n",
      "12284 Training Loss: tensor(0.3690)\n",
      "12285 Training Loss: tensor(0.3679)\n",
      "12286 Training Loss: tensor(0.3657)\n",
      "12287 Training Loss: tensor(0.3628)\n",
      "12288 Training Loss: tensor(0.3651)\n",
      "12289 Training Loss: tensor(0.3638)\n",
      "12290 Training Loss: tensor(0.3654)\n",
      "12291 Training Loss: tensor(0.3650)\n",
      "12292 Training Loss: tensor(0.3670)\n",
      "12293 Training Loss: tensor(0.3627)\n",
      "12294 Training Loss: tensor(0.3628)\n",
      "12295 Training Loss: tensor(0.3643)\n",
      "12296 Training Loss: tensor(0.3639)\n",
      "12297 Training Loss: tensor(0.3612)\n",
      "12298 Training Loss: tensor(0.3665)\n",
      "12299 Training Loss: tensor(0.3718)\n",
      "12300 Training Loss: tensor(0.3617)\n",
      "12301 Training Loss: tensor(0.3665)\n",
      "12302 Training Loss: tensor(0.3628)\n",
      "12303 Training Loss: tensor(0.3656)\n",
      "12304 Training Loss: tensor(0.3672)\n",
      "12305 Training Loss: tensor(0.3622)\n",
      "12306 Training Loss: tensor(0.3647)\n",
      "12307 Training Loss: tensor(0.3651)\n",
      "12308 Training Loss: tensor(0.3657)\n",
      "12309 Training Loss: tensor(0.3633)\n",
      "12310 Training Loss: tensor(0.3699)\n",
      "12311 Training Loss: tensor(0.3676)\n",
      "12312 Training Loss: tensor(0.3663)\n",
      "12313 Training Loss: tensor(0.3615)\n",
      "12314 Training Loss: tensor(0.3669)\n",
      "12315 Training Loss: tensor(0.3631)\n",
      "12316 Training Loss: tensor(0.3648)\n",
      "12317 Training Loss: tensor(0.3678)\n",
      "12318 Training Loss: tensor(0.3659)\n",
      "12319 Training Loss: tensor(0.3655)\n",
      "12320 Training Loss: tensor(0.3620)\n",
      "12321 Training Loss: tensor(0.3672)\n",
      "12322 Training Loss: tensor(0.3643)\n",
      "12323 Training Loss: tensor(0.3658)\n",
      "12324 Training Loss: tensor(0.3665)\n",
      "12325 Training Loss: tensor(0.3621)\n",
      "12326 Training Loss: tensor(0.3649)\n",
      "12327 Training Loss: tensor(0.3632)\n",
      "12328 Training Loss: tensor(0.3635)\n",
      "12329 Training Loss: tensor(0.3642)\n",
      "12330 Training Loss: tensor(0.3649)\n",
      "12331 Training Loss: tensor(0.3680)\n",
      "12332 Training Loss: tensor(0.3647)\n",
      "12333 Training Loss: tensor(0.3628)\n",
      "12334 Training Loss: tensor(0.3683)\n",
      "12335 Training Loss: tensor(0.3671)\n",
      "12336 Training Loss: tensor(0.3665)\n",
      "12337 Training Loss: tensor(0.3642)\n",
      "12338 Training Loss: tensor(0.3648)\n",
      "12339 Training Loss: tensor(0.3640)\n",
      "12340 Training Loss: tensor(0.3628)\n",
      "12341 Training Loss: tensor(0.3634)\n",
      "12342 Training Loss: tensor(0.3633)\n",
      "12343 Training Loss: tensor(0.3666)\n",
      "12344 Training Loss: tensor(0.3624)\n",
      "12345 Training Loss: tensor(0.3652)\n",
      "12346 Training Loss: tensor(0.3652)\n",
      "12347 Training Loss: tensor(0.3674)\n",
      "12348 Training Loss: tensor(0.3670)\n",
      "12349 Training Loss: tensor(0.3669)\n",
      "12350 Training Loss: tensor(0.3639)\n",
      "12351 Training Loss: tensor(0.3674)\n",
      "12352 Training Loss: tensor(0.3629)\n",
      "12353 Training Loss: tensor(0.3675)\n",
      "12354 Training Loss: tensor(0.3640)\n",
      "12355 Training Loss: tensor(0.3638)\n",
      "12356 Training Loss: tensor(0.3637)\n",
      "12357 Training Loss: tensor(0.3655)\n",
      "12358 Training Loss: tensor(0.3640)\n",
      "12359 Training Loss: tensor(0.3683)\n",
      "12360 Training Loss: tensor(0.3662)\n",
      "12361 Training Loss: tensor(0.3647)\n",
      "12362 Training Loss: tensor(0.3618)\n",
      "12363 Training Loss: tensor(0.3624)\n",
      "12364 Training Loss: tensor(0.3639)\n",
      "12365 Training Loss: tensor(0.3635)\n",
      "12366 Training Loss: tensor(0.3672)\n",
      "12367 Training Loss: tensor(0.3658)\n",
      "12368 Training Loss: tensor(0.3697)\n",
      "12369 Training Loss: tensor(0.3614)\n",
      "12370 Training Loss: tensor(0.3677)\n",
      "12371 Training Loss: tensor(0.3630)\n",
      "12372 Training Loss: tensor(0.3634)\n",
      "12373 Training Loss: tensor(0.3621)\n",
      "12374 Training Loss: tensor(0.3616)\n",
      "12375 Training Loss: tensor(0.3626)\n",
      "12376 Training Loss: tensor(0.3639)\n",
      "12377 Training Loss: tensor(0.3637)\n",
      "12378 Training Loss: tensor(0.3670)\n",
      "12379 Training Loss: tensor(0.3662)\n",
      "12380 Training Loss: tensor(0.3639)\n",
      "12381 Training Loss: tensor(0.3636)\n",
      "12382 Training Loss: tensor(0.3664)\n",
      "12383 Training Loss: tensor(0.3659)\n",
      "12384 Training Loss: tensor(0.3643)\n",
      "12385 Training Loss: tensor(0.3623)\n",
      "12386 Training Loss: tensor(0.3688)\n",
      "12387 Training Loss: tensor(0.3641)\n",
      "12388 Training Loss: tensor(0.3609)\n",
      "12389 Training Loss: tensor(0.3653)\n",
      "12390 Training Loss: tensor(0.3673)\n",
      "12391 Training Loss: tensor(0.3670)\n",
      "12392 Training Loss: tensor(0.3639)\n",
      "12393 Training Loss: tensor(0.3686)\n",
      "12394 Training Loss: tensor(0.3612)\n",
      "12395 Training Loss: tensor(0.3663)\n",
      "12396 Training Loss: tensor(0.3660)\n",
      "12397 Training Loss: tensor(0.3679)\n",
      "12398 Training Loss: tensor(0.3690)\n",
      "12399 Training Loss: tensor(0.3640)\n",
      "12400 Training Loss: tensor(0.3646)\n",
      "12401 Training Loss: tensor(0.3631)\n",
      "12402 Training Loss: tensor(0.3617)\n",
      "12403 Training Loss: tensor(0.3650)\n",
      "12404 Training Loss: tensor(0.3665)\n",
      "12405 Training Loss: tensor(0.3622)\n",
      "12406 Training Loss: tensor(0.3681)\n",
      "12407 Training Loss: tensor(0.3646)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12408 Training Loss: tensor(0.3634)\n",
      "12409 Training Loss: tensor(0.3681)\n",
      "12410 Training Loss: tensor(0.3667)\n",
      "12411 Training Loss: tensor(0.3635)\n",
      "12412 Training Loss: tensor(0.3623)\n",
      "12413 Training Loss: tensor(0.3704)\n",
      "12414 Training Loss: tensor(0.3667)\n",
      "12415 Training Loss: tensor(0.3677)\n",
      "12416 Training Loss: tensor(0.3620)\n",
      "12417 Training Loss: tensor(0.3626)\n",
      "12418 Training Loss: tensor(0.3646)\n",
      "12419 Training Loss: tensor(0.3698)\n",
      "12420 Training Loss: tensor(0.3666)\n",
      "12421 Training Loss: tensor(0.3629)\n",
      "12422 Training Loss: tensor(0.3631)\n",
      "12423 Training Loss: tensor(0.3650)\n",
      "12424 Training Loss: tensor(0.3647)\n",
      "12425 Training Loss: tensor(0.3667)\n",
      "12426 Training Loss: tensor(0.3625)\n",
      "12427 Training Loss: tensor(0.3646)\n",
      "12428 Training Loss: tensor(0.3660)\n",
      "12429 Training Loss: tensor(0.3629)\n",
      "12430 Training Loss: tensor(0.3625)\n",
      "12431 Training Loss: tensor(0.3627)\n",
      "12432 Training Loss: tensor(0.3706)\n",
      "12433 Training Loss: tensor(0.3627)\n",
      "12434 Training Loss: tensor(0.3623)\n",
      "12435 Training Loss: tensor(0.3658)\n",
      "12436 Training Loss: tensor(0.3657)\n",
      "12437 Training Loss: tensor(0.3640)\n",
      "12438 Training Loss: tensor(0.3635)\n",
      "12439 Training Loss: tensor(0.3636)\n",
      "12440 Training Loss: tensor(0.3642)\n",
      "12441 Training Loss: tensor(0.3647)\n",
      "12442 Training Loss: tensor(0.3646)\n",
      "12443 Training Loss: tensor(0.3622)\n",
      "12444 Training Loss: tensor(0.3674)\n",
      "12445 Training Loss: tensor(0.3638)\n",
      "12446 Training Loss: tensor(0.3603)\n",
      "12447 Training Loss: tensor(0.3609)\n",
      "12448 Training Loss: tensor(0.3610)\n",
      "12449 Training Loss: tensor(0.3687)\n",
      "12450 Training Loss: tensor(0.3632)\n",
      "12451 Training Loss: tensor(0.3668)\n",
      "12452 Training Loss: tensor(0.3615)\n",
      "12453 Training Loss: tensor(0.3619)\n",
      "12454 Training Loss: tensor(0.3639)\n",
      "12455 Training Loss: tensor(0.3707)\n",
      "12456 Training Loss: tensor(0.3614)\n",
      "12457 Training Loss: tensor(0.3685)\n",
      "12458 Training Loss: tensor(0.3666)\n",
      "12459 Training Loss: tensor(0.3630)\n",
      "12460 Training Loss: tensor(0.3648)\n",
      "12461 Training Loss: tensor(0.3629)\n",
      "12462 Training Loss: tensor(0.3639)\n",
      "12463 Training Loss: tensor(0.3594)\n",
      "12464 Training Loss: tensor(0.3628)\n",
      "12465 Training Loss: tensor(0.3626)\n",
      "12466 Training Loss: tensor(0.3639)\n",
      "12467 Training Loss: tensor(0.3626)\n",
      "12468 Training Loss: tensor(0.3615)\n",
      "12469 Training Loss: tensor(0.3638)\n",
      "12470 Training Loss: tensor(0.3658)\n",
      "12471 Training Loss: tensor(0.3724)\n",
      "12472 Training Loss: tensor(0.3665)\n",
      "12473 Training Loss: tensor(0.3638)\n",
      "12474 Training Loss: tensor(0.3642)\n",
      "12475 Training Loss: tensor(0.3666)\n",
      "12476 Training Loss: tensor(0.3669)\n",
      "12477 Training Loss: tensor(0.3659)\n",
      "12478 Training Loss: tensor(0.3671)\n",
      "12479 Training Loss: tensor(0.3658)\n",
      "12480 Training Loss: tensor(0.3649)\n",
      "12481 Training Loss: tensor(0.3667)\n",
      "12482 Training Loss: tensor(0.3682)\n",
      "12483 Training Loss: tensor(0.3671)\n",
      "12484 Training Loss: tensor(0.3675)\n",
      "12485 Training Loss: tensor(0.3683)\n",
      "12486 Training Loss: tensor(0.3631)\n",
      "12487 Training Loss: tensor(0.3671)\n",
      "12488 Training Loss: tensor(0.3631)\n",
      "12489 Training Loss: tensor(0.3657)\n",
      "12490 Training Loss: tensor(0.3645)\n",
      "12491 Training Loss: tensor(0.3642)\n",
      "12492 Training Loss: tensor(0.3667)\n",
      "12493 Training Loss: tensor(0.3622)\n",
      "12494 Training Loss: tensor(0.3633)\n",
      "12495 Training Loss: tensor(0.3620)\n",
      "12496 Training Loss: tensor(0.3615)\n",
      "12497 Training Loss: tensor(0.3640)\n",
      "12498 Training Loss: tensor(0.3710)\n",
      "12499 Training Loss: tensor(0.3643)\n",
      "12500 Training Loss: tensor(0.3643)\n",
      "12501 Training Loss: tensor(0.3652)\n",
      "12502 Training Loss: tensor(0.3643)\n",
      "12503 Training Loss: tensor(0.3622)\n",
      "12504 Training Loss: tensor(0.3624)\n",
      "12505 Training Loss: tensor(0.3643)\n",
      "12506 Training Loss: tensor(0.3638)\n",
      "12507 Training Loss: tensor(0.3672)\n",
      "12508 Training Loss: tensor(0.3607)\n",
      "12509 Training Loss: tensor(0.3625)\n",
      "12510 Training Loss: tensor(0.3621)\n",
      "12511 Training Loss: tensor(0.3660)\n",
      "12512 Training Loss: tensor(0.3621)\n",
      "12513 Training Loss: tensor(0.3622)\n",
      "12514 Training Loss: tensor(0.3644)\n",
      "12515 Training Loss: tensor(0.3696)\n",
      "12516 Training Loss: tensor(0.3598)\n",
      "12517 Training Loss: tensor(0.3697)\n",
      "12518 Training Loss: tensor(0.3658)\n",
      "12519 Training Loss: tensor(0.3691)\n",
      "12520 Training Loss: tensor(0.3599)\n",
      "12521 Training Loss: tensor(0.3639)\n",
      "12522 Training Loss: tensor(0.3631)\n",
      "12523 Training Loss: tensor(0.3631)\n",
      "12524 Training Loss: tensor(0.3608)\n",
      "12525 Training Loss: tensor(0.3639)\n",
      "12526 Training Loss: tensor(0.3630)\n",
      "12527 Training Loss: tensor(0.3615)\n",
      "12528 Training Loss: tensor(0.3657)\n",
      "12529 Training Loss: tensor(0.3680)\n",
      "12530 Training Loss: tensor(0.3675)\n",
      "12531 Training Loss: tensor(0.3603)\n",
      "12532 Training Loss: tensor(0.3671)\n",
      "12533 Training Loss: tensor(0.3633)\n",
      "12534 Training Loss: tensor(0.3678)\n",
      "12535 Training Loss: tensor(0.3667)\n",
      "12536 Training Loss: tensor(0.3639)\n",
      "12537 Training Loss: tensor(0.3669)\n",
      "12538 Training Loss: tensor(0.3619)\n",
      "12539 Training Loss: tensor(0.3614)\n",
      "12540 Training Loss: tensor(0.3612)\n",
      "12541 Training Loss: tensor(0.3650)\n",
      "12542 Training Loss: tensor(0.3621)\n",
      "12543 Training Loss: tensor(0.3624)\n",
      "12544 Training Loss: tensor(0.3623)\n",
      "12545 Training Loss: tensor(0.3635)\n",
      "12546 Training Loss: tensor(0.3604)\n",
      "12547 Training Loss: tensor(0.3666)\n",
      "12548 Training Loss: tensor(0.3681)\n",
      "12549 Training Loss: tensor(0.3651)\n",
      "12550 Training Loss: tensor(0.3627)\n",
      "12551 Training Loss: tensor(0.3656)\n",
      "12552 Training Loss: tensor(0.3615)\n",
      "12553 Training Loss: tensor(0.3659)\n",
      "12554 Training Loss: tensor(0.3655)\n",
      "12555 Training Loss: tensor(0.3655)\n",
      "12556 Training Loss: tensor(0.3639)\n",
      "12557 Training Loss: tensor(0.3640)\n",
      "12558 Training Loss: tensor(0.3645)\n",
      "12559 Training Loss: tensor(0.3641)\n",
      "12560 Training Loss: tensor(0.3686)\n",
      "12561 Training Loss: tensor(0.3649)\n",
      "12562 Training Loss: tensor(0.3631)\n",
      "12563 Training Loss: tensor(0.3639)\n",
      "12564 Training Loss: tensor(0.3647)\n",
      "12565 Training Loss: tensor(0.3608)\n",
      "12566 Training Loss: tensor(0.3630)\n",
      "12567 Training Loss: tensor(0.3623)\n",
      "12568 Training Loss: tensor(0.3644)\n",
      "12569 Training Loss: tensor(0.3625)\n",
      "12570 Training Loss: tensor(0.3662)\n",
      "12571 Training Loss: tensor(0.3694)\n",
      "12572 Training Loss: tensor(0.3652)\n",
      "12573 Training Loss: tensor(0.3625)\n",
      "12574 Training Loss: tensor(0.3637)\n",
      "12575 Training Loss: tensor(0.3682)\n",
      "12576 Training Loss: tensor(0.3639)\n",
      "12577 Training Loss: tensor(0.3620)\n",
      "12578 Training Loss: tensor(0.3689)\n",
      "12579 Training Loss: tensor(0.3642)\n",
      "12580 Training Loss: tensor(0.3606)\n",
      "12581 Training Loss: tensor(0.3704)\n",
      "12582 Training Loss: tensor(0.3627)\n",
      "12583 Training Loss: tensor(0.3617)\n",
      "12584 Training Loss: tensor(0.3619)\n",
      "12585 Training Loss: tensor(0.3633)\n",
      "12586 Training Loss: tensor(0.3611)\n",
      "12587 Training Loss: tensor(0.3644)\n",
      "12588 Training Loss: tensor(0.3656)\n",
      "12589 Training Loss: tensor(0.3644)\n",
      "12590 Training Loss: tensor(0.3627)\n",
      "12591 Training Loss: tensor(0.3642)\n",
      "12592 Training Loss: tensor(0.3631)\n",
      "12593 Training Loss: tensor(0.3656)\n",
      "12594 Training Loss: tensor(0.3689)\n",
      "12595 Training Loss: tensor(0.3664)\n",
      "12596 Training Loss: tensor(0.3594)\n",
      "12597 Training Loss: tensor(0.3638)\n",
      "12598 Training Loss: tensor(0.3626)\n",
      "12599 Training Loss: tensor(0.3633)\n",
      "12600 Training Loss: tensor(0.3646)\n",
      "12601 Training Loss: tensor(0.3666)\n",
      "12602 Training Loss: tensor(0.3635)\n",
      "12603 Training Loss: tensor(0.3642)\n",
      "12604 Training Loss: tensor(0.3674)\n",
      "12605 Training Loss: tensor(0.3632)\n",
      "12606 Training Loss: tensor(0.3671)\n",
      "12607 Training Loss: tensor(0.3635)\n",
      "12608 Training Loss: tensor(0.3636)\n",
      "12609 Training Loss: tensor(0.3637)\n",
      "12610 Training Loss: tensor(0.3679)\n",
      "12611 Training Loss: tensor(0.3613)\n",
      "12612 Training Loss: tensor(0.3613)\n",
      "12613 Training Loss: tensor(0.3657)\n",
      "12614 Training Loss: tensor(0.3645)\n",
      "12615 Training Loss: tensor(0.3621)\n",
      "12616 Training Loss: tensor(0.3626)\n",
      "12617 Training Loss: tensor(0.3675)\n",
      "12618 Training Loss: tensor(0.3599)\n",
      "12619 Training Loss: tensor(0.3620)\n",
      "12620 Training Loss: tensor(0.3673)\n",
      "12621 Training Loss: tensor(0.3662)\n",
      "12622 Training Loss: tensor(0.3660)\n",
      "12623 Training Loss: tensor(0.3607)\n",
      "12624 Training Loss: tensor(0.3633)\n",
      "12625 Training Loss: tensor(0.3626)\n",
      "12626 Training Loss: tensor(0.3634)\n",
      "12627 Training Loss: tensor(0.3625)\n",
      "12628 Training Loss: tensor(0.3609)\n",
      "12629 Training Loss: tensor(0.3653)\n",
      "12630 Training Loss: tensor(0.3626)\n",
      "12631 Training Loss: tensor(0.3620)\n",
      "12632 Training Loss: tensor(0.3640)\n",
      "12633 Training Loss: tensor(0.3642)\n",
      "12634 Training Loss: tensor(0.3643)\n",
      "12635 Training Loss: tensor(0.3612)\n",
      "12636 Training Loss: tensor(0.3644)\n",
      "12637 Training Loss: tensor(0.3649)\n",
      "12638 Training Loss: tensor(0.3727)\n",
      "12639 Training Loss: tensor(0.3669)\n",
      "12640 Training Loss: tensor(0.3603)\n",
      "12641 Training Loss: tensor(0.3677)\n",
      "12642 Training Loss: tensor(0.3630)\n",
      "12643 Training Loss: tensor(0.3678)\n",
      "12644 Training Loss: tensor(0.3612)\n",
      "12645 Training Loss: tensor(0.3632)\n",
      "12646 Training Loss: tensor(0.3643)\n",
      "12647 Training Loss: tensor(0.3646)\n",
      "12648 Training Loss: tensor(0.3612)\n",
      "12649 Training Loss: tensor(0.3625)\n",
      "12650 Training Loss: tensor(0.3628)\n",
      "12651 Training Loss: tensor(0.3612)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12652 Training Loss: tensor(0.3621)\n",
      "12653 Training Loss: tensor(0.3653)\n",
      "12654 Training Loss: tensor(0.3695)\n",
      "12655 Training Loss: tensor(0.3664)\n",
      "12656 Training Loss: tensor(0.3672)\n",
      "12657 Training Loss: tensor(0.3605)\n",
      "12658 Training Loss: tensor(0.3608)\n",
      "12659 Training Loss: tensor(0.3672)\n",
      "12660 Training Loss: tensor(0.3625)\n",
      "12661 Training Loss: tensor(0.3630)\n",
      "12662 Training Loss: tensor(0.3662)\n",
      "12663 Training Loss: tensor(0.3608)\n",
      "12664 Training Loss: tensor(0.3683)\n",
      "12665 Training Loss: tensor(0.3659)\n",
      "12666 Training Loss: tensor(0.3628)\n",
      "12667 Training Loss: tensor(0.3703)\n",
      "12668 Training Loss: tensor(0.3683)\n",
      "12669 Training Loss: tensor(0.3605)\n",
      "12670 Training Loss: tensor(0.3634)\n",
      "12671 Training Loss: tensor(0.3695)\n",
      "12672 Training Loss: tensor(0.3666)\n",
      "12673 Training Loss: tensor(0.3629)\n",
      "12674 Training Loss: tensor(0.3639)\n",
      "12675 Training Loss: tensor(0.3639)\n",
      "12676 Training Loss: tensor(0.3629)\n",
      "12677 Training Loss: tensor(0.3649)\n",
      "12678 Training Loss: tensor(0.3651)\n",
      "12679 Training Loss: tensor(0.3613)\n",
      "12680 Training Loss: tensor(0.3645)\n",
      "12681 Training Loss: tensor(0.3654)\n",
      "12682 Training Loss: tensor(0.3629)\n",
      "12683 Training Loss: tensor(0.3657)\n",
      "12684 Training Loss: tensor(0.3603)\n",
      "12685 Training Loss: tensor(0.3639)\n",
      "12686 Training Loss: tensor(0.3654)\n",
      "12687 Training Loss: tensor(0.3611)\n",
      "12688 Training Loss: tensor(0.3646)\n",
      "12689 Training Loss: tensor(0.3639)\n",
      "12690 Training Loss: tensor(0.3693)\n",
      "12691 Training Loss: tensor(0.3612)\n",
      "12692 Training Loss: tensor(0.3662)\n",
      "12693 Training Loss: tensor(0.3630)\n",
      "12694 Training Loss: tensor(0.3647)\n",
      "12695 Training Loss: tensor(0.3630)\n",
      "12696 Training Loss: tensor(0.3647)\n",
      "12697 Training Loss: tensor(0.3603)\n",
      "12698 Training Loss: tensor(0.3684)\n",
      "12699 Training Loss: tensor(0.3614)\n",
      "12700 Training Loss: tensor(0.3658)\n",
      "12701 Training Loss: tensor(0.3658)\n",
      "12702 Training Loss: tensor(0.3666)\n",
      "12703 Training Loss: tensor(0.3614)\n",
      "12704 Training Loss: tensor(0.3634)\n",
      "12705 Training Loss: tensor(0.3633)\n",
      "12706 Training Loss: tensor(0.3629)\n",
      "12707 Training Loss: tensor(0.3639)\n",
      "12708 Training Loss: tensor(0.3612)\n",
      "12709 Training Loss: tensor(0.3680)\n",
      "12710 Training Loss: tensor(0.3678)\n",
      "12711 Training Loss: tensor(0.3634)\n",
      "12712 Training Loss: tensor(0.3618)\n",
      "12713 Training Loss: tensor(0.3643)\n",
      "12714 Training Loss: tensor(0.3622)\n",
      "12715 Training Loss: tensor(0.3676)\n",
      "12716 Training Loss: tensor(0.3652)\n",
      "12717 Training Loss: tensor(0.3708)\n",
      "12718 Training Loss: tensor(0.3641)\n",
      "12719 Training Loss: tensor(0.3639)\n",
      "12720 Training Loss: tensor(0.3663)\n",
      "12721 Training Loss: tensor(0.3632)\n",
      "12722 Training Loss: tensor(0.3640)\n",
      "12723 Training Loss: tensor(0.3652)\n",
      "12724 Training Loss: tensor(0.3611)\n",
      "12725 Training Loss: tensor(0.3623)\n",
      "12726 Training Loss: tensor(0.3616)\n",
      "12727 Training Loss: tensor(0.3641)\n",
      "12728 Training Loss: tensor(0.3628)\n",
      "12729 Training Loss: tensor(0.3660)\n",
      "12730 Training Loss: tensor(0.3625)\n",
      "12731 Training Loss: tensor(0.3639)\n",
      "12732 Training Loss: tensor(0.3623)\n",
      "12733 Training Loss: tensor(0.3671)\n",
      "12734 Training Loss: tensor(0.3616)\n",
      "12735 Training Loss: tensor(0.3662)\n",
      "12736 Training Loss: tensor(0.3652)\n",
      "12737 Training Loss: tensor(0.3659)\n",
      "12738 Training Loss: tensor(0.3648)\n",
      "12739 Training Loss: tensor(0.3612)\n",
      "12740 Training Loss: tensor(0.3690)\n",
      "12741 Training Loss: tensor(0.3631)\n",
      "12742 Training Loss: tensor(0.3628)\n",
      "12743 Training Loss: tensor(0.3663)\n",
      "12744 Training Loss: tensor(0.3661)\n",
      "12745 Training Loss: tensor(0.3649)\n",
      "12746 Training Loss: tensor(0.3640)\n",
      "12747 Training Loss: tensor(0.3627)\n",
      "12748 Training Loss: tensor(0.3626)\n",
      "12749 Training Loss: tensor(0.3605)\n",
      "12750 Training Loss: tensor(0.3618)\n",
      "12751 Training Loss: tensor(0.3614)\n",
      "12752 Training Loss: tensor(0.3606)\n",
      "12753 Training Loss: tensor(0.3648)\n",
      "12754 Training Loss: tensor(0.3637)\n",
      "12755 Training Loss: tensor(0.3597)\n",
      "12756 Training Loss: tensor(0.3679)\n",
      "12757 Training Loss: tensor(0.3639)\n",
      "12758 Training Loss: tensor(0.3614)\n",
      "12759 Training Loss: tensor(0.3700)\n",
      "12760 Training Loss: tensor(0.3626)\n",
      "12761 Training Loss: tensor(0.3662)\n",
      "12762 Training Loss: tensor(0.3626)\n",
      "12763 Training Loss: tensor(0.3617)\n",
      "12764 Training Loss: tensor(0.3616)\n",
      "12765 Training Loss: tensor(0.3626)\n",
      "12766 Training Loss: tensor(0.3613)\n",
      "12767 Training Loss: tensor(0.3673)\n",
      "12768 Training Loss: tensor(0.3648)\n",
      "12769 Training Loss: tensor(0.3648)\n",
      "12770 Training Loss: tensor(0.3607)\n",
      "12771 Training Loss: tensor(0.3626)\n",
      "12772 Training Loss: tensor(0.3622)\n",
      "12773 Training Loss: tensor(0.3622)\n",
      "12774 Training Loss: tensor(0.3608)\n",
      "12775 Training Loss: tensor(0.3672)\n",
      "12776 Training Loss: tensor(0.3662)\n",
      "12777 Training Loss: tensor(0.3620)\n",
      "12778 Training Loss: tensor(0.3608)\n",
      "12779 Training Loss: tensor(0.3653)\n",
      "12780 Training Loss: tensor(0.3613)\n",
      "12781 Training Loss: tensor(0.3634)\n",
      "12782 Training Loss: tensor(0.3715)\n",
      "12783 Training Loss: tensor(0.3628)\n",
      "12784 Training Loss: tensor(0.3667)\n",
      "12785 Training Loss: tensor(0.3677)\n",
      "12786 Training Loss: tensor(0.3607)\n",
      "12787 Training Loss: tensor(0.3660)\n",
      "12788 Training Loss: tensor(0.3620)\n",
      "12789 Training Loss: tensor(0.3612)\n",
      "12790 Training Loss: tensor(0.3667)\n",
      "12791 Training Loss: tensor(0.3601)\n",
      "12792 Training Loss: tensor(0.3637)\n",
      "12793 Training Loss: tensor(0.3653)\n",
      "12794 Training Loss: tensor(0.3641)\n",
      "12795 Training Loss: tensor(0.3640)\n",
      "12796 Training Loss: tensor(0.3648)\n",
      "12797 Training Loss: tensor(0.3614)\n",
      "12798 Training Loss: tensor(0.3635)\n",
      "12799 Training Loss: tensor(0.3671)\n",
      "12800 Training Loss: tensor(0.3683)\n",
      "12801 Training Loss: tensor(0.3623)\n",
      "12802 Training Loss: tensor(0.3626)\n",
      "12803 Training Loss: tensor(0.3618)\n",
      "12804 Training Loss: tensor(0.3656)\n",
      "12805 Training Loss: tensor(0.3629)\n",
      "12806 Training Loss: tensor(0.3621)\n",
      "12807 Training Loss: tensor(0.3624)\n",
      "12808 Training Loss: tensor(0.3621)\n",
      "12809 Training Loss: tensor(0.3613)\n",
      "12810 Training Loss: tensor(0.3646)\n",
      "12811 Training Loss: tensor(0.3605)\n",
      "12812 Training Loss: tensor(0.3729)\n",
      "12813 Training Loss: tensor(0.3615)\n",
      "12814 Training Loss: tensor(0.3600)\n",
      "12815 Training Loss: tensor(0.3694)\n",
      "12816 Training Loss: tensor(0.3621)\n",
      "12817 Training Loss: tensor(0.3664)\n",
      "12818 Training Loss: tensor(0.3648)\n",
      "12819 Training Loss: tensor(0.3675)\n",
      "12820 Training Loss: tensor(0.3674)\n",
      "12821 Training Loss: tensor(0.3644)\n",
      "12822 Training Loss: tensor(0.3656)\n",
      "12823 Training Loss: tensor(0.3616)\n",
      "12824 Training Loss: tensor(0.3649)\n",
      "12825 Training Loss: tensor(0.3673)\n",
      "12826 Training Loss: tensor(0.3618)\n",
      "12827 Training Loss: tensor(0.3643)\n",
      "12828 Training Loss: tensor(0.3639)\n",
      "12829 Training Loss: tensor(0.3628)\n",
      "12830 Training Loss: tensor(0.3625)\n",
      "12831 Training Loss: tensor(0.3630)\n",
      "12832 Training Loss: tensor(0.3656)\n",
      "12833 Training Loss: tensor(0.3625)\n",
      "12834 Training Loss: tensor(0.3639)\n",
      "12835 Training Loss: tensor(0.3634)\n",
      "12836 Training Loss: tensor(0.3673)\n",
      "12837 Training Loss: tensor(0.3612)\n",
      "12838 Training Loss: tensor(0.3602)\n",
      "12839 Training Loss: tensor(0.3629)\n",
      "12840 Training Loss: tensor(0.3638)\n",
      "12841 Training Loss: tensor(0.3630)\n",
      "12842 Training Loss: tensor(0.3659)\n",
      "12843 Training Loss: tensor(0.3646)\n",
      "12844 Training Loss: tensor(0.3605)\n",
      "12845 Training Loss: tensor(0.3649)\n",
      "12846 Training Loss: tensor(0.3616)\n",
      "12847 Training Loss: tensor(0.3696)\n",
      "12848 Training Loss: tensor(0.3680)\n",
      "12849 Training Loss: tensor(0.3620)\n",
      "12850 Training Loss: tensor(0.3662)\n",
      "12851 Training Loss: tensor(0.3640)\n",
      "12852 Training Loss: tensor(0.3707)\n",
      "12853 Training Loss: tensor(0.3659)\n",
      "12854 Training Loss: tensor(0.3611)\n",
      "12855 Training Loss: tensor(0.3642)\n",
      "12856 Training Loss: tensor(0.3648)\n",
      "12857 Training Loss: tensor(0.3644)\n",
      "12858 Training Loss: tensor(0.3628)\n",
      "12859 Training Loss: tensor(0.3647)\n",
      "12860 Training Loss: tensor(0.3616)\n",
      "12861 Training Loss: tensor(0.3614)\n",
      "12862 Training Loss: tensor(0.3641)\n",
      "12863 Training Loss: tensor(0.3646)\n",
      "12864 Training Loss: tensor(0.3660)\n",
      "12865 Training Loss: tensor(0.3648)\n",
      "12866 Training Loss: tensor(0.3622)\n",
      "12867 Training Loss: tensor(0.3614)\n",
      "12868 Training Loss: tensor(0.3650)\n",
      "12869 Training Loss: tensor(0.3625)\n",
      "12870 Training Loss: tensor(0.3637)\n",
      "12871 Training Loss: tensor(0.3658)\n",
      "12872 Training Loss: tensor(0.3595)\n",
      "12873 Training Loss: tensor(0.3604)\n",
      "12874 Training Loss: tensor(0.3629)\n",
      "12875 Training Loss: tensor(0.3612)\n",
      "12876 Training Loss: tensor(0.3616)\n",
      "12877 Training Loss: tensor(0.3608)\n",
      "12878 Training Loss: tensor(0.3611)\n",
      "12879 Training Loss: tensor(0.3649)\n",
      "12880 Training Loss: tensor(0.3609)\n",
      "12881 Training Loss: tensor(0.3603)\n",
      "12882 Training Loss: tensor(0.3630)\n",
      "12883 Training Loss: tensor(0.3621)\n",
      "12884 Training Loss: tensor(0.3612)\n",
      "12885 Training Loss: tensor(0.3599)\n",
      "12886 Training Loss: tensor(0.3602)\n",
      "12887 Training Loss: tensor(0.3616)\n",
      "12888 Training Loss: tensor(0.3652)\n",
      "12889 Training Loss: tensor(0.3622)\n",
      "12890 Training Loss: tensor(0.3725)\n",
      "12891 Training Loss: tensor(0.3632)\n",
      "12892 Training Loss: tensor(0.3624)\n",
      "12893 Training Loss: tensor(0.3646)\n",
      "12894 Training Loss: tensor(0.3605)\n",
      "12895 Training Loss: tensor(0.3673)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12896 Training Loss: tensor(0.3614)\n",
      "12897 Training Loss: tensor(0.3650)\n",
      "12898 Training Loss: tensor(0.3609)\n",
      "12899 Training Loss: tensor(0.3636)\n",
      "12900 Training Loss: tensor(0.3596)\n",
      "12901 Training Loss: tensor(0.3612)\n",
      "12902 Training Loss: tensor(0.3634)\n",
      "12903 Training Loss: tensor(0.3712)\n",
      "12904 Training Loss: tensor(0.3627)\n",
      "12905 Training Loss: tensor(0.3659)\n",
      "12906 Training Loss: tensor(0.3643)\n",
      "12907 Training Loss: tensor(0.3637)\n",
      "12908 Training Loss: tensor(0.3641)\n",
      "12909 Training Loss: tensor(0.3600)\n",
      "12910 Training Loss: tensor(0.3646)\n",
      "12911 Training Loss: tensor(0.3664)\n",
      "12912 Training Loss: tensor(0.3640)\n",
      "12913 Training Loss: tensor(0.3616)\n",
      "12914 Training Loss: tensor(0.3622)\n",
      "12915 Training Loss: tensor(0.3684)\n",
      "12916 Training Loss: tensor(0.3639)\n",
      "12917 Training Loss: tensor(0.3679)\n",
      "12918 Training Loss: tensor(0.3609)\n",
      "12919 Training Loss: tensor(0.3606)\n",
      "12920 Training Loss: tensor(0.3624)\n",
      "12921 Training Loss: tensor(0.3617)\n",
      "12922 Training Loss: tensor(0.3677)\n",
      "12923 Training Loss: tensor(0.3601)\n",
      "12924 Training Loss: tensor(0.3631)\n",
      "12925 Training Loss: tensor(0.3631)\n",
      "12926 Training Loss: tensor(0.3655)\n",
      "12927 Training Loss: tensor(0.3650)\n",
      "12928 Training Loss: tensor(0.3652)\n",
      "12929 Training Loss: tensor(0.3630)\n",
      "12930 Training Loss: tensor(0.3614)\n",
      "12931 Training Loss: tensor(0.3639)\n",
      "12932 Training Loss: tensor(0.3612)\n",
      "12933 Training Loss: tensor(0.3627)\n",
      "12934 Training Loss: tensor(0.3595)\n",
      "12935 Training Loss: tensor(0.3643)\n",
      "12936 Training Loss: tensor(0.3689)\n",
      "12937 Training Loss: tensor(0.3605)\n",
      "12938 Training Loss: tensor(0.3609)\n",
      "12939 Training Loss: tensor(0.3629)\n",
      "12940 Training Loss: tensor(0.3603)\n",
      "12941 Training Loss: tensor(0.3606)\n",
      "12942 Training Loss: tensor(0.3663)\n",
      "12943 Training Loss: tensor(0.3618)\n",
      "12944 Training Loss: tensor(0.3623)\n",
      "12945 Training Loss: tensor(0.3653)\n",
      "12946 Training Loss: tensor(0.3602)\n",
      "12947 Training Loss: tensor(0.3656)\n",
      "12948 Training Loss: tensor(0.3640)\n",
      "12949 Training Loss: tensor(0.3657)\n",
      "12950 Training Loss: tensor(0.3600)\n",
      "12951 Training Loss: tensor(0.3613)\n",
      "12952 Training Loss: tensor(0.3658)\n",
      "12953 Training Loss: tensor(0.3598)\n",
      "12954 Training Loss: tensor(0.3619)\n",
      "12955 Training Loss: tensor(0.3614)\n",
      "12956 Training Loss: tensor(0.3657)\n",
      "12957 Training Loss: tensor(0.3626)\n",
      "12958 Training Loss: tensor(0.3618)\n",
      "12959 Training Loss: tensor(0.3622)\n",
      "12960 Training Loss: tensor(0.3631)\n",
      "12961 Training Loss: tensor(0.3644)\n",
      "12962 Training Loss: tensor(0.3651)\n",
      "12963 Training Loss: tensor(0.3647)\n",
      "12964 Training Loss: tensor(0.3652)\n",
      "12965 Training Loss: tensor(0.3611)\n",
      "12966 Training Loss: tensor(0.3625)\n",
      "12967 Training Loss: tensor(0.3624)\n",
      "12968 Training Loss: tensor(0.3653)\n",
      "12969 Training Loss: tensor(0.3664)\n",
      "12970 Training Loss: tensor(0.3632)\n",
      "12971 Training Loss: tensor(0.3767)\n",
      "12972 Training Loss: tensor(0.3644)\n",
      "12973 Training Loss: tensor(0.3643)\n",
      "12974 Training Loss: tensor(0.3644)\n",
      "12975 Training Loss: tensor(0.3620)\n",
      "12976 Training Loss: tensor(0.3617)\n",
      "12977 Training Loss: tensor(0.3631)\n",
      "12978 Training Loss: tensor(0.3657)\n",
      "12979 Training Loss: tensor(0.3634)\n",
      "12980 Training Loss: tensor(0.3652)\n",
      "12981 Training Loss: tensor(0.3644)\n",
      "12982 Training Loss: tensor(0.3659)\n",
      "12983 Training Loss: tensor(0.3622)\n",
      "12984 Training Loss: tensor(0.3619)\n",
      "12985 Training Loss: tensor(0.3649)\n",
      "12986 Training Loss: tensor(0.3661)\n",
      "12987 Training Loss: tensor(0.3643)\n",
      "12988 Training Loss: tensor(0.3640)\n",
      "12989 Training Loss: tensor(0.3670)\n",
      "12990 Training Loss: tensor(0.3654)\n",
      "12991 Training Loss: tensor(0.3646)\n",
      "12992 Training Loss: tensor(0.3614)\n",
      "12993 Training Loss: tensor(0.3603)\n",
      "12994 Training Loss: tensor(0.3602)\n",
      "12995 Training Loss: tensor(0.3650)\n",
      "12996 Training Loss: tensor(0.3627)\n",
      "12997 Training Loss: tensor(0.3655)\n",
      "12998 Training Loss: tensor(0.3596)\n",
      "12999 Training Loss: tensor(0.3627)\n",
      "13000 Training Loss: tensor(0.3636)\n",
      "13001 Training Loss: tensor(0.3618)\n",
      "13002 Training Loss: tensor(0.3606)\n",
      "13003 Training Loss: tensor(0.3673)\n",
      "13004 Training Loss: tensor(0.3597)\n",
      "13005 Training Loss: tensor(0.3597)\n",
      "13006 Training Loss: tensor(0.3664)\n",
      "13007 Training Loss: tensor(0.3670)\n",
      "13008 Training Loss: tensor(0.3595)\n",
      "13009 Training Loss: tensor(0.3592)\n",
      "13010 Training Loss: tensor(0.3625)\n",
      "13011 Training Loss: tensor(0.3582)\n",
      "13012 Training Loss: tensor(0.3614)\n",
      "13013 Training Loss: tensor(0.3669)\n",
      "13014 Training Loss: tensor(0.3590)\n",
      "13015 Training Loss: tensor(0.3598)\n",
      "13016 Training Loss: tensor(0.3623)\n",
      "13017 Training Loss: tensor(0.3593)\n",
      "13018 Training Loss: tensor(0.3624)\n",
      "13019 Training Loss: tensor(0.3615)\n",
      "13020 Training Loss: tensor(0.3644)\n",
      "13021 Training Loss: tensor(0.3614)\n",
      "13022 Training Loss: tensor(0.3611)\n",
      "13023 Training Loss: tensor(0.3678)\n",
      "13024 Training Loss: tensor(0.3681)\n",
      "13025 Training Loss: tensor(0.3675)\n",
      "13026 Training Loss: tensor(0.3625)\n",
      "13027 Training Loss: tensor(0.3634)\n",
      "13028 Training Loss: tensor(0.3622)\n",
      "13029 Training Loss: tensor(0.3646)\n",
      "13030 Training Loss: tensor(0.3637)\n",
      "13031 Training Loss: tensor(0.3627)\n",
      "13032 Training Loss: tensor(0.3642)\n",
      "13033 Training Loss: tensor(0.3637)\n",
      "13034 Training Loss: tensor(0.3667)\n",
      "13035 Training Loss: tensor(0.3657)\n",
      "13036 Training Loss: tensor(0.3661)\n",
      "13037 Training Loss: tensor(0.3665)\n",
      "13038 Training Loss: tensor(0.3607)\n",
      "13039 Training Loss: tensor(0.3636)\n",
      "13040 Training Loss: tensor(0.3627)\n",
      "13041 Training Loss: tensor(0.3633)\n",
      "13042 Training Loss: tensor(0.3622)\n",
      "13043 Training Loss: tensor(0.3617)\n",
      "13044 Training Loss: tensor(0.3629)\n",
      "13045 Training Loss: tensor(0.3631)\n",
      "13046 Training Loss: tensor(0.3631)\n",
      "13047 Training Loss: tensor(0.3606)\n",
      "13048 Training Loss: tensor(0.3600)\n",
      "13049 Training Loss: tensor(0.3611)\n",
      "13050 Training Loss: tensor(0.3601)\n",
      "13051 Training Loss: tensor(0.3631)\n",
      "13052 Training Loss: tensor(0.3654)\n",
      "13053 Training Loss: tensor(0.3593)\n",
      "13054 Training Loss: tensor(0.3638)\n",
      "13055 Training Loss: tensor(0.3603)\n",
      "13056 Training Loss: tensor(0.3649)\n",
      "13057 Training Loss: tensor(0.3654)\n",
      "13058 Training Loss: tensor(0.3593)\n",
      "13059 Training Loss: tensor(0.3593)\n",
      "13060 Training Loss: tensor(0.3683)\n",
      "13061 Training Loss: tensor(0.3614)\n",
      "13062 Training Loss: tensor(0.3668)\n",
      "13063 Training Loss: tensor(0.3643)\n",
      "13064 Training Loss: tensor(0.3637)\n",
      "13065 Training Loss: tensor(0.3635)\n",
      "13066 Training Loss: tensor(0.3597)\n",
      "13067 Training Loss: tensor(0.3621)\n",
      "13068 Training Loss: tensor(0.3628)\n",
      "13069 Training Loss: tensor(0.3602)\n",
      "13070 Training Loss: tensor(0.3623)\n",
      "13071 Training Loss: tensor(0.3635)\n",
      "13072 Training Loss: tensor(0.3603)\n",
      "13073 Training Loss: tensor(0.3610)\n",
      "13074 Training Loss: tensor(0.3606)\n",
      "13075 Training Loss: tensor(0.3670)\n",
      "13076 Training Loss: tensor(0.3631)\n",
      "13077 Training Loss: tensor(0.3591)\n",
      "13078 Training Loss: tensor(0.3622)\n",
      "13079 Training Loss: tensor(0.3691)\n",
      "13080 Training Loss: tensor(0.3607)\n",
      "13081 Training Loss: tensor(0.3599)\n",
      "13082 Training Loss: tensor(0.3599)\n",
      "13083 Training Loss: tensor(0.3610)\n",
      "13084 Training Loss: tensor(0.3612)\n",
      "13085 Training Loss: tensor(0.3613)\n",
      "13086 Training Loss: tensor(0.3626)\n",
      "13087 Training Loss: tensor(0.3630)\n",
      "13088 Training Loss: tensor(0.3597)\n",
      "13089 Training Loss: tensor(0.3602)\n",
      "13090 Training Loss: tensor(0.3593)\n",
      "13091 Training Loss: tensor(0.3686)\n",
      "13092 Training Loss: tensor(0.3580)\n",
      "13093 Training Loss: tensor(0.3598)\n",
      "13094 Training Loss: tensor(0.3612)\n",
      "13095 Training Loss: tensor(0.3695)\n",
      "13096 Training Loss: tensor(0.3622)\n",
      "13097 Training Loss: tensor(0.3627)\n",
      "13098 Training Loss: tensor(0.3631)\n",
      "13099 Training Loss: tensor(0.3759)\n",
      "13100 Training Loss: tensor(0.3630)\n",
      "13101 Training Loss: tensor(0.3616)\n",
      "13102 Training Loss: tensor(0.3640)\n",
      "13103 Training Loss: tensor(0.3607)\n",
      "13104 Training Loss: tensor(0.3610)\n",
      "13105 Training Loss: tensor(0.3639)\n",
      "13106 Training Loss: tensor(0.3656)\n",
      "13107 Training Loss: tensor(0.3618)\n",
      "13108 Training Loss: tensor(0.3666)\n",
      "13109 Training Loss: tensor(0.3622)\n",
      "13110 Training Loss: tensor(0.3622)\n",
      "13111 Training Loss: tensor(0.3627)\n",
      "13112 Training Loss: tensor(0.3626)\n",
      "13113 Training Loss: tensor(0.3635)\n",
      "13114 Training Loss: tensor(0.3614)\n",
      "13115 Training Loss: tensor(0.3611)\n",
      "13116 Training Loss: tensor(0.3610)\n",
      "13117 Training Loss: tensor(0.3597)\n",
      "13118 Training Loss: tensor(0.3636)\n",
      "13119 Training Loss: tensor(0.3615)\n",
      "13120 Training Loss: tensor(0.3618)\n",
      "13121 Training Loss: tensor(0.3597)\n",
      "13122 Training Loss: tensor(0.3658)\n",
      "13123 Training Loss: tensor(0.3614)\n",
      "13124 Training Loss: tensor(0.3614)\n",
      "13125 Training Loss: tensor(0.3679)\n",
      "13126 Training Loss: tensor(0.3628)\n",
      "13127 Training Loss: tensor(0.3591)\n",
      "13128 Training Loss: tensor(0.3675)\n",
      "13129 Training Loss: tensor(0.3640)\n",
      "13130 Training Loss: tensor(0.3602)\n",
      "13131 Training Loss: tensor(0.3624)\n",
      "13132 Training Loss: tensor(0.3669)\n",
      "13133 Training Loss: tensor(0.3626)\n",
      "13134 Training Loss: tensor(0.3685)\n",
      "13135 Training Loss: tensor(0.3647)\n",
      "13136 Training Loss: tensor(0.3624)\n",
      "13137 Training Loss: tensor(0.3620)\n",
      "13138 Training Loss: tensor(0.3626)\n",
      "13139 Training Loss: tensor(0.3611)\n",
      "13140 Training Loss: tensor(0.3589)\n",
      "13141 Training Loss: tensor(0.3646)\n",
      "13142 Training Loss: tensor(0.3603)\n",
      "13143 Training Loss: tensor(0.3623)\n",
      "13144 Training Loss: tensor(0.3611)\n",
      "13145 Training Loss: tensor(0.3611)\n",
      "13146 Training Loss: tensor(0.3600)\n",
      "13147 Training Loss: tensor(0.3615)\n",
      "13148 Training Loss: tensor(0.3628)\n",
      "13149 Training Loss: tensor(0.3681)\n",
      "13150 Training Loss: tensor(0.3615)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13151 Training Loss: tensor(0.3629)\n",
      "13152 Training Loss: tensor(0.3647)\n",
      "13153 Training Loss: tensor(0.3609)\n",
      "13154 Training Loss: tensor(0.3649)\n",
      "13155 Training Loss: tensor(0.3635)\n",
      "13156 Training Loss: tensor(0.3631)\n",
      "13157 Training Loss: tensor(0.3592)\n",
      "13158 Training Loss: tensor(0.3630)\n",
      "13159 Training Loss: tensor(0.3618)\n",
      "13160 Training Loss: tensor(0.3626)\n",
      "13161 Training Loss: tensor(0.3607)\n",
      "13162 Training Loss: tensor(0.3629)\n",
      "13163 Training Loss: tensor(0.3688)\n",
      "13164 Training Loss: tensor(0.3644)\n",
      "13165 Training Loss: tensor(0.3617)\n",
      "13166 Training Loss: tensor(0.3605)\n",
      "13167 Training Loss: tensor(0.3636)\n",
      "13168 Training Loss: tensor(0.3619)\n",
      "13169 Training Loss: tensor(0.3588)\n",
      "13170 Training Loss: tensor(0.3600)\n",
      "13171 Training Loss: tensor(0.3619)\n",
      "13172 Training Loss: tensor(0.3626)\n",
      "13173 Training Loss: tensor(0.3596)\n",
      "13174 Training Loss: tensor(0.3644)\n",
      "13175 Training Loss: tensor(0.3605)\n",
      "13176 Training Loss: tensor(0.3635)\n",
      "13177 Training Loss: tensor(0.3607)\n",
      "13178 Training Loss: tensor(0.3614)\n",
      "13179 Training Loss: tensor(0.3621)\n",
      "13180 Training Loss: tensor(0.3626)\n",
      "13181 Training Loss: tensor(0.3600)\n",
      "13182 Training Loss: tensor(0.3590)\n",
      "13183 Training Loss: tensor(0.3628)\n",
      "13184 Training Loss: tensor(0.3639)\n",
      "13185 Training Loss: tensor(0.3653)\n",
      "13186 Training Loss: tensor(0.3599)\n",
      "13187 Training Loss: tensor(0.3606)\n",
      "13188 Training Loss: tensor(0.3617)\n",
      "13189 Training Loss: tensor(0.3637)\n",
      "13190 Training Loss: tensor(0.3587)\n",
      "13191 Training Loss: tensor(0.3629)\n",
      "13192 Training Loss: tensor(0.3714)\n",
      "13193 Training Loss: tensor(0.3625)\n",
      "13194 Training Loss: tensor(0.3603)\n",
      "13195 Training Loss: tensor(0.3653)\n",
      "13196 Training Loss: tensor(0.3620)\n",
      "13197 Training Loss: tensor(0.3609)\n",
      "13198 Training Loss: tensor(0.3619)\n",
      "13199 Training Loss: tensor(0.3612)\n",
      "13200 Training Loss: tensor(0.3651)\n",
      "13201 Training Loss: tensor(0.3666)\n",
      "13202 Training Loss: tensor(0.3586)\n",
      "13203 Training Loss: tensor(0.3610)\n",
      "13204 Training Loss: tensor(0.3621)\n",
      "13205 Training Loss: tensor(0.3613)\n",
      "13206 Training Loss: tensor(0.3683)\n",
      "13207 Training Loss: tensor(0.3597)\n",
      "13208 Training Loss: tensor(0.3651)\n",
      "13209 Training Loss: tensor(0.3673)\n",
      "13210 Training Loss: tensor(0.3633)\n",
      "13211 Training Loss: tensor(0.3629)\n",
      "13212 Training Loss: tensor(0.3595)\n",
      "13213 Training Loss: tensor(0.3607)\n",
      "13214 Training Loss: tensor(0.3616)\n",
      "13215 Training Loss: tensor(0.3639)\n",
      "13216 Training Loss: tensor(0.3608)\n",
      "13217 Training Loss: tensor(0.3623)\n",
      "13218 Training Loss: tensor(0.3644)\n",
      "13219 Training Loss: tensor(0.3629)\n",
      "13220 Training Loss: tensor(0.3618)\n",
      "13221 Training Loss: tensor(0.3618)\n",
      "13222 Training Loss: tensor(0.3631)\n",
      "13223 Training Loss: tensor(0.3631)\n",
      "13224 Training Loss: tensor(0.3657)\n",
      "13225 Training Loss: tensor(0.3637)\n",
      "13226 Training Loss: tensor(0.3621)\n",
      "13227 Training Loss: tensor(0.3615)\n",
      "13228 Training Loss: tensor(0.3586)\n",
      "13229 Training Loss: tensor(0.3640)\n",
      "13230 Training Loss: tensor(0.3685)\n",
      "13231 Training Loss: tensor(0.3630)\n",
      "13232 Training Loss: tensor(0.3621)\n",
      "13233 Training Loss: tensor(0.3625)\n",
      "13234 Training Loss: tensor(0.3632)\n",
      "13235 Training Loss: tensor(0.3639)\n",
      "13236 Training Loss: tensor(0.3617)\n",
      "13237 Training Loss: tensor(0.3672)\n",
      "13238 Training Loss: tensor(0.3617)\n",
      "13239 Training Loss: tensor(0.3621)\n",
      "13240 Training Loss: tensor(0.3609)\n",
      "13241 Training Loss: tensor(0.3639)\n",
      "13242 Training Loss: tensor(0.3601)\n",
      "13243 Training Loss: tensor(0.3666)\n",
      "13244 Training Loss: tensor(0.3620)\n",
      "13245 Training Loss: tensor(0.3648)\n",
      "13246 Training Loss: tensor(0.3582)\n",
      "13247 Training Loss: tensor(0.3619)\n",
      "13248 Training Loss: tensor(0.3645)\n",
      "13249 Training Loss: tensor(0.3588)\n",
      "13250 Training Loss: tensor(0.3644)\n",
      "13251 Training Loss: tensor(0.3656)\n",
      "13252 Training Loss: tensor(0.3661)\n",
      "13253 Training Loss: tensor(0.3643)\n",
      "13254 Training Loss: tensor(0.3608)\n",
      "13255 Training Loss: tensor(0.3624)\n",
      "13256 Training Loss: tensor(0.3628)\n",
      "13257 Training Loss: tensor(0.3640)\n",
      "13258 Training Loss: tensor(0.3695)\n",
      "13259 Training Loss: tensor(0.3618)\n",
      "13260 Training Loss: tensor(0.3664)\n",
      "13261 Training Loss: tensor(0.3613)\n",
      "13262 Training Loss: tensor(0.3630)\n",
      "13263 Training Loss: tensor(0.3612)\n",
      "13264 Training Loss: tensor(0.3619)\n",
      "13265 Training Loss: tensor(0.3631)\n",
      "13266 Training Loss: tensor(0.3613)\n",
      "13267 Training Loss: tensor(0.3664)\n",
      "13268 Training Loss: tensor(0.3607)\n",
      "13269 Training Loss: tensor(0.3645)\n",
      "13270 Training Loss: tensor(0.3606)\n",
      "13271 Training Loss: tensor(0.3639)\n",
      "13272 Training Loss: tensor(0.3602)\n",
      "13273 Training Loss: tensor(0.3594)\n",
      "13274 Training Loss: tensor(0.3607)\n",
      "13275 Training Loss: tensor(0.3604)\n",
      "13276 Training Loss: tensor(0.3701)\n",
      "13277 Training Loss: tensor(0.3615)\n",
      "13278 Training Loss: tensor(0.3636)\n",
      "13279 Training Loss: tensor(0.3626)\n",
      "13280 Training Loss: tensor(0.3625)\n",
      "13281 Training Loss: tensor(0.3618)\n",
      "13282 Training Loss: tensor(0.3602)\n",
      "13283 Training Loss: tensor(0.3610)\n",
      "13284 Training Loss: tensor(0.3669)\n",
      "13285 Training Loss: tensor(0.3594)\n",
      "13286 Training Loss: tensor(0.3619)\n",
      "13287 Training Loss: tensor(0.3624)\n",
      "13288 Training Loss: tensor(0.3604)\n",
      "13289 Training Loss: tensor(0.3635)\n",
      "13290 Training Loss: tensor(0.3626)\n",
      "13291 Training Loss: tensor(0.3626)\n",
      "13292 Training Loss: tensor(0.3658)\n",
      "13293 Training Loss: tensor(0.3634)\n",
      "13294 Training Loss: tensor(0.3612)\n",
      "13295 Training Loss: tensor(0.3599)\n",
      "13296 Training Loss: tensor(0.3624)\n",
      "13297 Training Loss: tensor(0.3594)\n",
      "13298 Training Loss: tensor(0.3612)\n",
      "13299 Training Loss: tensor(0.3622)\n",
      "13300 Training Loss: tensor(0.3626)\n",
      "13301 Training Loss: tensor(0.3603)\n",
      "13302 Training Loss: tensor(0.3599)\n",
      "13303 Training Loss: tensor(0.3726)\n",
      "13304 Training Loss: tensor(0.3685)\n",
      "13305 Training Loss: tensor(0.3681)\n",
      "13306 Training Loss: tensor(0.3618)\n",
      "13307 Training Loss: tensor(0.3587)\n",
      "13308 Training Loss: tensor(0.3608)\n",
      "13309 Training Loss: tensor(0.3614)\n",
      "13310 Training Loss: tensor(0.3606)\n",
      "13311 Training Loss: tensor(0.3604)\n",
      "13312 Training Loss: tensor(0.3598)\n",
      "13313 Training Loss: tensor(0.3669)\n",
      "13314 Training Loss: tensor(0.3605)\n",
      "13315 Training Loss: tensor(0.3628)\n",
      "13316 Training Loss: tensor(0.3597)\n",
      "13317 Training Loss: tensor(0.3656)\n",
      "13318 Training Loss: tensor(0.3599)\n",
      "13319 Training Loss: tensor(0.3610)\n",
      "13320 Training Loss: tensor(0.3630)\n",
      "13321 Training Loss: tensor(0.3593)\n",
      "13322 Training Loss: tensor(0.3616)\n",
      "13323 Training Loss: tensor(0.3606)\n",
      "13324 Training Loss: tensor(0.3632)\n",
      "13325 Training Loss: tensor(0.3655)\n",
      "13326 Training Loss: tensor(0.3594)\n",
      "13327 Training Loss: tensor(0.3627)\n",
      "13328 Training Loss: tensor(0.3585)\n",
      "13329 Training Loss: tensor(0.3627)\n",
      "13330 Training Loss: tensor(0.3634)\n",
      "13331 Training Loss: tensor(0.3601)\n",
      "13332 Training Loss: tensor(0.3645)\n",
      "13333 Training Loss: tensor(0.3587)\n",
      "13334 Training Loss: tensor(0.3648)\n",
      "13335 Training Loss: tensor(0.3597)\n",
      "13336 Training Loss: tensor(0.3585)\n",
      "13337 Training Loss: tensor(0.3585)\n",
      "13338 Training Loss: tensor(0.3626)\n",
      "13339 Training Loss: tensor(0.3590)\n",
      "13340 Training Loss: tensor(0.3618)\n",
      "13341 Training Loss: tensor(0.3600)\n",
      "13342 Training Loss: tensor(0.3575)\n",
      "13343 Training Loss: tensor(0.3606)\n",
      "13344 Training Loss: tensor(0.3595)\n",
      "13345 Training Loss: tensor(0.3600)\n",
      "13346 Training Loss: tensor(0.3600)\n",
      "13347 Training Loss: tensor(0.3643)\n",
      "13348 Training Loss: tensor(0.3608)\n",
      "13349 Training Loss: tensor(0.3588)\n",
      "13350 Training Loss: tensor(0.3620)\n",
      "13351 Training Loss: tensor(0.3632)\n",
      "13352 Training Loss: tensor(0.3628)\n",
      "13353 Training Loss: tensor(0.3630)\n",
      "13354 Training Loss: tensor(0.3659)\n",
      "13355 Training Loss: tensor(0.3646)\n",
      "13356 Training Loss: tensor(0.3582)\n",
      "13357 Training Loss: tensor(0.3609)\n",
      "13358 Training Loss: tensor(0.3633)\n",
      "13359 Training Loss: tensor(0.3603)\n",
      "13360 Training Loss: tensor(0.3603)\n",
      "13361 Training Loss: tensor(0.3681)\n",
      "13362 Training Loss: tensor(0.3653)\n",
      "13363 Training Loss: tensor(0.3630)\n",
      "13364 Training Loss: tensor(0.3639)\n",
      "13365 Training Loss: tensor(0.3628)\n",
      "13366 Training Loss: tensor(0.3613)\n",
      "13367 Training Loss: tensor(0.3599)\n",
      "13368 Training Loss: tensor(0.3615)\n",
      "13369 Training Loss: tensor(0.3620)\n",
      "13370 Training Loss: tensor(0.3611)\n",
      "13371 Training Loss: tensor(0.3599)\n",
      "13372 Training Loss: tensor(0.3609)\n",
      "13373 Training Loss: tensor(0.3608)\n",
      "13374 Training Loss: tensor(0.3615)\n",
      "13375 Training Loss: tensor(0.3605)\n",
      "13376 Training Loss: tensor(0.3658)\n",
      "13377 Training Loss: tensor(0.3622)\n",
      "13378 Training Loss: tensor(0.3632)\n",
      "13379 Training Loss: tensor(0.3585)\n",
      "13380 Training Loss: tensor(0.3623)\n",
      "13381 Training Loss: tensor(0.3586)\n",
      "13382 Training Loss: tensor(0.3626)\n",
      "13383 Training Loss: tensor(0.3611)\n",
      "13384 Training Loss: tensor(0.3659)\n",
      "13385 Training Loss: tensor(0.3640)\n",
      "13386 Training Loss: tensor(0.3667)\n",
      "13387 Training Loss: tensor(0.3641)\n",
      "13388 Training Loss: tensor(0.3583)\n",
      "13389 Training Loss: tensor(0.3605)\n",
      "13390 Training Loss: tensor(0.3630)\n",
      "13391 Training Loss: tensor(0.3632)\n",
      "13392 Training Loss: tensor(0.3592)\n",
      "13393 Training Loss: tensor(0.3654)\n",
      "13394 Training Loss: tensor(0.3622)\n",
      "13395 Training Loss: tensor(0.3635)\n",
      "13396 Training Loss: tensor(0.3588)\n",
      "13397 Training Loss: tensor(0.3625)\n",
      "13398 Training Loss: tensor(0.3583)\n",
      "13399 Training Loss: tensor(0.3604)\n",
      "13400 Training Loss: tensor(0.3637)\n",
      "13401 Training Loss: tensor(0.3622)\n",
      "13402 Training Loss: tensor(0.3685)\n",
      "13403 Training Loss: tensor(0.3606)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13404 Training Loss: tensor(0.3593)\n",
      "13405 Training Loss: tensor(0.3646)\n",
      "13406 Training Loss: tensor(0.3638)\n",
      "13407 Training Loss: tensor(0.3628)\n",
      "13408 Training Loss: tensor(0.3681)\n",
      "13409 Training Loss: tensor(0.3672)\n",
      "13410 Training Loss: tensor(0.3603)\n",
      "13411 Training Loss: tensor(0.3593)\n",
      "13412 Training Loss: tensor(0.3636)\n",
      "13413 Training Loss: tensor(0.3632)\n",
      "13414 Training Loss: tensor(0.3615)\n",
      "13415 Training Loss: tensor(0.3614)\n",
      "13416 Training Loss: tensor(0.3615)\n",
      "13417 Training Loss: tensor(0.3612)\n",
      "13418 Training Loss: tensor(0.3608)\n",
      "13419 Training Loss: tensor(0.3657)\n",
      "13420 Training Loss: tensor(0.3654)\n",
      "13421 Training Loss: tensor(0.3690)\n",
      "13422 Training Loss: tensor(0.3633)\n",
      "13423 Training Loss: tensor(0.3603)\n",
      "13424 Training Loss: tensor(0.3619)\n",
      "13425 Training Loss: tensor(0.3589)\n",
      "13426 Training Loss: tensor(0.3653)\n",
      "13427 Training Loss: tensor(0.3647)\n",
      "13428 Training Loss: tensor(0.3610)\n",
      "13429 Training Loss: tensor(0.3636)\n",
      "13430 Training Loss: tensor(0.3688)\n",
      "13431 Training Loss: tensor(0.3633)\n",
      "13432 Training Loss: tensor(0.3586)\n",
      "13433 Training Loss: tensor(0.3616)\n",
      "13434 Training Loss: tensor(0.3642)\n",
      "13435 Training Loss: tensor(0.3625)\n",
      "13436 Training Loss: tensor(0.3612)\n",
      "13437 Training Loss: tensor(0.3590)\n",
      "13438 Training Loss: tensor(0.3612)\n",
      "13439 Training Loss: tensor(0.3605)\n",
      "13440 Training Loss: tensor(0.3592)\n",
      "13441 Training Loss: tensor(0.3608)\n",
      "13442 Training Loss: tensor(0.3580)\n",
      "13443 Training Loss: tensor(0.3588)\n",
      "13444 Training Loss: tensor(0.3599)\n",
      "13445 Training Loss: tensor(0.3598)\n",
      "13446 Training Loss: tensor(0.3602)\n",
      "13447 Training Loss: tensor(0.3744)\n",
      "13448 Training Loss: tensor(0.3606)\n",
      "13449 Training Loss: tensor(0.3597)\n",
      "13450 Training Loss: tensor(0.3652)\n",
      "13451 Training Loss: tensor(0.3591)\n",
      "13452 Training Loss: tensor(0.3622)\n",
      "13453 Training Loss: tensor(0.3634)\n",
      "13454 Training Loss: tensor(0.3633)\n",
      "13455 Training Loss: tensor(0.3604)\n",
      "13456 Training Loss: tensor(0.3601)\n",
      "13457 Training Loss: tensor(0.3604)\n",
      "13458 Training Loss: tensor(0.3594)\n",
      "13459 Training Loss: tensor(0.3605)\n",
      "13460 Training Loss: tensor(0.3633)\n",
      "13461 Training Loss: tensor(0.3643)\n",
      "13462 Training Loss: tensor(0.3665)\n",
      "13463 Training Loss: tensor(0.3636)\n",
      "13464 Training Loss: tensor(0.3634)\n",
      "13465 Training Loss: tensor(0.3623)\n",
      "13466 Training Loss: tensor(0.3609)\n",
      "13467 Training Loss: tensor(0.3616)\n",
      "13468 Training Loss: tensor(0.3615)\n",
      "13469 Training Loss: tensor(0.3691)\n",
      "13470 Training Loss: tensor(0.3650)\n",
      "13471 Training Loss: tensor(0.3618)\n",
      "13472 Training Loss: tensor(0.3604)\n",
      "13473 Training Loss: tensor(0.3616)\n",
      "13474 Training Loss: tensor(0.3621)\n",
      "13475 Training Loss: tensor(0.3607)\n",
      "13476 Training Loss: tensor(0.3671)\n",
      "13477 Training Loss: tensor(0.3598)\n",
      "13478 Training Loss: tensor(0.3609)\n",
      "13479 Training Loss: tensor(0.3607)\n",
      "13480 Training Loss: tensor(0.3602)\n",
      "13481 Training Loss: tensor(0.3601)\n",
      "13482 Training Loss: tensor(0.3639)\n",
      "13483 Training Loss: tensor(0.3651)\n",
      "13484 Training Loss: tensor(0.3597)\n",
      "13485 Training Loss: tensor(0.3625)\n",
      "13486 Training Loss: tensor(0.3599)\n",
      "13487 Training Loss: tensor(0.3670)\n",
      "13488 Training Loss: tensor(0.3646)\n",
      "13489 Training Loss: tensor(0.3588)\n",
      "13490 Training Loss: tensor(0.3621)\n",
      "13491 Training Loss: tensor(0.3594)\n",
      "13492 Training Loss: tensor(0.3609)\n",
      "13493 Training Loss: tensor(0.3626)\n",
      "13494 Training Loss: tensor(0.3620)\n",
      "13495 Training Loss: tensor(0.3644)\n",
      "13496 Training Loss: tensor(0.3626)\n",
      "13497 Training Loss: tensor(0.3625)\n",
      "13498 Training Loss: tensor(0.3635)\n",
      "13499 Training Loss: tensor(0.3659)\n",
      "13500 Training Loss: tensor(0.3600)\n",
      "13501 Training Loss: tensor(0.3601)\n",
      "13502 Training Loss: tensor(0.3604)\n",
      "13503 Training Loss: tensor(0.3638)\n",
      "13504 Training Loss: tensor(0.3590)\n",
      "13505 Training Loss: tensor(0.3608)\n",
      "13506 Training Loss: tensor(0.3607)\n",
      "13507 Training Loss: tensor(0.3615)\n",
      "13508 Training Loss: tensor(0.3635)\n",
      "13509 Training Loss: tensor(0.3596)\n",
      "13510 Training Loss: tensor(0.3588)\n",
      "13511 Training Loss: tensor(0.3611)\n",
      "13512 Training Loss: tensor(0.3588)\n",
      "13513 Training Loss: tensor(0.3624)\n",
      "13514 Training Loss: tensor(0.3605)\n",
      "13515 Training Loss: tensor(0.3653)\n",
      "13516 Training Loss: tensor(0.3643)\n",
      "13517 Training Loss: tensor(0.3654)\n",
      "13518 Training Loss: tensor(0.3617)\n",
      "13519 Training Loss: tensor(0.3631)\n",
      "13520 Training Loss: tensor(0.3591)\n",
      "13521 Training Loss: tensor(0.3630)\n",
      "13522 Training Loss: tensor(0.3577)\n",
      "13523 Training Loss: tensor(0.3612)\n",
      "13524 Training Loss: tensor(0.3589)\n",
      "13525 Training Loss: tensor(0.3587)\n",
      "13526 Training Loss: tensor(0.3639)\n",
      "13527 Training Loss: tensor(0.3619)\n",
      "13528 Training Loss: tensor(0.3601)\n",
      "13529 Training Loss: tensor(0.3600)\n",
      "13530 Training Loss: tensor(0.3654)\n",
      "13531 Training Loss: tensor(0.3631)\n",
      "13532 Training Loss: tensor(0.3593)\n",
      "13533 Training Loss: tensor(0.3618)\n",
      "13534 Training Loss: tensor(0.3581)\n",
      "13535 Training Loss: tensor(0.3665)\n",
      "13536 Training Loss: tensor(0.3665)\n",
      "13537 Training Loss: tensor(0.3595)\n",
      "13538 Training Loss: tensor(0.3585)\n",
      "13539 Training Loss: tensor(0.3598)\n",
      "13540 Training Loss: tensor(0.3605)\n",
      "13541 Training Loss: tensor(0.3641)\n",
      "13542 Training Loss: tensor(0.3575)\n",
      "13543 Training Loss: tensor(0.3599)\n",
      "13544 Training Loss: tensor(0.3609)\n",
      "13545 Training Loss: tensor(0.3613)\n",
      "13546 Training Loss: tensor(0.3623)\n",
      "13547 Training Loss: tensor(0.3585)\n",
      "13548 Training Loss: tensor(0.3585)\n",
      "13549 Training Loss: tensor(0.3655)\n",
      "13550 Training Loss: tensor(0.3572)\n",
      "13551 Training Loss: tensor(0.3609)\n",
      "13552 Training Loss: tensor(0.3624)\n",
      "13553 Training Loss: tensor(0.3585)\n",
      "13554 Training Loss: tensor(0.3590)\n",
      "13555 Training Loss: tensor(0.3592)\n",
      "13556 Training Loss: tensor(0.3616)\n",
      "13557 Training Loss: tensor(0.3660)\n",
      "13558 Training Loss: tensor(0.3621)\n",
      "13559 Training Loss: tensor(0.3645)\n",
      "13560 Training Loss: tensor(0.3628)\n",
      "13561 Training Loss: tensor(0.3595)\n",
      "13562 Training Loss: tensor(0.3743)\n",
      "13563 Training Loss: tensor(0.3682)\n",
      "13564 Training Loss: tensor(0.3592)\n",
      "13565 Training Loss: tensor(0.3704)\n",
      "13566 Training Loss: tensor(0.3615)\n",
      "13567 Training Loss: tensor(0.3617)\n",
      "13568 Training Loss: tensor(0.3598)\n",
      "13569 Training Loss: tensor(0.3632)\n",
      "13570 Training Loss: tensor(0.3606)\n",
      "13571 Training Loss: tensor(0.3625)\n",
      "13572 Training Loss: tensor(0.3633)\n",
      "13573 Training Loss: tensor(0.3594)\n",
      "13574 Training Loss: tensor(0.3607)\n",
      "13575 Training Loss: tensor(0.3619)\n",
      "13576 Training Loss: tensor(0.3621)\n",
      "13577 Training Loss: tensor(0.3599)\n",
      "13578 Training Loss: tensor(0.3600)\n",
      "13579 Training Loss: tensor(0.3578)\n",
      "13580 Training Loss: tensor(0.3611)\n",
      "13581 Training Loss: tensor(0.3608)\n",
      "13582 Training Loss: tensor(0.3599)\n",
      "13583 Training Loss: tensor(0.3609)\n",
      "13584 Training Loss: tensor(0.3625)\n",
      "13585 Training Loss: tensor(0.3617)\n",
      "13586 Training Loss: tensor(0.3595)\n",
      "13587 Training Loss: tensor(0.3624)\n",
      "13588 Training Loss: tensor(0.3638)\n",
      "13589 Training Loss: tensor(0.3612)\n",
      "13590 Training Loss: tensor(0.3589)\n",
      "13591 Training Loss: tensor(0.3575)\n",
      "13592 Training Loss: tensor(0.3578)\n",
      "13593 Training Loss: tensor(0.3600)\n",
      "13594 Training Loss: tensor(0.3646)\n",
      "13595 Training Loss: tensor(0.3629)\n",
      "13596 Training Loss: tensor(0.3608)\n",
      "13597 Training Loss: tensor(0.3611)\n",
      "13598 Training Loss: tensor(0.3625)\n",
      "13599 Training Loss: tensor(0.3601)\n",
      "13600 Training Loss: tensor(0.3605)\n",
      "13601 Training Loss: tensor(0.3597)\n",
      "13602 Training Loss: tensor(0.3655)\n",
      "13603 Training Loss: tensor(0.3635)\n",
      "13604 Training Loss: tensor(0.3582)\n",
      "13605 Training Loss: tensor(0.3593)\n",
      "13606 Training Loss: tensor(0.3619)\n",
      "13607 Training Loss: tensor(0.3626)\n",
      "13608 Training Loss: tensor(0.3608)\n",
      "13609 Training Loss: tensor(0.3642)\n",
      "13610 Training Loss: tensor(0.3607)\n",
      "13611 Training Loss: tensor(0.3592)\n",
      "13612 Training Loss: tensor(0.3577)\n",
      "13613 Training Loss: tensor(0.3650)\n",
      "13614 Training Loss: tensor(0.3621)\n",
      "13615 Training Loss: tensor(0.3645)\n",
      "13616 Training Loss: tensor(0.3618)\n",
      "13617 Training Loss: tensor(0.3582)\n",
      "13618 Training Loss: tensor(0.3644)\n",
      "13619 Training Loss: tensor(0.3594)\n",
      "13620 Training Loss: tensor(0.3593)\n",
      "13621 Training Loss: tensor(0.3627)\n",
      "13622 Training Loss: tensor(0.3650)\n",
      "13623 Training Loss: tensor(0.3595)\n",
      "13624 Training Loss: tensor(0.3609)\n",
      "13625 Training Loss: tensor(0.3651)\n",
      "13626 Training Loss: tensor(0.3610)\n",
      "13627 Training Loss: tensor(0.3590)\n",
      "13628 Training Loss: tensor(0.3579)\n",
      "13629 Training Loss: tensor(0.3664)\n",
      "13630 Training Loss: tensor(0.3615)\n",
      "13631 Training Loss: tensor(0.3641)\n",
      "13632 Training Loss: tensor(0.3577)\n",
      "13633 Training Loss: tensor(0.3632)\n",
      "13634 Training Loss: tensor(0.3607)\n",
      "13635 Training Loss: tensor(0.3592)\n",
      "13636 Training Loss: tensor(0.3610)\n",
      "13637 Training Loss: tensor(0.3641)\n",
      "13638 Training Loss: tensor(0.3576)\n",
      "13639 Training Loss: tensor(0.3621)\n",
      "13640 Training Loss: tensor(0.3605)\n",
      "13641 Training Loss: tensor(0.3585)\n",
      "13642 Training Loss: tensor(0.3622)\n",
      "13643 Training Loss: tensor(0.3622)\n",
      "13644 Training Loss: tensor(0.3607)\n",
      "13645 Training Loss: tensor(0.3641)\n",
      "13646 Training Loss: tensor(0.3666)\n",
      "13647 Training Loss: tensor(0.3756)\n",
      "13648 Training Loss: tensor(0.3595)\n",
      "13649 Training Loss: tensor(0.3649)\n",
      "13650 Training Loss: tensor(0.3598)\n",
      "13651 Training Loss: tensor(0.3613)\n",
      "13652 Training Loss: tensor(0.3613)\n",
      "13653 Training Loss: tensor(0.3606)\n",
      "13654 Training Loss: tensor(0.3618)\n",
      "13655 Training Loss: tensor(0.3629)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13656 Training Loss: tensor(0.3637)\n",
      "13657 Training Loss: tensor(0.3601)\n",
      "13658 Training Loss: tensor(0.3597)\n",
      "13659 Training Loss: tensor(0.3607)\n",
      "13660 Training Loss: tensor(0.3615)\n",
      "13661 Training Loss: tensor(0.3657)\n",
      "13662 Training Loss: tensor(0.3603)\n",
      "13663 Training Loss: tensor(0.3627)\n",
      "13664 Training Loss: tensor(0.3593)\n",
      "13665 Training Loss: tensor(0.3597)\n",
      "13666 Training Loss: tensor(0.3587)\n",
      "13667 Training Loss: tensor(0.3584)\n",
      "13668 Training Loss: tensor(0.3596)\n",
      "13669 Training Loss: tensor(0.3617)\n",
      "13670 Training Loss: tensor(0.3684)\n",
      "13671 Training Loss: tensor(0.3596)\n",
      "13672 Training Loss: tensor(0.3615)\n",
      "13673 Training Loss: tensor(0.3667)\n",
      "13674 Training Loss: tensor(0.3584)\n",
      "13675 Training Loss: tensor(0.3599)\n",
      "13676 Training Loss: tensor(0.3621)\n",
      "13677 Training Loss: tensor(0.3605)\n",
      "13678 Training Loss: tensor(0.3592)\n",
      "13679 Training Loss: tensor(0.3660)\n",
      "13680 Training Loss: tensor(0.3658)\n",
      "13681 Training Loss: tensor(0.3650)\n",
      "13682 Training Loss: tensor(0.3602)\n",
      "13683 Training Loss: tensor(0.3606)\n",
      "13684 Training Loss: tensor(0.3682)\n",
      "13685 Training Loss: tensor(0.3623)\n",
      "13686 Training Loss: tensor(0.3617)\n",
      "13687 Training Loss: tensor(0.3596)\n",
      "13688 Training Loss: tensor(0.3604)\n",
      "13689 Training Loss: tensor(0.3624)\n",
      "13690 Training Loss: tensor(0.3585)\n",
      "13691 Training Loss: tensor(0.3588)\n",
      "13692 Training Loss: tensor(0.3629)\n",
      "13693 Training Loss: tensor(0.3633)\n",
      "13694 Training Loss: tensor(0.3609)\n",
      "13695 Training Loss: tensor(0.3602)\n",
      "13696 Training Loss: tensor(0.3617)\n",
      "13697 Training Loss: tensor(0.3593)\n",
      "13698 Training Loss: tensor(0.3598)\n",
      "13699 Training Loss: tensor(0.3614)\n",
      "13700 Training Loss: tensor(0.3632)\n",
      "13701 Training Loss: tensor(0.3595)\n",
      "13702 Training Loss: tensor(0.3612)\n",
      "13703 Training Loss: tensor(0.3590)\n",
      "13704 Training Loss: tensor(0.3564)\n",
      "13705 Training Loss: tensor(0.3571)\n",
      "13706 Training Loss: tensor(0.3607)\n",
      "13707 Training Loss: tensor(0.3592)\n",
      "13708 Training Loss: tensor(0.3570)\n",
      "13709 Training Loss: tensor(0.3639)\n",
      "13710 Training Loss: tensor(0.3600)\n",
      "13711 Training Loss: tensor(0.3676)\n",
      "13712 Training Loss: tensor(0.3573)\n",
      "13713 Training Loss: tensor(0.3623)\n",
      "13714 Training Loss: tensor(0.3594)\n",
      "13715 Training Loss: tensor(0.3591)\n",
      "13716 Training Loss: tensor(0.3598)\n",
      "13717 Training Loss: tensor(0.3649)\n",
      "13718 Training Loss: tensor(0.3593)\n",
      "13719 Training Loss: tensor(0.3586)\n",
      "13720 Training Loss: tensor(0.3600)\n",
      "13721 Training Loss: tensor(0.3585)\n",
      "13722 Training Loss: tensor(0.3638)\n",
      "13723 Training Loss: tensor(0.3658)\n",
      "13724 Training Loss: tensor(0.3572)\n",
      "13725 Training Loss: tensor(0.3573)\n",
      "13726 Training Loss: tensor(0.3656)\n",
      "13727 Training Loss: tensor(0.3622)\n",
      "13728 Training Loss: tensor(0.3788)\n",
      "13729 Training Loss: tensor(0.3623)\n",
      "13730 Training Loss: tensor(0.3620)\n",
      "13731 Training Loss: tensor(0.3582)\n",
      "13732 Training Loss: tensor(0.3592)\n",
      "13733 Training Loss: tensor(0.3632)\n",
      "13734 Training Loss: tensor(0.3607)\n",
      "13735 Training Loss: tensor(0.3595)\n",
      "13736 Training Loss: tensor(0.3631)\n",
      "13737 Training Loss: tensor(0.3680)\n",
      "13738 Training Loss: tensor(0.3607)\n",
      "13739 Training Loss: tensor(0.3634)\n",
      "13740 Training Loss: tensor(0.3592)\n",
      "13741 Training Loss: tensor(0.3601)\n",
      "13742 Training Loss: tensor(0.3617)\n",
      "13743 Training Loss: tensor(0.3616)\n",
      "13744 Training Loss: tensor(0.3584)\n",
      "13745 Training Loss: tensor(0.3616)\n",
      "13746 Training Loss: tensor(0.3599)\n",
      "13747 Training Loss: tensor(0.3599)\n",
      "13748 Training Loss: tensor(0.3644)\n",
      "13749 Training Loss: tensor(0.3621)\n",
      "13750 Training Loss: tensor(0.3590)\n",
      "13751 Training Loss: tensor(0.3616)\n",
      "13752 Training Loss: tensor(0.3614)\n",
      "13753 Training Loss: tensor(0.3597)\n",
      "13754 Training Loss: tensor(0.3624)\n",
      "13755 Training Loss: tensor(0.3644)\n",
      "13756 Training Loss: tensor(0.3630)\n",
      "13757 Training Loss: tensor(0.3631)\n",
      "13758 Training Loss: tensor(0.3631)\n",
      "13759 Training Loss: tensor(0.3669)\n",
      "13760 Training Loss: tensor(0.3588)\n",
      "13761 Training Loss: tensor(0.3641)\n",
      "13762 Training Loss: tensor(0.3600)\n",
      "13763 Training Loss: tensor(0.3610)\n",
      "13764 Training Loss: tensor(0.3670)\n",
      "13765 Training Loss: tensor(0.3612)\n",
      "13766 Training Loss: tensor(0.3582)\n",
      "13767 Training Loss: tensor(0.3609)\n",
      "13768 Training Loss: tensor(0.3605)\n",
      "13769 Training Loss: tensor(0.3589)\n",
      "13770 Training Loss: tensor(0.3611)\n",
      "13771 Training Loss: tensor(0.3590)\n",
      "13772 Training Loss: tensor(0.3583)\n",
      "13773 Training Loss: tensor(0.3587)\n",
      "13774 Training Loss: tensor(0.3596)\n",
      "13775 Training Loss: tensor(0.3600)\n",
      "13776 Training Loss: tensor(0.3593)\n",
      "13777 Training Loss: tensor(0.3601)\n",
      "13778 Training Loss: tensor(0.3572)\n",
      "13779 Training Loss: tensor(0.3588)\n",
      "13780 Training Loss: tensor(0.3568)\n",
      "13781 Training Loss: tensor(0.3631)\n",
      "13782 Training Loss: tensor(0.3563)\n",
      "13783 Training Loss: tensor(0.3614)\n",
      "13784 Training Loss: tensor(0.3582)\n",
      "13785 Training Loss: tensor(0.3566)\n",
      "13786 Training Loss: tensor(0.3612)\n",
      "13787 Training Loss: tensor(0.3580)\n",
      "13788 Training Loss: tensor(0.3596)\n",
      "13789 Training Loss: tensor(0.3603)\n",
      "13790 Training Loss: tensor(0.3609)\n",
      "13791 Training Loss: tensor(0.3662)\n",
      "13792 Training Loss: tensor(0.3603)\n",
      "13793 Training Loss: tensor(0.3566)\n",
      "13794 Training Loss: tensor(0.3649)\n",
      "13795 Training Loss: tensor(0.3619)\n",
      "13796 Training Loss: tensor(0.3579)\n",
      "13797 Training Loss: tensor(0.3573)\n",
      "13798 Training Loss: tensor(0.3569)\n",
      "13799 Training Loss: tensor(0.3626)\n",
      "13800 Training Loss: tensor(0.3582)\n",
      "13801 Training Loss: tensor(0.3590)\n",
      "13802 Training Loss: tensor(0.3635)\n",
      "13803 Training Loss: tensor(0.3614)\n",
      "13804 Training Loss: tensor(0.3645)\n",
      "13805 Training Loss: tensor(0.3596)\n",
      "13806 Training Loss: tensor(0.3678)\n",
      "13807 Training Loss: tensor(0.3603)\n",
      "13808 Training Loss: tensor(0.3577)\n",
      "13809 Training Loss: tensor(0.3631)\n",
      "13810 Training Loss: tensor(0.3608)\n",
      "13811 Training Loss: tensor(0.3643)\n",
      "13812 Training Loss: tensor(0.3588)\n",
      "13813 Training Loss: tensor(0.3625)\n",
      "13814 Training Loss: tensor(0.3639)\n",
      "13815 Training Loss: tensor(0.3603)\n",
      "13816 Training Loss: tensor(0.3582)\n",
      "13817 Training Loss: tensor(0.3626)\n",
      "13818 Training Loss: tensor(0.3644)\n",
      "13819 Training Loss: tensor(0.3606)\n",
      "13820 Training Loss: tensor(0.3589)\n",
      "13821 Training Loss: tensor(0.3611)\n",
      "13822 Training Loss: tensor(0.3598)\n",
      "13823 Training Loss: tensor(0.3594)\n",
      "13824 Training Loss: tensor(0.3606)\n",
      "13825 Training Loss: tensor(0.3577)\n",
      "13826 Training Loss: tensor(0.3610)\n",
      "13827 Training Loss: tensor(0.3681)\n",
      "13828 Training Loss: tensor(0.3570)\n",
      "13829 Training Loss: tensor(0.3574)\n",
      "13830 Training Loss: tensor(0.3595)\n",
      "13831 Training Loss: tensor(0.3700)\n",
      "13832 Training Loss: tensor(0.3629)\n",
      "13833 Training Loss: tensor(0.3583)\n",
      "13834 Training Loss: tensor(0.3569)\n",
      "13835 Training Loss: tensor(0.3575)\n",
      "13836 Training Loss: tensor(0.3689)\n",
      "13837 Training Loss: tensor(0.3673)\n",
      "13838 Training Loss: tensor(0.3583)\n",
      "13839 Training Loss: tensor(0.3602)\n",
      "13840 Training Loss: tensor(0.3594)\n",
      "13841 Training Loss: tensor(0.3588)\n",
      "13842 Training Loss: tensor(0.3632)\n",
      "13843 Training Loss: tensor(0.3604)\n",
      "13844 Training Loss: tensor(0.3617)\n",
      "13845 Training Loss: tensor(0.3625)\n",
      "13846 Training Loss: tensor(0.3592)\n",
      "13847 Training Loss: tensor(0.3595)\n",
      "13848 Training Loss: tensor(0.3608)\n",
      "13849 Training Loss: tensor(0.3598)\n",
      "13850 Training Loss: tensor(0.3564)\n",
      "13851 Training Loss: tensor(0.3676)\n",
      "13852 Training Loss: tensor(0.3602)\n",
      "13853 Training Loss: tensor(0.3570)\n",
      "13854 Training Loss: tensor(0.3593)\n",
      "13855 Training Loss: tensor(0.3622)\n",
      "13856 Training Loss: tensor(0.3593)\n",
      "13857 Training Loss: tensor(0.3594)\n",
      "13858 Training Loss: tensor(0.3606)\n",
      "13859 Training Loss: tensor(0.3584)\n",
      "13860 Training Loss: tensor(0.3598)\n",
      "13861 Training Loss: tensor(0.3585)\n",
      "13862 Training Loss: tensor(0.3607)\n",
      "13863 Training Loss: tensor(0.3610)\n",
      "13864 Training Loss: tensor(0.3604)\n",
      "13865 Training Loss: tensor(0.3600)\n",
      "13866 Training Loss: tensor(0.3581)\n",
      "13867 Training Loss: tensor(0.3604)\n",
      "13868 Training Loss: tensor(0.3574)\n",
      "13869 Training Loss: tensor(0.3621)\n",
      "13870 Training Loss: tensor(0.3645)\n",
      "13871 Training Loss: tensor(0.3589)\n",
      "13872 Training Loss: tensor(0.3623)\n",
      "13873 Training Loss: tensor(0.3601)\n",
      "13874 Training Loss: tensor(0.3619)\n",
      "13875 Training Loss: tensor(0.3571)\n",
      "13876 Training Loss: tensor(0.3613)\n",
      "13877 Training Loss: tensor(0.3631)\n",
      "13878 Training Loss: tensor(0.3595)\n",
      "13879 Training Loss: tensor(0.3570)\n",
      "13880 Training Loss: tensor(0.3560)\n",
      "13881 Training Loss: tensor(0.3581)\n",
      "13882 Training Loss: tensor(0.3598)\n",
      "13883 Training Loss: tensor(0.3581)\n",
      "13884 Training Loss: tensor(0.3665)\n",
      "13885 Training Loss: tensor(0.3664)\n",
      "13886 Training Loss: tensor(0.3601)\n",
      "13887 Training Loss: tensor(0.3657)\n",
      "13888 Training Loss: tensor(0.3593)\n",
      "13889 Training Loss: tensor(0.3568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13890 Training Loss: tensor(0.3595)\n",
      "13891 Training Loss: tensor(0.3599)\n",
      "13892 Training Loss: tensor(0.3624)\n",
      "13893 Training Loss: tensor(0.3597)\n",
      "13894 Training Loss: tensor(0.3575)\n",
      "13895 Training Loss: tensor(0.3605)\n",
      "13896 Training Loss: tensor(0.3586)\n",
      "13897 Training Loss: tensor(0.3607)\n",
      "13898 Training Loss: tensor(0.3596)\n",
      "13899 Training Loss: tensor(0.3626)\n",
      "13900 Training Loss: tensor(0.3580)\n",
      "13901 Training Loss: tensor(0.3616)\n",
      "13902 Training Loss: tensor(0.3633)\n",
      "13903 Training Loss: tensor(0.3625)\n",
      "13904 Training Loss: tensor(0.3612)\n",
      "13905 Training Loss: tensor(0.3606)\n",
      "13906 Training Loss: tensor(0.3621)\n",
      "13907 Training Loss: tensor(0.3609)\n",
      "13908 Training Loss: tensor(0.3627)\n",
      "13909 Training Loss: tensor(0.3595)\n",
      "13910 Training Loss: tensor(0.3589)\n",
      "13911 Training Loss: tensor(0.3598)\n",
      "13912 Training Loss: tensor(0.3600)\n",
      "13913 Training Loss: tensor(0.3567)\n",
      "13914 Training Loss: tensor(0.3582)\n",
      "13915 Training Loss: tensor(0.3577)\n",
      "13916 Training Loss: tensor(0.3674)\n",
      "13917 Training Loss: tensor(0.3619)\n",
      "13918 Training Loss: tensor(0.3605)\n",
      "13919 Training Loss: tensor(0.3610)\n",
      "13920 Training Loss: tensor(0.3608)\n",
      "13921 Training Loss: tensor(0.3640)\n",
      "13922 Training Loss: tensor(0.3660)\n",
      "13923 Training Loss: tensor(0.3618)\n",
      "13924 Training Loss: tensor(0.3601)\n",
      "13925 Training Loss: tensor(0.3603)\n",
      "13926 Training Loss: tensor(0.3615)\n",
      "13927 Training Loss: tensor(0.3598)\n",
      "13928 Training Loss: tensor(0.3602)\n",
      "13929 Training Loss: tensor(0.3608)\n",
      "13930 Training Loss: tensor(0.3590)\n",
      "13931 Training Loss: tensor(0.3612)\n",
      "13932 Training Loss: tensor(0.3644)\n",
      "13933 Training Loss: tensor(0.3589)\n",
      "13934 Training Loss: tensor(0.3638)\n",
      "13935 Training Loss: tensor(0.3594)\n",
      "13936 Training Loss: tensor(0.3693)\n",
      "13937 Training Loss: tensor(0.3636)\n",
      "13938 Training Loss: tensor(0.3583)\n",
      "13939 Training Loss: tensor(0.3604)\n",
      "13940 Training Loss: tensor(0.3596)\n",
      "13941 Training Loss: tensor(0.3658)\n",
      "13942 Training Loss: tensor(0.3580)\n",
      "13943 Training Loss: tensor(0.3616)\n",
      "13944 Training Loss: tensor(0.3598)\n",
      "13945 Training Loss: tensor(0.3615)\n",
      "13946 Training Loss: tensor(0.3620)\n",
      "13947 Training Loss: tensor(0.3601)\n",
      "13948 Training Loss: tensor(0.3625)\n",
      "13949 Training Loss: tensor(0.3583)\n",
      "13950 Training Loss: tensor(0.3620)\n",
      "13951 Training Loss: tensor(0.3590)\n",
      "13952 Training Loss: tensor(0.3608)\n",
      "13953 Training Loss: tensor(0.3568)\n",
      "13954 Training Loss: tensor(0.3583)\n",
      "13955 Training Loss: tensor(0.3582)\n",
      "13956 Training Loss: tensor(0.3703)\n",
      "13957 Training Loss: tensor(0.3583)\n",
      "13958 Training Loss: tensor(0.3612)\n",
      "13959 Training Loss: tensor(0.3605)\n",
      "13960 Training Loss: tensor(0.3633)\n",
      "13961 Training Loss: tensor(0.3602)\n",
      "13962 Training Loss: tensor(0.3594)\n",
      "13963 Training Loss: tensor(0.3579)\n",
      "13964 Training Loss: tensor(0.3573)\n",
      "13965 Training Loss: tensor(0.3580)\n",
      "13966 Training Loss: tensor(0.3578)\n",
      "13967 Training Loss: tensor(0.3647)\n",
      "13968 Training Loss: tensor(0.3627)\n",
      "13969 Training Loss: tensor(0.3593)\n",
      "13970 Training Loss: tensor(0.3602)\n",
      "13971 Training Loss: tensor(0.3614)\n",
      "13972 Training Loss: tensor(0.3621)\n",
      "13973 Training Loss: tensor(0.3585)\n",
      "13974 Training Loss: tensor(0.3588)\n",
      "13975 Training Loss: tensor(0.3588)\n",
      "13976 Training Loss: tensor(0.3588)\n",
      "13977 Training Loss: tensor(0.3575)\n",
      "13978 Training Loss: tensor(0.3579)\n",
      "13979 Training Loss: tensor(0.3650)\n",
      "13980 Training Loss: tensor(0.3604)\n",
      "13981 Training Loss: tensor(0.3585)\n",
      "13982 Training Loss: tensor(0.3596)\n",
      "13983 Training Loss: tensor(0.3591)\n",
      "13984 Training Loss: tensor(0.3625)\n",
      "13985 Training Loss: tensor(0.3597)\n",
      "13986 Training Loss: tensor(0.3620)\n",
      "13987 Training Loss: tensor(0.3602)\n",
      "13988 Training Loss: tensor(0.3603)\n",
      "13989 Training Loss: tensor(0.3567)\n",
      "13990 Training Loss: tensor(0.3613)\n",
      "13991 Training Loss: tensor(0.3597)\n",
      "13992 Training Loss: tensor(0.3594)\n",
      "13993 Training Loss: tensor(0.3597)\n",
      "13994 Training Loss: tensor(0.3615)\n",
      "13995 Training Loss: tensor(0.3596)\n",
      "13996 Training Loss: tensor(0.3577)\n",
      "13997 Training Loss: tensor(0.3578)\n",
      "13998 Training Loss: tensor(0.3640)\n",
      "13999 Training Loss: tensor(0.3605)\n",
      "14000 Training Loss: tensor(0.3610)\n",
      "14001 Training Loss: tensor(0.3616)\n",
      "14002 Training Loss: tensor(0.3635)\n",
      "14003 Training Loss: tensor(0.3586)\n",
      "14004 Training Loss: tensor(0.3642)\n",
      "14005 Training Loss: tensor(0.3618)\n",
      "14006 Training Loss: tensor(0.3645)\n",
      "14007 Training Loss: tensor(0.3585)\n",
      "14008 Training Loss: tensor(0.3604)\n",
      "14009 Training Loss: tensor(0.3619)\n",
      "14010 Training Loss: tensor(0.3639)\n",
      "14011 Training Loss: tensor(0.3638)\n",
      "14012 Training Loss: tensor(0.3588)\n",
      "14013 Training Loss: tensor(0.3608)\n",
      "14014 Training Loss: tensor(0.3570)\n",
      "14015 Training Loss: tensor(0.3643)\n",
      "14016 Training Loss: tensor(0.3613)\n",
      "14017 Training Loss: tensor(0.3632)\n",
      "14018 Training Loss: tensor(0.3629)\n",
      "14019 Training Loss: tensor(0.3585)\n",
      "14020 Training Loss: tensor(0.3612)\n",
      "14021 Training Loss: tensor(0.3631)\n",
      "14022 Training Loss: tensor(0.3583)\n",
      "14023 Training Loss: tensor(0.3628)\n",
      "14024 Training Loss: tensor(0.3601)\n",
      "14025 Training Loss: tensor(0.3602)\n",
      "14026 Training Loss: tensor(0.3610)\n",
      "14027 Training Loss: tensor(0.3601)\n",
      "14028 Training Loss: tensor(0.3612)\n",
      "14029 Training Loss: tensor(0.3605)\n",
      "14030 Training Loss: tensor(0.3590)\n",
      "14031 Training Loss: tensor(0.3580)\n",
      "14032 Training Loss: tensor(0.3583)\n",
      "14033 Training Loss: tensor(0.3578)\n",
      "14034 Training Loss: tensor(0.3612)\n",
      "14035 Training Loss: tensor(0.3574)\n",
      "14036 Training Loss: tensor(0.3590)\n",
      "14037 Training Loss: tensor(0.3593)\n",
      "14038 Training Loss: tensor(0.3609)\n",
      "14039 Training Loss: tensor(0.3576)\n",
      "14040 Training Loss: tensor(0.3615)\n",
      "14041 Training Loss: tensor(0.3592)\n",
      "14042 Training Loss: tensor(0.3593)\n",
      "14043 Training Loss: tensor(0.3622)\n",
      "14044 Training Loss: tensor(0.3591)\n",
      "14045 Training Loss: tensor(0.3617)\n",
      "14046 Training Loss: tensor(0.3617)\n",
      "14047 Training Loss: tensor(0.3625)\n",
      "14048 Training Loss: tensor(0.3594)\n",
      "14049 Training Loss: tensor(0.3631)\n",
      "14050 Training Loss: tensor(0.3601)\n",
      "14051 Training Loss: tensor(0.3575)\n",
      "14052 Training Loss: tensor(0.3573)\n",
      "14053 Training Loss: tensor(0.3571)\n",
      "14054 Training Loss: tensor(0.3588)\n",
      "14055 Training Loss: tensor(0.3606)\n",
      "14056 Training Loss: tensor(0.3605)\n",
      "14057 Training Loss: tensor(0.3577)\n",
      "14058 Training Loss: tensor(0.3572)\n",
      "14059 Training Loss: tensor(0.3578)\n",
      "14060 Training Loss: tensor(0.3599)\n",
      "14061 Training Loss: tensor(0.3600)\n",
      "14062 Training Loss: tensor(0.3580)\n",
      "14063 Training Loss: tensor(0.3572)\n",
      "14064 Training Loss: tensor(0.3613)\n",
      "14065 Training Loss: tensor(0.3663)\n",
      "14066 Training Loss: tensor(0.3608)\n",
      "14067 Training Loss: tensor(0.3608)\n",
      "14068 Training Loss: tensor(0.3585)\n",
      "14069 Training Loss: tensor(0.3593)\n",
      "14070 Training Loss: tensor(0.3590)\n",
      "14071 Training Loss: tensor(0.3613)\n",
      "14072 Training Loss: tensor(0.3596)\n",
      "14073 Training Loss: tensor(0.3584)\n",
      "14074 Training Loss: tensor(0.3599)\n",
      "14075 Training Loss: tensor(0.3582)\n",
      "14076 Training Loss: tensor(0.3586)\n",
      "14077 Training Loss: tensor(0.3604)\n",
      "14078 Training Loss: tensor(0.3569)\n",
      "14079 Training Loss: tensor(0.3578)\n",
      "14080 Training Loss: tensor(0.3609)\n",
      "14081 Training Loss: tensor(0.3626)\n",
      "14082 Training Loss: tensor(0.3602)\n",
      "14083 Training Loss: tensor(0.3640)\n",
      "14084 Training Loss: tensor(0.3593)\n",
      "14085 Training Loss: tensor(0.3583)\n",
      "14086 Training Loss: tensor(0.3617)\n",
      "14087 Training Loss: tensor(0.3582)\n",
      "14088 Training Loss: tensor(0.3604)\n",
      "14089 Training Loss: tensor(0.3640)\n",
      "14090 Training Loss: tensor(0.3596)\n",
      "14091 Training Loss: tensor(0.3607)\n",
      "14092 Training Loss: tensor(0.3607)\n",
      "14093 Training Loss: tensor(0.3634)\n",
      "14094 Training Loss: tensor(0.3584)\n",
      "14095 Training Loss: tensor(0.3589)\n",
      "14096 Training Loss: tensor(0.3647)\n",
      "14097 Training Loss: tensor(0.3599)\n",
      "14098 Training Loss: tensor(0.3617)\n",
      "14099 Training Loss: tensor(0.3603)\n",
      "14100 Training Loss: tensor(0.3569)\n",
      "14101 Training Loss: tensor(0.3568)\n",
      "14102 Training Loss: tensor(0.3621)\n",
      "14103 Training Loss: tensor(0.3570)\n",
      "14104 Training Loss: tensor(0.3600)\n",
      "14105 Training Loss: tensor(0.3622)\n",
      "14106 Training Loss: tensor(0.3592)\n",
      "14107 Training Loss: tensor(0.3591)\n",
      "14108 Training Loss: tensor(0.3561)\n",
      "14109 Training Loss: tensor(0.3584)\n",
      "14110 Training Loss: tensor(0.3580)\n",
      "14111 Training Loss: tensor(0.3613)\n",
      "14112 Training Loss: tensor(0.3641)\n",
      "14113 Training Loss: tensor(0.3617)\n",
      "14114 Training Loss: tensor(0.3602)\n",
      "14115 Training Loss: tensor(0.3572)\n",
      "14116 Training Loss: tensor(0.3604)\n",
      "14117 Training Loss: tensor(0.3616)\n",
      "14118 Training Loss: tensor(0.3609)\n",
      "14119 Training Loss: tensor(0.3603)\n",
      "14120 Training Loss: tensor(0.3577)\n",
      "14121 Training Loss: tensor(0.3582)\n",
      "14122 Training Loss: tensor(0.3569)\n",
      "14123 Training Loss: tensor(0.3623)\n",
      "14124 Training Loss: tensor(0.3589)\n",
      "14125 Training Loss: tensor(0.3576)\n",
      "14126 Training Loss: tensor(0.3563)\n",
      "14127 Training Loss: tensor(0.3579)\n",
      "14128 Training Loss: tensor(0.3578)\n",
      "14129 Training Loss: tensor(0.3631)\n",
      "14130 Training Loss: tensor(0.3573)\n",
      "14131 Training Loss: tensor(0.3551)\n",
      "14132 Training Loss: tensor(0.3565)\n",
      "14133 Training Loss: tensor(0.3591)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14134 Training Loss: tensor(0.3561)\n",
      "14135 Training Loss: tensor(0.3638)\n",
      "14136 Training Loss: tensor(0.3568)\n",
      "14137 Training Loss: tensor(0.3575)\n",
      "14138 Training Loss: tensor(0.3577)\n",
      "14139 Training Loss: tensor(0.3567)\n",
      "14140 Training Loss: tensor(0.3559)\n",
      "14141 Training Loss: tensor(0.3587)\n",
      "14142 Training Loss: tensor(0.3569)\n",
      "14143 Training Loss: tensor(0.3547)\n",
      "14144 Training Loss: tensor(0.3598)\n",
      "14145 Training Loss: tensor(0.3758)\n",
      "14146 Training Loss: tensor(0.3655)\n",
      "14147 Training Loss: tensor(0.3602)\n",
      "14148 Training Loss: tensor(0.3589)\n",
      "14149 Training Loss: tensor(0.3553)\n",
      "14150 Training Loss: tensor(0.3587)\n",
      "14151 Training Loss: tensor(0.3636)\n",
      "14152 Training Loss: tensor(0.3617)\n",
      "14153 Training Loss: tensor(0.3611)\n",
      "14154 Training Loss: tensor(0.3641)\n",
      "14155 Training Loss: tensor(0.3582)\n",
      "14156 Training Loss: tensor(0.3603)\n",
      "14157 Training Loss: tensor(0.3615)\n",
      "14158 Training Loss: tensor(0.3568)\n",
      "14159 Training Loss: tensor(0.3595)\n",
      "14160 Training Loss: tensor(0.3581)\n",
      "14161 Training Loss: tensor(0.3574)\n",
      "14162 Training Loss: tensor(0.3585)\n",
      "14163 Training Loss: tensor(0.3631)\n",
      "14164 Training Loss: tensor(0.3561)\n",
      "14165 Training Loss: tensor(0.3604)\n",
      "14166 Training Loss: tensor(0.3640)\n",
      "14167 Training Loss: tensor(0.3574)\n",
      "14168 Training Loss: tensor(0.3676)\n",
      "14169 Training Loss: tensor(0.3571)\n",
      "14170 Training Loss: tensor(0.3589)\n",
      "14171 Training Loss: tensor(0.3575)\n",
      "14172 Training Loss: tensor(0.3568)\n",
      "14173 Training Loss: tensor(0.3594)\n",
      "14174 Training Loss: tensor(0.3578)\n",
      "14175 Training Loss: tensor(0.3609)\n",
      "14176 Training Loss: tensor(0.3642)\n",
      "14177 Training Loss: tensor(0.3578)\n",
      "14178 Training Loss: tensor(0.3636)\n",
      "14179 Training Loss: tensor(0.3633)\n",
      "14180 Training Loss: tensor(0.3569)\n",
      "14181 Training Loss: tensor(0.3651)\n",
      "14182 Training Loss: tensor(0.3576)\n",
      "14183 Training Loss: tensor(0.3588)\n",
      "14184 Training Loss: tensor(0.3620)\n",
      "14185 Training Loss: tensor(0.3573)\n",
      "14186 Training Loss: tensor(0.3705)\n",
      "14187 Training Loss: tensor(0.3602)\n",
      "14188 Training Loss: tensor(0.3612)\n",
      "14189 Training Loss: tensor(0.3600)\n",
      "14190 Training Loss: tensor(0.3615)\n",
      "14191 Training Loss: tensor(0.3617)\n",
      "14192 Training Loss: tensor(0.3620)\n",
      "14193 Training Loss: tensor(0.3596)\n",
      "14194 Training Loss: tensor(0.3623)\n",
      "14195 Training Loss: tensor(0.3580)\n",
      "14196 Training Loss: tensor(0.3589)\n",
      "14197 Training Loss: tensor(0.3590)\n",
      "14198 Training Loss: tensor(0.3589)\n",
      "14199 Training Loss: tensor(0.3592)\n",
      "14200 Training Loss: tensor(0.3686)\n",
      "14201 Training Loss: tensor(0.3588)\n",
      "14202 Training Loss: tensor(0.3591)\n",
      "14203 Training Loss: tensor(0.3589)\n",
      "14204 Training Loss: tensor(0.3571)\n",
      "14205 Training Loss: tensor(0.3610)\n",
      "14206 Training Loss: tensor(0.3622)\n",
      "14207 Training Loss: tensor(0.3581)\n",
      "14208 Training Loss: tensor(0.3657)\n",
      "14209 Training Loss: tensor(0.3586)\n",
      "14210 Training Loss: tensor(0.3578)\n",
      "14211 Training Loss: tensor(0.3610)\n",
      "14212 Training Loss: tensor(0.3624)\n",
      "14213 Training Loss: tensor(0.3608)\n",
      "14214 Training Loss: tensor(0.3610)\n",
      "14215 Training Loss: tensor(0.3595)\n",
      "14216 Training Loss: tensor(0.3580)\n",
      "14217 Training Loss: tensor(0.3617)\n",
      "14218 Training Loss: tensor(0.3566)\n",
      "14219 Training Loss: tensor(0.3589)\n",
      "14220 Training Loss: tensor(0.3622)\n",
      "14221 Training Loss: tensor(0.3639)\n",
      "14222 Training Loss: tensor(0.3580)\n",
      "14223 Training Loss: tensor(0.3596)\n",
      "14224 Training Loss: tensor(0.3616)\n",
      "14225 Training Loss: tensor(0.3588)\n",
      "14226 Training Loss: tensor(0.3603)\n",
      "14227 Training Loss: tensor(0.3590)\n",
      "14228 Training Loss: tensor(0.3599)\n",
      "14229 Training Loss: tensor(0.3571)\n",
      "14230 Training Loss: tensor(0.3588)\n",
      "14231 Training Loss: tensor(0.3607)\n",
      "14232 Training Loss: tensor(0.3593)\n",
      "14233 Training Loss: tensor(0.3638)\n",
      "14234 Training Loss: tensor(0.3584)\n",
      "14235 Training Loss: tensor(0.3579)\n",
      "14236 Training Loss: tensor(0.3571)\n",
      "14237 Training Loss: tensor(0.3586)\n",
      "14238 Training Loss: tensor(0.3581)\n",
      "14239 Training Loss: tensor(0.3580)\n",
      "14240 Training Loss: tensor(0.3569)\n",
      "14241 Training Loss: tensor(0.3574)\n",
      "14242 Training Loss: tensor(0.3594)\n",
      "14243 Training Loss: tensor(0.3569)\n",
      "14244 Training Loss: tensor(0.3567)\n",
      "14245 Training Loss: tensor(0.3651)\n",
      "14246 Training Loss: tensor(0.3638)\n",
      "14247 Training Loss: tensor(0.3578)\n",
      "14248 Training Loss: tensor(0.3615)\n",
      "14249 Training Loss: tensor(0.3605)\n",
      "14250 Training Loss: tensor(0.3583)\n",
      "14251 Training Loss: tensor(0.3637)\n",
      "14252 Training Loss: tensor(0.3628)\n",
      "14253 Training Loss: tensor(0.3643)\n",
      "14254 Training Loss: tensor(0.3599)\n",
      "14255 Training Loss: tensor(0.3577)\n",
      "14256 Training Loss: tensor(0.3590)\n",
      "14257 Training Loss: tensor(0.3599)\n",
      "14258 Training Loss: tensor(0.3571)\n",
      "14259 Training Loss: tensor(0.3599)\n",
      "14260 Training Loss: tensor(0.3609)\n",
      "14261 Training Loss: tensor(0.3622)\n",
      "14262 Training Loss: tensor(0.3573)\n",
      "14263 Training Loss: tensor(0.3627)\n",
      "14264 Training Loss: tensor(0.3619)\n",
      "14265 Training Loss: tensor(0.3702)\n",
      "14266 Training Loss: tensor(0.3596)\n",
      "14267 Training Loss: tensor(0.3577)\n",
      "14268 Training Loss: tensor(0.3618)\n",
      "14269 Training Loss: tensor(0.3651)\n",
      "14270 Training Loss: tensor(0.3608)\n",
      "14271 Training Loss: tensor(0.3596)\n",
      "14272 Training Loss: tensor(0.3587)\n",
      "14273 Training Loss: tensor(0.3604)\n",
      "14274 Training Loss: tensor(0.3622)\n",
      "14275 Training Loss: tensor(0.3599)\n",
      "14276 Training Loss: tensor(0.3593)\n",
      "14277 Training Loss: tensor(0.3584)\n",
      "14278 Training Loss: tensor(0.3576)\n",
      "14279 Training Loss: tensor(0.3606)\n",
      "14280 Training Loss: tensor(0.3608)\n",
      "14281 Training Loss: tensor(0.3577)\n",
      "14282 Training Loss: tensor(0.3603)\n",
      "14283 Training Loss: tensor(0.3620)\n",
      "14284 Training Loss: tensor(0.3589)\n",
      "14285 Training Loss: tensor(0.3592)\n",
      "14286 Training Loss: tensor(0.3569)\n",
      "14287 Training Loss: tensor(0.3579)\n",
      "14288 Training Loss: tensor(0.3568)\n",
      "14289 Training Loss: tensor(0.3605)\n",
      "14290 Training Loss: tensor(0.3634)\n",
      "14291 Training Loss: tensor(0.3574)\n",
      "14292 Training Loss: tensor(0.3585)\n",
      "14293 Training Loss: tensor(0.3572)\n",
      "14294 Training Loss: tensor(0.3646)\n",
      "14295 Training Loss: tensor(0.3573)\n",
      "14296 Training Loss: tensor(0.3611)\n",
      "14297 Training Loss: tensor(0.3565)\n",
      "14298 Training Loss: tensor(0.3584)\n",
      "14299 Training Loss: tensor(0.3593)\n",
      "14300 Training Loss: tensor(0.3565)\n",
      "14301 Training Loss: tensor(0.3620)\n",
      "14302 Training Loss: tensor(0.3623)\n",
      "14303 Training Loss: tensor(0.3605)\n",
      "14304 Training Loss: tensor(0.3588)\n",
      "14305 Training Loss: tensor(0.3572)\n",
      "14306 Training Loss: tensor(0.3619)\n",
      "14307 Training Loss: tensor(0.3621)\n",
      "14308 Training Loss: tensor(0.3623)\n",
      "14309 Training Loss: tensor(0.3571)\n",
      "14310 Training Loss: tensor(0.3572)\n",
      "14311 Training Loss: tensor(0.3582)\n",
      "14312 Training Loss: tensor(0.3621)\n",
      "14313 Training Loss: tensor(0.3633)\n",
      "14314 Training Loss: tensor(0.3568)\n",
      "14315 Training Loss: tensor(0.3594)\n",
      "14316 Training Loss: tensor(0.3615)\n",
      "14317 Training Loss: tensor(0.3577)\n",
      "14318 Training Loss: tensor(0.3573)\n",
      "14319 Training Loss: tensor(0.3588)\n",
      "14320 Training Loss: tensor(0.3563)\n",
      "14321 Training Loss: tensor(0.3610)\n",
      "14322 Training Loss: tensor(0.3599)\n",
      "14323 Training Loss: tensor(0.3625)\n",
      "14324 Training Loss: tensor(0.3573)\n",
      "14325 Training Loss: tensor(0.3609)\n",
      "14326 Training Loss: tensor(0.3593)\n",
      "14327 Training Loss: tensor(0.3573)\n",
      "14328 Training Loss: tensor(0.3648)\n",
      "14329 Training Loss: tensor(0.3584)\n",
      "14330 Training Loss: tensor(0.3588)\n",
      "14331 Training Loss: tensor(0.3638)\n",
      "14332 Training Loss: tensor(0.3574)\n",
      "14333 Training Loss: tensor(0.3576)\n",
      "14334 Training Loss: tensor(0.3597)\n",
      "14335 Training Loss: tensor(0.3586)\n",
      "14336 Training Loss: tensor(0.3572)\n",
      "14337 Training Loss: tensor(0.3617)\n",
      "14338 Training Loss: tensor(0.3564)\n",
      "14339 Training Loss: tensor(0.3588)\n",
      "14340 Training Loss: tensor(0.3583)\n",
      "14341 Training Loss: tensor(0.3629)\n",
      "14342 Training Loss: tensor(0.3560)\n",
      "14343 Training Loss: tensor(0.3621)\n",
      "14344 Training Loss: tensor(0.3600)\n",
      "14345 Training Loss: tensor(0.3640)\n",
      "14346 Training Loss: tensor(0.3626)\n",
      "14347 Training Loss: tensor(0.3582)\n",
      "14348 Training Loss: tensor(0.3643)\n",
      "14349 Training Loss: tensor(0.3624)\n",
      "14350 Training Loss: tensor(0.3590)\n",
      "14351 Training Loss: tensor(0.3614)\n",
      "14352 Training Loss: tensor(0.3592)\n",
      "14353 Training Loss: tensor(0.3605)\n",
      "14354 Training Loss: tensor(0.3575)\n",
      "14355 Training Loss: tensor(0.3590)\n",
      "14356 Training Loss: tensor(0.3601)\n",
      "14357 Training Loss: tensor(0.3582)\n",
      "14358 Training Loss: tensor(0.3586)\n",
      "14359 Training Loss: tensor(0.3614)\n",
      "14360 Training Loss: tensor(0.3570)\n",
      "14361 Training Loss: tensor(0.3624)\n",
      "14362 Training Loss: tensor(0.3625)\n",
      "14363 Training Loss: tensor(0.3602)\n",
      "14364 Training Loss: tensor(0.3596)\n",
      "14365 Training Loss: tensor(0.3675)\n",
      "14366 Training Loss: tensor(0.3563)\n",
      "14367 Training Loss: tensor(0.3600)\n",
      "14368 Training Loss: tensor(0.3604)\n",
      "14369 Training Loss: tensor(0.3596)\n",
      "14370 Training Loss: tensor(0.3586)\n",
      "14371 Training Loss: tensor(0.3616)\n",
      "14372 Training Loss: tensor(0.3602)\n",
      "14373 Training Loss: tensor(0.3600)\n",
      "14374 Training Loss: tensor(0.3572)\n",
      "14375 Training Loss: tensor(0.3590)\n",
      "14376 Training Loss: tensor(0.3598)\n",
      "14377 Training Loss: tensor(0.3576)\n",
      "14378 Training Loss: tensor(0.3562)\n",
      "14379 Training Loss: tensor(0.3606)\n",
      "14380 Training Loss: tensor(0.3559)\n",
      "14381 Training Loss: tensor(0.3579)\n",
      "14382 Training Loss: tensor(0.3589)\n",
      "14383 Training Loss: tensor(0.3609)\n",
      "14384 Training Loss: tensor(0.3558)\n",
      "14385 Training Loss: tensor(0.3631)\n",
      "14386 Training Loss: tensor(0.3617)\n",
      "14387 Training Loss: tensor(0.3648)\n",
      "14388 Training Loss: tensor(0.3572)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14389 Training Loss: tensor(0.3604)\n",
      "14390 Training Loss: tensor(0.3596)\n",
      "14391 Training Loss: tensor(0.3554)\n",
      "14392 Training Loss: tensor(0.3611)\n",
      "14393 Training Loss: tensor(0.3607)\n",
      "14394 Training Loss: tensor(0.3567)\n",
      "14395 Training Loss: tensor(0.3641)\n",
      "14396 Training Loss: tensor(0.3566)\n",
      "14397 Training Loss: tensor(0.3591)\n",
      "14398 Training Loss: tensor(0.3564)\n",
      "14399 Training Loss: tensor(0.3586)\n",
      "14400 Training Loss: tensor(0.3655)\n",
      "14401 Training Loss: tensor(0.3565)\n",
      "14402 Training Loss: tensor(0.3585)\n",
      "14403 Training Loss: tensor(0.3602)\n",
      "14404 Training Loss: tensor(0.3576)\n",
      "14405 Training Loss: tensor(0.3561)\n",
      "14406 Training Loss: tensor(0.3611)\n",
      "14407 Training Loss: tensor(0.3601)\n",
      "14408 Training Loss: tensor(0.3570)\n",
      "14409 Training Loss: tensor(0.3628)\n",
      "14410 Training Loss: tensor(0.3612)\n",
      "14411 Training Loss: tensor(0.3572)\n",
      "14412 Training Loss: tensor(0.3595)\n",
      "14413 Training Loss: tensor(0.3578)\n",
      "14414 Training Loss: tensor(0.3582)\n",
      "14415 Training Loss: tensor(0.3566)\n",
      "14416 Training Loss: tensor(0.3585)\n",
      "14417 Training Loss: tensor(0.3557)\n",
      "14418 Training Loss: tensor(0.3622)\n",
      "14419 Training Loss: tensor(0.3595)\n",
      "14420 Training Loss: tensor(0.3649)\n",
      "14421 Training Loss: tensor(0.3597)\n",
      "14422 Training Loss: tensor(0.3572)\n",
      "14423 Training Loss: tensor(0.3610)\n",
      "14424 Training Loss: tensor(0.3563)\n",
      "14425 Training Loss: tensor(0.3576)\n",
      "14426 Training Loss: tensor(0.3625)\n",
      "14427 Training Loss: tensor(0.3585)\n",
      "14428 Training Loss: tensor(0.3575)\n",
      "14429 Training Loss: tensor(0.3649)\n",
      "14430 Training Loss: tensor(0.3596)\n",
      "14431 Training Loss: tensor(0.3554)\n",
      "14432 Training Loss: tensor(0.3585)\n",
      "14433 Training Loss: tensor(0.3604)\n",
      "14434 Training Loss: tensor(0.3580)\n",
      "14435 Training Loss: tensor(0.3600)\n",
      "14436 Training Loss: tensor(0.3658)\n",
      "14437 Training Loss: tensor(0.3564)\n",
      "14438 Training Loss: tensor(0.3584)\n",
      "14439 Training Loss: tensor(0.3572)\n",
      "14440 Training Loss: tensor(0.3576)\n",
      "14441 Training Loss: tensor(0.3571)\n",
      "14442 Training Loss: tensor(0.3638)\n",
      "14443 Training Loss: tensor(0.3556)\n",
      "14444 Training Loss: tensor(0.3615)\n",
      "14445 Training Loss: tensor(0.3587)\n",
      "14446 Training Loss: tensor(0.3592)\n",
      "14447 Training Loss: tensor(0.3577)\n",
      "14448 Training Loss: tensor(0.3585)\n",
      "14449 Training Loss: tensor(0.3569)\n",
      "14450 Training Loss: tensor(0.3578)\n",
      "14451 Training Loss: tensor(0.3631)\n",
      "14452 Training Loss: tensor(0.3586)\n",
      "14453 Training Loss: tensor(0.3579)\n",
      "14454 Training Loss: tensor(0.3629)\n",
      "14455 Training Loss: tensor(0.3611)\n",
      "14456 Training Loss: tensor(0.3596)\n",
      "14457 Training Loss: tensor(0.3581)\n",
      "14458 Training Loss: tensor(0.3570)\n",
      "14459 Training Loss: tensor(0.3600)\n",
      "14460 Training Loss: tensor(0.3577)\n",
      "14461 Training Loss: tensor(0.3557)\n",
      "14462 Training Loss: tensor(0.3595)\n",
      "14463 Training Loss: tensor(0.3598)\n",
      "14464 Training Loss: tensor(0.3574)\n",
      "14465 Training Loss: tensor(0.3552)\n",
      "14466 Training Loss: tensor(0.3677)\n",
      "14467 Training Loss: tensor(0.3591)\n",
      "14468 Training Loss: tensor(0.3606)\n",
      "14469 Training Loss: tensor(0.3623)\n",
      "14470 Training Loss: tensor(0.3614)\n",
      "14471 Training Loss: tensor(0.3575)\n",
      "14472 Training Loss: tensor(0.3596)\n",
      "14473 Training Loss: tensor(0.3586)\n",
      "14474 Training Loss: tensor(0.3581)\n",
      "14475 Training Loss: tensor(0.3576)\n",
      "14476 Training Loss: tensor(0.3602)\n",
      "14477 Training Loss: tensor(0.3605)\n",
      "14478 Training Loss: tensor(0.3640)\n",
      "14479 Training Loss: tensor(0.3574)\n",
      "14480 Training Loss: tensor(0.3602)\n",
      "14481 Training Loss: tensor(0.3628)\n",
      "14482 Training Loss: tensor(0.3588)\n",
      "14483 Training Loss: tensor(0.3607)\n",
      "14484 Training Loss: tensor(0.3610)\n",
      "14485 Training Loss: tensor(0.3596)\n",
      "14486 Training Loss: tensor(0.3622)\n",
      "14487 Training Loss: tensor(0.3618)\n",
      "14488 Training Loss: tensor(0.3566)\n",
      "14489 Training Loss: tensor(0.3573)\n",
      "14490 Training Loss: tensor(0.3568)\n",
      "14491 Training Loss: tensor(0.3598)\n",
      "14492 Training Loss: tensor(0.3599)\n",
      "14493 Training Loss: tensor(0.3565)\n",
      "14494 Training Loss: tensor(0.3592)\n",
      "14495 Training Loss: tensor(0.3592)\n",
      "14496 Training Loss: tensor(0.3582)\n",
      "14497 Training Loss: tensor(0.3605)\n",
      "14498 Training Loss: tensor(0.3580)\n",
      "14499 Training Loss: tensor(0.3579)\n",
      "14500 Training Loss: tensor(0.3563)\n",
      "14501 Training Loss: tensor(0.3584)\n",
      "14502 Training Loss: tensor(0.3566)\n",
      "14503 Training Loss: tensor(0.3627)\n",
      "14504 Training Loss: tensor(0.3546)\n",
      "14505 Training Loss: tensor(0.3577)\n",
      "14506 Training Loss: tensor(0.3606)\n",
      "14507 Training Loss: tensor(0.3611)\n",
      "14508 Training Loss: tensor(0.3596)\n",
      "14509 Training Loss: tensor(0.3556)\n",
      "14510 Training Loss: tensor(0.3570)\n",
      "14511 Training Loss: tensor(0.3604)\n",
      "14512 Training Loss: tensor(0.3607)\n",
      "14513 Training Loss: tensor(0.3555)\n",
      "14514 Training Loss: tensor(0.3628)\n",
      "14515 Training Loss: tensor(0.3597)\n",
      "14516 Training Loss: tensor(0.3597)\n",
      "14517 Training Loss: tensor(0.3582)\n",
      "14518 Training Loss: tensor(0.3585)\n",
      "14519 Training Loss: tensor(0.3584)\n",
      "14520 Training Loss: tensor(0.3567)\n",
      "14521 Training Loss: tensor(0.3573)\n",
      "14522 Training Loss: tensor(0.3577)\n",
      "14523 Training Loss: tensor(0.3569)\n",
      "14524 Training Loss: tensor(0.3571)\n",
      "14525 Training Loss: tensor(0.3549)\n",
      "14526 Training Loss: tensor(0.3608)\n",
      "14527 Training Loss: tensor(0.3640)\n",
      "14528 Training Loss: tensor(0.3580)\n",
      "14529 Training Loss: tensor(0.3596)\n",
      "14530 Training Loss: tensor(0.3629)\n",
      "14531 Training Loss: tensor(0.3655)\n",
      "14532 Training Loss: tensor(0.3579)\n",
      "14533 Training Loss: tensor(0.3590)\n",
      "14534 Training Loss: tensor(0.3636)\n",
      "14535 Training Loss: tensor(0.3613)\n",
      "14536 Training Loss: tensor(0.3606)\n",
      "14537 Training Loss: tensor(0.3621)\n",
      "14538 Training Loss: tensor(0.3590)\n",
      "14539 Training Loss: tensor(0.3631)\n",
      "14540 Training Loss: tensor(0.3600)\n",
      "14541 Training Loss: tensor(0.3634)\n",
      "14542 Training Loss: tensor(0.3575)\n",
      "14543 Training Loss: tensor(0.3566)\n",
      "14544 Training Loss: tensor(0.3600)\n",
      "14545 Training Loss: tensor(0.3624)\n",
      "14546 Training Loss: tensor(0.3577)\n",
      "14547 Training Loss: tensor(0.3627)\n",
      "14548 Training Loss: tensor(0.3575)\n",
      "14549 Training Loss: tensor(0.3625)\n",
      "14550 Training Loss: tensor(0.3617)\n",
      "14551 Training Loss: tensor(0.3560)\n",
      "14552 Training Loss: tensor(0.3601)\n",
      "14553 Training Loss: tensor(0.3576)\n",
      "14554 Training Loss: tensor(0.3594)\n",
      "14555 Training Loss: tensor(0.3592)\n",
      "14556 Training Loss: tensor(0.3570)\n",
      "14557 Training Loss: tensor(0.3576)\n",
      "14558 Training Loss: tensor(0.3580)\n",
      "14559 Training Loss: tensor(0.3578)\n",
      "14560 Training Loss: tensor(0.3674)\n",
      "14561 Training Loss: tensor(0.3558)\n",
      "14562 Training Loss: tensor(0.3561)\n",
      "14563 Training Loss: tensor(0.3578)\n",
      "14564 Training Loss: tensor(0.3580)\n",
      "14565 Training Loss: tensor(0.3570)\n",
      "14566 Training Loss: tensor(0.3545)\n",
      "14567 Training Loss: tensor(0.3617)\n",
      "14568 Training Loss: tensor(0.3574)\n",
      "14569 Training Loss: tensor(0.3568)\n",
      "14570 Training Loss: tensor(0.3564)\n",
      "14571 Training Loss: tensor(0.3567)\n",
      "14572 Training Loss: tensor(0.3581)\n",
      "14573 Training Loss: tensor(0.3566)\n",
      "14574 Training Loss: tensor(0.3669)\n",
      "14575 Training Loss: tensor(0.3585)\n",
      "14576 Training Loss: tensor(0.3560)\n",
      "14577 Training Loss: tensor(0.3568)\n",
      "14578 Training Loss: tensor(0.3577)\n",
      "14579 Training Loss: tensor(0.3577)\n",
      "14580 Training Loss: tensor(0.3584)\n",
      "14581 Training Loss: tensor(0.3602)\n",
      "14582 Training Loss: tensor(0.3583)\n",
      "14583 Training Loss: tensor(0.3567)\n",
      "14584 Training Loss: tensor(0.3577)\n",
      "14585 Training Loss: tensor(0.3554)\n",
      "14586 Training Loss: tensor(0.3582)\n",
      "14587 Training Loss: tensor(0.3555)\n",
      "14588 Training Loss: tensor(0.3628)\n",
      "14589 Training Loss: tensor(0.3628)\n",
      "14590 Training Loss: tensor(0.3564)\n",
      "14591 Training Loss: tensor(0.3571)\n",
      "14592 Training Loss: tensor(0.3624)\n",
      "14593 Training Loss: tensor(0.3620)\n",
      "14594 Training Loss: tensor(0.3579)\n",
      "14595 Training Loss: tensor(0.3566)\n",
      "14596 Training Loss: tensor(0.3603)\n",
      "14597 Training Loss: tensor(0.3609)\n",
      "14598 Training Loss: tensor(0.3617)\n",
      "14599 Training Loss: tensor(0.3568)\n",
      "14600 Training Loss: tensor(0.3618)\n",
      "14601 Training Loss: tensor(0.3562)\n",
      "14602 Training Loss: tensor(0.3624)\n",
      "14603 Training Loss: tensor(0.3591)\n",
      "14604 Training Loss: tensor(0.3576)\n",
      "14605 Training Loss: tensor(0.3614)\n",
      "14606 Training Loss: tensor(0.3635)\n",
      "14607 Training Loss: tensor(0.3583)\n",
      "14608 Training Loss: tensor(0.3591)\n",
      "14609 Training Loss: tensor(0.3591)\n",
      "14610 Training Loss: tensor(0.3565)\n",
      "14611 Training Loss: tensor(0.3568)\n",
      "14612 Training Loss: tensor(0.3578)\n",
      "14613 Training Loss: tensor(0.3582)\n",
      "14614 Training Loss: tensor(0.3566)\n",
      "14615 Training Loss: tensor(0.3634)\n",
      "14616 Training Loss: tensor(0.3566)\n",
      "14617 Training Loss: tensor(0.3577)\n",
      "14618 Training Loss: tensor(0.3570)\n",
      "14619 Training Loss: tensor(0.3595)\n",
      "14620 Training Loss: tensor(0.3621)\n",
      "14621 Training Loss: tensor(0.3574)\n",
      "14622 Training Loss: tensor(0.3563)\n",
      "14623 Training Loss: tensor(0.3574)\n",
      "14624 Training Loss: tensor(0.3601)\n",
      "14625 Training Loss: tensor(0.3616)\n",
      "14626 Training Loss: tensor(0.3586)\n",
      "14627 Training Loss: tensor(0.3549)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14628 Training Loss: tensor(0.3628)\n",
      "14629 Training Loss: tensor(0.3562)\n",
      "14630 Training Loss: tensor(0.3583)\n",
      "14631 Training Loss: tensor(0.3557)\n",
      "14632 Training Loss: tensor(0.3550)\n",
      "14633 Training Loss: tensor(0.3590)\n",
      "14634 Training Loss: tensor(0.3590)\n",
      "14635 Training Loss: tensor(0.3549)\n",
      "14636 Training Loss: tensor(0.3598)\n",
      "14637 Training Loss: tensor(0.3578)\n",
      "14638 Training Loss: tensor(0.3562)\n",
      "14639 Training Loss: tensor(0.3575)\n",
      "14640 Training Loss: tensor(0.3616)\n",
      "14641 Training Loss: tensor(0.3567)\n",
      "14642 Training Loss: tensor(0.3638)\n",
      "14643 Training Loss: tensor(0.3577)\n",
      "14644 Training Loss: tensor(0.3586)\n",
      "14645 Training Loss: tensor(0.3687)\n",
      "14646 Training Loss: tensor(0.3588)\n",
      "14647 Training Loss: tensor(0.3624)\n",
      "14648 Training Loss: tensor(0.3588)\n",
      "14649 Training Loss: tensor(0.3608)\n",
      "14650 Training Loss: tensor(0.3572)\n",
      "14651 Training Loss: tensor(0.3591)\n",
      "14652 Training Loss: tensor(0.3587)\n",
      "14653 Training Loss: tensor(0.3627)\n",
      "14654 Training Loss: tensor(0.3609)\n",
      "14655 Training Loss: tensor(0.3599)\n",
      "14656 Training Loss: tensor(0.3587)\n",
      "14657 Training Loss: tensor(0.3609)\n",
      "14658 Training Loss: tensor(0.3573)\n",
      "14659 Training Loss: tensor(0.3578)\n",
      "14660 Training Loss: tensor(0.3566)\n",
      "14661 Training Loss: tensor(0.3572)\n",
      "14662 Training Loss: tensor(0.3591)\n",
      "14663 Training Loss: tensor(0.3577)\n",
      "14664 Training Loss: tensor(0.3570)\n",
      "14665 Training Loss: tensor(0.3579)\n",
      "14666 Training Loss: tensor(0.3593)\n",
      "14667 Training Loss: tensor(0.3607)\n",
      "14668 Training Loss: tensor(0.3609)\n",
      "14669 Training Loss: tensor(0.3565)\n",
      "14670 Training Loss: tensor(0.3590)\n",
      "14671 Training Loss: tensor(0.3548)\n",
      "14672 Training Loss: tensor(0.3551)\n",
      "14673 Training Loss: tensor(0.3564)\n",
      "14674 Training Loss: tensor(0.3622)\n",
      "14675 Training Loss: tensor(0.3641)\n",
      "14676 Training Loss: tensor(0.3550)\n",
      "14677 Training Loss: tensor(0.3566)\n",
      "14678 Training Loss: tensor(0.3621)\n",
      "14679 Training Loss: tensor(0.3599)\n",
      "14680 Training Loss: tensor(0.3570)\n",
      "14681 Training Loss: tensor(0.3578)\n",
      "14682 Training Loss: tensor(0.3555)\n",
      "14683 Training Loss: tensor(0.3557)\n",
      "14684 Training Loss: tensor(0.3582)\n",
      "14685 Training Loss: tensor(0.3583)\n",
      "14686 Training Loss: tensor(0.3552)\n",
      "14687 Training Loss: tensor(0.3552)\n",
      "14688 Training Loss: tensor(0.3638)\n",
      "14689 Training Loss: tensor(0.3587)\n",
      "14690 Training Loss: tensor(0.3584)\n",
      "14691 Training Loss: tensor(0.3595)\n",
      "14692 Training Loss: tensor(0.3577)\n",
      "14693 Training Loss: tensor(0.3639)\n",
      "14694 Training Loss: tensor(0.3591)\n",
      "14695 Training Loss: tensor(0.3551)\n",
      "14696 Training Loss: tensor(0.3560)\n",
      "14697 Training Loss: tensor(0.3601)\n",
      "14698 Training Loss: tensor(0.3600)\n",
      "14699 Training Loss: tensor(0.3591)\n",
      "14700 Training Loss: tensor(0.3599)\n",
      "14701 Training Loss: tensor(0.3552)\n",
      "14702 Training Loss: tensor(0.3581)\n",
      "14703 Training Loss: tensor(0.3563)\n",
      "14704 Training Loss: tensor(0.3569)\n",
      "14705 Training Loss: tensor(0.3574)\n",
      "14706 Training Loss: tensor(0.3555)\n",
      "14707 Training Loss: tensor(0.3573)\n",
      "14708 Training Loss: tensor(0.3576)\n",
      "14709 Training Loss: tensor(0.3614)\n",
      "14710 Training Loss: tensor(0.3563)\n",
      "14711 Training Loss: tensor(0.3545)\n",
      "14712 Training Loss: tensor(0.3577)\n",
      "14713 Training Loss: tensor(0.3556)\n",
      "14714 Training Loss: tensor(0.3600)\n",
      "14715 Training Loss: tensor(0.3626)\n",
      "14716 Training Loss: tensor(0.3574)\n",
      "14717 Training Loss: tensor(0.3557)\n",
      "14718 Training Loss: tensor(0.3574)\n",
      "14719 Training Loss: tensor(0.3557)\n",
      "14720 Training Loss: tensor(0.3560)\n",
      "14721 Training Loss: tensor(0.3593)\n",
      "14722 Training Loss: tensor(0.3569)\n",
      "14723 Training Loss: tensor(0.3564)\n",
      "14724 Training Loss: tensor(0.3557)\n",
      "14725 Training Loss: tensor(0.3552)\n",
      "14726 Training Loss: tensor(0.3551)\n",
      "14727 Training Loss: tensor(0.3565)\n",
      "14728 Training Loss: tensor(0.3541)\n",
      "14729 Training Loss: tensor(0.3613)\n",
      "14730 Training Loss: tensor(0.3585)\n",
      "14731 Training Loss: tensor(0.3632)\n",
      "14732 Training Loss: tensor(0.3542)\n",
      "14733 Training Loss: tensor(0.3649)\n",
      "14734 Training Loss: tensor(0.3570)\n",
      "14735 Training Loss: tensor(0.3571)\n",
      "14736 Training Loss: tensor(0.3560)\n",
      "14737 Training Loss: tensor(0.3565)\n",
      "14738 Training Loss: tensor(0.3654)\n",
      "14739 Training Loss: tensor(0.3565)\n",
      "14740 Training Loss: tensor(0.3564)\n",
      "14741 Training Loss: tensor(0.3564)\n",
      "14742 Training Loss: tensor(0.3557)\n",
      "14743 Training Loss: tensor(0.3578)\n",
      "14744 Training Loss: tensor(0.3598)\n",
      "14745 Training Loss: tensor(0.3566)\n",
      "14746 Training Loss: tensor(0.3598)\n",
      "14747 Training Loss: tensor(0.3565)\n",
      "14748 Training Loss: tensor(0.3609)\n",
      "14749 Training Loss: tensor(0.3604)\n",
      "14750 Training Loss: tensor(0.3557)\n",
      "14751 Training Loss: tensor(0.3564)\n",
      "14752 Training Loss: tensor(0.3603)\n",
      "14753 Training Loss: tensor(0.3561)\n",
      "14754 Training Loss: tensor(0.3556)\n",
      "14755 Training Loss: tensor(0.3575)\n",
      "14756 Training Loss: tensor(0.3553)\n",
      "14757 Training Loss: tensor(0.3573)\n",
      "14758 Training Loss: tensor(0.3587)\n",
      "14759 Training Loss: tensor(0.3582)\n",
      "14760 Training Loss: tensor(0.3649)\n",
      "14761 Training Loss: tensor(0.3616)\n",
      "14762 Training Loss: tensor(0.3598)\n",
      "14763 Training Loss: tensor(0.3589)\n",
      "14764 Training Loss: tensor(0.3566)\n",
      "14765 Training Loss: tensor(0.3614)\n",
      "14766 Training Loss: tensor(0.3559)\n",
      "14767 Training Loss: tensor(0.3577)\n",
      "14768 Training Loss: tensor(0.3633)\n",
      "14769 Training Loss: tensor(0.3555)\n",
      "14770 Training Loss: tensor(0.3585)\n",
      "14771 Training Loss: tensor(0.3547)\n",
      "14772 Training Loss: tensor(0.3556)\n",
      "14773 Training Loss: tensor(0.3583)\n",
      "14774 Training Loss: tensor(0.3623)\n",
      "14775 Training Loss: tensor(0.3573)\n",
      "14776 Training Loss: tensor(0.3624)\n",
      "14777 Training Loss: tensor(0.3594)\n",
      "14778 Training Loss: tensor(0.3583)\n",
      "14779 Training Loss: tensor(0.3571)\n",
      "14780 Training Loss: tensor(0.3619)\n",
      "14781 Training Loss: tensor(0.3563)\n",
      "14782 Training Loss: tensor(0.3570)\n",
      "14783 Training Loss: tensor(0.3602)\n",
      "14784 Training Loss: tensor(0.3581)\n",
      "14785 Training Loss: tensor(0.3593)\n",
      "14786 Training Loss: tensor(0.3562)\n",
      "14787 Training Loss: tensor(0.3571)\n",
      "14788 Training Loss: tensor(0.3562)\n",
      "14789 Training Loss: tensor(0.3627)\n",
      "14790 Training Loss: tensor(0.3575)\n",
      "14791 Training Loss: tensor(0.3576)\n",
      "14792 Training Loss: tensor(0.3628)\n",
      "14793 Training Loss: tensor(0.3564)\n",
      "14794 Training Loss: tensor(0.3582)\n",
      "14795 Training Loss: tensor(0.3564)\n",
      "14796 Training Loss: tensor(0.3595)\n",
      "14797 Training Loss: tensor(0.3629)\n",
      "14798 Training Loss: tensor(0.3573)\n",
      "14799 Training Loss: tensor(0.3564)\n",
      "14800 Training Loss: tensor(0.3578)\n",
      "14801 Training Loss: tensor(0.3557)\n",
      "14802 Training Loss: tensor(0.3590)\n",
      "14803 Training Loss: tensor(0.3592)\n",
      "14804 Training Loss: tensor(0.3572)\n",
      "14805 Training Loss: tensor(0.3587)\n",
      "14806 Training Loss: tensor(0.3619)\n",
      "14807 Training Loss: tensor(0.3645)\n",
      "14808 Training Loss: tensor(0.3569)\n",
      "14809 Training Loss: tensor(0.3585)\n",
      "14810 Training Loss: tensor(0.3576)\n",
      "14811 Training Loss: tensor(0.3600)\n",
      "14812 Training Loss: tensor(0.3580)\n",
      "14813 Training Loss: tensor(0.3580)\n",
      "14814 Training Loss: tensor(0.3557)\n",
      "14815 Training Loss: tensor(0.3562)\n",
      "14816 Training Loss: tensor(0.3613)\n",
      "14817 Training Loss: tensor(0.3560)\n",
      "14818 Training Loss: tensor(0.3610)\n",
      "14819 Training Loss: tensor(0.3554)\n",
      "14820 Training Loss: tensor(0.3605)\n",
      "14821 Training Loss: tensor(0.3558)\n",
      "14822 Training Loss: tensor(0.3561)\n",
      "14823 Training Loss: tensor(0.3574)\n",
      "14824 Training Loss: tensor(0.3564)\n",
      "14825 Training Loss: tensor(0.3627)\n",
      "14826 Training Loss: tensor(0.3566)\n",
      "14827 Training Loss: tensor(0.3554)\n",
      "14828 Training Loss: tensor(0.3607)\n",
      "14829 Training Loss: tensor(0.3558)\n",
      "14830 Training Loss: tensor(0.3631)\n",
      "14831 Training Loss: tensor(0.3601)\n",
      "14832 Training Loss: tensor(0.3621)\n",
      "14833 Training Loss: tensor(0.3642)\n",
      "14834 Training Loss: tensor(0.3600)\n",
      "14835 Training Loss: tensor(0.3590)\n",
      "14836 Training Loss: tensor(0.3572)\n",
      "14837 Training Loss: tensor(0.3558)\n",
      "14838 Training Loss: tensor(0.3584)\n",
      "14839 Training Loss: tensor(0.3578)\n",
      "14840 Training Loss: tensor(0.3572)\n",
      "14841 Training Loss: tensor(0.3577)\n",
      "14842 Training Loss: tensor(0.3578)\n",
      "14843 Training Loss: tensor(0.3650)\n",
      "14844 Training Loss: tensor(0.3572)\n",
      "14845 Training Loss: tensor(0.3605)\n",
      "14846 Training Loss: tensor(0.3583)\n",
      "14847 Training Loss: tensor(0.3572)\n",
      "14848 Training Loss: tensor(0.3560)\n",
      "14849 Training Loss: tensor(0.3588)\n",
      "14850 Training Loss: tensor(0.3600)\n",
      "14851 Training Loss: tensor(0.3577)\n",
      "14852 Training Loss: tensor(0.3627)\n",
      "14853 Training Loss: tensor(0.3548)\n",
      "14854 Training Loss: tensor(0.3608)\n",
      "14855 Training Loss: tensor(0.3631)\n",
      "14856 Training Loss: tensor(0.3671)\n",
      "14857 Training Loss: tensor(0.3646)\n",
      "14858 Training Loss: tensor(0.3571)\n",
      "14859 Training Loss: tensor(0.3595)\n",
      "14860 Training Loss: tensor(0.3564)\n",
      "14861 Training Loss: tensor(0.3564)\n",
      "14862 Training Loss: tensor(0.3598)\n",
      "14863 Training Loss: tensor(0.3568)\n",
      "14864 Training Loss: tensor(0.3608)\n",
      "14865 Training Loss: tensor(0.3603)\n",
      "14866 Training Loss: tensor(0.3650)\n",
      "14867 Training Loss: tensor(0.3579)\n",
      "14868 Training Loss: tensor(0.3581)\n",
      "14869 Training Loss: tensor(0.3591)\n",
      "14870 Training Loss: tensor(0.3591)\n",
      "14871 Training Loss: tensor(0.3573)\n",
      "14872 Training Loss: tensor(0.3579)\n",
      "14873 Training Loss: tensor(0.3580)\n",
      "14874 Training Loss: tensor(0.3594)\n",
      "14875 Training Loss: tensor(0.3567)\n",
      "14876 Training Loss: tensor(0.3561)\n",
      "14877 Training Loss: tensor(0.3561)\n",
      "14878 Training Loss: tensor(0.3575)\n",
      "14879 Training Loss: tensor(0.3548)\n",
      "14880 Training Loss: tensor(0.3580)\n",
      "14881 Training Loss: tensor(0.3572)\n",
      "14882 Training Loss: tensor(0.3685)\n",
      "14883 Training Loss: tensor(0.3559)\n",
      "14884 Training Loss: tensor(0.3600)\n",
      "14885 Training Loss: tensor(0.3674)\n",
      "14886 Training Loss: tensor(0.3611)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14887 Training Loss: tensor(0.3584)\n",
      "14888 Training Loss: tensor(0.3578)\n",
      "14889 Training Loss: tensor(0.3622)\n",
      "14890 Training Loss: tensor(0.3582)\n",
      "14891 Training Loss: tensor(0.3630)\n",
      "14892 Training Loss: tensor(0.3571)\n",
      "14893 Training Loss: tensor(0.3566)\n",
      "14894 Training Loss: tensor(0.3586)\n",
      "14895 Training Loss: tensor(0.3570)\n",
      "14896 Training Loss: tensor(0.3555)\n",
      "14897 Training Loss: tensor(0.3573)\n",
      "14898 Training Loss: tensor(0.3609)\n",
      "14899 Training Loss: tensor(0.3577)\n",
      "14900 Training Loss: tensor(0.3594)\n",
      "14901 Training Loss: tensor(0.3586)\n",
      "14902 Training Loss: tensor(0.3597)\n",
      "14903 Training Loss: tensor(0.3576)\n",
      "14904 Training Loss: tensor(0.3608)\n",
      "14905 Training Loss: tensor(0.3580)\n",
      "14906 Training Loss: tensor(0.3566)\n",
      "14907 Training Loss: tensor(0.3589)\n",
      "14908 Training Loss: tensor(0.3623)\n",
      "14909 Training Loss: tensor(0.3569)\n",
      "14910 Training Loss: tensor(0.3560)\n",
      "14911 Training Loss: tensor(0.3569)\n",
      "14912 Training Loss: tensor(0.3627)\n",
      "14913 Training Loss: tensor(0.3571)\n",
      "14914 Training Loss: tensor(0.3560)\n",
      "14915 Training Loss: tensor(0.3583)\n",
      "14916 Training Loss: tensor(0.3585)\n",
      "14917 Training Loss: tensor(0.3564)\n",
      "14918 Training Loss: tensor(0.3569)\n",
      "14919 Training Loss: tensor(0.3544)\n",
      "14920 Training Loss: tensor(0.3620)\n",
      "14921 Training Loss: tensor(0.3555)\n",
      "14922 Training Loss: tensor(0.3567)\n",
      "14923 Training Loss: tensor(0.3560)\n",
      "14924 Training Loss: tensor(0.3612)\n",
      "14925 Training Loss: tensor(0.3589)\n",
      "14926 Training Loss: tensor(0.3627)\n",
      "14927 Training Loss: tensor(0.3557)\n",
      "14928 Training Loss: tensor(0.3598)\n",
      "14929 Training Loss: tensor(0.3616)\n",
      "14930 Training Loss: tensor(0.3555)\n",
      "14931 Training Loss: tensor(0.3578)\n",
      "14932 Training Loss: tensor(0.3574)\n",
      "14933 Training Loss: tensor(0.3606)\n",
      "14934 Training Loss: tensor(0.3588)\n",
      "14935 Training Loss: tensor(0.3601)\n",
      "14936 Training Loss: tensor(0.3581)\n",
      "14937 Training Loss: tensor(0.3555)\n",
      "14938 Training Loss: tensor(0.3556)\n",
      "14939 Training Loss: tensor(0.3604)\n",
      "14940 Training Loss: tensor(0.3602)\n",
      "14941 Training Loss: tensor(0.3566)\n",
      "14942 Training Loss: tensor(0.3585)\n",
      "14943 Training Loss: tensor(0.3591)\n",
      "14944 Training Loss: tensor(0.3559)\n",
      "14945 Training Loss: tensor(0.3629)\n",
      "14946 Training Loss: tensor(0.3652)\n",
      "14947 Training Loss: tensor(0.3561)\n",
      "14948 Training Loss: tensor(0.3562)\n",
      "14949 Training Loss: tensor(0.3584)\n",
      "14950 Training Loss: tensor(0.3578)\n",
      "14951 Training Loss: tensor(0.3585)\n",
      "14952 Training Loss: tensor(0.3618)\n",
      "14953 Training Loss: tensor(0.3560)\n",
      "14954 Training Loss: tensor(0.3571)\n",
      "14955 Training Loss: tensor(0.3593)\n",
      "14956 Training Loss: tensor(0.3573)\n",
      "14957 Training Loss: tensor(0.3577)\n",
      "14958 Training Loss: tensor(0.3566)\n",
      "14959 Training Loss: tensor(0.3596)\n",
      "14960 Training Loss: tensor(0.3551)\n",
      "14961 Training Loss: tensor(0.3601)\n",
      "14962 Training Loss: tensor(0.3546)\n",
      "14963 Training Loss: tensor(0.3589)\n",
      "14964 Training Loss: tensor(0.3553)\n",
      "14965 Training Loss: tensor(0.3543)\n",
      "14966 Training Loss: tensor(0.3566)\n",
      "14967 Training Loss: tensor(0.3629)\n",
      "14968 Training Loss: tensor(0.3584)\n",
      "14969 Training Loss: tensor(0.3648)\n",
      "14970 Training Loss: tensor(0.3588)\n",
      "14971 Training Loss: tensor(0.3623)\n",
      "14972 Training Loss: tensor(0.3564)\n",
      "14973 Training Loss: tensor(0.3585)\n",
      "14974 Training Loss: tensor(0.3575)\n",
      "14975 Training Loss: tensor(0.3557)\n",
      "14976 Training Loss: tensor(0.3576)\n",
      "14977 Training Loss: tensor(0.3570)\n",
      "14978 Training Loss: tensor(0.3590)\n",
      "14979 Training Loss: tensor(0.3543)\n",
      "14980 Training Loss: tensor(0.3550)\n",
      "14981 Training Loss: tensor(0.3575)\n",
      "14982 Training Loss: tensor(0.3554)\n",
      "14983 Training Loss: tensor(0.3580)\n",
      "14984 Training Loss: tensor(0.3573)\n",
      "14985 Training Loss: tensor(0.3551)\n",
      "14986 Training Loss: tensor(0.3576)\n",
      "14987 Training Loss: tensor(0.3553)\n",
      "14988 Training Loss: tensor(0.3600)\n",
      "14989 Training Loss: tensor(0.3575)\n",
      "14990 Training Loss: tensor(0.3609)\n",
      "14991 Training Loss: tensor(0.3577)\n",
      "14992 Training Loss: tensor(0.3555)\n",
      "14993 Training Loss: tensor(0.3546)\n",
      "14994 Training Loss: tensor(0.3647)\n",
      "14995 Training Loss: tensor(0.3540)\n",
      "14996 Training Loss: tensor(0.3604)\n",
      "14997 Training Loss: tensor(0.3546)\n",
      "14998 Training Loss: tensor(0.3624)\n",
      "14999 Training Loss: tensor(0.3583)\n",
      "15000 Training Loss: tensor(0.3610)\n",
      "15001 Training Loss: tensor(0.3571)\n",
      "15002 Training Loss: tensor(0.3645)\n",
      "15003 Training Loss: tensor(0.3581)\n",
      "15004 Training Loss: tensor(0.3559)\n",
      "15005 Training Loss: tensor(0.3577)\n",
      "15006 Training Loss: tensor(0.3574)\n",
      "15007 Training Loss: tensor(0.3597)\n",
      "15008 Training Loss: tensor(0.3633)\n",
      "15009 Training Loss: tensor(0.3633)\n",
      "15010 Training Loss: tensor(0.3568)\n",
      "15011 Training Loss: tensor(0.3587)\n",
      "15012 Training Loss: tensor(0.3577)\n",
      "15013 Training Loss: tensor(0.3577)\n",
      "15014 Training Loss: tensor(0.3603)\n",
      "15015 Training Loss: tensor(0.3581)\n",
      "15016 Training Loss: tensor(0.3568)\n",
      "15017 Training Loss: tensor(0.3590)\n",
      "15018 Training Loss: tensor(0.3556)\n",
      "15019 Training Loss: tensor(0.3596)\n",
      "15020 Training Loss: tensor(0.3596)\n",
      "15021 Training Loss: tensor(0.3616)\n",
      "15022 Training Loss: tensor(0.3569)\n",
      "15023 Training Loss: tensor(0.3569)\n",
      "15024 Training Loss: tensor(0.3561)\n",
      "15025 Training Loss: tensor(0.3578)\n",
      "15026 Training Loss: tensor(0.3561)\n",
      "15027 Training Loss: tensor(0.3591)\n",
      "15028 Training Loss: tensor(0.3557)\n",
      "15029 Training Loss: tensor(0.3595)\n",
      "15030 Training Loss: tensor(0.3571)\n",
      "15031 Training Loss: tensor(0.3614)\n",
      "15032 Training Loss: tensor(0.3589)\n",
      "15033 Training Loss: tensor(0.3556)\n",
      "15034 Training Loss: tensor(0.3625)\n",
      "15035 Training Loss: tensor(0.3574)\n",
      "15036 Training Loss: tensor(0.3634)\n",
      "15037 Training Loss: tensor(0.3588)\n",
      "15038 Training Loss: tensor(0.3569)\n",
      "15039 Training Loss: tensor(0.3564)\n",
      "15040 Training Loss: tensor(0.3635)\n",
      "15041 Training Loss: tensor(0.3559)\n",
      "15042 Training Loss: tensor(0.3562)\n",
      "15043 Training Loss: tensor(0.3557)\n",
      "15044 Training Loss: tensor(0.3569)\n",
      "15045 Training Loss: tensor(0.3629)\n",
      "15046 Training Loss: tensor(0.3616)\n",
      "15047 Training Loss: tensor(0.3616)\n",
      "15048 Training Loss: tensor(0.3551)\n",
      "15049 Training Loss: tensor(0.3714)\n",
      "15050 Training Loss: tensor(0.3574)\n",
      "15051 Training Loss: tensor(0.3575)\n",
      "15052 Training Loss: tensor(0.3577)\n",
      "15053 Training Loss: tensor(0.3592)\n",
      "15054 Training Loss: tensor(0.3582)\n",
      "15055 Training Loss: tensor(0.3596)\n",
      "15056 Training Loss: tensor(0.3588)\n",
      "15057 Training Loss: tensor(0.3569)\n",
      "15058 Training Loss: tensor(0.3587)\n",
      "15059 Training Loss: tensor(0.3583)\n",
      "15060 Training Loss: tensor(0.3598)\n",
      "15061 Training Loss: tensor(0.3557)\n",
      "15062 Training Loss: tensor(0.3556)\n",
      "15063 Training Loss: tensor(0.3627)\n",
      "15064 Training Loss: tensor(0.3575)\n",
      "15065 Training Loss: tensor(0.3560)\n",
      "15066 Training Loss: tensor(0.3589)\n",
      "15067 Training Loss: tensor(0.3576)\n",
      "15068 Training Loss: tensor(0.3573)\n",
      "15069 Training Loss: tensor(0.3595)\n",
      "15070 Training Loss: tensor(0.3565)\n",
      "15071 Training Loss: tensor(0.3584)\n",
      "15072 Training Loss: tensor(0.3600)\n",
      "15073 Training Loss: tensor(0.3569)\n",
      "15074 Training Loss: tensor(0.3603)\n",
      "15075 Training Loss: tensor(0.3590)\n",
      "15076 Training Loss: tensor(0.3561)\n",
      "15077 Training Loss: tensor(0.3568)\n",
      "15078 Training Loss: tensor(0.3577)\n",
      "15079 Training Loss: tensor(0.3564)\n",
      "15080 Training Loss: tensor(0.3634)\n",
      "15081 Training Loss: tensor(0.3585)\n",
      "15082 Training Loss: tensor(0.3595)\n",
      "15083 Training Loss: tensor(0.3595)\n",
      "15084 Training Loss: tensor(0.3575)\n",
      "15085 Training Loss: tensor(0.3570)\n",
      "15086 Training Loss: tensor(0.3624)\n",
      "15087 Training Loss: tensor(0.3560)\n",
      "15088 Training Loss: tensor(0.3570)\n",
      "15089 Training Loss: tensor(0.3565)\n",
      "15090 Training Loss: tensor(0.3569)\n",
      "15091 Training Loss: tensor(0.3554)\n",
      "15092 Training Loss: tensor(0.3617)\n",
      "15093 Training Loss: tensor(0.3585)\n",
      "15094 Training Loss: tensor(0.3586)\n",
      "15095 Training Loss: tensor(0.3560)\n",
      "15096 Training Loss: tensor(0.3565)\n",
      "15097 Training Loss: tensor(0.3600)\n",
      "15098 Training Loss: tensor(0.3596)\n",
      "15099 Training Loss: tensor(0.3552)\n",
      "15100 Training Loss: tensor(0.3541)\n",
      "15101 Training Loss: tensor(0.3635)\n",
      "15102 Training Loss: tensor(0.3605)\n",
      "15103 Training Loss: tensor(0.3561)\n",
      "15104 Training Loss: tensor(0.3548)\n",
      "15105 Training Loss: tensor(0.3587)\n",
      "15106 Training Loss: tensor(0.3563)\n",
      "15107 Training Loss: tensor(0.3546)\n",
      "15108 Training Loss: tensor(0.3559)\n",
      "15109 Training Loss: tensor(0.3575)\n",
      "15110 Training Loss: tensor(0.3594)\n",
      "15111 Training Loss: tensor(0.3613)\n",
      "15112 Training Loss: tensor(0.3566)\n",
      "15113 Training Loss: tensor(0.3593)\n",
      "15114 Training Loss: tensor(0.3577)\n",
      "15115 Training Loss: tensor(0.3575)\n",
      "15116 Training Loss: tensor(0.3549)\n",
      "15117 Training Loss: tensor(0.3570)\n",
      "15118 Training Loss: tensor(0.3555)\n",
      "15119 Training Loss: tensor(0.3555)\n",
      "15120 Training Loss: tensor(0.3625)\n",
      "15121 Training Loss: tensor(0.3669)\n",
      "15122 Training Loss: tensor(0.3565)\n",
      "15123 Training Loss: tensor(0.3597)\n",
      "15124 Training Loss: tensor(0.3571)\n",
      "15125 Training Loss: tensor(0.3561)\n",
      "15126 Training Loss: tensor(0.3585)\n",
      "15127 Training Loss: tensor(0.3554)\n",
      "15128 Training Loss: tensor(0.3552)\n",
      "15129 Training Loss: tensor(0.3556)\n",
      "15130 Training Loss: tensor(0.3585)\n",
      "15131 Training Loss: tensor(0.3647)\n",
      "15132 Training Loss: tensor(0.3579)\n",
      "15133 Training Loss: tensor(0.3566)\n",
      "15134 Training Loss: tensor(0.3551)\n",
      "15135 Training Loss: tensor(0.3557)\n",
      "15136 Training Loss: tensor(0.3602)\n",
      "15137 Training Loss: tensor(0.3604)\n",
      "15138 Training Loss: tensor(0.3613)\n",
      "15139 Training Loss: tensor(0.3560)\n",
      "15140 Training Loss: tensor(0.3552)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15141 Training Loss: tensor(0.3544)\n",
      "15142 Training Loss: tensor(0.3560)\n",
      "15143 Training Loss: tensor(0.3602)\n",
      "15144 Training Loss: tensor(0.3599)\n",
      "15145 Training Loss: tensor(0.3608)\n",
      "15146 Training Loss: tensor(0.3580)\n",
      "15147 Training Loss: tensor(0.3573)\n",
      "15148 Training Loss: tensor(0.3575)\n",
      "15149 Training Loss: tensor(0.3574)\n",
      "15150 Training Loss: tensor(0.3556)\n",
      "15151 Training Loss: tensor(0.3574)\n",
      "15152 Training Loss: tensor(0.3586)\n",
      "15153 Training Loss: tensor(0.3607)\n",
      "15154 Training Loss: tensor(0.3576)\n",
      "15155 Training Loss: tensor(0.3601)\n",
      "15156 Training Loss: tensor(0.3621)\n",
      "15157 Training Loss: tensor(0.3554)\n",
      "15158 Training Loss: tensor(0.3556)\n",
      "15159 Training Loss: tensor(0.3572)\n",
      "15160 Training Loss: tensor(0.3636)\n",
      "15161 Training Loss: tensor(0.3561)\n",
      "15162 Training Loss: tensor(0.3569)\n",
      "15163 Training Loss: tensor(0.3562)\n",
      "15164 Training Loss: tensor(0.3644)\n",
      "15165 Training Loss: tensor(0.3576)\n",
      "15166 Training Loss: tensor(0.3562)\n",
      "15167 Training Loss: tensor(0.3573)\n",
      "15168 Training Loss: tensor(0.3588)\n",
      "15169 Training Loss: tensor(0.3563)\n",
      "15170 Training Loss: tensor(0.3588)\n",
      "15171 Training Loss: tensor(0.3573)\n",
      "15172 Training Loss: tensor(0.3568)\n",
      "15173 Training Loss: tensor(0.3565)\n",
      "15174 Training Loss: tensor(0.3553)\n",
      "15175 Training Loss: tensor(0.3550)\n",
      "15176 Training Loss: tensor(0.3577)\n",
      "15177 Training Loss: tensor(0.3600)\n",
      "15178 Training Loss: tensor(0.3564)\n",
      "15179 Training Loss: tensor(0.3650)\n",
      "15180 Training Loss: tensor(0.3562)\n",
      "15181 Training Loss: tensor(0.3563)\n",
      "15182 Training Loss: tensor(0.3586)\n",
      "15183 Training Loss: tensor(0.3540)\n",
      "15184 Training Loss: tensor(0.3585)\n",
      "15185 Training Loss: tensor(0.3593)\n",
      "15186 Training Loss: tensor(0.3594)\n",
      "15187 Training Loss: tensor(0.3555)\n",
      "15188 Training Loss: tensor(0.3578)\n",
      "15189 Training Loss: tensor(0.3632)\n",
      "15190 Training Loss: tensor(0.3564)\n",
      "15191 Training Loss: tensor(0.3553)\n",
      "15192 Training Loss: tensor(0.3566)\n",
      "15193 Training Loss: tensor(0.3565)\n",
      "15194 Training Loss: tensor(0.3541)\n",
      "15195 Training Loss: tensor(0.3607)\n",
      "15196 Training Loss: tensor(0.3568)\n",
      "15197 Training Loss: tensor(0.3582)\n",
      "15198 Training Loss: tensor(0.3558)\n",
      "15199 Training Loss: tensor(0.3575)\n",
      "15200 Training Loss: tensor(0.3622)\n",
      "15201 Training Loss: tensor(0.3576)\n",
      "15202 Training Loss: tensor(0.3552)\n",
      "15203 Training Loss: tensor(0.3545)\n",
      "15204 Training Loss: tensor(0.3609)\n",
      "15205 Training Loss: tensor(0.3589)\n",
      "15206 Training Loss: tensor(0.3603)\n",
      "15207 Training Loss: tensor(0.3611)\n",
      "15208 Training Loss: tensor(0.3581)\n",
      "15209 Training Loss: tensor(0.3563)\n",
      "15210 Training Loss: tensor(0.3555)\n",
      "15211 Training Loss: tensor(0.3565)\n",
      "15212 Training Loss: tensor(0.3572)\n",
      "15213 Training Loss: tensor(0.3570)\n",
      "15214 Training Loss: tensor(0.3557)\n",
      "15215 Training Loss: tensor(0.3554)\n",
      "15216 Training Loss: tensor(0.3568)\n",
      "15217 Training Loss: tensor(0.3547)\n",
      "15218 Training Loss: tensor(0.3573)\n",
      "15219 Training Loss: tensor(0.3542)\n",
      "15220 Training Loss: tensor(0.3617)\n",
      "15221 Training Loss: tensor(0.3574)\n",
      "15222 Training Loss: tensor(0.3595)\n",
      "15223 Training Loss: tensor(0.3544)\n",
      "15224 Training Loss: tensor(0.3566)\n",
      "15225 Training Loss: tensor(0.3547)\n",
      "15226 Training Loss: tensor(0.3610)\n",
      "15227 Training Loss: tensor(0.3554)\n",
      "15228 Training Loss: tensor(0.3568)\n",
      "15229 Training Loss: tensor(0.3567)\n",
      "15230 Training Loss: tensor(0.3576)\n",
      "15231 Training Loss: tensor(0.3637)\n",
      "15232 Training Loss: tensor(0.3537)\n",
      "15233 Training Loss: tensor(0.3546)\n",
      "15234 Training Loss: tensor(0.3554)\n",
      "15235 Training Loss: tensor(0.3603)\n",
      "15236 Training Loss: tensor(0.3534)\n",
      "15237 Training Loss: tensor(0.3601)\n",
      "15238 Training Loss: tensor(0.3539)\n",
      "15239 Training Loss: tensor(0.3546)\n",
      "15240 Training Loss: tensor(0.3596)\n",
      "15241 Training Loss: tensor(0.3563)\n",
      "15242 Training Loss: tensor(0.3567)\n",
      "15243 Training Loss: tensor(0.3589)\n",
      "15244 Training Loss: tensor(0.3607)\n",
      "15245 Training Loss: tensor(0.3537)\n",
      "15246 Training Loss: tensor(0.3625)\n",
      "15247 Training Loss: tensor(0.3619)\n",
      "15248 Training Loss: tensor(0.3571)\n",
      "15249 Training Loss: tensor(0.3562)\n",
      "15250 Training Loss: tensor(0.3549)\n",
      "15251 Training Loss: tensor(0.3573)\n",
      "15252 Training Loss: tensor(0.3552)\n",
      "15253 Training Loss: tensor(0.3548)\n",
      "15254 Training Loss: tensor(0.3556)\n",
      "15255 Training Loss: tensor(0.3561)\n",
      "15256 Training Loss: tensor(0.3612)\n",
      "15257 Training Loss: tensor(0.3558)\n",
      "15258 Training Loss: tensor(0.3541)\n",
      "15259 Training Loss: tensor(0.3601)\n",
      "15260 Training Loss: tensor(0.3562)\n",
      "15261 Training Loss: tensor(0.3578)\n",
      "15262 Training Loss: tensor(0.3689)\n",
      "15263 Training Loss: tensor(0.3544)\n",
      "15264 Training Loss: tensor(0.3540)\n",
      "15265 Training Loss: tensor(0.3543)\n",
      "15266 Training Loss: tensor(0.3644)\n",
      "15267 Training Loss: tensor(0.3590)\n",
      "15268 Training Loss: tensor(0.3589)\n",
      "15269 Training Loss: tensor(0.3569)\n",
      "15270 Training Loss: tensor(0.3550)\n",
      "15271 Training Loss: tensor(0.3557)\n",
      "15272 Training Loss: tensor(0.3564)\n",
      "15273 Training Loss: tensor(0.3567)\n",
      "15274 Training Loss: tensor(0.3584)\n",
      "15275 Training Loss: tensor(0.3568)\n",
      "15276 Training Loss: tensor(0.3584)\n",
      "15277 Training Loss: tensor(0.3570)\n",
      "15278 Training Loss: tensor(0.3584)\n",
      "15279 Training Loss: tensor(0.3595)\n",
      "15280 Training Loss: tensor(0.3575)\n",
      "15281 Training Loss: tensor(0.3554)\n",
      "15282 Training Loss: tensor(0.3544)\n",
      "15283 Training Loss: tensor(0.3550)\n",
      "15284 Training Loss: tensor(0.3578)\n",
      "15285 Training Loss: tensor(0.3565)\n",
      "15286 Training Loss: tensor(0.3554)\n",
      "15287 Training Loss: tensor(0.3556)\n",
      "15288 Training Loss: tensor(0.3544)\n",
      "15289 Training Loss: tensor(0.3581)\n",
      "15290 Training Loss: tensor(0.3622)\n",
      "15291 Training Loss: tensor(0.3555)\n",
      "15292 Training Loss: tensor(0.3562)\n",
      "15293 Training Loss: tensor(0.3557)\n",
      "15294 Training Loss: tensor(0.3585)\n",
      "15295 Training Loss: tensor(0.3552)\n",
      "15296 Training Loss: tensor(0.3542)\n",
      "15297 Training Loss: tensor(0.3549)\n",
      "15298 Training Loss: tensor(0.3576)\n",
      "15299 Training Loss: tensor(0.3567)\n",
      "15300 Training Loss: tensor(0.3577)\n",
      "15301 Training Loss: tensor(0.3636)\n",
      "15302 Training Loss: tensor(0.3662)\n",
      "15303 Training Loss: tensor(0.3538)\n",
      "15304 Training Loss: tensor(0.3622)\n",
      "15305 Training Loss: tensor(0.3570)\n",
      "15306 Training Loss: tensor(0.3579)\n",
      "15307 Training Loss: tensor(0.3590)\n",
      "15308 Training Loss: tensor(0.3575)\n",
      "15309 Training Loss: tensor(0.3622)\n",
      "15310 Training Loss: tensor(0.3598)\n",
      "15311 Training Loss: tensor(0.3580)\n",
      "15312 Training Loss: tensor(0.3586)\n",
      "15313 Training Loss: tensor(0.3601)\n",
      "15314 Training Loss: tensor(0.3611)\n",
      "15315 Training Loss: tensor(0.3580)\n",
      "15316 Training Loss: tensor(0.3562)\n",
      "15317 Training Loss: tensor(0.3572)\n",
      "15318 Training Loss: tensor(0.3568)\n",
      "15319 Training Loss: tensor(0.3576)\n",
      "15320 Training Loss: tensor(0.3601)\n",
      "15321 Training Loss: tensor(0.3587)\n",
      "15322 Training Loss: tensor(0.3578)\n",
      "15323 Training Loss: tensor(0.3597)\n",
      "15324 Training Loss: tensor(0.3573)\n",
      "15325 Training Loss: tensor(0.3562)\n",
      "15326 Training Loss: tensor(0.3554)\n",
      "15327 Training Loss: tensor(0.3575)\n",
      "15328 Training Loss: tensor(0.3571)\n",
      "15329 Training Loss: tensor(0.3552)\n",
      "15330 Training Loss: tensor(0.3599)\n",
      "15331 Training Loss: tensor(0.3564)\n",
      "15332 Training Loss: tensor(0.3608)\n",
      "15333 Training Loss: tensor(0.3580)\n",
      "15334 Training Loss: tensor(0.3575)\n",
      "15335 Training Loss: tensor(0.3596)\n",
      "15336 Training Loss: tensor(0.3542)\n",
      "15337 Training Loss: tensor(0.3537)\n",
      "15338 Training Loss: tensor(0.3590)\n",
      "15339 Training Loss: tensor(0.3540)\n",
      "15340 Training Loss: tensor(0.3547)\n",
      "15341 Training Loss: tensor(0.3548)\n",
      "15342 Training Loss: tensor(0.3550)\n",
      "15343 Training Loss: tensor(0.3548)\n",
      "15344 Training Loss: tensor(0.3547)\n",
      "15345 Training Loss: tensor(0.3543)\n",
      "15346 Training Loss: tensor(0.3620)\n",
      "15347 Training Loss: tensor(0.3611)\n",
      "15348 Training Loss: tensor(0.3532)\n",
      "15349 Training Loss: tensor(0.3552)\n",
      "15350 Training Loss: tensor(0.3549)\n",
      "15351 Training Loss: tensor(0.3574)\n",
      "15352 Training Loss: tensor(0.3606)\n",
      "15353 Training Loss: tensor(0.3536)\n",
      "15354 Training Loss: tensor(0.3553)\n",
      "15355 Training Loss: tensor(0.3544)\n",
      "15356 Training Loss: tensor(0.3599)\n",
      "15357 Training Loss: tensor(0.3642)\n",
      "15358 Training Loss: tensor(0.3584)\n",
      "15359 Training Loss: tensor(0.3553)\n",
      "15360 Training Loss: tensor(0.3564)\n",
      "15361 Training Loss: tensor(0.3549)\n",
      "15362 Training Loss: tensor(0.3540)\n",
      "15363 Training Loss: tensor(0.3581)\n",
      "15364 Training Loss: tensor(0.3573)\n",
      "15365 Training Loss: tensor(0.3532)\n",
      "15366 Training Loss: tensor(0.3580)\n",
      "15367 Training Loss: tensor(0.3558)\n",
      "15368 Training Loss: tensor(0.3552)\n",
      "15369 Training Loss: tensor(0.3629)\n",
      "15370 Training Loss: tensor(0.3557)\n",
      "15371 Training Loss: tensor(0.3615)\n",
      "15372 Training Loss: tensor(0.3547)\n",
      "15373 Training Loss: tensor(0.3569)\n",
      "15374 Training Loss: tensor(0.3607)\n",
      "15375 Training Loss: tensor(0.3563)\n",
      "15376 Training Loss: tensor(0.3540)\n",
      "15377 Training Loss: tensor(0.3606)\n",
      "15378 Training Loss: tensor(0.3550)\n",
      "15379 Training Loss: tensor(0.3662)\n",
      "15380 Training Loss: tensor(0.3592)\n",
      "15381 Training Loss: tensor(0.3552)\n",
      "15382 Training Loss: tensor(0.3553)\n",
      "15383 Training Loss: tensor(0.3592)\n",
      "15384 Training Loss: tensor(0.3568)\n",
      "15385 Training Loss: tensor(0.3547)\n",
      "15386 Training Loss: tensor(0.3620)\n",
      "15387 Training Loss: tensor(0.3593)\n",
      "15388 Training Loss: tensor(0.3589)\n",
      "15389 Training Loss: tensor(0.3584)\n",
      "15390 Training Loss: tensor(0.3605)\n",
      "15391 Training Loss: tensor(0.3559)\n",
      "15392 Training Loss: tensor(0.3584)\n",
      "15393 Training Loss: tensor(0.3561)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15394 Training Loss: tensor(0.3555)\n",
      "15395 Training Loss: tensor(0.3573)\n",
      "15396 Training Loss: tensor(0.3570)\n",
      "15397 Training Loss: tensor(0.3572)\n",
      "15398 Training Loss: tensor(0.3585)\n",
      "15399 Training Loss: tensor(0.3575)\n",
      "15400 Training Loss: tensor(0.3620)\n",
      "15401 Training Loss: tensor(0.3560)\n",
      "15402 Training Loss: tensor(0.3577)\n",
      "15403 Training Loss: tensor(0.3579)\n",
      "15404 Training Loss: tensor(0.3594)\n",
      "15405 Training Loss: tensor(0.3583)\n",
      "15406 Training Loss: tensor(0.3555)\n",
      "15407 Training Loss: tensor(0.3587)\n",
      "15408 Training Loss: tensor(0.3572)\n",
      "15409 Training Loss: tensor(0.3589)\n",
      "15410 Training Loss: tensor(0.3557)\n",
      "15411 Training Loss: tensor(0.3553)\n",
      "15412 Training Loss: tensor(0.3555)\n",
      "15413 Training Loss: tensor(0.3578)\n",
      "15414 Training Loss: tensor(0.3566)\n",
      "15415 Training Loss: tensor(0.3548)\n",
      "15416 Training Loss: tensor(0.3548)\n",
      "15417 Training Loss: tensor(0.3618)\n",
      "15418 Training Loss: tensor(0.3561)\n",
      "15419 Training Loss: tensor(0.3536)\n",
      "15420 Training Loss: tensor(0.3570)\n",
      "15421 Training Loss: tensor(0.3627)\n",
      "15422 Training Loss: tensor(0.3554)\n",
      "15423 Training Loss: tensor(0.3566)\n",
      "15424 Training Loss: tensor(0.3538)\n",
      "15425 Training Loss: tensor(0.3548)\n",
      "15426 Training Loss: tensor(0.3541)\n",
      "15427 Training Loss: tensor(0.3555)\n",
      "15428 Training Loss: tensor(0.3651)\n",
      "15429 Training Loss: tensor(0.3582)\n",
      "15430 Training Loss: tensor(0.3548)\n",
      "15431 Training Loss: tensor(0.3548)\n",
      "15432 Training Loss: tensor(0.3577)\n",
      "15433 Training Loss: tensor(0.3642)\n",
      "15434 Training Loss: tensor(0.3540)\n",
      "15435 Training Loss: tensor(0.3541)\n",
      "15436 Training Loss: tensor(0.3581)\n",
      "15437 Training Loss: tensor(0.3612)\n",
      "15438 Training Loss: tensor(0.3545)\n",
      "15439 Training Loss: tensor(0.3582)\n",
      "15440 Training Loss: tensor(0.3563)\n",
      "15441 Training Loss: tensor(0.3578)\n",
      "15442 Training Loss: tensor(0.3550)\n",
      "15443 Training Loss: tensor(0.3608)\n",
      "15444 Training Loss: tensor(0.3561)\n",
      "15445 Training Loss: tensor(0.3567)\n",
      "15446 Training Loss: tensor(0.3554)\n",
      "15447 Training Loss: tensor(0.3574)\n",
      "15448 Training Loss: tensor(0.3570)\n",
      "15449 Training Loss: tensor(0.3608)\n",
      "15450 Training Loss: tensor(0.3569)\n",
      "15451 Training Loss: tensor(0.3570)\n",
      "15452 Training Loss: tensor(0.3566)\n",
      "15453 Training Loss: tensor(0.3559)\n",
      "15454 Training Loss: tensor(0.3547)\n",
      "15455 Training Loss: tensor(0.3582)\n",
      "15456 Training Loss: tensor(0.3576)\n",
      "15457 Training Loss: tensor(0.3613)\n",
      "15458 Training Loss: tensor(0.3540)\n",
      "15459 Training Loss: tensor(0.3603)\n",
      "15460 Training Loss: tensor(0.3575)\n",
      "15461 Training Loss: tensor(0.3565)\n",
      "15462 Training Loss: tensor(0.3573)\n",
      "15463 Training Loss: tensor(0.3546)\n",
      "15464 Training Loss: tensor(0.3553)\n",
      "15465 Training Loss: tensor(0.3563)\n",
      "15466 Training Loss: tensor(0.3547)\n",
      "15467 Training Loss: tensor(0.3582)\n",
      "15468 Training Loss: tensor(0.3598)\n",
      "15469 Training Loss: tensor(0.3627)\n",
      "15470 Training Loss: tensor(0.3597)\n",
      "15471 Training Loss: tensor(0.3605)\n",
      "15472 Training Loss: tensor(0.3555)\n",
      "15473 Training Loss: tensor(0.3585)\n",
      "15474 Training Loss: tensor(0.3574)\n",
      "15475 Training Loss: tensor(0.3624)\n",
      "15476 Training Loss: tensor(0.3587)\n",
      "15477 Training Loss: tensor(0.3578)\n",
      "15478 Training Loss: tensor(0.3557)\n",
      "15479 Training Loss: tensor(0.3556)\n",
      "15480 Training Loss: tensor(0.3578)\n",
      "15481 Training Loss: tensor(0.3604)\n",
      "15482 Training Loss: tensor(0.3586)\n",
      "15483 Training Loss: tensor(0.3550)\n",
      "15484 Training Loss: tensor(0.3557)\n",
      "15485 Training Loss: tensor(0.3559)\n",
      "15486 Training Loss: tensor(0.3579)\n",
      "15487 Training Loss: tensor(0.3618)\n",
      "15488 Training Loss: tensor(0.3574)\n",
      "15489 Training Loss: tensor(0.3612)\n",
      "15490 Training Loss: tensor(0.3618)\n",
      "15491 Training Loss: tensor(0.3596)\n",
      "15492 Training Loss: tensor(0.3553)\n",
      "15493 Training Loss: tensor(0.3565)\n",
      "15494 Training Loss: tensor(0.3602)\n",
      "15495 Training Loss: tensor(0.3555)\n",
      "15496 Training Loss: tensor(0.3594)\n",
      "15497 Training Loss: tensor(0.3570)\n",
      "15498 Training Loss: tensor(0.3615)\n",
      "15499 Training Loss: tensor(0.3556)\n",
      "15500 Training Loss: tensor(0.3545)\n",
      "15501 Training Loss: tensor(0.3587)\n",
      "15502 Training Loss: tensor(0.3617)\n",
      "15503 Training Loss: tensor(0.3639)\n",
      "15504 Training Loss: tensor(0.3592)\n",
      "15505 Training Loss: tensor(0.3560)\n",
      "15506 Training Loss: tensor(0.3557)\n",
      "15507 Training Loss: tensor(0.3550)\n",
      "15508 Training Loss: tensor(0.3556)\n",
      "15509 Training Loss: tensor(0.3559)\n",
      "15510 Training Loss: tensor(0.3563)\n",
      "15511 Training Loss: tensor(0.3551)\n",
      "15512 Training Loss: tensor(0.3609)\n",
      "15513 Training Loss: tensor(0.3559)\n",
      "15514 Training Loss: tensor(0.3578)\n",
      "15515 Training Loss: tensor(0.3586)\n",
      "15516 Training Loss: tensor(0.3542)\n",
      "15517 Training Loss: tensor(0.3578)\n",
      "15518 Training Loss: tensor(0.3563)\n",
      "15519 Training Loss: tensor(0.3539)\n",
      "15520 Training Loss: tensor(0.3556)\n",
      "15521 Training Loss: tensor(0.3534)\n",
      "15522 Training Loss: tensor(0.3554)\n",
      "15523 Training Loss: tensor(0.3592)\n",
      "15524 Training Loss: tensor(0.3703)\n",
      "15525 Training Loss: tensor(0.3610)\n",
      "15526 Training Loss: tensor(0.3591)\n",
      "15527 Training Loss: tensor(0.3571)\n",
      "15528 Training Loss: tensor(0.3563)\n",
      "15529 Training Loss: tensor(0.3624)\n",
      "15530 Training Loss: tensor(0.3584)\n",
      "15531 Training Loss: tensor(0.3572)\n",
      "15532 Training Loss: tensor(0.3573)\n",
      "15533 Training Loss: tensor(0.3637)\n",
      "15534 Training Loss: tensor(0.3567)\n",
      "15535 Training Loss: tensor(0.3582)\n",
      "15536 Training Loss: tensor(0.3614)\n",
      "15537 Training Loss: tensor(0.3579)\n",
      "15538 Training Loss: tensor(0.3552)\n",
      "15539 Training Loss: tensor(0.3560)\n",
      "15540 Training Loss: tensor(0.3589)\n",
      "15541 Training Loss: tensor(0.3589)\n",
      "15542 Training Loss: tensor(0.3592)\n",
      "15543 Training Loss: tensor(0.3554)\n",
      "15544 Training Loss: tensor(0.3614)\n",
      "15545 Training Loss: tensor(0.3563)\n",
      "15546 Training Loss: tensor(0.3549)\n",
      "15547 Training Loss: tensor(0.3562)\n",
      "15548 Training Loss: tensor(0.3584)\n",
      "15549 Training Loss: tensor(0.3568)\n",
      "15550 Training Loss: tensor(0.3552)\n",
      "15551 Training Loss: tensor(0.3593)\n",
      "15552 Training Loss: tensor(0.3557)\n",
      "15553 Training Loss: tensor(0.3578)\n",
      "15554 Training Loss: tensor(0.3575)\n",
      "15555 Training Loss: tensor(0.3550)\n",
      "15556 Training Loss: tensor(0.3578)\n",
      "15557 Training Loss: tensor(0.3607)\n",
      "15558 Training Loss: tensor(0.3624)\n",
      "15559 Training Loss: tensor(0.3558)\n",
      "15560 Training Loss: tensor(0.3572)\n",
      "15561 Training Loss: tensor(0.3600)\n",
      "15562 Training Loss: tensor(0.3538)\n",
      "15563 Training Loss: tensor(0.3561)\n",
      "15564 Training Loss: tensor(0.3600)\n",
      "15565 Training Loss: tensor(0.3631)\n",
      "15566 Training Loss: tensor(0.3547)\n",
      "15567 Training Loss: tensor(0.3562)\n",
      "15568 Training Loss: tensor(0.3567)\n",
      "15569 Training Loss: tensor(0.3584)\n",
      "15570 Training Loss: tensor(0.3573)\n",
      "15571 Training Loss: tensor(0.3558)\n",
      "15572 Training Loss: tensor(0.3569)\n",
      "15573 Training Loss: tensor(0.3549)\n",
      "15574 Training Loss: tensor(0.3584)\n",
      "15575 Training Loss: tensor(0.3554)\n",
      "15576 Training Loss: tensor(0.3547)\n",
      "15577 Training Loss: tensor(0.3567)\n",
      "15578 Training Loss: tensor(0.3547)\n",
      "15579 Training Loss: tensor(0.3569)\n",
      "15580 Training Loss: tensor(0.3546)\n",
      "15581 Training Loss: tensor(0.3575)\n",
      "15582 Training Loss: tensor(0.3585)\n",
      "15583 Training Loss: tensor(0.3530)\n",
      "15584 Training Loss: tensor(0.3603)\n",
      "15585 Training Loss: tensor(0.3550)\n",
      "15586 Training Loss: tensor(0.3553)\n",
      "15587 Training Loss: tensor(0.3579)\n",
      "15588 Training Loss: tensor(0.3587)\n",
      "15589 Training Loss: tensor(0.3542)\n",
      "15590 Training Loss: tensor(0.3608)\n",
      "15591 Training Loss: tensor(0.3549)\n",
      "15592 Training Loss: tensor(0.3549)\n",
      "15593 Training Loss: tensor(0.3547)\n",
      "15594 Training Loss: tensor(0.3550)\n",
      "15595 Training Loss: tensor(0.3569)\n",
      "15596 Training Loss: tensor(0.3660)\n",
      "15597 Training Loss: tensor(0.3532)\n",
      "15598 Training Loss: tensor(0.3559)\n",
      "15599 Training Loss: tensor(0.3547)\n",
      "15600 Training Loss: tensor(0.3613)\n",
      "15601 Training Loss: tensor(0.3546)\n",
      "15602 Training Loss: tensor(0.3567)\n",
      "15603 Training Loss: tensor(0.3589)\n",
      "15604 Training Loss: tensor(0.3588)\n",
      "15605 Training Loss: tensor(0.3540)\n",
      "15606 Training Loss: tensor(0.3545)\n",
      "15607 Training Loss: tensor(0.3543)\n",
      "15608 Training Loss: tensor(0.3627)\n",
      "15609 Training Loss: tensor(0.3577)\n",
      "15610 Training Loss: tensor(0.3553)\n",
      "15611 Training Loss: tensor(0.3663)\n",
      "15612 Training Loss: tensor(0.3569)\n",
      "15613 Training Loss: tensor(0.3556)\n",
      "15614 Training Loss: tensor(0.3562)\n",
      "15615 Training Loss: tensor(0.3569)\n",
      "15616 Training Loss: tensor(0.3566)\n",
      "15617 Training Loss: tensor(0.3545)\n",
      "15618 Training Loss: tensor(0.3601)\n",
      "15619 Training Loss: tensor(0.3600)\n",
      "15620 Training Loss: tensor(0.3545)\n",
      "15621 Training Loss: tensor(0.3609)\n",
      "15622 Training Loss: tensor(0.3560)\n",
      "15623 Training Loss: tensor(0.3548)\n",
      "15624 Training Loss: tensor(0.3564)\n",
      "15625 Training Loss: tensor(0.3534)\n",
      "15626 Training Loss: tensor(0.3561)\n",
      "15627 Training Loss: tensor(0.3549)\n",
      "15628 Training Loss: tensor(0.3563)\n",
      "15629 Training Loss: tensor(0.3539)\n",
      "15630 Training Loss: tensor(0.3537)\n",
      "15631 Training Loss: tensor(0.3559)\n",
      "15632 Training Loss: tensor(0.3552)\n",
      "15633 Training Loss: tensor(0.3552)\n",
      "15634 Training Loss: tensor(0.3562)\n",
      "15635 Training Loss: tensor(0.3571)\n",
      "15636 Training Loss: tensor(0.3548)\n",
      "15637 Training Loss: tensor(0.3553)\n",
      "15638 Training Loss: tensor(0.3567)\n",
      "15639 Training Loss: tensor(0.3594)\n",
      "15640 Training Loss: tensor(0.3554)\n",
      "15641 Training Loss: tensor(0.3556)\n",
      "15642 Training Loss: tensor(0.3586)\n",
      "15643 Training Loss: tensor(0.3598)\n",
      "15644 Training Loss: tensor(0.3580)\n",
      "15645 Training Loss: tensor(0.3547)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15646 Training Loss: tensor(0.3538)\n",
      "15647 Training Loss: tensor(0.3559)\n",
      "15648 Training Loss: tensor(0.3633)\n",
      "15649 Training Loss: tensor(0.3564)\n",
      "15650 Training Loss: tensor(0.3590)\n",
      "15651 Training Loss: tensor(0.3563)\n",
      "15652 Training Loss: tensor(0.3562)\n",
      "15653 Training Loss: tensor(0.3559)\n",
      "15654 Training Loss: tensor(0.3546)\n",
      "15655 Training Loss: tensor(0.3571)\n",
      "15656 Training Loss: tensor(0.3563)\n",
      "15657 Training Loss: tensor(0.3541)\n",
      "15658 Training Loss: tensor(0.3565)\n",
      "15659 Training Loss: tensor(0.3570)\n",
      "15660 Training Loss: tensor(0.3563)\n",
      "15661 Training Loss: tensor(0.3564)\n",
      "15662 Training Loss: tensor(0.3532)\n",
      "15663 Training Loss: tensor(0.3655)\n",
      "15664 Training Loss: tensor(0.3575)\n",
      "15665 Training Loss: tensor(0.3533)\n",
      "15666 Training Loss: tensor(0.3591)\n",
      "15667 Training Loss: tensor(0.3590)\n",
      "15668 Training Loss: tensor(0.3644)\n",
      "15669 Training Loss: tensor(0.3534)\n",
      "15670 Training Loss: tensor(0.3539)\n",
      "15671 Training Loss: tensor(0.3567)\n",
      "15672 Training Loss: tensor(0.3602)\n",
      "15673 Training Loss: tensor(0.3567)\n",
      "15674 Training Loss: tensor(0.3548)\n",
      "15675 Training Loss: tensor(0.3543)\n",
      "15676 Training Loss: tensor(0.3574)\n",
      "15677 Training Loss: tensor(0.3595)\n",
      "15678 Training Loss: tensor(0.3533)\n",
      "15679 Training Loss: tensor(0.3536)\n",
      "15680 Training Loss: tensor(0.3597)\n",
      "15681 Training Loss: tensor(0.3539)\n",
      "15682 Training Loss: tensor(0.3547)\n",
      "15683 Training Loss: tensor(0.3573)\n",
      "15684 Training Loss: tensor(0.3580)\n",
      "15685 Training Loss: tensor(0.3623)\n",
      "15686 Training Loss: tensor(0.3530)\n",
      "15687 Training Loss: tensor(0.3535)\n",
      "15688 Training Loss: tensor(0.3592)\n",
      "15689 Training Loss: tensor(0.3537)\n",
      "15690 Training Loss: tensor(0.3531)\n",
      "15691 Training Loss: tensor(0.3564)\n",
      "15692 Training Loss: tensor(0.3563)\n",
      "15693 Training Loss: tensor(0.3543)\n",
      "15694 Training Loss: tensor(0.3592)\n",
      "15695 Training Loss: tensor(0.3592)\n",
      "15696 Training Loss: tensor(0.3576)\n",
      "15697 Training Loss: tensor(0.3565)\n",
      "15698 Training Loss: tensor(0.3583)\n",
      "15699 Training Loss: tensor(0.3571)\n",
      "15700 Training Loss: tensor(0.3540)\n",
      "15701 Training Loss: tensor(0.3595)\n",
      "15702 Training Loss: tensor(0.3562)\n",
      "15703 Training Loss: tensor(0.3553)\n",
      "15704 Training Loss: tensor(0.3587)\n",
      "15705 Training Loss: tensor(0.3540)\n",
      "15706 Training Loss: tensor(0.3586)\n",
      "15707 Training Loss: tensor(0.3598)\n",
      "15708 Training Loss: tensor(0.3576)\n",
      "15709 Training Loss: tensor(0.3615)\n",
      "15710 Training Loss: tensor(0.3540)\n",
      "15711 Training Loss: tensor(0.3567)\n",
      "15712 Training Loss: tensor(0.3541)\n",
      "15713 Training Loss: tensor(0.3564)\n",
      "15714 Training Loss: tensor(0.3616)\n",
      "15715 Training Loss: tensor(0.3558)\n",
      "15716 Training Loss: tensor(0.3571)\n",
      "15717 Training Loss: tensor(0.3544)\n",
      "15718 Training Loss: tensor(0.3584)\n",
      "15719 Training Loss: tensor(0.3578)\n",
      "15720 Training Loss: tensor(0.3578)\n",
      "15721 Training Loss: tensor(0.3582)\n",
      "15722 Training Loss: tensor(0.3541)\n",
      "15723 Training Loss: tensor(0.3544)\n",
      "15724 Training Loss: tensor(0.3564)\n",
      "15725 Training Loss: tensor(0.3603)\n",
      "15726 Training Loss: tensor(0.3598)\n",
      "15727 Training Loss: tensor(0.3558)\n",
      "15728 Training Loss: tensor(0.3541)\n",
      "15729 Training Loss: tensor(0.3549)\n",
      "15730 Training Loss: tensor(0.3568)\n",
      "15731 Training Loss: tensor(0.3537)\n",
      "15732 Training Loss: tensor(0.3551)\n",
      "15733 Training Loss: tensor(0.3536)\n",
      "15734 Training Loss: tensor(0.3576)\n",
      "15735 Training Loss: tensor(0.3541)\n",
      "15736 Training Loss: tensor(0.3602)\n",
      "15737 Training Loss: tensor(0.3587)\n",
      "15738 Training Loss: tensor(0.3527)\n",
      "15739 Training Loss: tensor(0.3566)\n",
      "15740 Training Loss: tensor(0.3577)\n",
      "15741 Training Loss: tensor(0.3557)\n",
      "15742 Training Loss: tensor(0.3578)\n",
      "15743 Training Loss: tensor(0.3557)\n",
      "15744 Training Loss: tensor(0.3625)\n",
      "15745 Training Loss: tensor(0.3550)\n",
      "15746 Training Loss: tensor(0.3562)\n",
      "15747 Training Loss: tensor(0.3585)\n",
      "15748 Training Loss: tensor(0.3540)\n",
      "15749 Training Loss: tensor(0.3614)\n",
      "15750 Training Loss: tensor(0.3541)\n",
      "15751 Training Loss: tensor(0.3547)\n",
      "15752 Training Loss: tensor(0.3552)\n",
      "15753 Training Loss: tensor(0.3588)\n",
      "15754 Training Loss: tensor(0.3591)\n",
      "15755 Training Loss: tensor(0.3567)\n",
      "15756 Training Loss: tensor(0.3614)\n",
      "15757 Training Loss: tensor(0.3584)\n",
      "15758 Training Loss: tensor(0.3561)\n",
      "15759 Training Loss: tensor(0.3560)\n",
      "15760 Training Loss: tensor(0.3542)\n",
      "15761 Training Loss: tensor(0.3548)\n",
      "15762 Training Loss: tensor(0.3585)\n",
      "15763 Training Loss: tensor(0.3589)\n",
      "15764 Training Loss: tensor(0.3604)\n",
      "15765 Training Loss: tensor(0.3556)\n",
      "15766 Training Loss: tensor(0.3546)\n",
      "15767 Training Loss: tensor(0.3577)\n",
      "15768 Training Loss: tensor(0.3634)\n",
      "15769 Training Loss: tensor(0.3542)\n",
      "15770 Training Loss: tensor(0.3553)\n",
      "15771 Training Loss: tensor(0.3581)\n",
      "15772 Training Loss: tensor(0.3630)\n",
      "15773 Training Loss: tensor(0.3571)\n",
      "15774 Training Loss: tensor(0.3559)\n",
      "15775 Training Loss: tensor(0.3556)\n",
      "15776 Training Loss: tensor(0.3579)\n",
      "15777 Training Loss: tensor(0.3556)\n",
      "15778 Training Loss: tensor(0.3575)\n",
      "15779 Training Loss: tensor(0.3580)\n",
      "15780 Training Loss: tensor(0.3551)\n",
      "15781 Training Loss: tensor(0.3559)\n",
      "15782 Training Loss: tensor(0.3547)\n",
      "15783 Training Loss: tensor(0.3546)\n",
      "15784 Training Loss: tensor(0.3546)\n",
      "15785 Training Loss: tensor(0.3577)\n",
      "15786 Training Loss: tensor(0.3546)\n",
      "15787 Training Loss: tensor(0.3557)\n",
      "15788 Training Loss: tensor(0.3568)\n",
      "15789 Training Loss: tensor(0.3602)\n",
      "15790 Training Loss: tensor(0.3537)\n",
      "15791 Training Loss: tensor(0.3563)\n",
      "15792 Training Loss: tensor(0.3595)\n",
      "15793 Training Loss: tensor(0.3554)\n",
      "15794 Training Loss: tensor(0.3609)\n",
      "15795 Training Loss: tensor(0.3549)\n",
      "15796 Training Loss: tensor(0.3556)\n",
      "15797 Training Loss: tensor(0.3534)\n",
      "15798 Training Loss: tensor(0.3580)\n",
      "15799 Training Loss: tensor(0.3627)\n",
      "15800 Training Loss: tensor(0.3590)\n",
      "15801 Training Loss: tensor(0.3569)\n",
      "15802 Training Loss: tensor(0.3541)\n",
      "15803 Training Loss: tensor(0.3562)\n",
      "15804 Training Loss: tensor(0.3596)\n",
      "15805 Training Loss: tensor(0.3574)\n",
      "15806 Training Loss: tensor(0.3573)\n",
      "15807 Training Loss: tensor(0.3551)\n",
      "15808 Training Loss: tensor(0.3535)\n",
      "15809 Training Loss: tensor(0.3557)\n",
      "15810 Training Loss: tensor(0.3543)\n",
      "15811 Training Loss: tensor(0.3551)\n",
      "15812 Training Loss: tensor(0.3632)\n",
      "15813 Training Loss: tensor(0.3574)\n",
      "15814 Training Loss: tensor(0.3610)\n",
      "15815 Training Loss: tensor(0.3584)\n",
      "15816 Training Loss: tensor(0.3550)\n",
      "15817 Training Loss: tensor(0.3580)\n",
      "15818 Training Loss: tensor(0.3585)\n",
      "15819 Training Loss: tensor(0.3546)\n",
      "15820 Training Loss: tensor(0.3575)\n",
      "15821 Training Loss: tensor(0.3570)\n",
      "15822 Training Loss: tensor(0.3573)\n",
      "15823 Training Loss: tensor(0.3542)\n",
      "15824 Training Loss: tensor(0.3546)\n",
      "15825 Training Loss: tensor(0.3551)\n",
      "15826 Training Loss: tensor(0.3554)\n",
      "15827 Training Loss: tensor(0.3546)\n",
      "15828 Training Loss: tensor(0.3567)\n",
      "15829 Training Loss: tensor(0.3567)\n",
      "15830 Training Loss: tensor(0.3531)\n",
      "15831 Training Loss: tensor(0.3569)\n",
      "15832 Training Loss: tensor(0.3583)\n",
      "15833 Training Loss: tensor(0.3558)\n",
      "15834 Training Loss: tensor(0.3601)\n",
      "15835 Training Loss: tensor(0.3630)\n",
      "15836 Training Loss: tensor(0.3621)\n",
      "15837 Training Loss: tensor(0.3587)\n",
      "15838 Training Loss: tensor(0.3604)\n",
      "15839 Training Loss: tensor(0.3602)\n",
      "15840 Training Loss: tensor(0.3552)\n",
      "15841 Training Loss: tensor(0.3548)\n",
      "15842 Training Loss: tensor(0.3570)\n",
      "15843 Training Loss: tensor(0.3586)\n",
      "15844 Training Loss: tensor(0.3584)\n",
      "15845 Training Loss: tensor(0.3587)\n",
      "15846 Training Loss: tensor(0.3554)\n",
      "15847 Training Loss: tensor(0.3606)\n",
      "15848 Training Loss: tensor(0.3558)\n",
      "15849 Training Loss: tensor(0.3559)\n",
      "15850 Training Loss: tensor(0.3557)\n",
      "15851 Training Loss: tensor(0.3552)\n",
      "15852 Training Loss: tensor(0.3561)\n",
      "15853 Training Loss: tensor(0.3553)\n",
      "15854 Training Loss: tensor(0.3566)\n",
      "15855 Training Loss: tensor(0.3598)\n",
      "15856 Training Loss: tensor(0.3570)\n",
      "15857 Training Loss: tensor(0.3563)\n",
      "15858 Training Loss: tensor(0.3558)\n",
      "15859 Training Loss: tensor(0.3589)\n",
      "15860 Training Loss: tensor(0.3573)\n",
      "15861 Training Loss: tensor(0.3566)\n",
      "15862 Training Loss: tensor(0.3556)\n",
      "15863 Training Loss: tensor(0.3582)\n",
      "15864 Training Loss: tensor(0.3552)\n",
      "15865 Training Loss: tensor(0.3570)\n",
      "15866 Training Loss: tensor(0.3619)\n",
      "15867 Training Loss: tensor(0.3557)\n",
      "15868 Training Loss: tensor(0.3558)\n",
      "15869 Training Loss: tensor(0.3565)\n",
      "15870 Training Loss: tensor(0.3581)\n",
      "15871 Training Loss: tensor(0.3556)\n",
      "15872 Training Loss: tensor(0.3588)\n",
      "15873 Training Loss: tensor(0.3620)\n",
      "15874 Training Loss: tensor(0.3609)\n",
      "15875 Training Loss: tensor(0.3567)\n",
      "15876 Training Loss: tensor(0.3610)\n",
      "15877 Training Loss: tensor(0.3538)\n",
      "15878 Training Loss: tensor(0.3579)\n",
      "15879 Training Loss: tensor(0.3619)\n",
      "15880 Training Loss: tensor(0.3569)\n",
      "15881 Training Loss: tensor(0.3548)\n",
      "15882 Training Loss: tensor(0.3567)\n",
      "15883 Training Loss: tensor(0.3571)\n",
      "15884 Training Loss: tensor(0.3565)\n",
      "15885 Training Loss: tensor(0.3593)\n",
      "15886 Training Loss: tensor(0.3596)\n",
      "15887 Training Loss: tensor(0.3563)\n",
      "15888 Training Loss: tensor(0.3572)\n",
      "15889 Training Loss: tensor(0.3562)\n",
      "15890 Training Loss: tensor(0.3543)\n",
      "15891 Training Loss: tensor(0.3635)\n",
      "15892 Training Loss: tensor(0.3545)\n",
      "15893 Training Loss: tensor(0.3617)\n",
      "15894 Training Loss: tensor(0.3552)\n",
      "15895 Training Loss: tensor(0.3553)\n",
      "15896 Training Loss: tensor(0.3560)\n",
      "15897 Training Loss: tensor(0.3555)\n",
      "15898 Training Loss: tensor(0.3558)\n",
      "15899 Training Loss: tensor(0.3553)\n",
      "15900 Training Loss: tensor(0.3566)\n",
      "15901 Training Loss: tensor(0.3551)\n",
      "15902 Training Loss: tensor(0.3539)\n",
      "15903 Training Loss: tensor(0.3547)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15904 Training Loss: tensor(0.3546)\n",
      "15905 Training Loss: tensor(0.3577)\n",
      "15906 Training Loss: tensor(0.3562)\n",
      "15907 Training Loss: tensor(0.3544)\n",
      "15908 Training Loss: tensor(0.3600)\n",
      "15909 Training Loss: tensor(0.3533)\n",
      "15910 Training Loss: tensor(0.3544)\n",
      "15911 Training Loss: tensor(0.3534)\n",
      "15912 Training Loss: tensor(0.3568)\n",
      "15913 Training Loss: tensor(0.3556)\n",
      "15914 Training Loss: tensor(0.3551)\n",
      "15915 Training Loss: tensor(0.3543)\n",
      "15916 Training Loss: tensor(0.3533)\n",
      "15917 Training Loss: tensor(0.3537)\n",
      "15918 Training Loss: tensor(0.3637)\n",
      "15919 Training Loss: tensor(0.3531)\n",
      "15920 Training Loss: tensor(0.3574)\n",
      "15921 Training Loss: tensor(0.3565)\n",
      "15922 Training Loss: tensor(0.3527)\n",
      "15923 Training Loss: tensor(0.3643)\n",
      "15924 Training Loss: tensor(0.3567)\n",
      "15925 Training Loss: tensor(0.3539)\n",
      "15926 Training Loss: tensor(0.3565)\n",
      "15927 Training Loss: tensor(0.3564)\n",
      "15928 Training Loss: tensor(0.3554)\n",
      "15929 Training Loss: tensor(0.3557)\n",
      "15930 Training Loss: tensor(0.3550)\n",
      "15931 Training Loss: tensor(0.3587)\n",
      "15932 Training Loss: tensor(0.3569)\n",
      "15933 Training Loss: tensor(0.3540)\n",
      "15934 Training Loss: tensor(0.3541)\n",
      "15935 Training Loss: tensor(0.3549)\n",
      "15936 Training Loss: tensor(0.3589)\n",
      "15937 Training Loss: tensor(0.3569)\n",
      "15938 Training Loss: tensor(0.3558)\n",
      "15939 Training Loss: tensor(0.3546)\n",
      "15940 Training Loss: tensor(0.3569)\n",
      "15941 Training Loss: tensor(0.3574)\n",
      "15942 Training Loss: tensor(0.3614)\n",
      "15943 Training Loss: tensor(0.3544)\n",
      "15944 Training Loss: tensor(0.3569)\n",
      "15945 Training Loss: tensor(0.3571)\n",
      "15946 Training Loss: tensor(0.3535)\n",
      "15947 Training Loss: tensor(0.3566)\n",
      "15948 Training Loss: tensor(0.3557)\n",
      "15949 Training Loss: tensor(0.3528)\n",
      "15950 Training Loss: tensor(0.3543)\n",
      "15951 Training Loss: tensor(0.3557)\n",
      "15952 Training Loss: tensor(0.3555)\n",
      "15953 Training Loss: tensor(0.3580)\n",
      "15954 Training Loss: tensor(0.3669)\n",
      "15955 Training Loss: tensor(0.3693)\n",
      "15956 Training Loss: tensor(0.3536)\n",
      "15957 Training Loss: tensor(0.3558)\n",
      "15958 Training Loss: tensor(0.3586)\n",
      "15959 Training Loss: tensor(0.3576)\n",
      "15960 Training Loss: tensor(0.3596)\n",
      "15961 Training Loss: tensor(0.3580)\n",
      "15962 Training Loss: tensor(0.3560)\n",
      "15963 Training Loss: tensor(0.3563)\n",
      "15964 Training Loss: tensor(0.3585)\n",
      "15965 Training Loss: tensor(0.3556)\n",
      "15966 Training Loss: tensor(0.3547)\n",
      "15967 Training Loss: tensor(0.3546)\n",
      "15968 Training Loss: tensor(0.3550)\n",
      "15969 Training Loss: tensor(0.3549)\n",
      "15970 Training Loss: tensor(0.3574)\n",
      "15971 Training Loss: tensor(0.3606)\n",
      "15972 Training Loss: tensor(0.3580)\n",
      "15973 Training Loss: tensor(0.3573)\n",
      "15974 Training Loss: tensor(0.3566)\n",
      "15975 Training Loss: tensor(0.3620)\n",
      "15976 Training Loss: tensor(0.3590)\n",
      "15977 Training Loss: tensor(0.3540)\n",
      "15978 Training Loss: tensor(0.3574)\n",
      "15979 Training Loss: tensor(0.3579)\n",
      "15980 Training Loss: tensor(0.3581)\n",
      "15981 Training Loss: tensor(0.3536)\n",
      "15982 Training Loss: tensor(0.3607)\n",
      "15983 Training Loss: tensor(0.3591)\n",
      "15984 Training Loss: tensor(0.3542)\n",
      "15985 Training Loss: tensor(0.3569)\n",
      "15986 Training Loss: tensor(0.3550)\n",
      "15987 Training Loss: tensor(0.3548)\n",
      "15988 Training Loss: tensor(0.3546)\n",
      "15989 Training Loss: tensor(0.3555)\n",
      "15990 Training Loss: tensor(0.3530)\n",
      "15991 Training Loss: tensor(0.3537)\n",
      "15992 Training Loss: tensor(0.3558)\n",
      "15993 Training Loss: tensor(0.3574)\n",
      "15994 Training Loss: tensor(0.3541)\n",
      "15995 Training Loss: tensor(0.3534)\n",
      "15996 Training Loss: tensor(0.3546)\n",
      "15997 Training Loss: tensor(0.3591)\n",
      "15998 Training Loss: tensor(0.3574)\n",
      "15999 Training Loss: tensor(0.3534)\n",
      "16000 Training Loss: tensor(0.3572)\n",
      "16001 Training Loss: tensor(0.3544)\n",
      "16002 Training Loss: tensor(0.3535)\n",
      "16003 Training Loss: tensor(0.3555)\n",
      "16004 Training Loss: tensor(0.3591)\n",
      "16005 Training Loss: tensor(0.3568)\n",
      "16006 Training Loss: tensor(0.3619)\n",
      "16007 Training Loss: tensor(0.3587)\n",
      "16008 Training Loss: tensor(0.3543)\n",
      "16009 Training Loss: tensor(0.3565)\n",
      "16010 Training Loss: tensor(0.3563)\n",
      "16011 Training Loss: tensor(0.3584)\n",
      "16012 Training Loss: tensor(0.3598)\n",
      "16013 Training Loss: tensor(0.3582)\n",
      "16014 Training Loss: tensor(0.3545)\n",
      "16015 Training Loss: tensor(0.3555)\n",
      "16016 Training Loss: tensor(0.3624)\n",
      "16017 Training Loss: tensor(0.3536)\n",
      "16018 Training Loss: tensor(0.3576)\n",
      "16019 Training Loss: tensor(0.3612)\n",
      "16020 Training Loss: tensor(0.3541)\n",
      "16021 Training Loss: tensor(0.3576)\n",
      "16022 Training Loss: tensor(0.3553)\n",
      "16023 Training Loss: tensor(0.3544)\n",
      "16024 Training Loss: tensor(0.3557)\n",
      "16025 Training Loss: tensor(0.3556)\n",
      "16026 Training Loss: tensor(0.3614)\n",
      "16027 Training Loss: tensor(0.3561)\n",
      "16028 Training Loss: tensor(0.3549)\n",
      "16029 Training Loss: tensor(0.3542)\n",
      "16030 Training Loss: tensor(0.3555)\n",
      "16031 Training Loss: tensor(0.3556)\n",
      "16032 Training Loss: tensor(0.3545)\n",
      "16033 Training Loss: tensor(0.3543)\n",
      "16034 Training Loss: tensor(0.3574)\n",
      "16035 Training Loss: tensor(0.3563)\n",
      "16036 Training Loss: tensor(0.3533)\n",
      "16037 Training Loss: tensor(0.3557)\n",
      "16038 Training Loss: tensor(0.3548)\n",
      "16039 Training Loss: tensor(0.3547)\n",
      "16040 Training Loss: tensor(0.3537)\n",
      "16041 Training Loss: tensor(0.3562)\n",
      "16042 Training Loss: tensor(0.3604)\n",
      "16043 Training Loss: tensor(0.3571)\n",
      "16044 Training Loss: tensor(0.3535)\n",
      "16045 Training Loss: tensor(0.3557)\n",
      "16046 Training Loss: tensor(0.3571)\n",
      "16047 Training Loss: tensor(0.3573)\n",
      "16048 Training Loss: tensor(0.3585)\n",
      "16049 Training Loss: tensor(0.3644)\n",
      "16050 Training Loss: tensor(0.3586)\n",
      "16051 Training Loss: tensor(0.3544)\n",
      "16052 Training Loss: tensor(0.3583)\n",
      "16053 Training Loss: tensor(0.3563)\n",
      "16054 Training Loss: tensor(0.3595)\n",
      "16055 Training Loss: tensor(0.3547)\n",
      "16056 Training Loss: tensor(0.3595)\n",
      "16057 Training Loss: tensor(0.3554)\n",
      "16058 Training Loss: tensor(0.3555)\n",
      "16059 Training Loss: tensor(0.3573)\n",
      "16060 Training Loss: tensor(0.3580)\n",
      "16061 Training Loss: tensor(0.3545)\n",
      "16062 Training Loss: tensor(0.3582)\n",
      "16063 Training Loss: tensor(0.3565)\n",
      "16064 Training Loss: tensor(0.3549)\n",
      "16065 Training Loss: tensor(0.3605)\n",
      "16066 Training Loss: tensor(0.3543)\n",
      "16067 Training Loss: tensor(0.3571)\n",
      "16068 Training Loss: tensor(0.3550)\n",
      "16069 Training Loss: tensor(0.3552)\n",
      "16070 Training Loss: tensor(0.3540)\n",
      "16071 Training Loss: tensor(0.3553)\n",
      "16072 Training Loss: tensor(0.3556)\n",
      "16073 Training Loss: tensor(0.3545)\n",
      "16074 Training Loss: tensor(0.3603)\n",
      "16075 Training Loss: tensor(0.3542)\n",
      "16076 Training Loss: tensor(0.3600)\n",
      "16077 Training Loss: tensor(0.3560)\n",
      "16078 Training Loss: tensor(0.3539)\n",
      "16079 Training Loss: tensor(0.3543)\n",
      "16080 Training Loss: tensor(0.3617)\n",
      "16081 Training Loss: tensor(0.3538)\n",
      "16082 Training Loss: tensor(0.3567)\n",
      "16083 Training Loss: tensor(0.3575)\n",
      "16084 Training Loss: tensor(0.3552)\n",
      "16085 Training Loss: tensor(0.3544)\n",
      "16086 Training Loss: tensor(0.3557)\n",
      "16087 Training Loss: tensor(0.3538)\n",
      "16088 Training Loss: tensor(0.3531)\n",
      "16089 Training Loss: tensor(0.3590)\n",
      "16090 Training Loss: tensor(0.3534)\n",
      "16091 Training Loss: tensor(0.3610)\n",
      "16092 Training Loss: tensor(0.3593)\n",
      "16093 Training Loss: tensor(0.3540)\n",
      "16094 Training Loss: tensor(0.3545)\n",
      "16095 Training Loss: tensor(0.3539)\n",
      "16096 Training Loss: tensor(0.3529)\n",
      "16097 Training Loss: tensor(0.3564)\n",
      "16098 Training Loss: tensor(0.3529)\n",
      "16099 Training Loss: tensor(0.3567)\n",
      "16100 Training Loss: tensor(0.3563)\n",
      "16101 Training Loss: tensor(0.3531)\n",
      "16102 Training Loss: tensor(0.3576)\n",
      "16103 Training Loss: tensor(0.3667)\n",
      "16104 Training Loss: tensor(0.3530)\n",
      "16105 Training Loss: tensor(0.3542)\n",
      "16106 Training Loss: tensor(0.3528)\n",
      "16107 Training Loss: tensor(0.3549)\n",
      "16108 Training Loss: tensor(0.3547)\n",
      "16109 Training Loss: tensor(0.3596)\n",
      "16110 Training Loss: tensor(0.3606)\n",
      "16111 Training Loss: tensor(0.3554)\n",
      "16112 Training Loss: tensor(0.3540)\n",
      "16113 Training Loss: tensor(0.3553)\n",
      "16114 Training Loss: tensor(0.3580)\n",
      "16115 Training Loss: tensor(0.3578)\n",
      "16116 Training Loss: tensor(0.3539)\n",
      "16117 Training Loss: tensor(0.3602)\n",
      "16118 Training Loss: tensor(0.3562)\n",
      "16119 Training Loss: tensor(0.3553)\n",
      "16120 Training Loss: tensor(0.3559)\n",
      "16121 Training Loss: tensor(0.3534)\n",
      "16122 Training Loss: tensor(0.3575)\n",
      "16123 Training Loss: tensor(0.3539)\n",
      "16124 Training Loss: tensor(0.3581)\n",
      "16125 Training Loss: tensor(0.3584)\n",
      "16126 Training Loss: tensor(0.3535)\n",
      "16127 Training Loss: tensor(0.3532)\n",
      "16128 Training Loss: tensor(0.3637)\n",
      "16129 Training Loss: tensor(0.3551)\n",
      "16130 Training Loss: tensor(0.3539)\n",
      "16131 Training Loss: tensor(0.3554)\n",
      "16132 Training Loss: tensor(0.3559)\n",
      "16133 Training Loss: tensor(0.3561)\n",
      "16134 Training Loss: tensor(0.3630)\n",
      "16135 Training Loss: tensor(0.3535)\n",
      "16136 Training Loss: tensor(0.3532)\n",
      "16137 Training Loss: tensor(0.3551)\n",
      "16138 Training Loss: tensor(0.3552)\n",
      "16139 Training Loss: tensor(0.3619)\n",
      "16140 Training Loss: tensor(0.3589)\n",
      "16141 Training Loss: tensor(0.3564)\n",
      "16142 Training Loss: tensor(0.3613)\n",
      "16143 Training Loss: tensor(0.3561)\n",
      "16144 Training Loss: tensor(0.3540)\n",
      "16145 Training Loss: tensor(0.3574)\n",
      "16146 Training Loss: tensor(0.3541)\n",
      "16147 Training Loss: tensor(0.3538)\n",
      "16148 Training Loss: tensor(0.3570)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16149 Training Loss: tensor(0.3556)\n",
      "16150 Training Loss: tensor(0.3561)\n",
      "16151 Training Loss: tensor(0.3546)\n",
      "16152 Training Loss: tensor(0.3538)\n",
      "16153 Training Loss: tensor(0.3541)\n",
      "16154 Training Loss: tensor(0.3589)\n",
      "16155 Training Loss: tensor(0.3536)\n",
      "16156 Training Loss: tensor(0.3539)\n",
      "16157 Training Loss: tensor(0.3556)\n",
      "16158 Training Loss: tensor(0.3580)\n",
      "16159 Training Loss: tensor(0.3557)\n",
      "16160 Training Loss: tensor(0.3556)\n",
      "16161 Training Loss: tensor(0.3541)\n",
      "16162 Training Loss: tensor(0.3558)\n",
      "16163 Training Loss: tensor(0.3540)\n",
      "16164 Training Loss: tensor(0.3551)\n",
      "16165 Training Loss: tensor(0.3529)\n",
      "16166 Training Loss: tensor(0.3534)\n",
      "16167 Training Loss: tensor(0.3535)\n",
      "16168 Training Loss: tensor(0.3580)\n",
      "16169 Training Loss: tensor(0.3579)\n",
      "16170 Training Loss: tensor(0.3572)\n",
      "16171 Training Loss: tensor(0.3530)\n",
      "16172 Training Loss: tensor(0.3562)\n",
      "16173 Training Loss: tensor(0.3532)\n",
      "16174 Training Loss: tensor(0.3612)\n",
      "16175 Training Loss: tensor(0.3569)\n",
      "16176 Training Loss: tensor(0.3534)\n",
      "16177 Training Loss: tensor(0.3526)\n",
      "16178 Training Loss: tensor(0.3584)\n",
      "16179 Training Loss: tensor(0.3533)\n",
      "16180 Training Loss: tensor(0.3557)\n",
      "16181 Training Loss: tensor(0.3526)\n",
      "16182 Training Loss: tensor(0.3530)\n",
      "16183 Training Loss: tensor(0.3646)\n",
      "16184 Training Loss: tensor(0.3580)\n",
      "16185 Training Loss: tensor(0.3587)\n",
      "16186 Training Loss: tensor(0.3545)\n",
      "16187 Training Loss: tensor(0.3536)\n",
      "16188 Training Loss: tensor(0.3575)\n",
      "16189 Training Loss: tensor(0.3577)\n",
      "16190 Training Loss: tensor(0.3525)\n",
      "16191 Training Loss: tensor(0.3534)\n",
      "16192 Training Loss: tensor(0.3547)\n",
      "16193 Training Loss: tensor(0.3538)\n",
      "16194 Training Loss: tensor(0.3543)\n",
      "16195 Training Loss: tensor(0.3570)\n",
      "16196 Training Loss: tensor(0.3540)\n",
      "16197 Training Loss: tensor(0.3611)\n",
      "16198 Training Loss: tensor(0.3568)\n",
      "16199 Training Loss: tensor(0.3536)\n",
      "16200 Training Loss: tensor(0.3542)\n",
      "16201 Training Loss: tensor(0.3532)\n",
      "16202 Training Loss: tensor(0.3579)\n",
      "16203 Training Loss: tensor(0.3554)\n",
      "16204 Training Loss: tensor(0.3626)\n",
      "16205 Training Loss: tensor(0.3543)\n",
      "16206 Training Loss: tensor(0.3555)\n",
      "16207 Training Loss: tensor(0.3576)\n",
      "16208 Training Loss: tensor(0.3543)\n",
      "16209 Training Loss: tensor(0.3547)\n",
      "16210 Training Loss: tensor(0.3539)\n",
      "16211 Training Loss: tensor(0.3626)\n",
      "16212 Training Loss: tensor(0.3561)\n",
      "16213 Training Loss: tensor(0.3551)\n",
      "16214 Training Loss: tensor(0.3551)\n",
      "16215 Training Loss: tensor(0.3561)\n",
      "16216 Training Loss: tensor(0.3539)\n",
      "16217 Training Loss: tensor(0.3580)\n",
      "16218 Training Loss: tensor(0.3592)\n",
      "16219 Training Loss: tensor(0.3536)\n",
      "16220 Training Loss: tensor(0.3533)\n",
      "16221 Training Loss: tensor(0.3546)\n",
      "16222 Training Loss: tensor(0.3607)\n",
      "16223 Training Loss: tensor(0.3580)\n",
      "16224 Training Loss: tensor(0.3536)\n",
      "16225 Training Loss: tensor(0.3533)\n",
      "16226 Training Loss: tensor(0.3549)\n",
      "16227 Training Loss: tensor(0.3548)\n",
      "16228 Training Loss: tensor(0.3565)\n",
      "16229 Training Loss: tensor(0.3532)\n",
      "16230 Training Loss: tensor(0.3544)\n",
      "16231 Training Loss: tensor(0.3545)\n",
      "16232 Training Loss: tensor(0.3551)\n",
      "16233 Training Loss: tensor(0.3548)\n",
      "16234 Training Loss: tensor(0.3534)\n",
      "16235 Training Loss: tensor(0.3547)\n",
      "16236 Training Loss: tensor(0.3644)\n",
      "16237 Training Loss: tensor(0.3557)\n",
      "16238 Training Loss: tensor(0.3540)\n",
      "16239 Training Loss: tensor(0.3535)\n",
      "16240 Training Loss: tensor(0.3539)\n",
      "16241 Training Loss: tensor(0.3544)\n",
      "16242 Training Loss: tensor(0.3553)\n",
      "16243 Training Loss: tensor(0.3568)\n",
      "16244 Training Loss: tensor(0.3573)\n",
      "16245 Training Loss: tensor(0.3545)\n",
      "16246 Training Loss: tensor(0.3530)\n",
      "16247 Training Loss: tensor(0.3551)\n",
      "16248 Training Loss: tensor(0.3601)\n",
      "16249 Training Loss: tensor(0.3548)\n",
      "16250 Training Loss: tensor(0.3593)\n",
      "16251 Training Loss: tensor(0.3579)\n",
      "16252 Training Loss: tensor(0.3563)\n",
      "16253 Training Loss: tensor(0.3575)\n",
      "16254 Training Loss: tensor(0.3542)\n",
      "16255 Training Loss: tensor(0.3536)\n",
      "16256 Training Loss: tensor(0.3530)\n",
      "16257 Training Loss: tensor(0.3558)\n",
      "16258 Training Loss: tensor(0.3544)\n",
      "16259 Training Loss: tensor(0.3544)\n",
      "16260 Training Loss: tensor(0.3541)\n",
      "16261 Training Loss: tensor(0.3555)\n",
      "16262 Training Loss: tensor(0.3537)\n",
      "16263 Training Loss: tensor(0.3540)\n",
      "16264 Training Loss: tensor(0.3569)\n",
      "16265 Training Loss: tensor(0.3574)\n",
      "16266 Training Loss: tensor(0.3523)\n",
      "16267 Training Loss: tensor(0.3646)\n",
      "16268 Training Loss: tensor(0.3552)\n",
      "16269 Training Loss: tensor(0.3536)\n",
      "16270 Training Loss: tensor(0.3549)\n",
      "16271 Training Loss: tensor(0.3558)\n",
      "16272 Training Loss: tensor(0.3528)\n",
      "16273 Training Loss: tensor(0.3531)\n",
      "16274 Training Loss: tensor(0.3534)\n",
      "16275 Training Loss: tensor(0.3526)\n",
      "16276 Training Loss: tensor(0.3539)\n",
      "16277 Training Loss: tensor(0.3547)\n",
      "16278 Training Loss: tensor(0.3521)\n",
      "16279 Training Loss: tensor(0.3563)\n",
      "16280 Training Loss: tensor(0.3525)\n",
      "16281 Training Loss: tensor(0.3613)\n",
      "16282 Training Loss: tensor(0.3562)\n",
      "16283 Training Loss: tensor(0.3666)\n",
      "16284 Training Loss: tensor(0.3528)\n",
      "16285 Training Loss: tensor(0.3602)\n",
      "16286 Training Loss: tensor(0.3597)\n",
      "16287 Training Loss: tensor(0.3564)\n",
      "16288 Training Loss: tensor(0.3570)\n",
      "16289 Training Loss: tensor(0.3582)\n",
      "16290 Training Loss: tensor(0.3532)\n",
      "16291 Training Loss: tensor(0.3549)\n",
      "16292 Training Loss: tensor(0.3545)\n",
      "16293 Training Loss: tensor(0.3564)\n",
      "16294 Training Loss: tensor(0.3542)\n",
      "16295 Training Loss: tensor(0.3582)\n",
      "16296 Training Loss: tensor(0.3547)\n",
      "16297 Training Loss: tensor(0.3558)\n",
      "16298 Training Loss: tensor(0.3542)\n",
      "16299 Training Loss: tensor(0.3534)\n",
      "16300 Training Loss: tensor(0.3585)\n",
      "16301 Training Loss: tensor(0.3534)\n",
      "16302 Training Loss: tensor(0.3626)\n",
      "16303 Training Loss: tensor(0.3529)\n",
      "16304 Training Loss: tensor(0.3528)\n",
      "16305 Training Loss: tensor(0.3576)\n",
      "16306 Training Loss: tensor(0.3565)\n",
      "16307 Training Loss: tensor(0.3622)\n",
      "16308 Training Loss: tensor(0.3543)\n",
      "16309 Training Loss: tensor(0.3539)\n",
      "16310 Training Loss: tensor(0.3544)\n",
      "16311 Training Loss: tensor(0.3524)\n",
      "16312 Training Loss: tensor(0.3570)\n",
      "16313 Training Loss: tensor(0.3529)\n",
      "16314 Training Loss: tensor(0.3575)\n",
      "16315 Training Loss: tensor(0.3572)\n",
      "16316 Training Loss: tensor(0.3587)\n",
      "16317 Training Loss: tensor(0.3583)\n",
      "16318 Training Loss: tensor(0.3528)\n",
      "16319 Training Loss: tensor(0.3542)\n",
      "16320 Training Loss: tensor(0.3523)\n",
      "16321 Training Loss: tensor(0.3532)\n",
      "16322 Training Loss: tensor(0.3651)\n",
      "16323 Training Loss: tensor(0.3581)\n",
      "16324 Training Loss: tensor(0.3578)\n",
      "16325 Training Loss: tensor(0.3539)\n",
      "16326 Training Loss: tensor(0.3561)\n",
      "16327 Training Loss: tensor(0.3565)\n",
      "16328 Training Loss: tensor(0.3549)\n",
      "16329 Training Loss: tensor(0.3545)\n",
      "16330 Training Loss: tensor(0.3550)\n",
      "16331 Training Loss: tensor(0.3548)\n",
      "16332 Training Loss: tensor(0.3543)\n",
      "16333 Training Loss: tensor(0.3614)\n",
      "16334 Training Loss: tensor(0.3584)\n",
      "16335 Training Loss: tensor(0.3554)\n",
      "16336 Training Loss: tensor(0.3576)\n",
      "16337 Training Loss: tensor(0.3578)\n",
      "16338 Training Loss: tensor(0.3583)\n",
      "16339 Training Loss: tensor(0.3544)\n",
      "16340 Training Loss: tensor(0.3531)\n",
      "16341 Training Loss: tensor(0.3532)\n",
      "16342 Training Loss: tensor(0.3599)\n",
      "16343 Training Loss: tensor(0.3546)\n",
      "16344 Training Loss: tensor(0.3527)\n",
      "16345 Training Loss: tensor(0.3538)\n",
      "16346 Training Loss: tensor(0.3543)\n",
      "16347 Training Loss: tensor(0.3540)\n",
      "16348 Training Loss: tensor(0.3558)\n",
      "16349 Training Loss: tensor(0.3565)\n",
      "16350 Training Loss: tensor(0.3521)\n",
      "16351 Training Loss: tensor(0.3536)\n",
      "16352 Training Loss: tensor(0.3529)\n",
      "16353 Training Loss: tensor(0.3570)\n",
      "16354 Training Loss: tensor(0.3541)\n",
      "16355 Training Loss: tensor(0.3562)\n",
      "16356 Training Loss: tensor(0.3531)\n",
      "16357 Training Loss: tensor(0.3611)\n",
      "16358 Training Loss: tensor(0.3550)\n",
      "16359 Training Loss: tensor(0.3529)\n",
      "16360 Training Loss: tensor(0.3538)\n",
      "16361 Training Loss: tensor(0.3542)\n",
      "16362 Training Loss: tensor(0.3524)\n",
      "16363 Training Loss: tensor(0.3544)\n",
      "16364 Training Loss: tensor(0.3518)\n",
      "16365 Training Loss: tensor(0.3533)\n",
      "16366 Training Loss: tensor(0.3599)\n",
      "16367 Training Loss: tensor(0.3556)\n",
      "16368 Training Loss: tensor(0.3569)\n",
      "16369 Training Loss: tensor(0.3555)\n",
      "16370 Training Loss: tensor(0.3563)\n",
      "16371 Training Loss: tensor(0.3553)\n",
      "16372 Training Loss: tensor(0.3524)\n",
      "16373 Training Loss: tensor(0.3580)\n",
      "16374 Training Loss: tensor(0.3642)\n",
      "16375 Training Loss: tensor(0.3530)\n",
      "16376 Training Loss: tensor(0.3586)\n",
      "16377 Training Loss: tensor(0.3548)\n",
      "16378 Training Loss: tensor(0.3552)\n",
      "16379 Training Loss: tensor(0.3553)\n",
      "16380 Training Loss: tensor(0.3555)\n",
      "16381 Training Loss: tensor(0.3547)\n",
      "16382 Training Loss: tensor(0.3546)\n",
      "16383 Training Loss: tensor(0.3553)\n",
      "16384 Training Loss: tensor(0.3545)\n",
      "16385 Training Loss: tensor(0.3539)\n",
      "16386 Training Loss: tensor(0.3578)\n",
      "16387 Training Loss: tensor(0.3559)\n",
      "16388 Training Loss: tensor(0.3540)\n",
      "16389 Training Loss: tensor(0.3563)\n",
      "16390 Training Loss: tensor(0.3566)\n",
      "16391 Training Loss: tensor(0.3588)\n",
      "16392 Training Loss: tensor(0.3575)\n",
      "16393 Training Loss: tensor(0.3560)\n",
      "16394 Training Loss: tensor(0.3565)\n",
      "16395 Training Loss: tensor(0.3591)\n",
      "16396 Training Loss: tensor(0.3538)\n",
      "16397 Training Loss: tensor(0.3554)\n",
      "16398 Training Loss: tensor(0.3559)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16399 Training Loss: tensor(0.3546)\n",
      "16400 Training Loss: tensor(0.3529)\n",
      "16401 Training Loss: tensor(0.3560)\n",
      "16402 Training Loss: tensor(0.3548)\n",
      "16403 Training Loss: tensor(0.3528)\n",
      "16404 Training Loss: tensor(0.3585)\n",
      "16405 Training Loss: tensor(0.3531)\n",
      "16406 Training Loss: tensor(0.3568)\n",
      "16407 Training Loss: tensor(0.3529)\n",
      "16408 Training Loss: tensor(0.3553)\n",
      "16409 Training Loss: tensor(0.3570)\n",
      "16410 Training Loss: tensor(0.3546)\n",
      "16411 Training Loss: tensor(0.3530)\n",
      "16412 Training Loss: tensor(0.3586)\n",
      "16413 Training Loss: tensor(0.3551)\n",
      "16414 Training Loss: tensor(0.3657)\n",
      "16415 Training Loss: tensor(0.3529)\n",
      "16416 Training Loss: tensor(0.3535)\n",
      "16417 Training Loss: tensor(0.3583)\n",
      "16418 Training Loss: tensor(0.3559)\n",
      "16419 Training Loss: tensor(0.3522)\n",
      "16420 Training Loss: tensor(0.3574)\n",
      "16421 Training Loss: tensor(0.3572)\n",
      "16422 Training Loss: tensor(0.3535)\n",
      "16423 Training Loss: tensor(0.3563)\n",
      "16424 Training Loss: tensor(0.3532)\n",
      "16425 Training Loss: tensor(0.3556)\n",
      "16426 Training Loss: tensor(0.3583)\n",
      "16427 Training Loss: tensor(0.3544)\n",
      "16428 Training Loss: tensor(0.3530)\n",
      "16429 Training Loss: tensor(0.3579)\n",
      "16430 Training Loss: tensor(0.3595)\n",
      "16431 Training Loss: tensor(0.3641)\n",
      "16432 Training Loss: tensor(0.3587)\n",
      "16433 Training Loss: tensor(0.3532)\n",
      "16434 Training Loss: tensor(0.3545)\n",
      "16435 Training Loss: tensor(0.3570)\n",
      "16436 Training Loss: tensor(0.3611)\n",
      "16437 Training Loss: tensor(0.3532)\n",
      "16438 Training Loss: tensor(0.3562)\n",
      "16439 Training Loss: tensor(0.3552)\n",
      "16440 Training Loss: tensor(0.3540)\n",
      "16441 Training Loss: tensor(0.3566)\n",
      "16442 Training Loss: tensor(0.3552)\n",
      "16443 Training Loss: tensor(0.3597)\n",
      "16444 Training Loss: tensor(0.3663)\n",
      "16445 Training Loss: tensor(0.3555)\n",
      "16446 Training Loss: tensor(0.3551)\n",
      "16447 Training Loss: tensor(0.3554)\n",
      "16448 Training Loss: tensor(0.3540)\n",
      "16449 Training Loss: tensor(0.3564)\n",
      "16450 Training Loss: tensor(0.3530)\n",
      "16451 Training Loss: tensor(0.3539)\n",
      "16452 Training Loss: tensor(0.3551)\n",
      "16453 Training Loss: tensor(0.3529)\n",
      "16454 Training Loss: tensor(0.3539)\n",
      "16455 Training Loss: tensor(0.3539)\n",
      "16456 Training Loss: tensor(0.3589)\n",
      "16457 Training Loss: tensor(0.3537)\n",
      "16458 Training Loss: tensor(0.3567)\n",
      "16459 Training Loss: tensor(0.3521)\n",
      "16460 Training Loss: tensor(0.3576)\n",
      "16461 Training Loss: tensor(0.3579)\n",
      "16462 Training Loss: tensor(0.3556)\n",
      "16463 Training Loss: tensor(0.3552)\n",
      "16464 Training Loss: tensor(0.3593)\n",
      "16465 Training Loss: tensor(0.3558)\n",
      "16466 Training Loss: tensor(0.3565)\n",
      "16467 Training Loss: tensor(0.3595)\n",
      "16468 Training Loss: tensor(0.3591)\n",
      "16469 Training Loss: tensor(0.3578)\n",
      "16470 Training Loss: tensor(0.3539)\n",
      "16471 Training Loss: tensor(0.3567)\n",
      "16472 Training Loss: tensor(0.3550)\n",
      "16473 Training Loss: tensor(0.3593)\n",
      "16474 Training Loss: tensor(0.3549)\n",
      "16475 Training Loss: tensor(0.3560)\n",
      "16476 Training Loss: tensor(0.3541)\n",
      "16477 Training Loss: tensor(0.3559)\n",
      "16478 Training Loss: tensor(0.3549)\n",
      "16479 Training Loss: tensor(0.3551)\n",
      "16480 Training Loss: tensor(0.3566)\n",
      "16481 Training Loss: tensor(0.3551)\n",
      "16482 Training Loss: tensor(0.3542)\n",
      "16483 Training Loss: tensor(0.3606)\n",
      "16484 Training Loss: tensor(0.3569)\n",
      "16485 Training Loss: tensor(0.3556)\n",
      "16486 Training Loss: tensor(0.3560)\n",
      "16487 Training Loss: tensor(0.3597)\n",
      "16488 Training Loss: tensor(0.3614)\n",
      "16489 Training Loss: tensor(0.3552)\n",
      "16490 Training Loss: tensor(0.3545)\n",
      "16491 Training Loss: tensor(0.3544)\n",
      "16492 Training Loss: tensor(0.3546)\n",
      "16493 Training Loss: tensor(0.3557)\n",
      "16494 Training Loss: tensor(0.3576)\n",
      "16495 Training Loss: tensor(0.3547)\n",
      "16496 Training Loss: tensor(0.3569)\n",
      "16497 Training Loss: tensor(0.3546)\n",
      "16498 Training Loss: tensor(0.3545)\n",
      "16499 Training Loss: tensor(0.3585)\n",
      "16500 Training Loss: tensor(0.3550)\n",
      "16501 Training Loss: tensor(0.3536)\n",
      "16502 Training Loss: tensor(0.3541)\n",
      "16503 Training Loss: tensor(0.3527)\n",
      "16504 Training Loss: tensor(0.3583)\n",
      "16505 Training Loss: tensor(0.3525)\n",
      "16506 Training Loss: tensor(0.3563)\n",
      "16507 Training Loss: tensor(0.3571)\n",
      "16508 Training Loss: tensor(0.3548)\n",
      "16509 Training Loss: tensor(0.3546)\n",
      "16510 Training Loss: tensor(0.3533)\n",
      "16511 Training Loss: tensor(0.3550)\n",
      "16512 Training Loss: tensor(0.3637)\n",
      "16513 Training Loss: tensor(0.3579)\n",
      "16514 Training Loss: tensor(0.3567)\n",
      "16515 Training Loss: tensor(0.3535)\n",
      "16516 Training Loss: tensor(0.3568)\n",
      "16517 Training Loss: tensor(0.3536)\n",
      "16518 Training Loss: tensor(0.3574)\n",
      "16519 Training Loss: tensor(0.3580)\n",
      "16520 Training Loss: tensor(0.3575)\n",
      "16521 Training Loss: tensor(0.3547)\n",
      "16522 Training Loss: tensor(0.3599)\n",
      "16523 Training Loss: tensor(0.3549)\n",
      "16524 Training Loss: tensor(0.3601)\n",
      "16525 Training Loss: tensor(0.3562)\n",
      "16526 Training Loss: tensor(0.3548)\n",
      "16527 Training Loss: tensor(0.3539)\n",
      "16528 Training Loss: tensor(0.3536)\n",
      "16529 Training Loss: tensor(0.3538)\n",
      "16530 Training Loss: tensor(0.3569)\n",
      "16531 Training Loss: tensor(0.3537)\n",
      "16532 Training Loss: tensor(0.3537)\n",
      "16533 Training Loss: tensor(0.3549)\n",
      "16534 Training Loss: tensor(0.3539)\n",
      "16535 Training Loss: tensor(0.3569)\n",
      "16536 Training Loss: tensor(0.3528)\n",
      "16537 Training Loss: tensor(0.3549)\n",
      "16538 Training Loss: tensor(0.3632)\n",
      "16539 Training Loss: tensor(0.3540)\n",
      "16540 Training Loss: tensor(0.3551)\n",
      "16541 Training Loss: tensor(0.3528)\n",
      "16542 Training Loss: tensor(0.3541)\n",
      "16543 Training Loss: tensor(0.3618)\n",
      "16544 Training Loss: tensor(0.3621)\n",
      "16545 Training Loss: tensor(0.3524)\n",
      "16546 Training Loss: tensor(0.3571)\n",
      "16547 Training Loss: tensor(0.3591)\n",
      "16548 Training Loss: tensor(0.3564)\n",
      "16549 Training Loss: tensor(0.3567)\n",
      "16550 Training Loss: tensor(0.3539)\n",
      "16551 Training Loss: tensor(0.3533)\n",
      "16552 Training Loss: tensor(0.3555)\n",
      "16553 Training Loss: tensor(0.3551)\n",
      "16554 Training Loss: tensor(0.3579)\n",
      "16555 Training Loss: tensor(0.3534)\n",
      "16556 Training Loss: tensor(0.3584)\n",
      "16557 Training Loss: tensor(0.3561)\n",
      "16558 Training Loss: tensor(0.3534)\n",
      "16559 Training Loss: tensor(0.3564)\n",
      "16560 Training Loss: tensor(0.3606)\n",
      "16561 Training Loss: tensor(0.3545)\n",
      "16562 Training Loss: tensor(0.3565)\n",
      "16563 Training Loss: tensor(0.3545)\n",
      "16564 Training Loss: tensor(0.3593)\n",
      "16565 Training Loss: tensor(0.3571)\n",
      "16566 Training Loss: tensor(0.3581)\n",
      "16567 Training Loss: tensor(0.3540)\n",
      "16568 Training Loss: tensor(0.3584)\n",
      "16569 Training Loss: tensor(0.3546)\n",
      "16570 Training Loss: tensor(0.3541)\n",
      "16571 Training Loss: tensor(0.3556)\n",
      "16572 Training Loss: tensor(0.3558)\n",
      "16573 Training Loss: tensor(0.3563)\n",
      "16574 Training Loss: tensor(0.3561)\n",
      "16575 Training Loss: tensor(0.3549)\n",
      "16576 Training Loss: tensor(0.3535)\n",
      "16577 Training Loss: tensor(0.3534)\n",
      "16578 Training Loss: tensor(0.3525)\n",
      "16579 Training Loss: tensor(0.3520)\n",
      "16580 Training Loss: tensor(0.3531)\n",
      "16581 Training Loss: tensor(0.3559)\n",
      "16582 Training Loss: tensor(0.3562)\n",
      "16583 Training Loss: tensor(0.3533)\n",
      "16584 Training Loss: tensor(0.3533)\n",
      "16585 Training Loss: tensor(0.3569)\n",
      "16586 Training Loss: tensor(0.3533)\n",
      "16587 Training Loss: tensor(0.3574)\n",
      "16588 Training Loss: tensor(0.3540)\n",
      "16589 Training Loss: tensor(0.3534)\n",
      "16590 Training Loss: tensor(0.3578)\n",
      "16591 Training Loss: tensor(0.3519)\n",
      "16592 Training Loss: tensor(0.3549)\n",
      "16593 Training Loss: tensor(0.3525)\n",
      "16594 Training Loss: tensor(0.3523)\n",
      "16595 Training Loss: tensor(0.3683)\n",
      "16596 Training Loss: tensor(0.3536)\n",
      "16597 Training Loss: tensor(0.3565)\n",
      "16598 Training Loss: tensor(0.3611)\n",
      "16599 Training Loss: tensor(0.3557)\n",
      "16600 Training Loss: tensor(0.3541)\n",
      "16601 Training Loss: tensor(0.3571)\n",
      "16602 Training Loss: tensor(0.3560)\n",
      "16603 Training Loss: tensor(0.3555)\n",
      "16604 Training Loss: tensor(0.3548)\n",
      "16605 Training Loss: tensor(0.3542)\n",
      "16606 Training Loss: tensor(0.3547)\n",
      "16607 Training Loss: tensor(0.3558)\n",
      "16608 Training Loss: tensor(0.3540)\n",
      "16609 Training Loss: tensor(0.3557)\n",
      "16610 Training Loss: tensor(0.3557)\n",
      "16611 Training Loss: tensor(0.3565)\n",
      "16612 Training Loss: tensor(0.3618)\n",
      "16613 Training Loss: tensor(0.3546)\n",
      "16614 Training Loss: tensor(0.3550)\n",
      "16615 Training Loss: tensor(0.3600)\n",
      "16616 Training Loss: tensor(0.3533)\n",
      "16617 Training Loss: tensor(0.3530)\n",
      "16618 Training Loss: tensor(0.3534)\n",
      "16619 Training Loss: tensor(0.3565)\n",
      "16620 Training Loss: tensor(0.3528)\n",
      "16621 Training Loss: tensor(0.3549)\n",
      "16622 Training Loss: tensor(0.3548)\n",
      "16623 Training Loss: tensor(0.3531)\n",
      "16624 Training Loss: tensor(0.3575)\n",
      "16625 Training Loss: tensor(0.3531)\n",
      "16626 Training Loss: tensor(0.3602)\n",
      "16627 Training Loss: tensor(0.3580)\n",
      "16628 Training Loss: tensor(0.3586)\n",
      "16629 Training Loss: tensor(0.3534)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16630 Training Loss: tensor(0.3580)\n",
      "16631 Training Loss: tensor(0.3541)\n",
      "16632 Training Loss: tensor(0.3629)\n",
      "16633 Training Loss: tensor(0.3536)\n",
      "16634 Training Loss: tensor(0.3646)\n",
      "16635 Training Loss: tensor(0.3552)\n",
      "16636 Training Loss: tensor(0.3548)\n",
      "16637 Training Loss: tensor(0.3593)\n",
      "16638 Training Loss: tensor(0.3537)\n",
      "16639 Training Loss: tensor(0.3556)\n",
      "16640 Training Loss: tensor(0.3566)\n",
      "16641 Training Loss: tensor(0.3542)\n",
      "16642 Training Loss: tensor(0.3565)\n",
      "16643 Training Loss: tensor(0.3570)\n",
      "16644 Training Loss: tensor(0.3552)\n",
      "16645 Training Loss: tensor(0.3539)\n",
      "16646 Training Loss: tensor(0.3549)\n",
      "16647 Training Loss: tensor(0.3558)\n",
      "16648 Training Loss: tensor(0.3558)\n",
      "16649 Training Loss: tensor(0.3542)\n",
      "16650 Training Loss: tensor(0.3570)\n",
      "16651 Training Loss: tensor(0.3595)\n",
      "16652 Training Loss: tensor(0.3526)\n",
      "16653 Training Loss: tensor(0.3600)\n",
      "16654 Training Loss: tensor(0.3531)\n",
      "16655 Training Loss: tensor(0.3526)\n",
      "16656 Training Loss: tensor(0.3542)\n",
      "16657 Training Loss: tensor(0.3601)\n",
      "16658 Training Loss: tensor(0.3520)\n",
      "16659 Training Loss: tensor(0.3535)\n",
      "16660 Training Loss: tensor(0.3549)\n",
      "16661 Training Loss: tensor(0.3606)\n",
      "16662 Training Loss: tensor(0.3522)\n",
      "16663 Training Loss: tensor(0.3525)\n",
      "16664 Training Loss: tensor(0.3535)\n",
      "16665 Training Loss: tensor(0.3550)\n",
      "16666 Training Loss: tensor(0.3545)\n",
      "16667 Training Loss: tensor(0.3631)\n",
      "16668 Training Loss: tensor(0.3538)\n",
      "16669 Training Loss: tensor(0.3547)\n",
      "16670 Training Loss: tensor(0.3534)\n",
      "16671 Training Loss: tensor(0.3612)\n",
      "16672 Training Loss: tensor(0.3542)\n",
      "16673 Training Loss: tensor(0.3603)\n",
      "16674 Training Loss: tensor(0.3534)\n",
      "16675 Training Loss: tensor(0.3557)\n",
      "16676 Training Loss: tensor(0.3557)\n",
      "16677 Training Loss: tensor(0.3536)\n",
      "16678 Training Loss: tensor(0.3550)\n",
      "16679 Training Loss: tensor(0.3539)\n",
      "16680 Training Loss: tensor(0.3525)\n",
      "16681 Training Loss: tensor(0.3538)\n",
      "16682 Training Loss: tensor(0.3530)\n",
      "16683 Training Loss: tensor(0.3600)\n",
      "16684 Training Loss: tensor(0.3535)\n",
      "16685 Training Loss: tensor(0.3536)\n",
      "16686 Training Loss: tensor(0.3531)\n",
      "16687 Training Loss: tensor(0.3571)\n",
      "16688 Training Loss: tensor(0.3590)\n",
      "16689 Training Loss: tensor(0.3534)\n",
      "16690 Training Loss: tensor(0.3520)\n",
      "16691 Training Loss: tensor(0.3521)\n",
      "16692 Training Loss: tensor(0.3554)\n",
      "16693 Training Loss: tensor(0.3531)\n",
      "16694 Training Loss: tensor(0.3551)\n",
      "16695 Training Loss: tensor(0.3639)\n",
      "16696 Training Loss: tensor(0.3582)\n",
      "16697 Training Loss: tensor(0.3630)\n",
      "16698 Training Loss: tensor(0.3601)\n",
      "16699 Training Loss: tensor(0.3537)\n",
      "16700 Training Loss: tensor(0.3570)\n",
      "16701 Training Loss: tensor(0.3532)\n",
      "16702 Training Loss: tensor(0.3533)\n",
      "16703 Training Loss: tensor(0.3523)\n",
      "16704 Training Loss: tensor(0.3579)\n",
      "16705 Training Loss: tensor(0.3538)\n",
      "16706 Training Loss: tensor(0.3529)\n",
      "16707 Training Loss: tensor(0.3570)\n",
      "16708 Training Loss: tensor(0.3552)\n",
      "16709 Training Loss: tensor(0.3552)\n",
      "16710 Training Loss: tensor(0.3557)\n",
      "16711 Training Loss: tensor(0.3564)\n",
      "16712 Training Loss: tensor(0.3572)\n",
      "16713 Training Loss: tensor(0.3535)\n",
      "16714 Training Loss: tensor(0.3611)\n",
      "16715 Training Loss: tensor(0.3530)\n",
      "16716 Training Loss: tensor(0.3560)\n",
      "16717 Training Loss: tensor(0.3550)\n",
      "16718 Training Loss: tensor(0.3558)\n",
      "16719 Training Loss: tensor(0.3531)\n",
      "16720 Training Loss: tensor(0.3543)\n",
      "16721 Training Loss: tensor(0.3573)\n",
      "16722 Training Loss: tensor(0.3545)\n",
      "16723 Training Loss: tensor(0.3534)\n",
      "16724 Training Loss: tensor(0.3563)\n",
      "16725 Training Loss: tensor(0.3549)\n",
      "16726 Training Loss: tensor(0.3533)\n",
      "16727 Training Loss: tensor(0.3543)\n",
      "16728 Training Loss: tensor(0.3561)\n",
      "16729 Training Loss: tensor(0.3523)\n",
      "16730 Training Loss: tensor(0.3548)\n",
      "16731 Training Loss: tensor(0.3547)\n",
      "16732 Training Loss: tensor(0.3606)\n",
      "16733 Training Loss: tensor(0.3566)\n",
      "16734 Training Loss: tensor(0.3534)\n",
      "16735 Training Loss: tensor(0.3605)\n",
      "16736 Training Loss: tensor(0.3537)\n",
      "16737 Training Loss: tensor(0.3542)\n",
      "16738 Training Loss: tensor(0.3590)\n",
      "16739 Training Loss: tensor(0.3558)\n",
      "16740 Training Loss: tensor(0.3581)\n",
      "16741 Training Loss: tensor(0.3539)\n",
      "16742 Training Loss: tensor(0.3555)\n",
      "16743 Training Loss: tensor(0.3554)\n",
      "16744 Training Loss: tensor(0.3530)\n",
      "16745 Training Loss: tensor(0.3629)\n",
      "16746 Training Loss: tensor(0.3622)\n",
      "16747 Training Loss: tensor(0.3541)\n",
      "16748 Training Loss: tensor(0.3559)\n",
      "16749 Training Loss: tensor(0.3539)\n",
      "16750 Training Loss: tensor(0.3561)\n",
      "16751 Training Loss: tensor(0.3541)\n",
      "16752 Training Loss: tensor(0.3564)\n",
      "16753 Training Loss: tensor(0.3539)\n",
      "16754 Training Loss: tensor(0.3551)\n",
      "16755 Training Loss: tensor(0.3563)\n",
      "16756 Training Loss: tensor(0.3534)\n",
      "16757 Training Loss: tensor(0.3533)\n",
      "16758 Training Loss: tensor(0.3607)\n",
      "16759 Training Loss: tensor(0.3581)\n",
      "16760 Training Loss: tensor(0.3531)\n",
      "16761 Training Loss: tensor(0.3544)\n",
      "16762 Training Loss: tensor(0.3556)\n",
      "16763 Training Loss: tensor(0.3535)\n",
      "16764 Training Loss: tensor(0.3687)\n",
      "16765 Training Loss: tensor(0.3524)\n",
      "16766 Training Loss: tensor(0.3555)\n",
      "16767 Training Loss: tensor(0.3531)\n",
      "16768 Training Loss: tensor(0.3554)\n",
      "16769 Training Loss: tensor(0.3564)\n",
      "16770 Training Loss: tensor(0.3562)\n",
      "16771 Training Loss: tensor(0.3543)\n",
      "16772 Training Loss: tensor(0.3551)\n",
      "16773 Training Loss: tensor(0.3575)\n",
      "16774 Training Loss: tensor(0.3555)\n",
      "16775 Training Loss: tensor(0.3532)\n",
      "16776 Training Loss: tensor(0.3556)\n",
      "16777 Training Loss: tensor(0.3536)\n",
      "16778 Training Loss: tensor(0.3550)\n",
      "16779 Training Loss: tensor(0.3525)\n",
      "16780 Training Loss: tensor(0.3538)\n",
      "16781 Training Loss: tensor(0.3543)\n",
      "16782 Training Loss: tensor(0.3569)\n",
      "16783 Training Loss: tensor(0.3630)\n",
      "16784 Training Loss: tensor(0.3568)\n",
      "16785 Training Loss: tensor(0.3569)\n",
      "16786 Training Loss: tensor(0.3538)\n",
      "16787 Training Loss: tensor(0.3543)\n",
      "16788 Training Loss: tensor(0.3529)\n",
      "16789 Training Loss: tensor(0.3542)\n",
      "16790 Training Loss: tensor(0.3579)\n",
      "16791 Training Loss: tensor(0.3553)\n",
      "16792 Training Loss: tensor(0.3527)\n",
      "16793 Training Loss: tensor(0.3532)\n",
      "16794 Training Loss: tensor(0.3619)\n",
      "16795 Training Loss: tensor(0.3535)\n",
      "16796 Training Loss: tensor(0.3637)\n",
      "16797 Training Loss: tensor(0.3572)\n",
      "16798 Training Loss: tensor(0.3562)\n",
      "16799 Training Loss: tensor(0.3541)\n",
      "16800 Training Loss: tensor(0.3543)\n",
      "16801 Training Loss: tensor(0.3545)\n",
      "16802 Training Loss: tensor(0.3561)\n",
      "16803 Training Loss: tensor(0.3531)\n",
      "16804 Training Loss: tensor(0.3554)\n",
      "16805 Training Loss: tensor(0.3528)\n",
      "16806 Training Loss: tensor(0.3568)\n",
      "16807 Training Loss: tensor(0.3546)\n",
      "16808 Training Loss: tensor(0.3617)\n",
      "16809 Training Loss: tensor(0.3530)\n",
      "16810 Training Loss: tensor(0.3531)\n",
      "16811 Training Loss: tensor(0.3539)\n",
      "16812 Training Loss: tensor(0.3531)\n",
      "16813 Training Loss: tensor(0.3563)\n",
      "16814 Training Loss: tensor(0.3543)\n",
      "16815 Training Loss: tensor(0.3536)\n",
      "16816 Training Loss: tensor(0.3591)\n",
      "16817 Training Loss: tensor(0.3519)\n",
      "16818 Training Loss: tensor(0.3554)\n",
      "16819 Training Loss: tensor(0.3520)\n",
      "16820 Training Loss: tensor(0.3554)\n",
      "16821 Training Loss: tensor(0.3538)\n",
      "16822 Training Loss: tensor(0.3561)\n",
      "16823 Training Loss: tensor(0.3521)\n",
      "16824 Training Loss: tensor(0.3563)\n",
      "16825 Training Loss: tensor(0.3534)\n",
      "16826 Training Loss: tensor(0.3525)\n",
      "16827 Training Loss: tensor(0.3634)\n",
      "16828 Training Loss: tensor(0.3539)\n",
      "16829 Training Loss: tensor(0.3613)\n",
      "16830 Training Loss: tensor(0.3517)\n",
      "16831 Training Loss: tensor(0.3583)\n",
      "16832 Training Loss: tensor(0.3532)\n",
      "16833 Training Loss: tensor(0.3564)\n",
      "16834 Training Loss: tensor(0.3586)\n",
      "16835 Training Loss: tensor(0.3543)\n",
      "16836 Training Loss: tensor(0.3524)\n",
      "16837 Training Loss: tensor(0.3530)\n",
      "16838 Training Loss: tensor(0.3544)\n",
      "16839 Training Loss: tensor(0.3588)\n",
      "16840 Training Loss: tensor(0.3538)\n",
      "16841 Training Loss: tensor(0.3593)\n",
      "16842 Training Loss: tensor(0.3544)\n",
      "16843 Training Loss: tensor(0.3619)\n",
      "16844 Training Loss: tensor(0.3578)\n",
      "16845 Training Loss: tensor(0.3533)\n",
      "16846 Training Loss: tensor(0.3545)\n",
      "16847 Training Loss: tensor(0.3534)\n",
      "16848 Training Loss: tensor(0.3547)\n",
      "16849 Training Loss: tensor(0.3569)\n",
      "16850 Training Loss: tensor(0.3545)\n",
      "16851 Training Loss: tensor(0.3582)\n",
      "16852 Training Loss: tensor(0.3571)\n",
      "16853 Training Loss: tensor(0.3565)\n",
      "16854 Training Loss: tensor(0.3525)\n",
      "16855 Training Loss: tensor(0.3552)\n",
      "16856 Training Loss: tensor(0.3571)\n",
      "16857 Training Loss: tensor(0.3538)\n",
      "16858 Training Loss: tensor(0.3528)\n",
      "16859 Training Loss: tensor(0.3556)\n",
      "16860 Training Loss: tensor(0.3532)\n",
      "16861 Training Loss: tensor(0.3529)\n",
      "16862 Training Loss: tensor(0.3529)\n",
      "16863 Training Loss: tensor(0.3530)\n",
      "16864 Training Loss: tensor(0.3651)\n",
      "16865 Training Loss: tensor(0.3567)\n",
      "16866 Training Loss: tensor(0.3538)\n",
      "16867 Training Loss: tensor(0.3522)\n",
      "16868 Training Loss: tensor(0.3555)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16869 Training Loss: tensor(0.3581)\n",
      "16870 Training Loss: tensor(0.3565)\n",
      "16871 Training Loss: tensor(0.3572)\n",
      "16872 Training Loss: tensor(0.3526)\n",
      "16873 Training Loss: tensor(0.3555)\n",
      "16874 Training Loss: tensor(0.3535)\n",
      "16875 Training Loss: tensor(0.3575)\n",
      "16876 Training Loss: tensor(0.3533)\n",
      "16877 Training Loss: tensor(0.3533)\n",
      "16878 Training Loss: tensor(0.3528)\n",
      "16879 Training Loss: tensor(0.3540)\n",
      "16880 Training Loss: tensor(0.3548)\n",
      "16881 Training Loss: tensor(0.3536)\n",
      "16882 Training Loss: tensor(0.3586)\n",
      "16883 Training Loss: tensor(0.3569)\n",
      "16884 Training Loss: tensor(0.3542)\n",
      "16885 Training Loss: tensor(0.3555)\n",
      "16886 Training Loss: tensor(0.3533)\n",
      "16887 Training Loss: tensor(0.3627)\n",
      "16888 Training Loss: tensor(0.3563)\n",
      "16889 Training Loss: tensor(0.3529)\n",
      "16890 Training Loss: tensor(0.3552)\n",
      "16891 Training Loss: tensor(0.3541)\n",
      "16892 Training Loss: tensor(0.3525)\n",
      "16893 Training Loss: tensor(0.3568)\n",
      "16894 Training Loss: tensor(0.3555)\n",
      "16895 Training Loss: tensor(0.3544)\n",
      "16896 Training Loss: tensor(0.3560)\n",
      "16897 Training Loss: tensor(0.3563)\n",
      "16898 Training Loss: tensor(0.3525)\n",
      "16899 Training Loss: tensor(0.3536)\n",
      "16900 Training Loss: tensor(0.3521)\n",
      "16901 Training Loss: tensor(0.3546)\n",
      "16902 Training Loss: tensor(0.3540)\n",
      "16903 Training Loss: tensor(0.3515)\n",
      "16904 Training Loss: tensor(0.3544)\n",
      "16905 Training Loss: tensor(0.3548)\n",
      "16906 Training Loss: tensor(0.3558)\n",
      "16907 Training Loss: tensor(0.3537)\n",
      "16908 Training Loss: tensor(0.3645)\n",
      "16909 Training Loss: tensor(0.3534)\n",
      "16910 Training Loss: tensor(0.3566)\n",
      "16911 Training Loss: tensor(0.3532)\n",
      "16912 Training Loss: tensor(0.3553)\n",
      "16913 Training Loss: tensor(0.3576)\n",
      "16914 Training Loss: tensor(0.3528)\n",
      "16915 Training Loss: tensor(0.3550)\n",
      "16916 Training Loss: tensor(0.3544)\n",
      "16917 Training Loss: tensor(0.3556)\n",
      "16918 Training Loss: tensor(0.3548)\n",
      "16919 Training Loss: tensor(0.3553)\n",
      "16920 Training Loss: tensor(0.3523)\n",
      "16921 Training Loss: tensor(0.3607)\n",
      "16922 Training Loss: tensor(0.3518)\n",
      "16923 Training Loss: tensor(0.3551)\n",
      "16924 Training Loss: tensor(0.3534)\n",
      "16925 Training Loss: tensor(0.3572)\n",
      "16926 Training Loss: tensor(0.3551)\n",
      "16927 Training Loss: tensor(0.3528)\n",
      "16928 Training Loss: tensor(0.3574)\n",
      "16929 Training Loss: tensor(0.3543)\n",
      "16930 Training Loss: tensor(0.3561)\n",
      "16931 Training Loss: tensor(0.3525)\n",
      "16932 Training Loss: tensor(0.3567)\n",
      "16933 Training Loss: tensor(0.3536)\n",
      "16934 Training Loss: tensor(0.3581)\n",
      "16935 Training Loss: tensor(0.3616)\n",
      "16936 Training Loss: tensor(0.3545)\n",
      "16937 Training Loss: tensor(0.3515)\n",
      "16938 Training Loss: tensor(0.3553)\n",
      "16939 Training Loss: tensor(0.3559)\n",
      "16940 Training Loss: tensor(0.3561)\n",
      "16941 Training Loss: tensor(0.3601)\n",
      "16942 Training Loss: tensor(0.3522)\n",
      "16943 Training Loss: tensor(0.3565)\n",
      "16944 Training Loss: tensor(0.3558)\n",
      "16945 Training Loss: tensor(0.3533)\n",
      "16946 Training Loss: tensor(0.3525)\n",
      "16947 Training Loss: tensor(0.3527)\n",
      "16948 Training Loss: tensor(0.3544)\n",
      "16949 Training Loss: tensor(0.3529)\n",
      "16950 Training Loss: tensor(0.3595)\n",
      "16951 Training Loss: tensor(0.3534)\n",
      "16952 Training Loss: tensor(0.3532)\n",
      "16953 Training Loss: tensor(0.3546)\n",
      "16954 Training Loss: tensor(0.3566)\n",
      "16955 Training Loss: tensor(0.3552)\n",
      "16956 Training Loss: tensor(0.3525)\n",
      "16957 Training Loss: tensor(0.3538)\n",
      "16958 Training Loss: tensor(0.3554)\n",
      "16959 Training Loss: tensor(0.3538)\n",
      "16960 Training Loss: tensor(0.3555)\n",
      "16961 Training Loss: tensor(0.3542)\n",
      "16962 Training Loss: tensor(0.3532)\n",
      "16963 Training Loss: tensor(0.3516)\n",
      "16964 Training Loss: tensor(0.3563)\n",
      "16965 Training Loss: tensor(0.3569)\n",
      "16966 Training Loss: tensor(0.3552)\n",
      "16967 Training Loss: tensor(0.3527)\n",
      "16968 Training Loss: tensor(0.3531)\n",
      "16969 Training Loss: tensor(0.3519)\n",
      "16970 Training Loss: tensor(0.3613)\n",
      "16971 Training Loss: tensor(0.3523)\n",
      "16972 Training Loss: tensor(0.3521)\n",
      "16973 Training Loss: tensor(0.3556)\n",
      "16974 Training Loss: tensor(0.3646)\n",
      "16975 Training Loss: tensor(0.3536)\n",
      "16976 Training Loss: tensor(0.3544)\n",
      "16977 Training Loss: tensor(0.3536)\n",
      "16978 Training Loss: tensor(0.3562)\n",
      "16979 Training Loss: tensor(0.3553)\n",
      "16980 Training Loss: tensor(0.3532)\n",
      "16981 Training Loss: tensor(0.3527)\n",
      "16982 Training Loss: tensor(0.3536)\n",
      "16983 Training Loss: tensor(0.3539)\n",
      "16984 Training Loss: tensor(0.3593)\n",
      "16985 Training Loss: tensor(0.3521)\n",
      "16986 Training Loss: tensor(0.3558)\n",
      "16987 Training Loss: tensor(0.3521)\n",
      "16988 Training Loss: tensor(0.3555)\n",
      "16989 Training Loss: tensor(0.3542)\n",
      "16990 Training Loss: tensor(0.3519)\n",
      "16991 Training Loss: tensor(0.3590)\n",
      "16992 Training Loss: tensor(0.3547)\n",
      "16993 Training Loss: tensor(0.3561)\n",
      "16994 Training Loss: tensor(0.3521)\n",
      "16995 Training Loss: tensor(0.3550)\n",
      "16996 Training Loss: tensor(0.3510)\n",
      "16997 Training Loss: tensor(0.3596)\n",
      "16998 Training Loss: tensor(0.3551)\n",
      "16999 Training Loss: tensor(0.3580)\n",
      "17000 Training Loss: tensor(0.3567)\n",
      "17001 Training Loss: tensor(0.3526)\n",
      "17002 Training Loss: tensor(0.3548)\n",
      "17003 Training Loss: tensor(0.3531)\n",
      "17004 Training Loss: tensor(0.3606)\n",
      "17005 Training Loss: tensor(0.3579)\n",
      "17006 Training Loss: tensor(0.3517)\n",
      "17007 Training Loss: tensor(0.3538)\n",
      "17008 Training Loss: tensor(0.3541)\n",
      "17009 Training Loss: tensor(0.3547)\n",
      "17010 Training Loss: tensor(0.3517)\n",
      "17011 Training Loss: tensor(0.3517)\n",
      "17012 Training Loss: tensor(0.3566)\n",
      "17013 Training Loss: tensor(0.3536)\n",
      "17014 Training Loss: tensor(0.3565)\n",
      "17015 Training Loss: tensor(0.3526)\n",
      "17016 Training Loss: tensor(0.3548)\n",
      "17017 Training Loss: tensor(0.3659)\n",
      "17018 Training Loss: tensor(0.3576)\n",
      "17019 Training Loss: tensor(0.3526)\n",
      "17020 Training Loss: tensor(0.3522)\n",
      "17021 Training Loss: tensor(0.3566)\n",
      "17022 Training Loss: tensor(0.3551)\n",
      "17023 Training Loss: tensor(0.3538)\n",
      "17024 Training Loss: tensor(0.3536)\n",
      "17025 Training Loss: tensor(0.3539)\n",
      "17026 Training Loss: tensor(0.3529)\n",
      "17027 Training Loss: tensor(0.3535)\n",
      "17028 Training Loss: tensor(0.3615)\n",
      "17029 Training Loss: tensor(0.3530)\n",
      "17030 Training Loss: tensor(0.3527)\n",
      "17031 Training Loss: tensor(0.3540)\n",
      "17032 Training Loss: tensor(0.3552)\n",
      "17033 Training Loss: tensor(0.3531)\n",
      "17034 Training Loss: tensor(0.3537)\n",
      "17035 Training Loss: tensor(0.3594)\n",
      "17036 Training Loss: tensor(0.3525)\n",
      "17037 Training Loss: tensor(0.3548)\n",
      "17038 Training Loss: tensor(0.3532)\n",
      "17039 Training Loss: tensor(0.3587)\n",
      "17040 Training Loss: tensor(0.3549)\n",
      "17041 Training Loss: tensor(0.3538)\n",
      "17042 Training Loss: tensor(0.3529)\n",
      "17043 Training Loss: tensor(0.3521)\n",
      "17044 Training Loss: tensor(0.3538)\n",
      "17045 Training Loss: tensor(0.3541)\n",
      "17046 Training Loss: tensor(0.3538)\n",
      "17047 Training Loss: tensor(0.3536)\n",
      "17048 Training Loss: tensor(0.3523)\n",
      "17049 Training Loss: tensor(0.3565)\n",
      "17050 Training Loss: tensor(0.3509)\n",
      "17051 Training Loss: tensor(0.3513)\n",
      "17052 Training Loss: tensor(0.3515)\n",
      "17053 Training Loss: tensor(0.3537)\n",
      "17054 Training Loss: tensor(0.3515)\n",
      "17055 Training Loss: tensor(0.3531)\n",
      "17056 Training Loss: tensor(0.3519)\n",
      "17057 Training Loss: tensor(0.3513)\n",
      "17058 Training Loss: tensor(0.3516)\n",
      "17059 Training Loss: tensor(0.3549)\n",
      "17060 Training Loss: tensor(0.3518)\n",
      "17061 Training Loss: tensor(0.3592)\n",
      "17062 Training Loss: tensor(0.3546)\n",
      "17063 Training Loss: tensor(0.3546)\n",
      "17064 Training Loss: tensor(0.3528)\n",
      "17065 Training Loss: tensor(0.3533)\n",
      "17066 Training Loss: tensor(0.3555)\n",
      "17067 Training Loss: tensor(0.3659)\n",
      "17068 Training Loss: tensor(0.3537)\n",
      "17069 Training Loss: tensor(0.3517)\n",
      "17070 Training Loss: tensor(0.3525)\n",
      "17071 Training Loss: tensor(0.3524)\n",
      "17072 Training Loss: tensor(0.3533)\n",
      "17073 Training Loss: tensor(0.3518)\n",
      "17074 Training Loss: tensor(0.3529)\n",
      "17075 Training Loss: tensor(0.3593)\n",
      "17076 Training Loss: tensor(0.3521)\n",
      "17077 Training Loss: tensor(0.3531)\n",
      "17078 Training Loss: tensor(0.3514)\n",
      "17079 Training Loss: tensor(0.3519)\n",
      "17080 Training Loss: tensor(0.3591)\n",
      "17081 Training Loss: tensor(0.3543)\n",
      "17082 Training Loss: tensor(0.3584)\n",
      "17083 Training Loss: tensor(0.3536)\n",
      "17084 Training Loss: tensor(0.3564)\n",
      "17085 Training Loss: tensor(0.3527)\n",
      "17086 Training Loss: tensor(0.3578)\n",
      "17087 Training Loss: tensor(0.3525)\n",
      "17088 Training Loss: tensor(0.3551)\n",
      "17089 Training Loss: tensor(0.3520)\n",
      "17090 Training Loss: tensor(0.3528)\n",
      "17091 Training Loss: tensor(0.3518)\n",
      "17092 Training Loss: tensor(0.3514)\n",
      "17093 Training Loss: tensor(0.3615)\n",
      "17094 Training Loss: tensor(0.3513)\n",
      "17095 Training Loss: tensor(0.3526)\n",
      "17096 Training Loss: tensor(0.3597)\n",
      "17097 Training Loss: tensor(0.3536)\n",
      "17098 Training Loss: tensor(0.3543)\n",
      "17099 Training Loss: tensor(0.3519)\n",
      "17100 Training Loss: tensor(0.3520)\n",
      "17101 Training Loss: tensor(0.3520)\n",
      "17102 Training Loss: tensor(0.3534)\n",
      "17103 Training Loss: tensor(0.3510)\n",
      "17104 Training Loss: tensor(0.3532)\n",
      "17105 Training Loss: tensor(0.3513)\n",
      "17106 Training Loss: tensor(0.3524)\n",
      "17107 Training Loss: tensor(0.3534)\n",
      "17108 Training Loss: tensor(0.3538)\n",
      "17109 Training Loss: tensor(0.3562)\n",
      "17110 Training Loss: tensor(0.3561)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17111 Training Loss: tensor(0.3568)\n",
      "17112 Training Loss: tensor(0.3502)\n",
      "17113 Training Loss: tensor(0.3547)\n",
      "17114 Training Loss: tensor(0.3542)\n",
      "17115 Training Loss: tensor(0.3549)\n",
      "17116 Training Loss: tensor(0.3518)\n",
      "17117 Training Loss: tensor(0.3547)\n",
      "17118 Training Loss: tensor(0.3513)\n",
      "17119 Training Loss: tensor(0.3574)\n",
      "17120 Training Loss: tensor(0.3529)\n",
      "17121 Training Loss: tensor(0.3535)\n",
      "17122 Training Loss: tensor(0.3574)\n",
      "17123 Training Loss: tensor(0.3518)\n",
      "17124 Training Loss: tensor(0.3596)\n",
      "17125 Training Loss: tensor(0.3513)\n",
      "17126 Training Loss: tensor(0.3550)\n",
      "17127 Training Loss: tensor(0.3529)\n",
      "17128 Training Loss: tensor(0.3565)\n",
      "17129 Training Loss: tensor(0.3518)\n",
      "17130 Training Loss: tensor(0.3546)\n",
      "17131 Training Loss: tensor(0.3538)\n",
      "17132 Training Loss: tensor(0.3529)\n",
      "17133 Training Loss: tensor(0.3549)\n",
      "17134 Training Loss: tensor(0.3522)\n",
      "17135 Training Loss: tensor(0.3546)\n",
      "17136 Training Loss: tensor(0.3593)\n",
      "17137 Training Loss: tensor(0.3529)\n",
      "17138 Training Loss: tensor(0.3558)\n",
      "17139 Training Loss: tensor(0.3523)\n",
      "17140 Training Loss: tensor(0.3596)\n",
      "17141 Training Loss: tensor(0.3528)\n",
      "17142 Training Loss: tensor(0.3527)\n",
      "17143 Training Loss: tensor(0.3526)\n",
      "17144 Training Loss: tensor(0.3541)\n",
      "17145 Training Loss: tensor(0.3536)\n",
      "17146 Training Loss: tensor(0.3528)\n",
      "17147 Training Loss: tensor(0.3662)\n",
      "17148 Training Loss: tensor(0.3548)\n",
      "17149 Training Loss: tensor(0.3538)\n",
      "17150 Training Loss: tensor(0.3522)\n",
      "17151 Training Loss: tensor(0.3560)\n",
      "17152 Training Loss: tensor(0.3527)\n",
      "17153 Training Loss: tensor(0.3571)\n",
      "17154 Training Loss: tensor(0.3526)\n",
      "17155 Training Loss: tensor(0.3626)\n",
      "17156 Training Loss: tensor(0.3586)\n",
      "17157 Training Loss: tensor(0.3534)\n",
      "17158 Training Loss: tensor(0.3571)\n",
      "17159 Training Loss: tensor(0.3531)\n",
      "17160 Training Loss: tensor(0.3556)\n",
      "17161 Training Loss: tensor(0.3580)\n",
      "17162 Training Loss: tensor(0.3519)\n",
      "17163 Training Loss: tensor(0.3563)\n",
      "17164 Training Loss: tensor(0.3532)\n",
      "17165 Training Loss: tensor(0.3575)\n",
      "17166 Training Loss: tensor(0.3563)\n",
      "17167 Training Loss: tensor(0.3537)\n",
      "17168 Training Loss: tensor(0.3539)\n",
      "17169 Training Loss: tensor(0.3515)\n",
      "17170 Training Loss: tensor(0.3576)\n",
      "17171 Training Loss: tensor(0.3527)\n",
      "17172 Training Loss: tensor(0.3576)\n",
      "17173 Training Loss: tensor(0.3532)\n",
      "17174 Training Loss: tensor(0.3523)\n",
      "17175 Training Loss: tensor(0.3519)\n",
      "17176 Training Loss: tensor(0.3571)\n",
      "17177 Training Loss: tensor(0.3558)\n",
      "17178 Training Loss: tensor(0.3552)\n",
      "17179 Training Loss: tensor(0.3530)\n",
      "17180 Training Loss: tensor(0.3547)\n",
      "17181 Training Loss: tensor(0.3563)\n",
      "17182 Training Loss: tensor(0.3586)\n",
      "17183 Training Loss: tensor(0.3546)\n",
      "17184 Training Loss: tensor(0.3595)\n",
      "17185 Training Loss: tensor(0.3522)\n",
      "17186 Training Loss: tensor(0.3596)\n",
      "17187 Training Loss: tensor(0.3524)\n",
      "17188 Training Loss: tensor(0.3529)\n",
      "17189 Training Loss: tensor(0.3536)\n",
      "17190 Training Loss: tensor(0.3540)\n",
      "17191 Training Loss: tensor(0.3524)\n",
      "17192 Training Loss: tensor(0.3543)\n",
      "17193 Training Loss: tensor(0.3523)\n",
      "17194 Training Loss: tensor(0.3527)\n",
      "17195 Training Loss: tensor(0.3513)\n",
      "17196 Training Loss: tensor(0.3525)\n",
      "17197 Training Loss: tensor(0.3557)\n",
      "17198 Training Loss: tensor(0.3512)\n",
      "17199 Training Loss: tensor(0.3549)\n",
      "17200 Training Loss: tensor(0.3522)\n",
      "17201 Training Loss: tensor(0.3602)\n",
      "17202 Training Loss: tensor(0.3548)\n",
      "17203 Training Loss: tensor(0.3572)\n",
      "17204 Training Loss: tensor(0.3514)\n",
      "17205 Training Loss: tensor(0.3519)\n",
      "17206 Training Loss: tensor(0.3518)\n",
      "17207 Training Loss: tensor(0.3567)\n",
      "17208 Training Loss: tensor(0.3516)\n",
      "17209 Training Loss: tensor(0.3542)\n",
      "17210 Training Loss: tensor(0.3596)\n",
      "17211 Training Loss: tensor(0.3524)\n",
      "17212 Training Loss: tensor(0.3535)\n",
      "17213 Training Loss: tensor(0.3544)\n",
      "17214 Training Loss: tensor(0.3524)\n",
      "17215 Training Loss: tensor(0.3588)\n",
      "17216 Training Loss: tensor(0.3554)\n",
      "17217 Training Loss: tensor(0.3589)\n",
      "17218 Training Loss: tensor(0.3533)\n",
      "17219 Training Loss: tensor(0.3531)\n",
      "17220 Training Loss: tensor(0.3546)\n",
      "17221 Training Loss: tensor(0.3526)\n",
      "17222 Training Loss: tensor(0.3559)\n",
      "17223 Training Loss: tensor(0.3585)\n",
      "17224 Training Loss: tensor(0.3528)\n",
      "17225 Training Loss: tensor(0.3533)\n",
      "17226 Training Loss: tensor(0.3543)\n",
      "17227 Training Loss: tensor(0.3585)\n",
      "17228 Training Loss: tensor(0.3530)\n",
      "17229 Training Loss: tensor(0.3567)\n",
      "17230 Training Loss: tensor(0.3547)\n",
      "17231 Training Loss: tensor(0.3557)\n",
      "17232 Training Loss: tensor(0.3554)\n",
      "17233 Training Loss: tensor(0.3542)\n",
      "17234 Training Loss: tensor(0.3554)\n",
      "17235 Training Loss: tensor(0.3526)\n",
      "17236 Training Loss: tensor(0.3558)\n",
      "17237 Training Loss: tensor(0.3532)\n",
      "17238 Training Loss: tensor(0.3522)\n",
      "17239 Training Loss: tensor(0.3519)\n",
      "17240 Training Loss: tensor(0.3571)\n",
      "17241 Training Loss: tensor(0.3572)\n",
      "17242 Training Loss: tensor(0.3511)\n",
      "17243 Training Loss: tensor(0.3536)\n",
      "17244 Training Loss: tensor(0.3507)\n",
      "17245 Training Loss: tensor(0.3513)\n",
      "17246 Training Loss: tensor(0.3526)\n",
      "17247 Training Loss: tensor(0.3509)\n",
      "17248 Training Loss: tensor(0.3527)\n",
      "17249 Training Loss: tensor(0.3543)\n",
      "17250 Training Loss: tensor(0.3544)\n",
      "17251 Training Loss: tensor(0.3558)\n",
      "17252 Training Loss: tensor(0.3518)\n",
      "17253 Training Loss: tensor(0.3552)\n",
      "17254 Training Loss: tensor(0.3562)\n",
      "17255 Training Loss: tensor(0.3519)\n",
      "17256 Training Loss: tensor(0.3520)\n",
      "17257 Training Loss: tensor(0.3566)\n",
      "17258 Training Loss: tensor(0.3572)\n",
      "17259 Training Loss: tensor(0.3523)\n",
      "17260 Training Loss: tensor(0.3521)\n",
      "17261 Training Loss: tensor(0.3559)\n",
      "17262 Training Loss: tensor(0.3552)\n",
      "17263 Training Loss: tensor(0.3537)\n",
      "17264 Training Loss: tensor(0.3507)\n",
      "17265 Training Loss: tensor(0.3575)\n",
      "17266 Training Loss: tensor(0.3602)\n",
      "17267 Training Loss: tensor(0.3524)\n",
      "17268 Training Loss: tensor(0.3517)\n",
      "17269 Training Loss: tensor(0.3570)\n",
      "17270 Training Loss: tensor(0.3557)\n",
      "17271 Training Loss: tensor(0.3520)\n",
      "17272 Training Loss: tensor(0.3579)\n",
      "17273 Training Loss: tensor(0.3614)\n",
      "17274 Training Loss: tensor(0.3548)\n",
      "17275 Training Loss: tensor(0.3579)\n",
      "17276 Training Loss: tensor(0.3567)\n",
      "17277 Training Loss: tensor(0.3538)\n",
      "17278 Training Loss: tensor(0.3558)\n",
      "17279 Training Loss: tensor(0.3550)\n",
      "17280 Training Loss: tensor(0.3535)\n",
      "17281 Training Loss: tensor(0.3533)\n",
      "17282 Training Loss: tensor(0.3540)\n",
      "17283 Training Loss: tensor(0.3605)\n",
      "17284 Training Loss: tensor(0.3573)\n",
      "17285 Training Loss: tensor(0.3532)\n",
      "17286 Training Loss: tensor(0.3534)\n",
      "17287 Training Loss: tensor(0.3572)\n",
      "17288 Training Loss: tensor(0.3541)\n",
      "17289 Training Loss: tensor(0.3522)\n",
      "17290 Training Loss: tensor(0.3535)\n",
      "17291 Training Loss: tensor(0.3591)\n",
      "17292 Training Loss: tensor(0.3526)\n",
      "17293 Training Loss: tensor(0.3537)\n",
      "17294 Training Loss: tensor(0.3519)\n",
      "17295 Training Loss: tensor(0.3527)\n",
      "17296 Training Loss: tensor(0.3516)\n",
      "17297 Training Loss: tensor(0.3536)\n",
      "17298 Training Loss: tensor(0.3540)\n",
      "17299 Training Loss: tensor(0.3541)\n",
      "17300 Training Loss: tensor(0.3578)\n",
      "17301 Training Loss: tensor(0.3570)\n",
      "17302 Training Loss: tensor(0.3553)\n",
      "17303 Training Loss: tensor(0.3606)\n",
      "17304 Training Loss: tensor(0.3606)\n",
      "17305 Training Loss: tensor(0.3539)\n",
      "17306 Training Loss: tensor(0.3587)\n",
      "17307 Training Loss: tensor(0.3534)\n",
      "17308 Training Loss: tensor(0.3530)\n",
      "17309 Training Loss: tensor(0.3603)\n",
      "17310 Training Loss: tensor(0.3526)\n",
      "17311 Training Loss: tensor(0.3588)\n",
      "17312 Training Loss: tensor(0.3532)\n",
      "17313 Training Loss: tensor(0.3535)\n",
      "17314 Training Loss: tensor(0.3533)\n",
      "17315 Training Loss: tensor(0.3559)\n",
      "17316 Training Loss: tensor(0.3544)\n",
      "17317 Training Loss: tensor(0.3571)\n",
      "17318 Training Loss: tensor(0.3573)\n",
      "17319 Training Loss: tensor(0.3526)\n",
      "17320 Training Loss: tensor(0.3528)\n",
      "17321 Training Loss: tensor(0.3529)\n",
      "17322 Training Loss: tensor(0.3524)\n",
      "17323 Training Loss: tensor(0.3521)\n",
      "17324 Training Loss: tensor(0.3571)\n",
      "17325 Training Loss: tensor(0.3511)\n",
      "17326 Training Loss: tensor(0.3534)\n",
      "17327 Training Loss: tensor(0.3586)\n",
      "17328 Training Loss: tensor(0.3591)\n",
      "17329 Training Loss: tensor(0.3512)\n",
      "17330 Training Loss: tensor(0.3559)\n",
      "17331 Training Loss: tensor(0.3541)\n",
      "17332 Training Loss: tensor(0.3535)\n",
      "17333 Training Loss: tensor(0.3519)\n",
      "17334 Training Loss: tensor(0.3522)\n",
      "17335 Training Loss: tensor(0.3549)\n",
      "17336 Training Loss: tensor(0.3513)\n",
      "17337 Training Loss: tensor(0.3526)\n",
      "17338 Training Loss: tensor(0.3510)\n",
      "17339 Training Loss: tensor(0.3529)\n",
      "17340 Training Loss: tensor(0.3560)\n",
      "17341 Training Loss: tensor(0.3544)\n",
      "17342 Training Loss: tensor(0.3575)\n",
      "17343 Training Loss: tensor(0.3579)\n",
      "17344 Training Loss: tensor(0.3552)\n",
      "17345 Training Loss: tensor(0.3564)\n",
      "17346 Training Loss: tensor(0.3516)\n",
      "17347 Training Loss: tensor(0.3540)\n",
      "17348 Training Loss: tensor(0.3594)\n",
      "17349 Training Loss: tensor(0.3534)\n",
      "17350 Training Loss: tensor(0.3641)\n",
      "17351 Training Loss: tensor(0.3521)\n",
      "17352 Training Loss: tensor(0.3574)\n",
      "17353 Training Loss: tensor(0.3531)\n",
      "17354 Training Loss: tensor(0.3574)\n",
      "17355 Training Loss: tensor(0.3519)\n",
      "17356 Training Loss: tensor(0.3533)\n",
      "17357 Training Loss: tensor(0.3522)\n",
      "17358 Training Loss: tensor(0.3558)\n",
      "17359 Training Loss: tensor(0.3554)\n",
      "17360 Training Loss: tensor(0.3550)\n",
      "17361 Training Loss: tensor(0.3551)\n",
      "17362 Training Loss: tensor(0.3534)\n",
      "17363 Training Loss: tensor(0.3580)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17364 Training Loss: tensor(0.3524)\n",
      "17365 Training Loss: tensor(0.3532)\n",
      "17366 Training Loss: tensor(0.3515)\n",
      "17367 Training Loss: tensor(0.3529)\n",
      "17368 Training Loss: tensor(0.3539)\n",
      "17369 Training Loss: tensor(0.3525)\n",
      "17370 Training Loss: tensor(0.3555)\n",
      "17371 Training Loss: tensor(0.3526)\n",
      "17372 Training Loss: tensor(0.3523)\n",
      "17373 Training Loss: tensor(0.3542)\n",
      "17374 Training Loss: tensor(0.3514)\n",
      "17375 Training Loss: tensor(0.3509)\n",
      "17376 Training Loss: tensor(0.3530)\n",
      "17377 Training Loss: tensor(0.3536)\n",
      "17378 Training Loss: tensor(0.3573)\n",
      "17379 Training Loss: tensor(0.3524)\n",
      "17380 Training Loss: tensor(0.3612)\n",
      "17381 Training Loss: tensor(0.3575)\n",
      "17382 Training Loss: tensor(0.3634)\n",
      "17383 Training Loss: tensor(0.3541)\n",
      "17384 Training Loss: tensor(0.3562)\n",
      "17385 Training Loss: tensor(0.3546)\n",
      "17386 Training Loss: tensor(0.3536)\n",
      "17387 Training Loss: tensor(0.3517)\n",
      "17388 Training Loss: tensor(0.3526)\n",
      "17389 Training Loss: tensor(0.3532)\n",
      "17390 Training Loss: tensor(0.3541)\n",
      "17391 Training Loss: tensor(0.3565)\n",
      "17392 Training Loss: tensor(0.3564)\n",
      "17393 Training Loss: tensor(0.3522)\n",
      "17394 Training Loss: tensor(0.3535)\n",
      "17395 Training Loss: tensor(0.3544)\n",
      "17396 Training Loss: tensor(0.3542)\n",
      "17397 Training Loss: tensor(0.3531)\n",
      "17398 Training Loss: tensor(0.3535)\n",
      "17399 Training Loss: tensor(0.3520)\n",
      "17400 Training Loss: tensor(0.3594)\n",
      "17401 Training Loss: tensor(0.3515)\n",
      "17402 Training Loss: tensor(0.3514)\n",
      "17403 Training Loss: tensor(0.3569)\n",
      "17404 Training Loss: tensor(0.3539)\n",
      "17405 Training Loss: tensor(0.3535)\n",
      "17406 Training Loss: tensor(0.3532)\n",
      "17407 Training Loss: tensor(0.3546)\n",
      "17408 Training Loss: tensor(0.3581)\n",
      "17409 Training Loss: tensor(0.3562)\n",
      "17410 Training Loss: tensor(0.3580)\n",
      "17411 Training Loss: tensor(0.3531)\n",
      "17412 Training Loss: tensor(0.3517)\n",
      "17413 Training Loss: tensor(0.3532)\n",
      "17414 Training Loss: tensor(0.3553)\n",
      "17415 Training Loss: tensor(0.3529)\n",
      "17416 Training Loss: tensor(0.3510)\n",
      "17417 Training Loss: tensor(0.3561)\n",
      "17418 Training Loss: tensor(0.3548)\n",
      "17419 Training Loss: tensor(0.3561)\n",
      "17420 Training Loss: tensor(0.3524)\n",
      "17421 Training Loss: tensor(0.3585)\n",
      "17422 Training Loss: tensor(0.3611)\n",
      "17423 Training Loss: tensor(0.3519)\n",
      "17424 Training Loss: tensor(0.3525)\n",
      "17425 Training Loss: tensor(0.3554)\n",
      "17426 Training Loss: tensor(0.3585)\n",
      "17427 Training Loss: tensor(0.3561)\n",
      "17428 Training Loss: tensor(0.3548)\n",
      "17429 Training Loss: tensor(0.3534)\n",
      "17430 Training Loss: tensor(0.3560)\n",
      "17431 Training Loss: tensor(0.3573)\n",
      "17432 Training Loss: tensor(0.3553)\n",
      "17433 Training Loss: tensor(0.3542)\n",
      "17434 Training Loss: tensor(0.3535)\n",
      "17435 Training Loss: tensor(0.3542)\n",
      "17436 Training Loss: tensor(0.3525)\n",
      "17437 Training Loss: tensor(0.3537)\n",
      "17438 Training Loss: tensor(0.3561)\n",
      "17439 Training Loss: tensor(0.3545)\n",
      "17440 Training Loss: tensor(0.3555)\n",
      "17441 Training Loss: tensor(0.3520)\n",
      "17442 Training Loss: tensor(0.3569)\n",
      "17443 Training Loss: tensor(0.3519)\n",
      "17444 Training Loss: tensor(0.3526)\n",
      "17445 Training Loss: tensor(0.3544)\n",
      "17446 Training Loss: tensor(0.3544)\n",
      "17447 Training Loss: tensor(0.3542)\n",
      "17448 Training Loss: tensor(0.3558)\n",
      "17449 Training Loss: tensor(0.3549)\n",
      "17450 Training Loss: tensor(0.3579)\n",
      "17451 Training Loss: tensor(0.3536)\n",
      "17452 Training Loss: tensor(0.3541)\n",
      "17453 Training Loss: tensor(0.3567)\n",
      "17454 Training Loss: tensor(0.3550)\n",
      "17455 Training Loss: tensor(0.3579)\n",
      "17456 Training Loss: tensor(0.3561)\n",
      "17457 Training Loss: tensor(0.3552)\n",
      "17458 Training Loss: tensor(0.3520)\n",
      "17459 Training Loss: tensor(0.3546)\n",
      "17460 Training Loss: tensor(0.3556)\n",
      "17461 Training Loss: tensor(0.3578)\n",
      "17462 Training Loss: tensor(0.3527)\n",
      "17463 Training Loss: tensor(0.3523)\n",
      "17464 Training Loss: tensor(0.3521)\n",
      "17465 Training Loss: tensor(0.3544)\n",
      "17466 Training Loss: tensor(0.3544)\n",
      "17467 Training Loss: tensor(0.3556)\n",
      "17468 Training Loss: tensor(0.3533)\n",
      "17469 Training Loss: tensor(0.3601)\n",
      "17470 Training Loss: tensor(0.3556)\n",
      "17471 Training Loss: tensor(0.3562)\n",
      "17472 Training Loss: tensor(0.3557)\n",
      "17473 Training Loss: tensor(0.3553)\n",
      "17474 Training Loss: tensor(0.3571)\n",
      "17475 Training Loss: tensor(0.3538)\n",
      "17476 Training Loss: tensor(0.3530)\n",
      "17477 Training Loss: tensor(0.3553)\n",
      "17478 Training Loss: tensor(0.3587)\n",
      "17479 Training Loss: tensor(0.3519)\n",
      "17480 Training Loss: tensor(0.3629)\n",
      "17481 Training Loss: tensor(0.3552)\n",
      "17482 Training Loss: tensor(0.3539)\n",
      "17483 Training Loss: tensor(0.3537)\n",
      "17484 Training Loss: tensor(0.3548)\n",
      "17485 Training Loss: tensor(0.3529)\n",
      "17486 Training Loss: tensor(0.3559)\n",
      "17487 Training Loss: tensor(0.3570)\n",
      "17488 Training Loss: tensor(0.3534)\n",
      "17489 Training Loss: tensor(0.3527)\n",
      "17490 Training Loss: tensor(0.3543)\n",
      "17491 Training Loss: tensor(0.3525)\n",
      "17492 Training Loss: tensor(0.3530)\n",
      "17493 Training Loss: tensor(0.3531)\n",
      "17494 Training Loss: tensor(0.3557)\n",
      "17495 Training Loss: tensor(0.3533)\n",
      "17496 Training Loss: tensor(0.3521)\n",
      "17497 Training Loss: tensor(0.3526)\n",
      "17498 Training Loss: tensor(0.3539)\n",
      "17499 Training Loss: tensor(0.3543)\n",
      "17500 Training Loss: tensor(0.3517)\n",
      "17501 Training Loss: tensor(0.3531)\n",
      "17502 Training Loss: tensor(0.3547)\n",
      "17503 Training Loss: tensor(0.3539)\n",
      "17504 Training Loss: tensor(0.3515)\n",
      "17505 Training Loss: tensor(0.3626)\n",
      "17506 Training Loss: tensor(0.3520)\n",
      "17507 Training Loss: tensor(0.3558)\n",
      "17508 Training Loss: tensor(0.3532)\n",
      "17509 Training Loss: tensor(0.3548)\n",
      "17510 Training Loss: tensor(0.3546)\n",
      "17511 Training Loss: tensor(0.3578)\n",
      "17512 Training Loss: tensor(0.3519)\n",
      "17513 Training Loss: tensor(0.3536)\n",
      "17514 Training Loss: tensor(0.3524)\n",
      "17515 Training Loss: tensor(0.3555)\n",
      "17516 Training Loss: tensor(0.3545)\n",
      "17517 Training Loss: tensor(0.3531)\n",
      "17518 Training Loss: tensor(0.3540)\n",
      "17519 Training Loss: tensor(0.3527)\n",
      "17520 Training Loss: tensor(0.3572)\n",
      "17521 Training Loss: tensor(0.3585)\n",
      "17522 Training Loss: tensor(0.3634)\n",
      "17523 Training Loss: tensor(0.3541)\n",
      "17524 Training Loss: tensor(0.3527)\n",
      "17525 Training Loss: tensor(0.3529)\n",
      "17526 Training Loss: tensor(0.3566)\n",
      "17527 Training Loss: tensor(0.3570)\n",
      "17528 Training Loss: tensor(0.3543)\n",
      "17529 Training Loss: tensor(0.3540)\n",
      "17530 Training Loss: tensor(0.3573)\n",
      "17531 Training Loss: tensor(0.3589)\n",
      "17532 Training Loss: tensor(0.3549)\n",
      "17533 Training Loss: tensor(0.3560)\n",
      "17534 Training Loss: tensor(0.3551)\n",
      "17535 Training Loss: tensor(0.3539)\n",
      "17536 Training Loss: tensor(0.3552)\n",
      "17537 Training Loss: tensor(0.3538)\n",
      "17538 Training Loss: tensor(0.3540)\n",
      "17539 Training Loss: tensor(0.3532)\n",
      "17540 Training Loss: tensor(0.3573)\n",
      "17541 Training Loss: tensor(0.3510)\n",
      "17542 Training Loss: tensor(0.3575)\n",
      "17543 Training Loss: tensor(0.3545)\n",
      "17544 Training Loss: tensor(0.3521)\n",
      "17545 Training Loss: tensor(0.3547)\n",
      "17546 Training Loss: tensor(0.3532)\n",
      "17547 Training Loss: tensor(0.3512)\n",
      "17548 Training Loss: tensor(0.3569)\n",
      "17549 Training Loss: tensor(0.3521)\n",
      "17550 Training Loss: tensor(0.3592)\n",
      "17551 Training Loss: tensor(0.3601)\n",
      "17552 Training Loss: tensor(0.3530)\n",
      "17553 Training Loss: tensor(0.3540)\n",
      "17554 Training Loss: tensor(0.3567)\n",
      "17555 Training Loss: tensor(0.3532)\n",
      "17556 Training Loss: tensor(0.3535)\n",
      "17557 Training Loss: tensor(0.3529)\n",
      "17558 Training Loss: tensor(0.3527)\n",
      "17559 Training Loss: tensor(0.3544)\n",
      "17560 Training Loss: tensor(0.3553)\n",
      "17561 Training Loss: tensor(0.3559)\n",
      "17562 Training Loss: tensor(0.3522)\n",
      "17563 Training Loss: tensor(0.3517)\n",
      "17564 Training Loss: tensor(0.3527)\n",
      "17565 Training Loss: tensor(0.3566)\n",
      "17566 Training Loss: tensor(0.3560)\n",
      "17567 Training Loss: tensor(0.3534)\n",
      "17568 Training Loss: tensor(0.3578)\n",
      "17569 Training Loss: tensor(0.3539)\n",
      "17570 Training Loss: tensor(0.3522)\n",
      "17571 Training Loss: tensor(0.3527)\n",
      "17572 Training Loss: tensor(0.3565)\n",
      "17573 Training Loss: tensor(0.3548)\n",
      "17574 Training Loss: tensor(0.3523)\n",
      "17575 Training Loss: tensor(0.3539)\n",
      "17576 Training Loss: tensor(0.3521)\n",
      "17577 Training Loss: tensor(0.3544)\n",
      "17578 Training Loss: tensor(0.3558)\n",
      "17579 Training Loss: tensor(0.3525)\n",
      "17580 Training Loss: tensor(0.3533)\n",
      "17581 Training Loss: tensor(0.3534)\n",
      "17582 Training Loss: tensor(0.3507)\n",
      "17583 Training Loss: tensor(0.3523)\n",
      "17584 Training Loss: tensor(0.3515)\n",
      "17585 Training Loss: tensor(0.3619)\n",
      "17586 Training Loss: tensor(0.3526)\n",
      "17587 Training Loss: tensor(0.3521)\n",
      "17588 Training Loss: tensor(0.3532)\n",
      "17589 Training Loss: tensor(0.3521)\n",
      "17590 Training Loss: tensor(0.3515)\n",
      "17591 Training Loss: tensor(0.3588)\n",
      "17592 Training Loss: tensor(0.3564)\n",
      "17593 Training Loss: tensor(0.3570)\n",
      "17594 Training Loss: tensor(0.3531)\n",
      "17595 Training Loss: tensor(0.3544)\n",
      "17596 Training Loss: tensor(0.3569)\n",
      "17597 Training Loss: tensor(0.3539)\n",
      "17598 Training Loss: tensor(0.3525)\n",
      "17599 Training Loss: tensor(0.3558)\n",
      "17600 Training Loss: tensor(0.3632)\n",
      "17601 Training Loss: tensor(0.3535)\n",
      "17602 Training Loss: tensor(0.3579)\n",
      "17603 Training Loss: tensor(0.3569)\n",
      "17604 Training Loss: tensor(0.3562)\n",
      "17605 Training Loss: tensor(0.3555)\n",
      "17606 Training Loss: tensor(0.3560)\n",
      "17607 Training Loss: tensor(0.3572)\n",
      "17608 Training Loss: tensor(0.3566)\n",
      "17609 Training Loss: tensor(0.3560)\n",
      "17610 Training Loss: tensor(0.3542)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17611 Training Loss: tensor(0.3531)\n",
      "17612 Training Loss: tensor(0.3540)\n",
      "17613 Training Loss: tensor(0.3535)\n",
      "17614 Training Loss: tensor(0.3517)\n",
      "17615 Training Loss: tensor(0.3522)\n",
      "17616 Training Loss: tensor(0.3531)\n",
      "17617 Training Loss: tensor(0.3628)\n",
      "17618 Training Loss: tensor(0.3568)\n",
      "17619 Training Loss: tensor(0.3578)\n",
      "17620 Training Loss: tensor(0.3545)\n",
      "17621 Training Loss: tensor(0.3535)\n",
      "17622 Training Loss: tensor(0.3521)\n",
      "17623 Training Loss: tensor(0.3517)\n",
      "17624 Training Loss: tensor(0.3541)\n",
      "17625 Training Loss: tensor(0.3549)\n",
      "17626 Training Loss: tensor(0.3533)\n",
      "17627 Training Loss: tensor(0.3515)\n",
      "17628 Training Loss: tensor(0.3547)\n",
      "17629 Training Loss: tensor(0.3579)\n",
      "17630 Training Loss: tensor(0.3537)\n",
      "17631 Training Loss: tensor(0.3511)\n",
      "17632 Training Loss: tensor(0.3555)\n",
      "17633 Training Loss: tensor(0.3509)\n",
      "17634 Training Loss: tensor(0.3516)\n",
      "17635 Training Loss: tensor(0.3531)\n",
      "17636 Training Loss: tensor(0.3537)\n",
      "17637 Training Loss: tensor(0.3578)\n",
      "17638 Training Loss: tensor(0.3524)\n",
      "17639 Training Loss: tensor(0.3509)\n",
      "17640 Training Loss: tensor(0.3559)\n",
      "17641 Training Loss: tensor(0.3558)\n",
      "17642 Training Loss: tensor(0.3519)\n",
      "17643 Training Loss: tensor(0.3514)\n",
      "17644 Training Loss: tensor(0.3551)\n",
      "17645 Training Loss: tensor(0.3563)\n",
      "17646 Training Loss: tensor(0.3522)\n",
      "17647 Training Loss: tensor(0.3559)\n",
      "17648 Training Loss: tensor(0.3532)\n",
      "17649 Training Loss: tensor(0.3543)\n",
      "17650 Training Loss: tensor(0.3519)\n",
      "17651 Training Loss: tensor(0.3529)\n",
      "17652 Training Loss: tensor(0.3511)\n",
      "17653 Training Loss: tensor(0.3541)\n",
      "17654 Training Loss: tensor(0.3539)\n",
      "17655 Training Loss: tensor(0.3565)\n",
      "17656 Training Loss: tensor(0.3560)\n",
      "17657 Training Loss: tensor(0.3517)\n",
      "17658 Training Loss: tensor(0.3538)\n",
      "17659 Training Loss: tensor(0.3513)\n",
      "17660 Training Loss: tensor(0.3519)\n",
      "17661 Training Loss: tensor(0.3518)\n",
      "17662 Training Loss: tensor(0.3524)\n",
      "17663 Training Loss: tensor(0.3509)\n",
      "17664 Training Loss: tensor(0.3506)\n",
      "17665 Training Loss: tensor(0.3508)\n",
      "17666 Training Loss: tensor(0.3586)\n",
      "17667 Training Loss: tensor(0.3537)\n",
      "17668 Training Loss: tensor(0.3570)\n",
      "17669 Training Loss: tensor(0.3545)\n",
      "17670 Training Loss: tensor(0.3582)\n",
      "17671 Training Loss: tensor(0.3558)\n",
      "17672 Training Loss: tensor(0.3530)\n",
      "17673 Training Loss: tensor(0.3521)\n",
      "17674 Training Loss: tensor(0.3520)\n",
      "17675 Training Loss: tensor(0.3507)\n",
      "17676 Training Loss: tensor(0.3519)\n",
      "17677 Training Loss: tensor(0.3515)\n",
      "17678 Training Loss: tensor(0.3511)\n",
      "17679 Training Loss: tensor(0.3512)\n",
      "17680 Training Loss: tensor(0.3516)\n",
      "17681 Training Loss: tensor(0.3520)\n",
      "17682 Training Loss: tensor(0.3583)\n",
      "17683 Training Loss: tensor(0.3509)\n",
      "17684 Training Loss: tensor(0.3506)\n",
      "17685 Training Loss: tensor(0.3623)\n",
      "17686 Training Loss: tensor(0.3554)\n",
      "17687 Training Loss: tensor(0.3544)\n",
      "17688 Training Loss: tensor(0.3568)\n",
      "17689 Training Loss: tensor(0.3509)\n",
      "17690 Training Loss: tensor(0.3544)\n",
      "17691 Training Loss: tensor(0.3529)\n",
      "17692 Training Loss: tensor(0.3621)\n",
      "17693 Training Loss: tensor(0.3588)\n",
      "17694 Training Loss: tensor(0.3553)\n",
      "17695 Training Loss: tensor(0.3536)\n",
      "17696 Training Loss: tensor(0.3554)\n",
      "17697 Training Loss: tensor(0.3530)\n",
      "17698 Training Loss: tensor(0.3521)\n",
      "17699 Training Loss: tensor(0.3604)\n",
      "17700 Training Loss: tensor(0.3521)\n",
      "17701 Training Loss: tensor(0.3520)\n",
      "17702 Training Loss: tensor(0.3525)\n",
      "17703 Training Loss: tensor(0.3532)\n",
      "17704 Training Loss: tensor(0.3536)\n",
      "17705 Training Loss: tensor(0.3534)\n",
      "17706 Training Loss: tensor(0.3556)\n",
      "17707 Training Loss: tensor(0.3536)\n",
      "17708 Training Loss: tensor(0.3540)\n",
      "17709 Training Loss: tensor(0.3606)\n",
      "17710 Training Loss: tensor(0.3539)\n",
      "17711 Training Loss: tensor(0.3521)\n",
      "17712 Training Loss: tensor(0.3517)\n",
      "17713 Training Loss: tensor(0.3573)\n",
      "17714 Training Loss: tensor(0.3579)\n",
      "17715 Training Loss: tensor(0.3534)\n",
      "17716 Training Loss: tensor(0.3520)\n",
      "17717 Training Loss: tensor(0.3522)\n",
      "17718 Training Loss: tensor(0.3517)\n",
      "17719 Training Loss: tensor(0.3540)\n",
      "17720 Training Loss: tensor(0.3555)\n",
      "17721 Training Loss: tensor(0.3526)\n",
      "17722 Training Loss: tensor(0.3549)\n",
      "17723 Training Loss: tensor(0.3581)\n",
      "17724 Training Loss: tensor(0.3520)\n",
      "17725 Training Loss: tensor(0.3534)\n",
      "17726 Training Loss: tensor(0.3535)\n",
      "17727 Training Loss: tensor(0.3567)\n",
      "17728 Training Loss: tensor(0.3536)\n",
      "17729 Training Loss: tensor(0.3515)\n",
      "17730 Training Loss: tensor(0.3528)\n",
      "17731 Training Loss: tensor(0.3524)\n",
      "17732 Training Loss: tensor(0.3560)\n",
      "17733 Training Loss: tensor(0.3643)\n",
      "17734 Training Loss: tensor(0.3560)\n",
      "17735 Training Loss: tensor(0.3523)\n",
      "17736 Training Loss: tensor(0.3584)\n",
      "17737 Training Loss: tensor(0.3632)\n",
      "17738 Training Loss: tensor(0.3518)\n",
      "17739 Training Loss: tensor(0.3524)\n",
      "17740 Training Loss: tensor(0.3536)\n",
      "17741 Training Loss: tensor(0.3535)\n",
      "17742 Training Loss: tensor(0.3555)\n",
      "17743 Training Loss: tensor(0.3548)\n",
      "17744 Training Loss: tensor(0.3560)\n",
      "17745 Training Loss: tensor(0.3519)\n",
      "17746 Training Loss: tensor(0.3542)\n",
      "17747 Training Loss: tensor(0.3539)\n",
      "17748 Training Loss: tensor(0.3550)\n",
      "17749 Training Loss: tensor(0.3530)\n",
      "17750 Training Loss: tensor(0.3526)\n",
      "17751 Training Loss: tensor(0.3597)\n",
      "17752 Training Loss: tensor(0.3551)\n",
      "17753 Training Loss: tensor(0.3523)\n",
      "17754 Training Loss: tensor(0.3558)\n",
      "17755 Training Loss: tensor(0.3515)\n",
      "17756 Training Loss: tensor(0.3564)\n",
      "17757 Training Loss: tensor(0.3543)\n",
      "17758 Training Loss: tensor(0.3546)\n",
      "17759 Training Loss: tensor(0.3511)\n",
      "17760 Training Loss: tensor(0.3551)\n",
      "17761 Training Loss: tensor(0.3516)\n",
      "17762 Training Loss: tensor(0.3552)\n",
      "17763 Training Loss: tensor(0.3551)\n",
      "17764 Training Loss: tensor(0.3565)\n",
      "17765 Training Loss: tensor(0.3523)\n",
      "17766 Training Loss: tensor(0.3536)\n",
      "17767 Training Loss: tensor(0.3507)\n",
      "17768 Training Loss: tensor(0.3512)\n",
      "17769 Training Loss: tensor(0.3558)\n",
      "17770 Training Loss: tensor(0.3522)\n",
      "17771 Training Loss: tensor(0.3528)\n",
      "17772 Training Loss: tensor(0.3575)\n",
      "17773 Training Loss: tensor(0.3555)\n",
      "17774 Training Loss: tensor(0.3540)\n",
      "17775 Training Loss: tensor(0.3531)\n",
      "17776 Training Loss: tensor(0.3531)\n",
      "17777 Training Loss: tensor(0.3525)\n",
      "17778 Training Loss: tensor(0.3568)\n",
      "17779 Training Loss: tensor(0.3537)\n",
      "17780 Training Loss: tensor(0.3542)\n",
      "17781 Training Loss: tensor(0.3547)\n",
      "17782 Training Loss: tensor(0.3569)\n",
      "17783 Training Loss: tensor(0.3534)\n",
      "17784 Training Loss: tensor(0.3510)\n",
      "17785 Training Loss: tensor(0.3517)\n",
      "17786 Training Loss: tensor(0.3598)\n",
      "17787 Training Loss: tensor(0.3531)\n",
      "17788 Training Loss: tensor(0.3514)\n",
      "17789 Training Loss: tensor(0.3515)\n",
      "17790 Training Loss: tensor(0.3563)\n",
      "17791 Training Loss: tensor(0.3527)\n",
      "17792 Training Loss: tensor(0.3587)\n",
      "17793 Training Loss: tensor(0.3515)\n",
      "17794 Training Loss: tensor(0.3538)\n",
      "17795 Training Loss: tensor(0.3527)\n",
      "17796 Training Loss: tensor(0.3586)\n",
      "17797 Training Loss: tensor(0.3530)\n",
      "17798 Training Loss: tensor(0.3529)\n",
      "17799 Training Loss: tensor(0.3534)\n",
      "17800 Training Loss: tensor(0.3515)\n",
      "17801 Training Loss: tensor(0.3629)\n",
      "17802 Training Loss: tensor(0.3517)\n",
      "17803 Training Loss: tensor(0.3521)\n",
      "17804 Training Loss: tensor(0.3536)\n",
      "17805 Training Loss: tensor(0.3512)\n",
      "17806 Training Loss: tensor(0.3537)\n",
      "17807 Training Loss: tensor(0.3528)\n",
      "17808 Training Loss: tensor(0.3621)\n",
      "17809 Training Loss: tensor(0.3567)\n",
      "17810 Training Loss: tensor(0.3515)\n",
      "17811 Training Loss: tensor(0.3517)\n",
      "17812 Training Loss: tensor(0.3591)\n",
      "17813 Training Loss: tensor(0.3546)\n",
      "17814 Training Loss: tensor(0.3522)\n",
      "17815 Training Loss: tensor(0.3604)\n",
      "17816 Training Loss: tensor(0.3507)\n",
      "17817 Training Loss: tensor(0.3515)\n",
      "17818 Training Loss: tensor(0.3601)\n",
      "17819 Training Loss: tensor(0.3532)\n",
      "17820 Training Loss: tensor(0.3539)\n",
      "17821 Training Loss: tensor(0.3533)\n",
      "17822 Training Loss: tensor(0.3533)\n",
      "17823 Training Loss: tensor(0.3518)\n",
      "17824 Training Loss: tensor(0.3540)\n",
      "17825 Training Loss: tensor(0.3531)\n",
      "17826 Training Loss: tensor(0.3529)\n",
      "17827 Training Loss: tensor(0.3519)\n",
      "17828 Training Loss: tensor(0.3587)\n",
      "17829 Training Loss: tensor(0.3548)\n",
      "17830 Training Loss: tensor(0.3523)\n",
      "17831 Training Loss: tensor(0.3538)\n",
      "17832 Training Loss: tensor(0.3564)\n",
      "17833 Training Loss: tensor(0.3567)\n",
      "17834 Training Loss: tensor(0.3520)\n",
      "17835 Training Loss: tensor(0.3515)\n",
      "17836 Training Loss: tensor(0.3542)\n",
      "17837 Training Loss: tensor(0.3579)\n",
      "17838 Training Loss: tensor(0.3558)\n",
      "17839 Training Loss: tensor(0.3570)\n",
      "17840 Training Loss: tensor(0.3532)\n",
      "17841 Training Loss: tensor(0.3558)\n",
      "17842 Training Loss: tensor(0.3517)\n",
      "17843 Training Loss: tensor(0.3512)\n",
      "17844 Training Loss: tensor(0.3520)\n",
      "17845 Training Loss: tensor(0.3507)\n",
      "17846 Training Loss: tensor(0.3535)\n",
      "17847 Training Loss: tensor(0.3532)\n",
      "17848 Training Loss: tensor(0.3570)\n",
      "17849 Training Loss: tensor(0.3544)\n",
      "17850 Training Loss: tensor(0.3562)\n",
      "17851 Training Loss: tensor(0.3543)\n",
      "17852 Training Loss: tensor(0.3577)\n",
      "17853 Training Loss: tensor(0.3611)\n",
      "17854 Training Loss: tensor(0.3518)\n",
      "17855 Training Loss: tensor(0.3525)\n",
      "17856 Training Loss: tensor(0.3546)\n",
      "17857 Training Loss: tensor(0.3526)\n",
      "17858 Training Loss: tensor(0.3538)\n",
      "17859 Training Loss: tensor(0.3528)\n",
      "17860 Training Loss: tensor(0.3529)\n",
      "17861 Training Loss: tensor(0.3525)\n",
      "17862 Training Loss: tensor(0.3520)\n",
      "17863 Training Loss: tensor(0.3528)\n",
      "17864 Training Loss: tensor(0.3559)\n",
      "17865 Training Loss: tensor(0.3518)\n",
      "17866 Training Loss: tensor(0.3506)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17867 Training Loss: tensor(0.3542)\n",
      "17868 Training Loss: tensor(0.3539)\n",
      "17869 Training Loss: tensor(0.3532)\n",
      "17870 Training Loss: tensor(0.3507)\n",
      "17871 Training Loss: tensor(0.3531)\n",
      "17872 Training Loss: tensor(0.3520)\n",
      "17873 Training Loss: tensor(0.3530)\n",
      "17874 Training Loss: tensor(0.3573)\n",
      "17875 Training Loss: tensor(0.3503)\n",
      "17876 Training Loss: tensor(0.3533)\n",
      "17877 Training Loss: tensor(0.3512)\n",
      "17878 Training Loss: tensor(0.3538)\n",
      "17879 Training Loss: tensor(0.3593)\n",
      "17880 Training Loss: tensor(0.3513)\n",
      "17881 Training Loss: tensor(0.3559)\n",
      "17882 Training Loss: tensor(0.3550)\n",
      "17883 Training Loss: tensor(0.3521)\n",
      "17884 Training Loss: tensor(0.3561)\n",
      "17885 Training Loss: tensor(0.3508)\n",
      "17886 Training Loss: tensor(0.3552)\n",
      "17887 Training Loss: tensor(0.3551)\n",
      "17888 Training Loss: tensor(0.3565)\n",
      "17889 Training Loss: tensor(0.3572)\n",
      "17890 Training Loss: tensor(0.3544)\n",
      "17891 Training Loss: tensor(0.3551)\n",
      "17892 Training Loss: tensor(0.3514)\n",
      "17893 Training Loss: tensor(0.3551)\n",
      "17894 Training Loss: tensor(0.3539)\n",
      "17895 Training Loss: tensor(0.3546)\n",
      "17896 Training Loss: tensor(0.3548)\n",
      "17897 Training Loss: tensor(0.3513)\n",
      "17898 Training Loss: tensor(0.3547)\n",
      "17899 Training Loss: tensor(0.3612)\n",
      "17900 Training Loss: tensor(0.3514)\n",
      "17901 Training Loss: tensor(0.3528)\n",
      "17902 Training Loss: tensor(0.3529)\n",
      "17903 Training Loss: tensor(0.3526)\n",
      "17904 Training Loss: tensor(0.3548)\n",
      "17905 Training Loss: tensor(0.3558)\n",
      "17906 Training Loss: tensor(0.3540)\n",
      "17907 Training Loss: tensor(0.3573)\n",
      "17908 Training Loss: tensor(0.3535)\n",
      "17909 Training Loss: tensor(0.3537)\n",
      "17910 Training Loss: tensor(0.3586)\n",
      "17911 Training Loss: tensor(0.3595)\n",
      "17912 Training Loss: tensor(0.3565)\n",
      "17913 Training Loss: tensor(0.3577)\n",
      "17914 Training Loss: tensor(0.3519)\n",
      "17915 Training Loss: tensor(0.3540)\n",
      "17916 Training Loss: tensor(0.3528)\n",
      "17917 Training Loss: tensor(0.3525)\n",
      "17918 Training Loss: tensor(0.3558)\n",
      "17919 Training Loss: tensor(0.3565)\n",
      "17920 Training Loss: tensor(0.3520)\n",
      "17921 Training Loss: tensor(0.3541)\n",
      "17922 Training Loss: tensor(0.3539)\n",
      "17923 Training Loss: tensor(0.3529)\n",
      "17924 Training Loss: tensor(0.3525)\n",
      "17925 Training Loss: tensor(0.3570)\n",
      "17926 Training Loss: tensor(0.3588)\n",
      "17927 Training Loss: tensor(0.3588)\n",
      "17928 Training Loss: tensor(0.3607)\n",
      "17929 Training Loss: tensor(0.3525)\n",
      "17930 Training Loss: tensor(0.3560)\n",
      "17931 Training Loss: tensor(0.3543)\n",
      "17932 Training Loss: tensor(0.3576)\n",
      "17933 Training Loss: tensor(0.3548)\n",
      "17934 Training Loss: tensor(0.3534)\n",
      "17935 Training Loss: tensor(0.3542)\n",
      "17936 Training Loss: tensor(0.3530)\n",
      "17937 Training Loss: tensor(0.3564)\n",
      "17938 Training Loss: tensor(0.3529)\n",
      "17939 Training Loss: tensor(0.3535)\n",
      "17940 Training Loss: tensor(0.3534)\n",
      "17941 Training Loss: tensor(0.3578)\n",
      "17942 Training Loss: tensor(0.3521)\n",
      "17943 Training Loss: tensor(0.3565)\n",
      "17944 Training Loss: tensor(0.3531)\n",
      "17945 Training Loss: tensor(0.3526)\n",
      "17946 Training Loss: tensor(0.3527)\n",
      "17947 Training Loss: tensor(0.3513)\n",
      "17948 Training Loss: tensor(0.3534)\n",
      "17949 Training Loss: tensor(0.3510)\n",
      "17950 Training Loss: tensor(0.3576)\n",
      "17951 Training Loss: tensor(0.3546)\n",
      "17952 Training Loss: tensor(0.3507)\n",
      "17953 Training Loss: tensor(0.3525)\n",
      "17954 Training Loss: tensor(0.3543)\n",
      "17955 Training Loss: tensor(0.3504)\n",
      "17956 Training Loss: tensor(0.3520)\n",
      "17957 Training Loss: tensor(0.3518)\n",
      "17958 Training Loss: tensor(0.3511)\n",
      "17959 Training Loss: tensor(0.3503)\n",
      "17960 Training Loss: tensor(0.3507)\n",
      "17961 Training Loss: tensor(0.3546)\n",
      "17962 Training Loss: tensor(0.3511)\n",
      "17963 Training Loss: tensor(0.3528)\n",
      "17964 Training Loss: tensor(0.3527)\n",
      "17965 Training Loss: tensor(0.3606)\n",
      "17966 Training Loss: tensor(0.3579)\n",
      "17967 Training Loss: tensor(0.3512)\n",
      "17968 Training Loss: tensor(0.3551)\n",
      "17969 Training Loss: tensor(0.3516)\n",
      "17970 Training Loss: tensor(0.3515)\n",
      "17971 Training Loss: tensor(0.3554)\n",
      "17972 Training Loss: tensor(0.3548)\n",
      "17973 Training Loss: tensor(0.3513)\n",
      "17974 Training Loss: tensor(0.3544)\n",
      "17975 Training Loss: tensor(0.3525)\n",
      "17976 Training Loss: tensor(0.3526)\n",
      "17977 Training Loss: tensor(0.3602)\n",
      "17978 Training Loss: tensor(0.3585)\n",
      "17979 Training Loss: tensor(0.3546)\n",
      "17980 Training Loss: tensor(0.3512)\n",
      "17981 Training Loss: tensor(0.3515)\n",
      "17982 Training Loss: tensor(0.3519)\n",
      "17983 Training Loss: tensor(0.3593)\n",
      "17984 Training Loss: tensor(0.3551)\n",
      "17985 Training Loss: tensor(0.3553)\n",
      "17986 Training Loss: tensor(0.3509)\n",
      "17987 Training Loss: tensor(0.3560)\n",
      "17988 Training Loss: tensor(0.3520)\n",
      "17989 Training Loss: tensor(0.3530)\n",
      "17990 Training Loss: tensor(0.3550)\n",
      "17991 Training Loss: tensor(0.3519)\n",
      "17992 Training Loss: tensor(0.3518)\n",
      "17993 Training Loss: tensor(0.3540)\n",
      "17994 Training Loss: tensor(0.3518)\n",
      "17995 Training Loss: tensor(0.3537)\n",
      "17996 Training Loss: tensor(0.3549)\n",
      "17997 Training Loss: tensor(0.3511)\n",
      "17998 Training Loss: tensor(0.3516)\n",
      "17999 Training Loss: tensor(0.3551)\n",
      "18000 Training Loss: tensor(0.3550)\n",
      "18001 Training Loss: tensor(0.3524)\n",
      "18002 Training Loss: tensor(0.3525)\n",
      "18003 Training Loss: tensor(0.3554)\n",
      "18004 Training Loss: tensor(0.3530)\n",
      "18005 Training Loss: tensor(0.3582)\n",
      "18006 Training Loss: tensor(0.3516)\n",
      "18007 Training Loss: tensor(0.3562)\n",
      "18008 Training Loss: tensor(0.3554)\n",
      "18009 Training Loss: tensor(0.3520)\n",
      "18010 Training Loss: tensor(0.3509)\n",
      "18011 Training Loss: tensor(0.3504)\n",
      "18012 Training Loss: tensor(0.3590)\n",
      "18013 Training Loss: tensor(0.3559)\n",
      "18014 Training Loss: tensor(0.3528)\n",
      "18015 Training Loss: tensor(0.3506)\n",
      "18016 Training Loss: tensor(0.3543)\n",
      "18017 Training Loss: tensor(0.3535)\n",
      "18018 Training Loss: tensor(0.3568)\n",
      "18019 Training Loss: tensor(0.3527)\n",
      "18020 Training Loss: tensor(0.3510)\n",
      "18021 Training Loss: tensor(0.3505)\n",
      "18022 Training Loss: tensor(0.3520)\n",
      "18023 Training Loss: tensor(0.3587)\n",
      "18024 Training Loss: tensor(0.3556)\n",
      "18025 Training Loss: tensor(0.3542)\n",
      "18026 Training Loss: tensor(0.3530)\n",
      "18027 Training Loss: tensor(0.3567)\n",
      "18028 Training Loss: tensor(0.3546)\n",
      "18029 Training Loss: tensor(0.3510)\n",
      "18030 Training Loss: tensor(0.3565)\n",
      "18031 Training Loss: tensor(0.3541)\n",
      "18032 Training Loss: tensor(0.3555)\n",
      "18033 Training Loss: tensor(0.3514)\n",
      "18034 Training Loss: tensor(0.3517)\n",
      "18035 Training Loss: tensor(0.3565)\n",
      "18036 Training Loss: tensor(0.3510)\n",
      "18037 Training Loss: tensor(0.3518)\n",
      "18038 Training Loss: tensor(0.3555)\n",
      "18039 Training Loss: tensor(0.3515)\n",
      "18040 Training Loss: tensor(0.3559)\n",
      "18041 Training Loss: tensor(0.3509)\n",
      "18042 Training Loss: tensor(0.3614)\n",
      "18043 Training Loss: tensor(0.3610)\n",
      "18044 Training Loss: tensor(0.3581)\n",
      "18045 Training Loss: tensor(0.3600)\n",
      "18046 Training Loss: tensor(0.3568)\n",
      "18047 Training Loss: tensor(0.3559)\n",
      "18048 Training Loss: tensor(0.3523)\n",
      "18049 Training Loss: tensor(0.3558)\n",
      "18050 Training Loss: tensor(0.3538)\n",
      "18051 Training Loss: tensor(0.3518)\n",
      "18052 Training Loss: tensor(0.3566)\n",
      "18053 Training Loss: tensor(0.3541)\n",
      "18054 Training Loss: tensor(0.3561)\n",
      "18055 Training Loss: tensor(0.3541)\n",
      "18056 Training Loss: tensor(0.3538)\n",
      "18057 Training Loss: tensor(0.3571)\n",
      "18058 Training Loss: tensor(0.3592)\n",
      "18059 Training Loss: tensor(0.3537)\n",
      "18060 Training Loss: tensor(0.3539)\n",
      "18061 Training Loss: tensor(0.3531)\n",
      "18062 Training Loss: tensor(0.3517)\n",
      "18063 Training Loss: tensor(0.3587)\n",
      "18064 Training Loss: tensor(0.3521)\n",
      "18065 Training Loss: tensor(0.3524)\n",
      "18066 Training Loss: tensor(0.3615)\n",
      "18067 Training Loss: tensor(0.3517)\n",
      "18068 Training Loss: tensor(0.3519)\n",
      "18069 Training Loss: tensor(0.3552)\n",
      "18070 Training Loss: tensor(0.3573)\n",
      "18071 Training Loss: tensor(0.3528)\n",
      "18072 Training Loss: tensor(0.3612)\n",
      "18073 Training Loss: tensor(0.3585)\n",
      "18074 Training Loss: tensor(0.3516)\n",
      "18075 Training Loss: tensor(0.3536)\n",
      "18076 Training Loss: tensor(0.3517)\n",
      "18077 Training Loss: tensor(0.3546)\n",
      "18078 Training Loss: tensor(0.3544)\n",
      "18079 Training Loss: tensor(0.3523)\n",
      "18080 Training Loss: tensor(0.3543)\n",
      "18081 Training Loss: tensor(0.3539)\n",
      "18082 Training Loss: tensor(0.3546)\n",
      "18083 Training Loss: tensor(0.3525)\n",
      "18084 Training Loss: tensor(0.3555)\n",
      "18085 Training Loss: tensor(0.3521)\n",
      "18086 Training Loss: tensor(0.3555)\n",
      "18087 Training Loss: tensor(0.3528)\n",
      "18088 Training Loss: tensor(0.3520)\n",
      "18089 Training Loss: tensor(0.3533)\n",
      "18090 Training Loss: tensor(0.3532)\n",
      "18091 Training Loss: tensor(0.3515)\n",
      "18092 Training Loss: tensor(0.3524)\n",
      "18093 Training Loss: tensor(0.3536)\n",
      "18094 Training Loss: tensor(0.3525)\n",
      "18095 Training Loss: tensor(0.3537)\n",
      "18096 Training Loss: tensor(0.3506)\n",
      "18097 Training Loss: tensor(0.3537)\n",
      "18098 Training Loss: tensor(0.3597)\n",
      "18099 Training Loss: tensor(0.3532)\n",
      "18100 Training Loss: tensor(0.3516)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18101 Training Loss: tensor(0.3565)\n",
      "18102 Training Loss: tensor(0.3588)\n",
      "18103 Training Loss: tensor(0.3529)\n",
      "18104 Training Loss: tensor(0.3509)\n",
      "18105 Training Loss: tensor(0.3537)\n",
      "18106 Training Loss: tensor(0.3539)\n",
      "18107 Training Loss: tensor(0.3537)\n",
      "18108 Training Loss: tensor(0.3541)\n",
      "18109 Training Loss: tensor(0.3513)\n",
      "18110 Training Loss: tensor(0.3520)\n",
      "18111 Training Loss: tensor(0.3532)\n",
      "18112 Training Loss: tensor(0.3520)\n",
      "18113 Training Loss: tensor(0.3527)\n",
      "18114 Training Loss: tensor(0.3549)\n",
      "18115 Training Loss: tensor(0.3517)\n",
      "18116 Training Loss: tensor(0.3533)\n",
      "18117 Training Loss: tensor(0.3553)\n",
      "18118 Training Loss: tensor(0.3512)\n",
      "18119 Training Loss: tensor(0.3534)\n",
      "18120 Training Loss: tensor(0.3583)\n",
      "18121 Training Loss: tensor(0.3534)\n",
      "18122 Training Loss: tensor(0.3561)\n",
      "18123 Training Loss: tensor(0.3572)\n",
      "18124 Training Loss: tensor(0.3525)\n",
      "18125 Training Loss: tensor(0.3558)\n",
      "18126 Training Loss: tensor(0.3532)\n",
      "18127 Training Loss: tensor(0.3511)\n",
      "18128 Training Loss: tensor(0.3521)\n",
      "18129 Training Loss: tensor(0.3535)\n",
      "18130 Training Loss: tensor(0.3513)\n",
      "18131 Training Loss: tensor(0.3521)\n",
      "18132 Training Loss: tensor(0.3530)\n",
      "18133 Training Loss: tensor(0.3576)\n",
      "18134 Training Loss: tensor(0.3509)\n",
      "18135 Training Loss: tensor(0.3528)\n",
      "18136 Training Loss: tensor(0.3524)\n",
      "18137 Training Loss: tensor(0.3525)\n",
      "18138 Training Loss: tensor(0.3540)\n",
      "18139 Training Loss: tensor(0.3563)\n",
      "18140 Training Loss: tensor(0.3513)\n",
      "18141 Training Loss: tensor(0.3571)\n",
      "18142 Training Loss: tensor(0.3518)\n",
      "18143 Training Loss: tensor(0.3518)\n",
      "18144 Training Loss: tensor(0.3525)\n",
      "18145 Training Loss: tensor(0.3562)\n",
      "18146 Training Loss: tensor(0.3541)\n",
      "18147 Training Loss: tensor(0.3519)\n",
      "18148 Training Loss: tensor(0.3588)\n",
      "18149 Training Loss: tensor(0.3560)\n",
      "18150 Training Loss: tensor(0.3540)\n",
      "18151 Training Loss: tensor(0.3511)\n",
      "18152 Training Loss: tensor(0.3511)\n",
      "18153 Training Loss: tensor(0.3516)\n",
      "18154 Training Loss: tensor(0.3520)\n",
      "18155 Training Loss: tensor(0.3583)\n",
      "18156 Training Loss: tensor(0.3544)\n",
      "18157 Training Loss: tensor(0.3545)\n",
      "18158 Training Loss: tensor(0.3580)\n",
      "18159 Training Loss: tensor(0.3518)\n",
      "18160 Training Loss: tensor(0.3564)\n",
      "18161 Training Loss: tensor(0.3519)\n",
      "18162 Training Loss: tensor(0.3556)\n",
      "18163 Training Loss: tensor(0.3540)\n",
      "18164 Training Loss: tensor(0.3521)\n",
      "18165 Training Loss: tensor(0.3514)\n",
      "18166 Training Loss: tensor(0.3522)\n",
      "18167 Training Loss: tensor(0.3562)\n",
      "18168 Training Loss: tensor(0.3514)\n",
      "18169 Training Loss: tensor(0.3513)\n",
      "18170 Training Loss: tensor(0.3517)\n",
      "18171 Training Loss: tensor(0.3547)\n",
      "18172 Training Loss: tensor(0.3505)\n",
      "18173 Training Loss: tensor(0.3519)\n",
      "18174 Training Loss: tensor(0.3524)\n",
      "18175 Training Loss: tensor(0.3575)\n",
      "18176 Training Loss: tensor(0.3505)\n",
      "18177 Training Loss: tensor(0.3571)\n",
      "18178 Training Loss: tensor(0.3541)\n",
      "18179 Training Loss: tensor(0.3508)\n",
      "18180 Training Loss: tensor(0.3553)\n",
      "18181 Training Loss: tensor(0.3580)\n",
      "18182 Training Loss: tensor(0.3538)\n",
      "18183 Training Loss: tensor(0.3525)\n",
      "18184 Training Loss: tensor(0.3529)\n",
      "18185 Training Loss: tensor(0.3511)\n",
      "18186 Training Loss: tensor(0.3507)\n",
      "18187 Training Loss: tensor(0.3524)\n",
      "18188 Training Loss: tensor(0.3524)\n",
      "18189 Training Loss: tensor(0.3542)\n",
      "18190 Training Loss: tensor(0.3514)\n",
      "18191 Training Loss: tensor(0.3516)\n",
      "18192 Training Loss: tensor(0.3519)\n",
      "18193 Training Loss: tensor(0.3659)\n",
      "18194 Training Loss: tensor(0.3603)\n",
      "18195 Training Loss: tensor(0.3531)\n",
      "18196 Training Loss: tensor(0.3537)\n",
      "18197 Training Loss: tensor(0.3540)\n",
      "18198 Training Loss: tensor(0.3512)\n",
      "18199 Training Loss: tensor(0.3508)\n",
      "18200 Training Loss: tensor(0.3550)\n",
      "18201 Training Loss: tensor(0.3525)\n",
      "18202 Training Loss: tensor(0.3545)\n",
      "18203 Training Loss: tensor(0.3524)\n",
      "18204 Training Loss: tensor(0.3529)\n",
      "18205 Training Loss: tensor(0.3570)\n",
      "18206 Training Loss: tensor(0.3515)\n",
      "18207 Training Loss: tensor(0.3507)\n",
      "18208 Training Loss: tensor(0.3524)\n",
      "18209 Training Loss: tensor(0.3534)\n",
      "18210 Training Loss: tensor(0.3503)\n",
      "18211 Training Loss: tensor(0.3530)\n",
      "18212 Training Loss: tensor(0.3548)\n",
      "18213 Training Loss: tensor(0.3501)\n",
      "18214 Training Loss: tensor(0.3621)\n",
      "18215 Training Loss: tensor(0.3527)\n",
      "18216 Training Loss: tensor(0.3542)\n",
      "18217 Training Loss: tensor(0.3562)\n",
      "18218 Training Loss: tensor(0.3556)\n",
      "18219 Training Loss: tensor(0.3555)\n",
      "18220 Training Loss: tensor(0.3528)\n",
      "18221 Training Loss: tensor(0.3524)\n",
      "18222 Training Loss: tensor(0.3517)\n",
      "18223 Training Loss: tensor(0.3545)\n",
      "18224 Training Loss: tensor(0.3546)\n",
      "18225 Training Loss: tensor(0.3525)\n",
      "18226 Training Loss: tensor(0.3556)\n",
      "18227 Training Loss: tensor(0.3539)\n",
      "18228 Training Loss: tensor(0.3532)\n",
      "18229 Training Loss: tensor(0.3550)\n",
      "18230 Training Loss: tensor(0.3532)\n",
      "18231 Training Loss: tensor(0.3538)\n",
      "18232 Training Loss: tensor(0.3523)\n",
      "18233 Training Loss: tensor(0.3515)\n",
      "18234 Training Loss: tensor(0.3531)\n",
      "18235 Training Loss: tensor(0.3582)\n",
      "18236 Training Loss: tensor(0.3529)\n",
      "18237 Training Loss: tensor(0.3550)\n",
      "18238 Training Loss: tensor(0.3556)\n",
      "18239 Training Loss: tensor(0.3509)\n",
      "18240 Training Loss: tensor(0.3516)\n",
      "18241 Training Loss: tensor(0.3528)\n",
      "18242 Training Loss: tensor(0.3533)\n",
      "18243 Training Loss: tensor(0.3548)\n",
      "18244 Training Loss: tensor(0.3556)\n",
      "18245 Training Loss: tensor(0.3518)\n",
      "18246 Training Loss: tensor(0.3543)\n",
      "18247 Training Loss: tensor(0.3523)\n",
      "18248 Training Loss: tensor(0.3539)\n",
      "18249 Training Loss: tensor(0.3534)\n",
      "18250 Training Loss: tensor(0.3562)\n",
      "18251 Training Loss: tensor(0.3543)\n",
      "18252 Training Loss: tensor(0.3583)\n",
      "18253 Training Loss: tensor(0.3508)\n",
      "18254 Training Loss: tensor(0.3534)\n",
      "18255 Training Loss: tensor(0.3563)\n",
      "18256 Training Loss: tensor(0.3532)\n",
      "18257 Training Loss: tensor(0.3586)\n",
      "18258 Training Loss: tensor(0.3523)\n",
      "18259 Training Loss: tensor(0.3563)\n",
      "18260 Training Loss: tensor(0.3532)\n",
      "18261 Training Loss: tensor(0.3548)\n",
      "18262 Training Loss: tensor(0.3524)\n",
      "18263 Training Loss: tensor(0.3551)\n",
      "18264 Training Loss: tensor(0.3535)\n",
      "18265 Training Loss: tensor(0.3520)\n",
      "18266 Training Loss: tensor(0.3516)\n",
      "18267 Training Loss: tensor(0.3518)\n",
      "18268 Training Loss: tensor(0.3550)\n",
      "18269 Training Loss: tensor(0.3530)\n",
      "18270 Training Loss: tensor(0.3521)\n",
      "18271 Training Loss: tensor(0.3535)\n",
      "18272 Training Loss: tensor(0.3544)\n",
      "18273 Training Loss: tensor(0.3589)\n",
      "18274 Training Loss: tensor(0.3510)\n",
      "18275 Training Loss: tensor(0.3589)\n",
      "18276 Training Loss: tensor(0.3511)\n",
      "18277 Training Loss: tensor(0.3522)\n",
      "18278 Training Loss: tensor(0.3560)\n",
      "18279 Training Loss: tensor(0.3512)\n",
      "18280 Training Loss: tensor(0.3521)\n",
      "18281 Training Loss: tensor(0.3567)\n",
      "18282 Training Loss: tensor(0.3530)\n",
      "18283 Training Loss: tensor(0.3550)\n",
      "18284 Training Loss: tensor(0.3556)\n",
      "18285 Training Loss: tensor(0.3520)\n",
      "18286 Training Loss: tensor(0.3548)\n",
      "18287 Training Loss: tensor(0.3506)\n",
      "18288 Training Loss: tensor(0.3515)\n",
      "18289 Training Loss: tensor(0.3521)\n",
      "18290 Training Loss: tensor(0.3562)\n",
      "18291 Training Loss: tensor(0.3606)\n",
      "18292 Training Loss: tensor(0.3552)\n",
      "18293 Training Loss: tensor(0.3540)\n",
      "18294 Training Loss: tensor(0.3511)\n",
      "18295 Training Loss: tensor(0.3580)\n",
      "18296 Training Loss: tensor(0.3515)\n",
      "18297 Training Loss: tensor(0.3552)\n",
      "18298 Training Loss: tensor(0.3524)\n",
      "18299 Training Loss: tensor(0.3557)\n",
      "18300 Training Loss: tensor(0.3542)\n",
      "18301 Training Loss: tensor(0.3570)\n",
      "18302 Training Loss: tensor(0.3526)\n",
      "18303 Training Loss: tensor(0.3557)\n",
      "18304 Training Loss: tensor(0.3564)\n",
      "18305 Training Loss: tensor(0.3540)\n",
      "18306 Training Loss: tensor(0.3550)\n",
      "18307 Training Loss: tensor(0.3546)\n",
      "18308 Training Loss: tensor(0.3540)\n",
      "18309 Training Loss: tensor(0.3555)\n",
      "18310 Training Loss: tensor(0.3521)\n",
      "18311 Training Loss: tensor(0.3556)\n",
      "18312 Training Loss: tensor(0.3520)\n",
      "18313 Training Loss: tensor(0.3556)\n",
      "18314 Training Loss: tensor(0.3537)\n",
      "18315 Training Loss: tensor(0.3527)\n",
      "18316 Training Loss: tensor(0.3521)\n",
      "18317 Training Loss: tensor(0.3548)\n",
      "18318 Training Loss: tensor(0.3516)\n",
      "18319 Training Loss: tensor(0.3577)\n",
      "18320 Training Loss: tensor(0.3515)\n",
      "18321 Training Loss: tensor(0.3509)\n",
      "18322 Training Loss: tensor(0.3527)\n",
      "18323 Training Loss: tensor(0.3514)\n",
      "18324 Training Loss: tensor(0.3648)\n",
      "18325 Training Loss: tensor(0.3562)\n",
      "18326 Training Loss: tensor(0.3530)\n",
      "18327 Training Loss: tensor(0.3538)\n",
      "18328 Training Loss: tensor(0.3510)\n",
      "18329 Training Loss: tensor(0.3511)\n",
      "18330 Training Loss: tensor(0.3531)\n",
      "18331 Training Loss: tensor(0.3504)\n",
      "18332 Training Loss: tensor(0.3536)\n",
      "18333 Training Loss: tensor(0.3513)\n",
      "18334 Training Loss: tensor(0.3572)\n",
      "18335 Training Loss: tensor(0.3516)\n",
      "18336 Training Loss: tensor(0.3539)\n",
      "18337 Training Loss: tensor(0.3602)\n",
      "18338 Training Loss: tensor(0.3562)\n",
      "18339 Training Loss: tensor(0.3518)\n",
      "18340 Training Loss: tensor(0.3520)\n",
      "18341 Training Loss: tensor(0.3563)\n",
      "18342 Training Loss: tensor(0.3512)\n",
      "18343 Training Loss: tensor(0.3531)\n",
      "18344 Training Loss: tensor(0.3517)\n",
      "18345 Training Loss: tensor(0.3528)\n",
      "18346 Training Loss: tensor(0.3510)\n",
      "18347 Training Loss: tensor(0.3534)\n",
      "18348 Training Loss: tensor(0.3554)\n",
      "18349 Training Loss: tensor(0.3516)\n",
      "18350 Training Loss: tensor(0.3554)\n",
      "18351 Training Loss: tensor(0.3509)\n",
      "18352 Training Loss: tensor(0.3516)\n",
      "18353 Training Loss: tensor(0.3529)\n",
      "18354 Training Loss: tensor(0.3562)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18355 Training Loss: tensor(0.3506)\n",
      "18356 Training Loss: tensor(0.3562)\n",
      "18357 Training Loss: tensor(0.3544)\n",
      "18358 Training Loss: tensor(0.3533)\n",
      "18359 Training Loss: tensor(0.3563)\n",
      "18360 Training Loss: tensor(0.3505)\n",
      "18361 Training Loss: tensor(0.3539)\n",
      "18362 Training Loss: tensor(0.3525)\n",
      "18363 Training Loss: tensor(0.3536)\n",
      "18364 Training Loss: tensor(0.3569)\n",
      "18365 Training Loss: tensor(0.3535)\n",
      "18366 Training Loss: tensor(0.3539)\n",
      "18367 Training Loss: tensor(0.3560)\n",
      "18368 Training Loss: tensor(0.3511)\n",
      "18369 Training Loss: tensor(0.3520)\n",
      "18370 Training Loss: tensor(0.3546)\n",
      "18371 Training Loss: tensor(0.3515)\n",
      "18372 Training Loss: tensor(0.3507)\n",
      "18373 Training Loss: tensor(0.3540)\n",
      "18374 Training Loss: tensor(0.3513)\n",
      "18375 Training Loss: tensor(0.3543)\n",
      "18376 Training Loss: tensor(0.3521)\n",
      "18377 Training Loss: tensor(0.3539)\n",
      "18378 Training Loss: tensor(0.3553)\n",
      "18379 Training Loss: tensor(0.3515)\n",
      "18380 Training Loss: tensor(0.3547)\n",
      "18381 Training Loss: tensor(0.3502)\n",
      "18382 Training Loss: tensor(0.3524)\n",
      "18383 Training Loss: tensor(0.3507)\n",
      "18384 Training Loss: tensor(0.3517)\n",
      "18385 Training Loss: tensor(0.3628)\n",
      "18386 Training Loss: tensor(0.3512)\n",
      "18387 Training Loss: tensor(0.3511)\n",
      "18388 Training Loss: tensor(0.3505)\n",
      "18389 Training Loss: tensor(0.3499)\n",
      "18390 Training Loss: tensor(0.3518)\n",
      "18391 Training Loss: tensor(0.3575)\n",
      "18392 Training Loss: tensor(0.3507)\n",
      "18393 Training Loss: tensor(0.3505)\n",
      "18394 Training Loss: tensor(0.3510)\n",
      "18395 Training Loss: tensor(0.3544)\n",
      "18396 Training Loss: tensor(0.3653)\n",
      "18397 Training Loss: tensor(0.3516)\n",
      "18398 Training Loss: tensor(0.3575)\n",
      "18399 Training Loss: tensor(0.3511)\n",
      "18400 Training Loss: tensor(0.3589)\n",
      "18401 Training Loss: tensor(0.3516)\n",
      "18402 Training Loss: tensor(0.3559)\n",
      "18403 Training Loss: tensor(0.3563)\n",
      "18404 Training Loss: tensor(0.3531)\n",
      "18405 Training Loss: tensor(0.3528)\n",
      "18406 Training Loss: tensor(0.3584)\n",
      "18407 Training Loss: tensor(0.3537)\n",
      "18408 Training Loss: tensor(0.3543)\n",
      "18409 Training Loss: tensor(0.3522)\n",
      "18410 Training Loss: tensor(0.3555)\n",
      "18411 Training Loss: tensor(0.3522)\n",
      "18412 Training Loss: tensor(0.3512)\n",
      "18413 Training Loss: tensor(0.3525)\n",
      "18414 Training Loss: tensor(0.3521)\n",
      "18415 Training Loss: tensor(0.3511)\n",
      "18416 Training Loss: tensor(0.3508)\n",
      "18417 Training Loss: tensor(0.3529)\n",
      "18418 Training Loss: tensor(0.3574)\n",
      "18419 Training Loss: tensor(0.3600)\n",
      "18420 Training Loss: tensor(0.3572)\n",
      "18421 Training Loss: tensor(0.3576)\n",
      "18422 Training Loss: tensor(0.3541)\n",
      "18423 Training Loss: tensor(0.3535)\n",
      "18424 Training Loss: tensor(0.3526)\n",
      "18425 Training Loss: tensor(0.3539)\n",
      "18426 Training Loss: tensor(0.3540)\n",
      "18427 Training Loss: tensor(0.3546)\n",
      "18428 Training Loss: tensor(0.3525)\n",
      "18429 Training Loss: tensor(0.3569)\n",
      "18430 Training Loss: tensor(0.3516)\n",
      "18431 Training Loss: tensor(0.3533)\n",
      "18432 Training Loss: tensor(0.3535)\n",
      "18433 Training Loss: tensor(0.3535)\n",
      "18434 Training Loss: tensor(0.3511)\n",
      "18435 Training Loss: tensor(0.3561)\n",
      "18436 Training Loss: tensor(0.3507)\n",
      "18437 Training Loss: tensor(0.3524)\n",
      "18438 Training Loss: tensor(0.3519)\n",
      "18439 Training Loss: tensor(0.3511)\n",
      "18440 Training Loss: tensor(0.3547)\n",
      "18441 Training Loss: tensor(0.3578)\n",
      "18442 Training Loss: tensor(0.3549)\n",
      "18443 Training Loss: tensor(0.3530)\n",
      "18444 Training Loss: tensor(0.3510)\n",
      "18445 Training Loss: tensor(0.3520)\n",
      "18446 Training Loss: tensor(0.3530)\n",
      "18447 Training Loss: tensor(0.3503)\n",
      "18448 Training Loss: tensor(0.3531)\n",
      "18449 Training Loss: tensor(0.3524)\n",
      "18450 Training Loss: tensor(0.3568)\n",
      "18451 Training Loss: tensor(0.3585)\n",
      "18452 Training Loss: tensor(0.3512)\n",
      "18453 Training Loss: tensor(0.3536)\n",
      "18454 Training Loss: tensor(0.3522)\n",
      "18455 Training Loss: tensor(0.3520)\n",
      "18456 Training Loss: tensor(0.3548)\n",
      "18457 Training Loss: tensor(0.3533)\n",
      "18458 Training Loss: tensor(0.3590)\n",
      "18459 Training Loss: tensor(0.3520)\n",
      "18460 Training Loss: tensor(0.3537)\n",
      "18461 Training Loss: tensor(0.3524)\n",
      "18462 Training Loss: tensor(0.3549)\n",
      "18463 Training Loss: tensor(0.3505)\n",
      "18464 Training Loss: tensor(0.3508)\n",
      "18465 Training Loss: tensor(0.3555)\n",
      "18466 Training Loss: tensor(0.3542)\n",
      "18467 Training Loss: tensor(0.3545)\n",
      "18468 Training Loss: tensor(0.3542)\n",
      "18469 Training Loss: tensor(0.3559)\n",
      "18470 Training Loss: tensor(0.3517)\n",
      "18471 Training Loss: tensor(0.3552)\n",
      "18472 Training Loss: tensor(0.3517)\n",
      "18473 Training Loss: tensor(0.3522)\n",
      "18474 Training Loss: tensor(0.3579)\n",
      "18475 Training Loss: tensor(0.3515)\n",
      "18476 Training Loss: tensor(0.3517)\n",
      "18477 Training Loss: tensor(0.3573)\n",
      "18478 Training Loss: tensor(0.3554)\n",
      "18479 Training Loss: tensor(0.3507)\n",
      "18480 Training Loss: tensor(0.3519)\n",
      "18481 Training Loss: tensor(0.3507)\n",
      "18482 Training Loss: tensor(0.3514)\n",
      "18483 Training Loss: tensor(0.3502)\n",
      "18484 Training Loss: tensor(0.3517)\n",
      "18485 Training Loss: tensor(0.3536)\n",
      "18486 Training Loss: tensor(0.3500)\n",
      "18487 Training Loss: tensor(0.3529)\n",
      "18488 Training Loss: tensor(0.3515)\n",
      "18489 Training Loss: tensor(0.3597)\n",
      "18490 Training Loss: tensor(0.3528)\n",
      "18491 Training Loss: tensor(0.3507)\n",
      "18492 Training Loss: tensor(0.3529)\n",
      "18493 Training Loss: tensor(0.3591)\n",
      "18494 Training Loss: tensor(0.3574)\n",
      "18495 Training Loss: tensor(0.3579)\n",
      "18496 Training Loss: tensor(0.3516)\n",
      "18497 Training Loss: tensor(0.3529)\n",
      "18498 Training Loss: tensor(0.3532)\n",
      "18499 Training Loss: tensor(0.3551)\n",
      "18500 Training Loss: tensor(0.3527)\n",
      "18501 Training Loss: tensor(0.3601)\n",
      "18502 Training Loss: tensor(0.3530)\n",
      "18503 Training Loss: tensor(0.3535)\n",
      "18504 Training Loss: tensor(0.3551)\n",
      "18505 Training Loss: tensor(0.3538)\n",
      "18506 Training Loss: tensor(0.3545)\n",
      "18507 Training Loss: tensor(0.3540)\n",
      "18508 Training Loss: tensor(0.3541)\n",
      "18509 Training Loss: tensor(0.3555)\n",
      "18510 Training Loss: tensor(0.3556)\n",
      "18511 Training Loss: tensor(0.3538)\n",
      "18512 Training Loss: tensor(0.3544)\n",
      "18513 Training Loss: tensor(0.3551)\n",
      "18514 Training Loss: tensor(0.3516)\n",
      "18515 Training Loss: tensor(0.3534)\n",
      "18516 Training Loss: tensor(0.3539)\n",
      "18517 Training Loss: tensor(0.3517)\n",
      "18518 Training Loss: tensor(0.3512)\n",
      "18519 Training Loss: tensor(0.3536)\n",
      "18520 Training Loss: tensor(0.3519)\n",
      "18521 Training Loss: tensor(0.3612)\n",
      "18522 Training Loss: tensor(0.3537)\n",
      "18523 Training Loss: tensor(0.3633)\n",
      "18524 Training Loss: tensor(0.3516)\n",
      "18525 Training Loss: tensor(0.3539)\n",
      "18526 Training Loss: tensor(0.3531)\n",
      "18527 Training Loss: tensor(0.3517)\n",
      "18528 Training Loss: tensor(0.3513)\n",
      "18529 Training Loss: tensor(0.3527)\n",
      "18530 Training Loss: tensor(0.3529)\n",
      "18531 Training Loss: tensor(0.3524)\n",
      "18532 Training Loss: tensor(0.3508)\n",
      "18533 Training Loss: tensor(0.3557)\n",
      "18534 Training Loss: tensor(0.3500)\n",
      "18535 Training Loss: tensor(0.3555)\n",
      "18536 Training Loss: tensor(0.3511)\n",
      "18537 Training Loss: tensor(0.3523)\n",
      "18538 Training Loss: tensor(0.3537)\n",
      "18539 Training Loss: tensor(0.3530)\n",
      "18540 Training Loss: tensor(0.3570)\n",
      "18541 Training Loss: tensor(0.3570)\n",
      "18542 Training Loss: tensor(0.3564)\n",
      "18543 Training Loss: tensor(0.3505)\n",
      "18544 Training Loss: tensor(0.3580)\n",
      "18545 Training Loss: tensor(0.3512)\n",
      "18546 Training Loss: tensor(0.3574)\n",
      "18547 Training Loss: tensor(0.3537)\n",
      "18548 Training Loss: tensor(0.3556)\n",
      "18549 Training Loss: tensor(0.3559)\n",
      "18550 Training Loss: tensor(0.3546)\n",
      "18551 Training Loss: tensor(0.3525)\n",
      "18552 Training Loss: tensor(0.3538)\n",
      "18553 Training Loss: tensor(0.3521)\n",
      "18554 Training Loss: tensor(0.3526)\n",
      "18555 Training Loss: tensor(0.3536)\n",
      "18556 Training Loss: tensor(0.3524)\n",
      "18557 Training Loss: tensor(0.3526)\n",
      "18558 Training Loss: tensor(0.3550)\n",
      "18559 Training Loss: tensor(0.3570)\n",
      "18560 Training Loss: tensor(0.3503)\n",
      "18561 Training Loss: tensor(0.3515)\n",
      "18562 Training Loss: tensor(0.3522)\n",
      "18563 Training Loss: tensor(0.3518)\n",
      "18564 Training Loss: tensor(0.3520)\n",
      "18565 Training Loss: tensor(0.3562)\n",
      "18566 Training Loss: tensor(0.3505)\n",
      "18567 Training Loss: tensor(0.3506)\n",
      "18568 Training Loss: tensor(0.3526)\n",
      "18569 Training Loss: tensor(0.3523)\n",
      "18570 Training Loss: tensor(0.3553)\n",
      "18571 Training Loss: tensor(0.3514)\n",
      "18572 Training Loss: tensor(0.3505)\n",
      "18573 Training Loss: tensor(0.3521)\n",
      "18574 Training Loss: tensor(0.3538)\n",
      "18575 Training Loss: tensor(0.3562)\n",
      "18576 Training Loss: tensor(0.3536)\n",
      "18577 Training Loss: tensor(0.3499)\n",
      "18578 Training Loss: tensor(0.3707)\n",
      "18579 Training Loss: tensor(0.3565)\n",
      "18580 Training Loss: tensor(0.3540)\n",
      "18581 Training Loss: tensor(0.3516)\n",
      "18582 Training Loss: tensor(0.3509)\n",
      "18583 Training Loss: tensor(0.3540)\n",
      "18584 Training Loss: tensor(0.3545)\n",
      "18585 Training Loss: tensor(0.3511)\n",
      "18586 Training Loss: tensor(0.3504)\n",
      "18587 Training Loss: tensor(0.3524)\n",
      "18588 Training Loss: tensor(0.3557)\n",
      "18589 Training Loss: tensor(0.3529)\n",
      "18590 Training Loss: tensor(0.3569)\n",
      "18591 Training Loss: tensor(0.3526)\n",
      "18592 Training Loss: tensor(0.3512)\n",
      "18593 Training Loss: tensor(0.3549)\n",
      "18594 Training Loss: tensor(0.3520)\n",
      "18595 Training Loss: tensor(0.3580)\n",
      "18596 Training Loss: tensor(0.3513)\n",
      "18597 Training Loss: tensor(0.3520)\n",
      "18598 Training Loss: tensor(0.3512)\n",
      "18599 Training Loss: tensor(0.3505)\n",
      "18600 Training Loss: tensor(0.3544)\n",
      "18601 Training Loss: tensor(0.3549)\n",
      "18602 Training Loss: tensor(0.3506)\n",
      "18603 Training Loss: tensor(0.3516)\n",
      "18604 Training Loss: tensor(0.3522)\n",
      "18605 Training Loss: tensor(0.3511)\n",
      "18606 Training Loss: tensor(0.3505)\n",
      "18607 Training Loss: tensor(0.3553)\n",
      "18608 Training Loss: tensor(0.3509)\n",
      "18609 Training Loss: tensor(0.3512)\n",
      "18610 Training Loss: tensor(0.3526)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18611 Training Loss: tensor(0.3538)\n",
      "18612 Training Loss: tensor(0.3516)\n",
      "18613 Training Loss: tensor(0.3566)\n",
      "18614 Training Loss: tensor(0.3557)\n",
      "18615 Training Loss: tensor(0.3512)\n",
      "18616 Training Loss: tensor(0.3551)\n",
      "18617 Training Loss: tensor(0.3539)\n",
      "18618 Training Loss: tensor(0.3606)\n",
      "18619 Training Loss: tensor(0.3503)\n",
      "18620 Training Loss: tensor(0.3510)\n",
      "18621 Training Loss: tensor(0.3533)\n",
      "18622 Training Loss: tensor(0.3539)\n",
      "18623 Training Loss: tensor(0.3528)\n",
      "18624 Training Loss: tensor(0.3507)\n",
      "18625 Training Loss: tensor(0.3528)\n",
      "18626 Training Loss: tensor(0.3507)\n",
      "18627 Training Loss: tensor(0.3541)\n",
      "18628 Training Loss: tensor(0.3521)\n",
      "18629 Training Loss: tensor(0.3516)\n",
      "18630 Training Loss: tensor(0.3591)\n",
      "18631 Training Loss: tensor(0.3534)\n",
      "18632 Training Loss: tensor(0.3512)\n",
      "18633 Training Loss: tensor(0.3513)\n",
      "18634 Training Loss: tensor(0.3544)\n",
      "18635 Training Loss: tensor(0.3569)\n",
      "18636 Training Loss: tensor(0.3508)\n",
      "18637 Training Loss: tensor(0.3509)\n",
      "18638 Training Loss: tensor(0.3505)\n",
      "18639 Training Loss: tensor(0.3516)\n",
      "18640 Training Loss: tensor(0.3565)\n",
      "18641 Training Loss: tensor(0.3538)\n",
      "18642 Training Loss: tensor(0.3503)\n",
      "18643 Training Loss: tensor(0.3528)\n",
      "18644 Training Loss: tensor(0.3530)\n",
      "18645 Training Loss: tensor(0.3501)\n",
      "18646 Training Loss: tensor(0.3518)\n",
      "18647 Training Loss: tensor(0.3559)\n",
      "18648 Training Loss: tensor(0.3518)\n",
      "18649 Training Loss: tensor(0.3711)\n",
      "18650 Training Loss: tensor(0.3509)\n",
      "18651 Training Loss: tensor(0.3590)\n",
      "18652 Training Loss: tensor(0.3566)\n",
      "18653 Training Loss: tensor(0.3522)\n",
      "18654 Training Loss: tensor(0.3554)\n",
      "18655 Training Loss: tensor(0.3523)\n",
      "18656 Training Loss: tensor(0.3525)\n",
      "18657 Training Loss: tensor(0.3552)\n",
      "18658 Training Loss: tensor(0.3521)\n",
      "18659 Training Loss: tensor(0.3541)\n",
      "18660 Training Loss: tensor(0.3542)\n",
      "18661 Training Loss: tensor(0.3522)\n",
      "18662 Training Loss: tensor(0.3573)\n",
      "18663 Training Loss: tensor(0.3518)\n",
      "18664 Training Loss: tensor(0.3565)\n",
      "18665 Training Loss: tensor(0.3538)\n",
      "18666 Training Loss: tensor(0.3552)\n",
      "18667 Training Loss: tensor(0.3521)\n",
      "18668 Training Loss: tensor(0.3511)\n",
      "18669 Training Loss: tensor(0.3557)\n",
      "18670 Training Loss: tensor(0.3510)\n",
      "18671 Training Loss: tensor(0.3520)\n",
      "18672 Training Loss: tensor(0.3508)\n",
      "18673 Training Loss: tensor(0.3507)\n",
      "18674 Training Loss: tensor(0.3508)\n",
      "18675 Training Loss: tensor(0.3519)\n",
      "18676 Training Loss: tensor(0.3530)\n",
      "18677 Training Loss: tensor(0.3524)\n",
      "18678 Training Loss: tensor(0.3499)\n",
      "18679 Training Loss: tensor(0.3501)\n",
      "18680 Training Loss: tensor(0.3496)\n",
      "18681 Training Loss: tensor(0.3578)\n",
      "18682 Training Loss: tensor(0.3530)\n",
      "18683 Training Loss: tensor(0.3520)\n",
      "18684 Training Loss: tensor(0.3502)\n",
      "18685 Training Loss: tensor(0.3576)\n",
      "18686 Training Loss: tensor(0.3707)\n",
      "18687 Training Loss: tensor(0.3528)\n",
      "18688 Training Loss: tensor(0.3543)\n",
      "18689 Training Loss: tensor(0.3515)\n",
      "18690 Training Loss: tensor(0.3536)\n",
      "18691 Training Loss: tensor(0.3543)\n",
      "18692 Training Loss: tensor(0.3538)\n",
      "18693 Training Loss: tensor(0.3569)\n",
      "18694 Training Loss: tensor(0.3521)\n",
      "18695 Training Loss: tensor(0.3512)\n",
      "18696 Training Loss: tensor(0.3519)\n",
      "18697 Training Loss: tensor(0.3535)\n",
      "18698 Training Loss: tensor(0.3547)\n",
      "18699 Training Loss: tensor(0.3540)\n",
      "18700 Training Loss: tensor(0.3549)\n",
      "18701 Training Loss: tensor(0.3540)\n",
      "18702 Training Loss: tensor(0.3539)\n",
      "18703 Training Loss: tensor(0.3543)\n",
      "18704 Training Loss: tensor(0.3555)\n",
      "18705 Training Loss: tensor(0.3546)\n",
      "18706 Training Loss: tensor(0.3524)\n",
      "18707 Training Loss: tensor(0.3565)\n",
      "18708 Training Loss: tensor(0.3575)\n",
      "18709 Training Loss: tensor(0.3557)\n",
      "18710 Training Loss: tensor(0.3513)\n",
      "18711 Training Loss: tensor(0.3514)\n",
      "18712 Training Loss: tensor(0.3513)\n",
      "18713 Training Loss: tensor(0.3511)\n",
      "18714 Training Loss: tensor(0.3553)\n",
      "18715 Training Loss: tensor(0.3561)\n",
      "18716 Training Loss: tensor(0.3510)\n",
      "18717 Training Loss: tensor(0.3515)\n",
      "18718 Training Loss: tensor(0.3536)\n",
      "18719 Training Loss: tensor(0.3559)\n",
      "18720 Training Loss: tensor(0.3572)\n",
      "18721 Training Loss: tensor(0.3499)\n",
      "18722 Training Loss: tensor(0.3519)\n",
      "18723 Training Loss: tensor(0.3530)\n",
      "18724 Training Loss: tensor(0.3505)\n",
      "18725 Training Loss: tensor(0.3521)\n",
      "18726 Training Loss: tensor(0.3597)\n",
      "18727 Training Loss: tensor(0.3548)\n",
      "18728 Training Loss: tensor(0.3542)\n",
      "18729 Training Loss: tensor(0.3517)\n",
      "18730 Training Loss: tensor(0.3541)\n",
      "18731 Training Loss: tensor(0.3551)\n",
      "18732 Training Loss: tensor(0.3514)\n",
      "18733 Training Loss: tensor(0.3523)\n",
      "18734 Training Loss: tensor(0.3561)\n",
      "18735 Training Loss: tensor(0.3520)\n",
      "18736 Training Loss: tensor(0.3563)\n",
      "18737 Training Loss: tensor(0.3533)\n",
      "18738 Training Loss: tensor(0.3523)\n",
      "18739 Training Loss: tensor(0.3512)\n",
      "18740 Training Loss: tensor(0.3537)\n",
      "18741 Training Loss: tensor(0.3527)\n",
      "18742 Training Loss: tensor(0.3529)\n",
      "18743 Training Loss: tensor(0.3551)\n",
      "18744 Training Loss: tensor(0.3534)\n",
      "18745 Training Loss: tensor(0.3514)\n",
      "18746 Training Loss: tensor(0.3518)\n",
      "18747 Training Loss: tensor(0.3587)\n",
      "18748 Training Loss: tensor(0.3567)\n",
      "18749 Training Loss: tensor(0.3514)\n",
      "18750 Training Loss: tensor(0.3537)\n",
      "18751 Training Loss: tensor(0.3510)\n",
      "18752 Training Loss: tensor(0.3533)\n",
      "18753 Training Loss: tensor(0.3513)\n",
      "18754 Training Loss: tensor(0.3518)\n",
      "18755 Training Loss: tensor(0.3552)\n",
      "18756 Training Loss: tensor(0.3522)\n",
      "18757 Training Loss: tensor(0.3505)\n",
      "18758 Training Loss: tensor(0.3587)\n",
      "18759 Training Loss: tensor(0.3527)\n",
      "18760 Training Loss: tensor(0.3529)\n",
      "18761 Training Loss: tensor(0.3558)\n",
      "18762 Training Loss: tensor(0.3525)\n",
      "18763 Training Loss: tensor(0.3535)\n",
      "18764 Training Loss: tensor(0.3525)\n",
      "18765 Training Loss: tensor(0.3570)\n",
      "18766 Training Loss: tensor(0.3515)\n",
      "18767 Training Loss: tensor(0.3592)\n",
      "18768 Training Loss: tensor(0.3514)\n",
      "18769 Training Loss: tensor(0.3524)\n",
      "18770 Training Loss: tensor(0.3553)\n",
      "18771 Training Loss: tensor(0.3614)\n",
      "18772 Training Loss: tensor(0.3529)\n",
      "18773 Training Loss: tensor(0.3539)\n",
      "18774 Training Loss: tensor(0.3542)\n",
      "18775 Training Loss: tensor(0.3519)\n",
      "18776 Training Loss: tensor(0.3587)\n",
      "18777 Training Loss: tensor(0.3516)\n",
      "18778 Training Loss: tensor(0.3556)\n",
      "18779 Training Loss: tensor(0.3534)\n",
      "18780 Training Loss: tensor(0.3541)\n",
      "18781 Training Loss: tensor(0.3518)\n",
      "18782 Training Loss: tensor(0.3569)\n",
      "18783 Training Loss: tensor(0.3517)\n",
      "18784 Training Loss: tensor(0.3571)\n",
      "18785 Training Loss: tensor(0.3530)\n",
      "18786 Training Loss: tensor(0.3515)\n",
      "18787 Training Loss: tensor(0.3536)\n",
      "18788 Training Loss: tensor(0.3521)\n",
      "18789 Training Loss: tensor(0.3543)\n",
      "18790 Training Loss: tensor(0.3509)\n",
      "18791 Training Loss: tensor(0.3521)\n",
      "18792 Training Loss: tensor(0.3546)\n",
      "18793 Training Loss: tensor(0.3541)\n",
      "18794 Training Loss: tensor(0.3527)\n",
      "18795 Training Loss: tensor(0.3514)\n",
      "18796 Training Loss: tensor(0.3544)\n",
      "18797 Training Loss: tensor(0.3573)\n",
      "18798 Training Loss: tensor(0.3523)\n",
      "18799 Training Loss: tensor(0.3505)\n",
      "18800 Training Loss: tensor(0.3530)\n",
      "18801 Training Loss: tensor(0.3501)\n",
      "18802 Training Loss: tensor(0.3507)\n",
      "18803 Training Loss: tensor(0.3554)\n",
      "18804 Training Loss: tensor(0.3538)\n",
      "18805 Training Loss: tensor(0.3534)\n",
      "18806 Training Loss: tensor(0.3541)\n",
      "18807 Training Loss: tensor(0.3511)\n",
      "18808 Training Loss: tensor(0.3537)\n",
      "18809 Training Loss: tensor(0.3506)\n",
      "18810 Training Loss: tensor(0.3526)\n",
      "18811 Training Loss: tensor(0.3524)\n",
      "18812 Training Loss: tensor(0.3559)\n",
      "18813 Training Loss: tensor(0.3521)\n",
      "18814 Training Loss: tensor(0.3501)\n",
      "18815 Training Loss: tensor(0.3565)\n",
      "18816 Training Loss: tensor(0.3514)\n",
      "18817 Training Loss: tensor(0.3600)\n",
      "18818 Training Loss: tensor(0.3566)\n",
      "18819 Training Loss: tensor(0.3541)\n",
      "18820 Training Loss: tensor(0.3597)\n",
      "18821 Training Loss: tensor(0.3516)\n",
      "18822 Training Loss: tensor(0.3508)\n",
      "18823 Training Loss: tensor(0.3541)\n",
      "18824 Training Loss: tensor(0.3513)\n",
      "18825 Training Loss: tensor(0.3539)\n",
      "18826 Training Loss: tensor(0.3523)\n",
      "18827 Training Loss: tensor(0.3585)\n",
      "18828 Training Loss: tensor(0.3550)\n",
      "18829 Training Loss: tensor(0.3522)\n",
      "18830 Training Loss: tensor(0.3532)\n",
      "18831 Training Loss: tensor(0.3527)\n",
      "18832 Training Loss: tensor(0.3519)\n",
      "18833 Training Loss: tensor(0.3528)\n",
      "18834 Training Loss: tensor(0.3532)\n",
      "18835 Training Loss: tensor(0.3554)\n",
      "18836 Training Loss: tensor(0.3539)\n",
      "18837 Training Loss: tensor(0.3546)\n",
      "18838 Training Loss: tensor(0.3509)\n",
      "18839 Training Loss: tensor(0.3536)\n",
      "18840 Training Loss: tensor(0.3568)\n",
      "18841 Training Loss: tensor(0.3529)\n",
      "18842 Training Loss: tensor(0.3551)\n",
      "18843 Training Loss: tensor(0.3539)\n",
      "18844 Training Loss: tensor(0.3511)\n",
      "18845 Training Loss: tensor(0.3540)\n",
      "18846 Training Loss: tensor(0.3542)\n",
      "18847 Training Loss: tensor(0.3548)\n",
      "18848 Training Loss: tensor(0.3536)\n",
      "18849 Training Loss: tensor(0.3526)\n",
      "18850 Training Loss: tensor(0.3535)\n",
      "18851 Training Loss: tensor(0.3517)\n",
      "18852 Training Loss: tensor(0.3523)\n",
      "18853 Training Loss: tensor(0.3643)\n",
      "18854 Training Loss: tensor(0.3523)\n",
      "18855 Training Loss: tensor(0.3558)\n",
      "18856 Training Loss: tensor(0.3520)\n",
      "18857 Training Loss: tensor(0.3516)\n",
      "18858 Training Loss: tensor(0.3530)\n",
      "18859 Training Loss: tensor(0.3537)\n",
      "18860 Training Loss: tensor(0.3511)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18861 Training Loss: tensor(0.3565)\n",
      "18862 Training Loss: tensor(0.3533)\n",
      "18863 Training Loss: tensor(0.3560)\n",
      "18864 Training Loss: tensor(0.3518)\n",
      "18865 Training Loss: tensor(0.3541)\n",
      "18866 Training Loss: tensor(0.3534)\n",
      "18867 Training Loss: tensor(0.3525)\n",
      "18868 Training Loss: tensor(0.3531)\n",
      "18869 Training Loss: tensor(0.3513)\n",
      "18870 Training Loss: tensor(0.3532)\n",
      "18871 Training Loss: tensor(0.3536)\n",
      "18872 Training Loss: tensor(0.3553)\n",
      "18873 Training Loss: tensor(0.3541)\n",
      "18874 Training Loss: tensor(0.3522)\n",
      "18875 Training Loss: tensor(0.3538)\n",
      "18876 Training Loss: tensor(0.3507)\n",
      "18877 Training Loss: tensor(0.3515)\n",
      "18878 Training Loss: tensor(0.3517)\n",
      "18879 Training Loss: tensor(0.3512)\n",
      "18880 Training Loss: tensor(0.3513)\n",
      "18881 Training Loss: tensor(0.3524)\n",
      "18882 Training Loss: tensor(0.3561)\n",
      "18883 Training Loss: tensor(0.3534)\n",
      "18884 Training Loss: tensor(0.3592)\n",
      "18885 Training Loss: tensor(0.3544)\n",
      "18886 Training Loss: tensor(0.3506)\n",
      "18887 Training Loss: tensor(0.3528)\n",
      "18888 Training Loss: tensor(0.3515)\n",
      "18889 Training Loss: tensor(0.3516)\n",
      "18890 Training Loss: tensor(0.3518)\n",
      "18891 Training Loss: tensor(0.3537)\n",
      "18892 Training Loss: tensor(0.3558)\n",
      "18893 Training Loss: tensor(0.3529)\n",
      "18894 Training Loss: tensor(0.3516)\n",
      "18895 Training Loss: tensor(0.3514)\n",
      "18896 Training Loss: tensor(0.3512)\n",
      "18897 Training Loss: tensor(0.3542)\n",
      "18898 Training Loss: tensor(0.3593)\n",
      "18899 Training Loss: tensor(0.3546)\n",
      "18900 Training Loss: tensor(0.3534)\n",
      "18901 Training Loss: tensor(0.3528)\n",
      "18902 Training Loss: tensor(0.3510)\n",
      "18903 Training Loss: tensor(0.3562)\n",
      "18904 Training Loss: tensor(0.3517)\n",
      "18905 Training Loss: tensor(0.3527)\n",
      "18906 Training Loss: tensor(0.3561)\n",
      "18907 Training Loss: tensor(0.3563)\n",
      "18908 Training Loss: tensor(0.3510)\n",
      "18909 Training Loss: tensor(0.3516)\n",
      "18910 Training Loss: tensor(0.3515)\n",
      "18911 Training Loss: tensor(0.3528)\n",
      "18912 Training Loss: tensor(0.3507)\n",
      "18913 Training Loss: tensor(0.3514)\n",
      "18914 Training Loss: tensor(0.3564)\n",
      "18915 Training Loss: tensor(0.3551)\n",
      "18916 Training Loss: tensor(0.3613)\n",
      "18917 Training Loss: tensor(0.3536)\n",
      "18918 Training Loss: tensor(0.3519)\n",
      "18919 Training Loss: tensor(0.3523)\n",
      "18920 Training Loss: tensor(0.3534)\n",
      "18921 Training Loss: tensor(0.3515)\n",
      "18922 Training Loss: tensor(0.3556)\n",
      "18923 Training Loss: tensor(0.3516)\n",
      "18924 Training Loss: tensor(0.3518)\n",
      "18925 Training Loss: tensor(0.3601)\n",
      "18926 Training Loss: tensor(0.3544)\n",
      "18927 Training Loss: tensor(0.3556)\n",
      "18928 Training Loss: tensor(0.3528)\n",
      "18929 Training Loss: tensor(0.3557)\n",
      "18930 Training Loss: tensor(0.3541)\n",
      "18931 Training Loss: tensor(0.3513)\n",
      "18932 Training Loss: tensor(0.3583)\n",
      "18933 Training Loss: tensor(0.3572)\n",
      "18934 Training Loss: tensor(0.3551)\n",
      "18935 Training Loss: tensor(0.3540)\n",
      "18936 Training Loss: tensor(0.3542)\n",
      "18937 Training Loss: tensor(0.3520)\n",
      "18938 Training Loss: tensor(0.3509)\n",
      "18939 Training Loss: tensor(0.3528)\n",
      "18940 Training Loss: tensor(0.3560)\n",
      "18941 Training Loss: tensor(0.3543)\n",
      "18942 Training Loss: tensor(0.3552)\n",
      "18943 Training Loss: tensor(0.3513)\n",
      "18944 Training Loss: tensor(0.3523)\n",
      "18945 Training Loss: tensor(0.3520)\n",
      "18946 Training Loss: tensor(0.3510)\n",
      "18947 Training Loss: tensor(0.3508)\n",
      "18948 Training Loss: tensor(0.3523)\n",
      "18949 Training Loss: tensor(0.3602)\n",
      "18950 Training Loss: tensor(0.3524)\n",
      "18951 Training Loss: tensor(0.3589)\n",
      "18952 Training Loss: tensor(0.3554)\n",
      "18953 Training Loss: tensor(0.3538)\n",
      "18954 Training Loss: tensor(0.3539)\n",
      "18955 Training Loss: tensor(0.3521)\n",
      "18956 Training Loss: tensor(0.3533)\n",
      "18957 Training Loss: tensor(0.3501)\n",
      "18958 Training Loss: tensor(0.3529)\n",
      "18959 Training Loss: tensor(0.3511)\n",
      "18960 Training Loss: tensor(0.3526)\n",
      "18961 Training Loss: tensor(0.3578)\n",
      "18962 Training Loss: tensor(0.3545)\n",
      "18963 Training Loss: tensor(0.3524)\n",
      "18964 Training Loss: tensor(0.3521)\n",
      "18965 Training Loss: tensor(0.3561)\n",
      "18966 Training Loss: tensor(0.3558)\n",
      "18967 Training Loss: tensor(0.3516)\n",
      "18968 Training Loss: tensor(0.3518)\n",
      "18969 Training Loss: tensor(0.3523)\n",
      "18970 Training Loss: tensor(0.3540)\n",
      "18971 Training Loss: tensor(0.3542)\n",
      "18972 Training Loss: tensor(0.3509)\n",
      "18973 Training Loss: tensor(0.3520)\n",
      "18974 Training Loss: tensor(0.3517)\n",
      "18975 Training Loss: tensor(0.3550)\n",
      "18976 Training Loss: tensor(0.3591)\n",
      "18977 Training Loss: tensor(0.3501)\n",
      "18978 Training Loss: tensor(0.3508)\n",
      "18979 Training Loss: tensor(0.3523)\n",
      "18980 Training Loss: tensor(0.3507)\n",
      "18981 Training Loss: tensor(0.3542)\n",
      "18982 Training Loss: tensor(0.3505)\n",
      "18983 Training Loss: tensor(0.3562)\n",
      "18984 Training Loss: tensor(0.3500)\n",
      "18985 Training Loss: tensor(0.3522)\n",
      "18986 Training Loss: tensor(0.3544)\n",
      "18987 Training Loss: tensor(0.3538)\n",
      "18988 Training Loss: tensor(0.3498)\n",
      "18989 Training Loss: tensor(0.3529)\n",
      "18990 Training Loss: tensor(0.3511)\n",
      "18991 Training Loss: tensor(0.3513)\n",
      "18992 Training Loss: tensor(0.3613)\n",
      "18993 Training Loss: tensor(0.3588)\n",
      "18994 Training Loss: tensor(0.3496)\n",
      "18995 Training Loss: tensor(0.3501)\n",
      "18996 Training Loss: tensor(0.3494)\n",
      "18997 Training Loss: tensor(0.3505)\n",
      "18998 Training Loss: tensor(0.3508)\n",
      "18999 Training Loss: tensor(0.3520)\n",
      "19000 Training Loss: tensor(0.3509)\n",
      "19001 Training Loss: tensor(0.3553)\n",
      "19002 Training Loss: tensor(0.3538)\n",
      "19003 Training Loss: tensor(0.3522)\n",
      "19004 Training Loss: tensor(0.3554)\n",
      "19005 Training Loss: tensor(0.3593)\n",
      "19006 Training Loss: tensor(0.3542)\n",
      "19007 Training Loss: tensor(0.3496)\n",
      "19008 Training Loss: tensor(0.3519)\n",
      "19009 Training Loss: tensor(0.3520)\n",
      "19010 Training Loss: tensor(0.3577)\n",
      "19011 Training Loss: tensor(0.3534)\n",
      "19012 Training Loss: tensor(0.3539)\n",
      "19013 Training Loss: tensor(0.3541)\n",
      "19014 Training Loss: tensor(0.3503)\n",
      "19015 Training Loss: tensor(0.3533)\n",
      "19016 Training Loss: tensor(0.3510)\n",
      "19017 Training Loss: tensor(0.3511)\n",
      "19018 Training Loss: tensor(0.3515)\n",
      "19019 Training Loss: tensor(0.3553)\n",
      "19020 Training Loss: tensor(0.3516)\n",
      "19021 Training Loss: tensor(0.3529)\n",
      "19022 Training Loss: tensor(0.3528)\n",
      "19023 Training Loss: tensor(0.3528)\n",
      "19024 Training Loss: tensor(0.3516)\n",
      "19025 Training Loss: tensor(0.3526)\n",
      "19026 Training Loss: tensor(0.3509)\n",
      "19027 Training Loss: tensor(0.3536)\n",
      "19028 Training Loss: tensor(0.3561)\n",
      "19029 Training Loss: tensor(0.3511)\n",
      "19030 Training Loss: tensor(0.3508)\n",
      "19031 Training Loss: tensor(0.3519)\n",
      "19032 Training Loss: tensor(0.3576)\n",
      "19033 Training Loss: tensor(0.3508)\n",
      "19034 Training Loss: tensor(0.3569)\n",
      "19035 Training Loss: tensor(0.3550)\n",
      "19036 Training Loss: tensor(0.3535)\n",
      "19037 Training Loss: tensor(0.3546)\n",
      "19038 Training Loss: tensor(0.3534)\n",
      "19039 Training Loss: tensor(0.3526)\n",
      "19040 Training Loss: tensor(0.3499)\n",
      "19041 Training Loss: tensor(0.3510)\n",
      "19042 Training Loss: tensor(0.3512)\n",
      "19043 Training Loss: tensor(0.3529)\n",
      "19044 Training Loss: tensor(0.3507)\n",
      "19045 Training Loss: tensor(0.3505)\n",
      "19046 Training Loss: tensor(0.3534)\n",
      "19047 Training Loss: tensor(0.3615)\n",
      "19048 Training Loss: tensor(0.3530)\n",
      "19049 Training Loss: tensor(0.3520)\n",
      "19050 Training Loss: tensor(0.3519)\n",
      "19051 Training Loss: tensor(0.3516)\n",
      "19052 Training Loss: tensor(0.3514)\n",
      "19053 Training Loss: tensor(0.3518)\n",
      "19054 Training Loss: tensor(0.3535)\n",
      "19055 Training Loss: tensor(0.3506)\n",
      "19056 Training Loss: tensor(0.3528)\n",
      "19057 Training Loss: tensor(0.3550)\n",
      "19058 Training Loss: tensor(0.3500)\n",
      "19059 Training Loss: tensor(0.3536)\n",
      "19060 Training Loss: tensor(0.3521)\n",
      "19061 Training Loss: tensor(0.3511)\n",
      "19062 Training Loss: tensor(0.3526)\n",
      "19063 Training Loss: tensor(0.3598)\n",
      "19064 Training Loss: tensor(0.3513)\n",
      "19065 Training Loss: tensor(0.3551)\n",
      "19066 Training Loss: tensor(0.3549)\n",
      "19067 Training Loss: tensor(0.3516)\n",
      "19068 Training Loss: tensor(0.3586)\n",
      "19069 Training Loss: tensor(0.3524)\n",
      "19070 Training Loss: tensor(0.3508)\n",
      "19071 Training Loss: tensor(0.3525)\n",
      "19072 Training Loss: tensor(0.3541)\n",
      "19073 Training Loss: tensor(0.3532)\n",
      "19074 Training Loss: tensor(0.3543)\n",
      "19075 Training Loss: tensor(0.3549)\n",
      "19076 Training Loss: tensor(0.3517)\n",
      "19077 Training Loss: tensor(0.3524)\n",
      "19078 Training Loss: tensor(0.3527)\n",
      "19079 Training Loss: tensor(0.3519)\n",
      "19080 Training Loss: tensor(0.3526)\n",
      "19081 Training Loss: tensor(0.3542)\n",
      "19082 Training Loss: tensor(0.3505)\n",
      "19083 Training Loss: tensor(0.3530)\n",
      "19084 Training Loss: tensor(0.3559)\n",
      "19085 Training Loss: tensor(0.3554)\n",
      "19086 Training Loss: tensor(0.3516)\n",
      "19087 Training Loss: tensor(0.3572)\n",
      "19088 Training Loss: tensor(0.3559)\n",
      "19089 Training Loss: tensor(0.3634)\n",
      "19090 Training Loss: tensor(0.3514)\n",
      "19091 Training Loss: tensor(0.3504)\n",
      "19092 Training Loss: tensor(0.3514)\n",
      "19093 Training Loss: tensor(0.3517)\n",
      "19094 Training Loss: tensor(0.3511)\n",
      "19095 Training Loss: tensor(0.3500)\n",
      "19096 Training Loss: tensor(0.3528)\n",
      "19097 Training Loss: tensor(0.3519)\n",
      "19098 Training Loss: tensor(0.3522)\n",
      "19099 Training Loss: tensor(0.3503)\n",
      "19100 Training Loss: tensor(0.3504)\n",
      "19101 Training Loss: tensor(0.3554)\n",
      "19102 Training Loss: tensor(0.3544)\n",
      "19103 Training Loss: tensor(0.3502)\n",
      "19104 Training Loss: tensor(0.3522)\n",
      "19105 Training Loss: tensor(0.3496)\n",
      "19106 Training Loss: tensor(0.3504)\n",
      "19107 Training Loss: tensor(0.3559)\n",
      "19108 Training Loss: tensor(0.3492)\n",
      "19109 Training Loss: tensor(0.3521)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19110 Training Loss: tensor(0.3645)\n",
      "19111 Training Loss: tensor(0.3636)\n",
      "19112 Training Loss: tensor(0.3538)\n",
      "19113 Training Loss: tensor(0.3621)\n",
      "19114 Training Loss: tensor(0.3562)\n",
      "19115 Training Loss: tensor(0.3549)\n",
      "19116 Training Loss: tensor(0.3536)\n",
      "19117 Training Loss: tensor(0.3524)\n",
      "19118 Training Loss: tensor(0.3541)\n",
      "19119 Training Loss: tensor(0.3533)\n",
      "19120 Training Loss: tensor(0.3557)\n",
      "19121 Training Loss: tensor(0.3525)\n",
      "19122 Training Loss: tensor(0.3549)\n",
      "19123 Training Loss: tensor(0.3554)\n",
      "19124 Training Loss: tensor(0.3530)\n",
      "19125 Training Loss: tensor(0.3524)\n",
      "19126 Training Loss: tensor(0.3552)\n",
      "19127 Training Loss: tensor(0.3577)\n",
      "19128 Training Loss: tensor(0.3537)\n",
      "19129 Training Loss: tensor(0.3530)\n",
      "19130 Training Loss: tensor(0.3509)\n",
      "19131 Training Loss: tensor(0.3524)\n",
      "19132 Training Loss: tensor(0.3525)\n",
      "19133 Training Loss: tensor(0.3520)\n",
      "19134 Training Loss: tensor(0.3533)\n",
      "19135 Training Loss: tensor(0.3524)\n",
      "19136 Training Loss: tensor(0.3529)\n",
      "19137 Training Loss: tensor(0.3531)\n",
      "19138 Training Loss: tensor(0.3514)\n",
      "19139 Training Loss: tensor(0.3578)\n",
      "19140 Training Loss: tensor(0.3498)\n",
      "19141 Training Loss: tensor(0.3541)\n",
      "19142 Training Loss: tensor(0.3573)\n",
      "19143 Training Loss: tensor(0.3563)\n",
      "19144 Training Loss: tensor(0.3559)\n",
      "19145 Training Loss: tensor(0.3524)\n",
      "19146 Training Loss: tensor(0.3519)\n",
      "19147 Training Loss: tensor(0.3529)\n",
      "19148 Training Loss: tensor(0.3500)\n",
      "19149 Training Loss: tensor(0.3520)\n",
      "19150 Training Loss: tensor(0.3510)\n",
      "19151 Training Loss: tensor(0.3559)\n",
      "19152 Training Loss: tensor(0.3558)\n",
      "19153 Training Loss: tensor(0.3574)\n",
      "19154 Training Loss: tensor(0.3553)\n",
      "19155 Training Loss: tensor(0.3538)\n",
      "19156 Training Loss: tensor(0.3540)\n",
      "19157 Training Loss: tensor(0.3517)\n",
      "19158 Training Loss: tensor(0.3516)\n",
      "19159 Training Loss: tensor(0.3506)\n",
      "19160 Training Loss: tensor(0.3558)\n",
      "19161 Training Loss: tensor(0.3513)\n",
      "19162 Training Loss: tensor(0.3540)\n",
      "19163 Training Loss: tensor(0.3527)\n",
      "19164 Training Loss: tensor(0.3524)\n",
      "19165 Training Loss: tensor(0.3520)\n",
      "19166 Training Loss: tensor(0.3525)\n",
      "19167 Training Loss: tensor(0.3521)\n",
      "19168 Training Loss: tensor(0.3620)\n",
      "19169 Training Loss: tensor(0.3510)\n",
      "19170 Training Loss: tensor(0.3537)\n",
      "19171 Training Loss: tensor(0.3540)\n",
      "19172 Training Loss: tensor(0.3509)\n",
      "19173 Training Loss: tensor(0.3507)\n",
      "19174 Training Loss: tensor(0.3501)\n",
      "19175 Training Loss: tensor(0.3516)\n",
      "19176 Training Loss: tensor(0.3588)\n",
      "19177 Training Loss: tensor(0.3538)\n",
      "19178 Training Loss: tensor(0.3547)\n",
      "19179 Training Loss: tensor(0.3525)\n",
      "19180 Training Loss: tensor(0.3589)\n",
      "19181 Training Loss: tensor(0.3547)\n",
      "19182 Training Loss: tensor(0.3539)\n",
      "19183 Training Loss: tensor(0.3526)\n",
      "19184 Training Loss: tensor(0.3566)\n",
      "19185 Training Loss: tensor(0.3519)\n",
      "19186 Training Loss: tensor(0.3543)\n",
      "19187 Training Loss: tensor(0.3513)\n",
      "19188 Training Loss: tensor(0.3514)\n",
      "19189 Training Loss: tensor(0.3517)\n",
      "19190 Training Loss: tensor(0.3530)\n",
      "19191 Training Loss: tensor(0.3515)\n",
      "19192 Training Loss: tensor(0.3538)\n",
      "19193 Training Loss: tensor(0.3508)\n",
      "19194 Training Loss: tensor(0.3530)\n",
      "19195 Training Loss: tensor(0.3509)\n",
      "19196 Training Loss: tensor(0.3522)\n",
      "19197 Training Loss: tensor(0.3519)\n",
      "19198 Training Loss: tensor(0.3510)\n",
      "19199 Training Loss: tensor(0.3515)\n",
      "19200 Training Loss: tensor(0.3495)\n",
      "19201 Training Loss: tensor(0.3559)\n",
      "19202 Training Loss: tensor(0.3500)\n",
      "19203 Training Loss: tensor(0.3581)\n",
      "19204 Training Loss: tensor(0.3495)\n",
      "19205 Training Loss: tensor(0.3523)\n",
      "19206 Training Loss: tensor(0.3588)\n",
      "19207 Training Loss: tensor(0.3536)\n",
      "19208 Training Loss: tensor(0.3502)\n",
      "19209 Training Loss: tensor(0.3499)\n",
      "19210 Training Loss: tensor(0.3531)\n",
      "19211 Training Loss: tensor(0.3588)\n",
      "19212 Training Loss: tensor(0.3509)\n",
      "19213 Training Loss: tensor(0.3551)\n",
      "19214 Training Loss: tensor(0.3549)\n",
      "19215 Training Loss: tensor(0.3545)\n",
      "19216 Training Loss: tensor(0.3528)\n",
      "19217 Training Loss: tensor(0.3541)\n",
      "19218 Training Loss: tensor(0.3503)\n",
      "19219 Training Loss: tensor(0.3516)\n",
      "19220 Training Loss: tensor(0.3541)\n",
      "19221 Training Loss: tensor(0.3536)\n",
      "19222 Training Loss: tensor(0.3511)\n",
      "19223 Training Loss: tensor(0.3531)\n",
      "19224 Training Loss: tensor(0.3519)\n",
      "19225 Training Loss: tensor(0.3576)\n",
      "19226 Training Loss: tensor(0.3527)\n",
      "19227 Training Loss: tensor(0.3516)\n",
      "19228 Training Loss: tensor(0.3517)\n",
      "19229 Training Loss: tensor(0.3532)\n",
      "19230 Training Loss: tensor(0.3509)\n",
      "19231 Training Loss: tensor(0.3535)\n",
      "19232 Training Loss: tensor(0.3529)\n",
      "19233 Training Loss: tensor(0.3511)\n",
      "19234 Training Loss: tensor(0.3557)\n",
      "19235 Training Loss: tensor(0.3513)\n",
      "19236 Training Loss: tensor(0.3550)\n",
      "19237 Training Loss: tensor(0.3543)\n",
      "19238 Training Loss: tensor(0.3561)\n",
      "19239 Training Loss: tensor(0.3550)\n",
      "19240 Training Loss: tensor(0.3536)\n",
      "19241 Training Loss: tensor(0.3582)\n",
      "19242 Training Loss: tensor(0.3519)\n",
      "19243 Training Loss: tensor(0.3519)\n",
      "19244 Training Loss: tensor(0.3514)\n",
      "19245 Training Loss: tensor(0.3515)\n",
      "19246 Training Loss: tensor(0.3522)\n",
      "19247 Training Loss: tensor(0.3584)\n",
      "19248 Training Loss: tensor(0.3502)\n",
      "19249 Training Loss: tensor(0.3504)\n",
      "19250 Training Loss: tensor(0.3559)\n",
      "19251 Training Loss: tensor(0.3511)\n",
      "19252 Training Loss: tensor(0.3543)\n",
      "19253 Training Loss: tensor(0.3538)\n",
      "19254 Training Loss: tensor(0.3544)\n",
      "19255 Training Loss: tensor(0.3504)\n",
      "19256 Training Loss: tensor(0.3544)\n",
      "19257 Training Loss: tensor(0.3509)\n",
      "19258 Training Loss: tensor(0.3543)\n",
      "19259 Training Loss: tensor(0.3592)\n",
      "19260 Training Loss: tensor(0.3520)\n",
      "19261 Training Loss: tensor(0.3499)\n",
      "19262 Training Loss: tensor(0.3508)\n",
      "19263 Training Loss: tensor(0.3522)\n",
      "19264 Training Loss: tensor(0.3516)\n",
      "19265 Training Loss: tensor(0.3513)\n",
      "19266 Training Loss: tensor(0.3547)\n",
      "19267 Training Loss: tensor(0.3519)\n",
      "19268 Training Loss: tensor(0.3506)\n",
      "19269 Training Loss: tensor(0.3522)\n",
      "19270 Training Loss: tensor(0.3509)\n",
      "19271 Training Loss: tensor(0.3526)\n",
      "19272 Training Loss: tensor(0.3569)\n",
      "19273 Training Loss: tensor(0.3511)\n",
      "19274 Training Loss: tensor(0.3551)\n",
      "19275 Training Loss: tensor(0.3511)\n",
      "19276 Training Loss: tensor(0.3522)\n",
      "19277 Training Loss: tensor(0.3498)\n",
      "19278 Training Loss: tensor(0.3553)\n",
      "19279 Training Loss: tensor(0.3515)\n",
      "19280 Training Loss: tensor(0.3603)\n",
      "19281 Training Loss: tensor(0.3559)\n",
      "19282 Training Loss: tensor(0.3504)\n",
      "19283 Training Loss: tensor(0.3534)\n",
      "19284 Training Loss: tensor(0.3518)\n",
      "19285 Training Loss: tensor(0.3586)\n",
      "19286 Training Loss: tensor(0.3510)\n",
      "19287 Training Loss: tensor(0.3521)\n",
      "19288 Training Loss: tensor(0.3506)\n",
      "19289 Training Loss: tensor(0.3507)\n",
      "19290 Training Loss: tensor(0.3509)\n",
      "19291 Training Loss: tensor(0.3536)\n",
      "19292 Training Loss: tensor(0.3536)\n",
      "19293 Training Loss: tensor(0.3524)\n",
      "19294 Training Loss: tensor(0.3504)\n",
      "19295 Training Loss: tensor(0.3583)\n",
      "19296 Training Loss: tensor(0.3513)\n",
      "19297 Training Loss: tensor(0.3511)\n",
      "19298 Training Loss: tensor(0.3534)\n",
      "19299 Training Loss: tensor(0.3506)\n",
      "19300 Training Loss: tensor(0.3499)\n",
      "19301 Training Loss: tensor(0.3502)\n",
      "19302 Training Loss: tensor(0.3524)\n",
      "19303 Training Loss: tensor(0.3580)\n",
      "19304 Training Loss: tensor(0.3545)\n",
      "19305 Training Loss: tensor(0.3508)\n",
      "19306 Training Loss: tensor(0.3532)\n",
      "19307 Training Loss: tensor(0.3503)\n",
      "19308 Training Loss: tensor(0.3512)\n",
      "19309 Training Loss: tensor(0.3515)\n",
      "19310 Training Loss: tensor(0.3532)\n",
      "19311 Training Loss: tensor(0.3545)\n",
      "19312 Training Loss: tensor(0.3550)\n",
      "19313 Training Loss: tensor(0.3554)\n",
      "19314 Training Loss: tensor(0.3493)\n",
      "19315 Training Loss: tensor(0.3503)\n",
      "19316 Training Loss: tensor(0.3615)\n",
      "19317 Training Loss: tensor(0.3499)\n",
      "19318 Training Loss: tensor(0.3514)\n",
      "19319 Training Loss: tensor(0.3532)\n",
      "19320 Training Loss: tensor(0.3533)\n",
      "19321 Training Loss: tensor(0.3529)\n",
      "19322 Training Loss: tensor(0.3516)\n",
      "19323 Training Loss: tensor(0.3505)\n",
      "19324 Training Loss: tensor(0.3521)\n",
      "19325 Training Loss: tensor(0.3538)\n",
      "19326 Training Loss: tensor(0.3504)\n",
      "19327 Training Loss: tensor(0.3567)\n",
      "19328 Training Loss: tensor(0.3551)\n",
      "19329 Training Loss: tensor(0.3519)\n",
      "19330 Training Loss: tensor(0.3525)\n",
      "19331 Training Loss: tensor(0.3520)\n",
      "19332 Training Loss: tensor(0.3509)\n",
      "19333 Training Loss: tensor(0.3576)\n",
      "19334 Training Loss: tensor(0.3511)\n",
      "19335 Training Loss: tensor(0.3517)\n",
      "19336 Training Loss: tensor(0.3518)\n",
      "19337 Training Loss: tensor(0.3519)\n",
      "19338 Training Loss: tensor(0.3500)\n",
      "19339 Training Loss: tensor(0.3509)\n",
      "19340 Training Loss: tensor(0.3505)\n",
      "19341 Training Loss: tensor(0.3564)\n",
      "19342 Training Loss: tensor(0.3583)\n",
      "19343 Training Loss: tensor(0.3549)\n",
      "19344 Training Loss: tensor(0.3514)\n",
      "19345 Training Loss: tensor(0.3522)\n",
      "19346 Training Loss: tensor(0.3597)\n",
      "19347 Training Loss: tensor(0.3599)\n",
      "19348 Training Loss: tensor(0.3562)\n",
      "19349 Training Loss: tensor(0.3540)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19350 Training Loss: tensor(0.3525)\n",
      "19351 Training Loss: tensor(0.3544)\n",
      "19352 Training Loss: tensor(0.3534)\n",
      "19353 Training Loss: tensor(0.3523)\n",
      "19354 Training Loss: tensor(0.3552)\n",
      "19355 Training Loss: tensor(0.3516)\n",
      "19356 Training Loss: tensor(0.3518)\n",
      "19357 Training Loss: tensor(0.3525)\n",
      "19358 Training Loss: tensor(0.3522)\n",
      "19359 Training Loss: tensor(0.3513)\n",
      "19360 Training Loss: tensor(0.3515)\n",
      "19361 Training Loss: tensor(0.3521)\n",
      "19362 Training Loss: tensor(0.3516)\n",
      "19363 Training Loss: tensor(0.3518)\n",
      "19364 Training Loss: tensor(0.3519)\n",
      "19365 Training Loss: tensor(0.3521)\n",
      "19366 Training Loss: tensor(0.3500)\n",
      "19367 Training Loss: tensor(0.3540)\n",
      "19368 Training Loss: tensor(0.3496)\n",
      "19369 Training Loss: tensor(0.3510)\n",
      "19370 Training Loss: tensor(0.3599)\n",
      "19371 Training Loss: tensor(0.3548)\n",
      "19372 Training Loss: tensor(0.3543)\n",
      "19373 Training Loss: tensor(0.3546)\n",
      "19374 Training Loss: tensor(0.3554)\n",
      "19375 Training Loss: tensor(0.3498)\n",
      "19376 Training Loss: tensor(0.3581)\n",
      "19377 Training Loss: tensor(0.3570)\n",
      "19378 Training Loss: tensor(0.3503)\n",
      "19379 Training Loss: tensor(0.3515)\n",
      "19380 Training Loss: tensor(0.3553)\n",
      "19381 Training Loss: tensor(0.3540)\n",
      "19382 Training Loss: tensor(0.3545)\n",
      "19383 Training Loss: tensor(0.3506)\n",
      "19384 Training Loss: tensor(0.3533)\n",
      "19385 Training Loss: tensor(0.3529)\n",
      "19386 Training Loss: tensor(0.3508)\n",
      "19387 Training Loss: tensor(0.3514)\n",
      "19388 Training Loss: tensor(0.3525)\n",
      "19389 Training Loss: tensor(0.3559)\n",
      "19390 Training Loss: tensor(0.3513)\n",
      "19391 Training Loss: tensor(0.3504)\n",
      "19392 Training Loss: tensor(0.3507)\n",
      "19393 Training Loss: tensor(0.3550)\n",
      "19394 Training Loss: tensor(0.3526)\n",
      "19395 Training Loss: tensor(0.3511)\n",
      "19396 Training Loss: tensor(0.3517)\n",
      "19397 Training Loss: tensor(0.3572)\n",
      "19398 Training Loss: tensor(0.3597)\n",
      "19399 Training Loss: tensor(0.3511)\n",
      "19400 Training Loss: tensor(0.3504)\n",
      "19401 Training Loss: tensor(0.3523)\n",
      "19402 Training Loss: tensor(0.3595)\n",
      "19403 Training Loss: tensor(0.3522)\n",
      "19404 Training Loss: tensor(0.3509)\n",
      "19405 Training Loss: tensor(0.3511)\n",
      "19406 Training Loss: tensor(0.3516)\n",
      "19407 Training Loss: tensor(0.3582)\n",
      "19408 Training Loss: tensor(0.3536)\n",
      "19409 Training Loss: tensor(0.3552)\n",
      "19410 Training Loss: tensor(0.3553)\n",
      "19411 Training Loss: tensor(0.3505)\n",
      "19412 Training Loss: tensor(0.3521)\n",
      "19413 Training Loss: tensor(0.3576)\n",
      "19414 Training Loss: tensor(0.3508)\n",
      "19415 Training Loss: tensor(0.3546)\n",
      "19416 Training Loss: tensor(0.3541)\n",
      "19417 Training Loss: tensor(0.3532)\n",
      "19418 Training Loss: tensor(0.3508)\n",
      "19419 Training Loss: tensor(0.3513)\n",
      "19420 Training Loss: tensor(0.3518)\n",
      "19421 Training Loss: tensor(0.3513)\n",
      "19422 Training Loss: tensor(0.3502)\n",
      "19423 Training Loss: tensor(0.3520)\n",
      "19424 Training Loss: tensor(0.3521)\n",
      "19425 Training Loss: tensor(0.3499)\n",
      "19426 Training Loss: tensor(0.3529)\n",
      "19427 Training Loss: tensor(0.3528)\n",
      "19428 Training Loss: tensor(0.3499)\n",
      "19429 Training Loss: tensor(0.3528)\n",
      "19430 Training Loss: tensor(0.3527)\n",
      "19431 Training Loss: tensor(0.3502)\n",
      "19432 Training Loss: tensor(0.3539)\n",
      "19433 Training Loss: tensor(0.3549)\n",
      "19434 Training Loss: tensor(0.3574)\n",
      "19435 Training Loss: tensor(0.3515)\n",
      "19436 Training Loss: tensor(0.3521)\n",
      "19437 Training Loss: tensor(0.3512)\n",
      "19438 Training Loss: tensor(0.3549)\n",
      "19439 Training Loss: tensor(0.3526)\n",
      "19440 Training Loss: tensor(0.3512)\n",
      "19441 Training Loss: tensor(0.3506)\n",
      "19442 Training Loss: tensor(0.3551)\n",
      "19443 Training Loss: tensor(0.3519)\n",
      "19444 Training Loss: tensor(0.3547)\n",
      "19445 Training Loss: tensor(0.3510)\n",
      "19446 Training Loss: tensor(0.3508)\n",
      "19447 Training Loss: tensor(0.3526)\n",
      "19448 Training Loss: tensor(0.3529)\n",
      "19449 Training Loss: tensor(0.3496)\n",
      "19450 Training Loss: tensor(0.3504)\n",
      "19451 Training Loss: tensor(0.3525)\n",
      "19452 Training Loss: tensor(0.3579)\n",
      "19453 Training Loss: tensor(0.3502)\n",
      "19454 Training Loss: tensor(0.3510)\n",
      "19455 Training Loss: tensor(0.3547)\n",
      "19456 Training Loss: tensor(0.3507)\n",
      "19457 Training Loss: tensor(0.3498)\n",
      "19458 Training Loss: tensor(0.3577)\n",
      "19459 Training Loss: tensor(0.3511)\n",
      "19460 Training Loss: tensor(0.3524)\n",
      "19461 Training Loss: tensor(0.3527)\n",
      "19462 Training Loss: tensor(0.3518)\n",
      "19463 Training Loss: tensor(0.3550)\n",
      "19464 Training Loss: tensor(0.3529)\n",
      "19465 Training Loss: tensor(0.3564)\n",
      "19466 Training Loss: tensor(0.3516)\n",
      "19467 Training Loss: tensor(0.3507)\n",
      "19468 Training Loss: tensor(0.3527)\n",
      "19469 Training Loss: tensor(0.3505)\n",
      "19470 Training Loss: tensor(0.3544)\n",
      "19471 Training Loss: tensor(0.3502)\n",
      "19472 Training Loss: tensor(0.3550)\n",
      "19473 Training Loss: tensor(0.3521)\n",
      "19474 Training Loss: tensor(0.3542)\n",
      "19475 Training Loss: tensor(0.3545)\n",
      "19476 Training Loss: tensor(0.3569)\n",
      "19477 Training Loss: tensor(0.3504)\n",
      "19478 Training Loss: tensor(0.3508)\n",
      "19479 Training Loss: tensor(0.3514)\n",
      "19480 Training Loss: tensor(0.3556)\n",
      "19481 Training Loss: tensor(0.3556)\n",
      "19482 Training Loss: tensor(0.3541)\n",
      "19483 Training Loss: tensor(0.3522)\n",
      "19484 Training Loss: tensor(0.3521)\n",
      "19485 Training Loss: tensor(0.3514)\n",
      "19486 Training Loss: tensor(0.3561)\n",
      "19487 Training Loss: tensor(0.3504)\n",
      "19488 Training Loss: tensor(0.3535)\n",
      "19489 Training Loss: tensor(0.3521)\n",
      "19490 Training Loss: tensor(0.3542)\n",
      "19491 Training Loss: tensor(0.3546)\n",
      "19492 Training Loss: tensor(0.3529)\n",
      "19493 Training Loss: tensor(0.3515)\n",
      "19494 Training Loss: tensor(0.3564)\n",
      "19495 Training Loss: tensor(0.3525)\n",
      "19496 Training Loss: tensor(0.3534)\n",
      "19497 Training Loss: tensor(0.3503)\n",
      "19498 Training Loss: tensor(0.3520)\n",
      "19499 Training Loss: tensor(0.3572)\n",
      "19500 Training Loss: tensor(0.3517)\n",
      "19501 Training Loss: tensor(0.3540)\n",
      "19502 Training Loss: tensor(0.3509)\n",
      "19503 Training Loss: tensor(0.3522)\n",
      "19504 Training Loss: tensor(0.3510)\n",
      "19505 Training Loss: tensor(0.3534)\n",
      "19506 Training Loss: tensor(0.3502)\n",
      "19507 Training Loss: tensor(0.3566)\n",
      "19508 Training Loss: tensor(0.3511)\n",
      "19509 Training Loss: tensor(0.3559)\n",
      "19510 Training Loss: tensor(0.3501)\n",
      "19511 Training Loss: tensor(0.3503)\n",
      "19512 Training Loss: tensor(0.3537)\n",
      "19513 Training Loss: tensor(0.3512)\n",
      "19514 Training Loss: tensor(0.3554)\n",
      "19515 Training Loss: tensor(0.3564)\n",
      "19516 Training Loss: tensor(0.3502)\n",
      "19517 Training Loss: tensor(0.3594)\n",
      "19518 Training Loss: tensor(0.3526)\n",
      "19519 Training Loss: tensor(0.3504)\n",
      "19520 Training Loss: tensor(0.3537)\n",
      "19521 Training Loss: tensor(0.3506)\n",
      "19522 Training Loss: tensor(0.3516)\n",
      "19523 Training Loss: tensor(0.3520)\n",
      "19524 Training Loss: tensor(0.3520)\n",
      "19525 Training Loss: tensor(0.3525)\n",
      "19526 Training Loss: tensor(0.3524)\n",
      "19527 Training Loss: tensor(0.3519)\n",
      "19528 Training Loss: tensor(0.3566)\n",
      "19529 Training Loss: tensor(0.3507)\n",
      "19530 Training Loss: tensor(0.3563)\n",
      "19531 Training Loss: tensor(0.3610)\n",
      "19532 Training Loss: tensor(0.3516)\n",
      "19533 Training Loss: tensor(0.3506)\n",
      "19534 Training Loss: tensor(0.3556)\n",
      "19535 Training Loss: tensor(0.3547)\n",
      "19536 Training Loss: tensor(0.3530)\n",
      "19537 Training Loss: tensor(0.3506)\n",
      "19538 Training Loss: tensor(0.3513)\n",
      "19539 Training Loss: tensor(0.3561)\n",
      "19540 Training Loss: tensor(0.3523)\n",
      "19541 Training Loss: tensor(0.3534)\n",
      "19542 Training Loss: tensor(0.3528)\n",
      "19543 Training Loss: tensor(0.3533)\n",
      "19544 Training Loss: tensor(0.3554)\n",
      "19545 Training Loss: tensor(0.3545)\n",
      "19546 Training Loss: tensor(0.3504)\n",
      "19547 Training Loss: tensor(0.3528)\n",
      "19548 Training Loss: tensor(0.3564)\n",
      "19549 Training Loss: tensor(0.3513)\n",
      "19550 Training Loss: tensor(0.3508)\n",
      "19551 Training Loss: tensor(0.3507)\n",
      "19552 Training Loss: tensor(0.3516)\n",
      "19553 Training Loss: tensor(0.3520)\n",
      "19554 Training Loss: tensor(0.3523)\n",
      "19555 Training Loss: tensor(0.3539)\n",
      "19556 Training Loss: tensor(0.3577)\n",
      "19557 Training Loss: tensor(0.3516)\n",
      "19558 Training Loss: tensor(0.3513)\n",
      "19559 Training Loss: tensor(0.3562)\n",
      "19560 Training Loss: tensor(0.3513)\n",
      "19561 Training Loss: tensor(0.3518)\n",
      "19562 Training Loss: tensor(0.3506)\n",
      "19563 Training Loss: tensor(0.3496)\n",
      "19564 Training Loss: tensor(0.3521)\n",
      "19565 Training Loss: tensor(0.3518)\n",
      "19566 Training Loss: tensor(0.3554)\n",
      "19567 Training Loss: tensor(0.3533)\n",
      "19568 Training Loss: tensor(0.3510)\n",
      "19569 Training Loss: tensor(0.3511)\n",
      "19570 Training Loss: tensor(0.3505)\n",
      "19571 Training Loss: tensor(0.3534)\n",
      "19572 Training Loss: tensor(0.3509)\n",
      "19573 Training Loss: tensor(0.3507)\n",
      "19574 Training Loss: tensor(0.3524)\n",
      "19575 Training Loss: tensor(0.3507)\n",
      "19576 Training Loss: tensor(0.3520)\n",
      "19577 Training Loss: tensor(0.3531)\n",
      "19578 Training Loss: tensor(0.3553)\n",
      "19579 Training Loss: tensor(0.3538)\n",
      "19580 Training Loss: tensor(0.3530)\n",
      "19581 Training Loss: tensor(0.3521)\n",
      "19582 Training Loss: tensor(0.3563)\n",
      "19583 Training Loss: tensor(0.3517)\n",
      "19584 Training Loss: tensor(0.3505)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19585 Training Loss: tensor(0.3517)\n",
      "19586 Training Loss: tensor(0.3554)\n",
      "19587 Training Loss: tensor(0.3507)\n",
      "19588 Training Loss: tensor(0.3523)\n",
      "19589 Training Loss: tensor(0.3520)\n",
      "19590 Training Loss: tensor(0.3515)\n",
      "19591 Training Loss: tensor(0.3524)\n",
      "19592 Training Loss: tensor(0.3509)\n",
      "19593 Training Loss: tensor(0.3562)\n",
      "19594 Training Loss: tensor(0.3530)\n",
      "19595 Training Loss: tensor(0.3588)\n",
      "19596 Training Loss: tensor(0.3506)\n",
      "19597 Training Loss: tensor(0.3614)\n",
      "19598 Training Loss: tensor(0.3558)\n",
      "19599 Training Loss: tensor(0.3538)\n",
      "19600 Training Loss: tensor(0.3502)\n",
      "19601 Training Loss: tensor(0.3520)\n",
      "19602 Training Loss: tensor(0.3500)\n",
      "19603 Training Loss: tensor(0.3514)\n",
      "19604 Training Loss: tensor(0.3511)\n",
      "19605 Training Loss: tensor(0.3510)\n",
      "19606 Training Loss: tensor(0.3552)\n",
      "19607 Training Loss: tensor(0.3504)\n",
      "19608 Training Loss: tensor(0.3507)\n",
      "19609 Training Loss: tensor(0.3499)\n",
      "19610 Training Loss: tensor(0.3525)\n",
      "19611 Training Loss: tensor(0.3503)\n",
      "19612 Training Loss: tensor(0.3499)\n",
      "19613 Training Loss: tensor(0.3505)\n",
      "19614 Training Loss: tensor(0.3518)\n",
      "19615 Training Loss: tensor(0.3497)\n",
      "19616 Training Loss: tensor(0.3513)\n",
      "19617 Training Loss: tensor(0.3707)\n",
      "19618 Training Loss: tensor(0.3547)\n",
      "19619 Training Loss: tensor(0.3502)\n",
      "19620 Training Loss: tensor(0.3607)\n",
      "19621 Training Loss: tensor(0.3564)\n",
      "19622 Training Loss: tensor(0.3556)\n",
      "19623 Training Loss: tensor(0.3535)\n",
      "19624 Training Loss: tensor(0.3523)\n",
      "19625 Training Loss: tensor(0.3511)\n",
      "19626 Training Loss: tensor(0.3571)\n",
      "19627 Training Loss: tensor(0.3535)\n",
      "19628 Training Loss: tensor(0.3582)\n",
      "19629 Training Loss: tensor(0.3505)\n",
      "19630 Training Loss: tensor(0.3514)\n",
      "19631 Training Loss: tensor(0.3514)\n",
      "19632 Training Loss: tensor(0.3527)\n",
      "19633 Training Loss: tensor(0.3518)\n",
      "19634 Training Loss: tensor(0.3530)\n",
      "19635 Training Loss: tensor(0.3521)\n",
      "19636 Training Loss: tensor(0.3518)\n",
      "19637 Training Loss: tensor(0.3509)\n",
      "19638 Training Loss: tensor(0.3506)\n",
      "19639 Training Loss: tensor(0.3502)\n",
      "19640 Training Loss: tensor(0.3518)\n",
      "19641 Training Loss: tensor(0.3507)\n",
      "19642 Training Loss: tensor(0.3600)\n",
      "19643 Training Loss: tensor(0.3496)\n",
      "19644 Training Loss: tensor(0.3504)\n",
      "19645 Training Loss: tensor(0.3524)\n",
      "19646 Training Loss: tensor(0.3537)\n",
      "19647 Training Loss: tensor(0.3507)\n",
      "19648 Training Loss: tensor(0.3543)\n",
      "19649 Training Loss: tensor(0.3509)\n",
      "19650 Training Loss: tensor(0.3512)\n",
      "19651 Training Loss: tensor(0.3500)\n",
      "19652 Training Loss: tensor(0.3502)\n",
      "19653 Training Loss: tensor(0.3493)\n",
      "19654 Training Loss: tensor(0.3536)\n",
      "19655 Training Loss: tensor(0.3499)\n",
      "19656 Training Loss: tensor(0.3499)\n",
      "19657 Training Loss: tensor(0.3527)\n",
      "19658 Training Loss: tensor(0.3507)\n",
      "19659 Training Loss: tensor(0.3589)\n",
      "19660 Training Loss: tensor(0.3491)\n",
      "19661 Training Loss: tensor(0.3558)\n",
      "19662 Training Loss: tensor(0.3558)\n",
      "19663 Training Loss: tensor(0.3552)\n",
      "19664 Training Loss: tensor(0.3497)\n",
      "19665 Training Loss: tensor(0.3526)\n",
      "19666 Training Loss: tensor(0.3528)\n",
      "19667 Training Loss: tensor(0.3515)\n",
      "19668 Training Loss: tensor(0.3546)\n",
      "19669 Training Loss: tensor(0.3496)\n",
      "19670 Training Loss: tensor(0.3540)\n",
      "19671 Training Loss: tensor(0.3510)\n",
      "19672 Training Loss: tensor(0.3528)\n",
      "19673 Training Loss: tensor(0.3524)\n",
      "19674 Training Loss: tensor(0.3596)\n",
      "19675 Training Loss: tensor(0.3505)\n",
      "19676 Training Loss: tensor(0.3526)\n",
      "19677 Training Loss: tensor(0.3587)\n",
      "19678 Training Loss: tensor(0.3509)\n",
      "19679 Training Loss: tensor(0.3515)\n",
      "19680 Training Loss: tensor(0.3554)\n",
      "19681 Training Loss: tensor(0.3508)\n",
      "19682 Training Loss: tensor(0.3547)\n",
      "19683 Training Loss: tensor(0.3527)\n",
      "19684 Training Loss: tensor(0.3548)\n",
      "19685 Training Loss: tensor(0.3538)\n",
      "19686 Training Loss: tensor(0.3501)\n",
      "19687 Training Loss: tensor(0.3514)\n",
      "19688 Training Loss: tensor(0.3505)\n",
      "19689 Training Loss: tensor(0.3541)\n",
      "19690 Training Loss: tensor(0.3511)\n",
      "19691 Training Loss: tensor(0.3558)\n",
      "19692 Training Loss: tensor(0.3562)\n",
      "19693 Training Loss: tensor(0.3521)\n",
      "19694 Training Loss: tensor(0.3507)\n",
      "19695 Training Loss: tensor(0.3515)\n",
      "19696 Training Loss: tensor(0.3505)\n",
      "19697 Training Loss: tensor(0.3507)\n",
      "19698 Training Loss: tensor(0.3515)\n",
      "19699 Training Loss: tensor(0.3514)\n",
      "19700 Training Loss: tensor(0.3510)\n",
      "19701 Training Loss: tensor(0.3507)\n",
      "19702 Training Loss: tensor(0.3541)\n",
      "19703 Training Loss: tensor(0.3507)\n",
      "19704 Training Loss: tensor(0.3546)\n",
      "19705 Training Loss: tensor(0.3529)\n",
      "19706 Training Loss: tensor(0.3517)\n",
      "19707 Training Loss: tensor(0.3521)\n",
      "19708 Training Loss: tensor(0.3567)\n",
      "19709 Training Loss: tensor(0.3498)\n",
      "19710 Training Loss: tensor(0.3509)\n",
      "19711 Training Loss: tensor(0.3514)\n",
      "19712 Training Loss: tensor(0.3498)\n",
      "19713 Training Loss: tensor(0.3597)\n",
      "19714 Training Loss: tensor(0.3540)\n",
      "19715 Training Loss: tensor(0.3534)\n",
      "19716 Training Loss: tensor(0.3596)\n",
      "19717 Training Loss: tensor(0.3555)\n",
      "19718 Training Loss: tensor(0.3501)\n",
      "19719 Training Loss: tensor(0.3504)\n",
      "19720 Training Loss: tensor(0.3506)\n",
      "19721 Training Loss: tensor(0.3519)\n",
      "19722 Training Loss: tensor(0.3511)\n",
      "19723 Training Loss: tensor(0.3518)\n",
      "19724 Training Loss: tensor(0.3547)\n",
      "19725 Training Loss: tensor(0.3559)\n",
      "19726 Training Loss: tensor(0.3506)\n",
      "19727 Training Loss: tensor(0.3517)\n",
      "19728 Training Loss: tensor(0.3535)\n",
      "19729 Training Loss: tensor(0.3532)\n",
      "19730 Training Loss: tensor(0.3513)\n",
      "19731 Training Loss: tensor(0.3506)\n",
      "19732 Training Loss: tensor(0.3547)\n",
      "19733 Training Loss: tensor(0.3518)\n",
      "19734 Training Loss: tensor(0.3501)\n",
      "19735 Training Loss: tensor(0.3502)\n",
      "19736 Training Loss: tensor(0.3511)\n",
      "19737 Training Loss: tensor(0.3516)\n",
      "19738 Training Loss: tensor(0.3561)\n",
      "19739 Training Loss: tensor(0.3502)\n",
      "19740 Training Loss: tensor(0.3497)\n",
      "19741 Training Loss: tensor(0.3507)\n",
      "19742 Training Loss: tensor(0.3509)\n",
      "19743 Training Loss: tensor(0.3499)\n",
      "19744 Training Loss: tensor(0.3601)\n",
      "19745 Training Loss: tensor(0.3568)\n",
      "19746 Training Loss: tensor(0.3555)\n",
      "19747 Training Loss: tensor(0.3510)\n",
      "19748 Training Loss: tensor(0.3512)\n",
      "19749 Training Loss: tensor(0.3558)\n",
      "19750 Training Loss: tensor(0.3504)\n",
      "19751 Training Loss: tensor(0.3508)\n",
      "19752 Training Loss: tensor(0.3503)\n",
      "19753 Training Loss: tensor(0.3524)\n",
      "19754 Training Loss: tensor(0.3538)\n",
      "19755 Training Loss: tensor(0.3555)\n",
      "19756 Training Loss: tensor(0.3536)\n",
      "19757 Training Loss: tensor(0.3515)\n",
      "19758 Training Loss: tensor(0.3535)\n",
      "19759 Training Loss: tensor(0.3506)\n",
      "19760 Training Loss: tensor(0.3500)\n",
      "19761 Training Loss: tensor(0.3517)\n",
      "19762 Training Loss: tensor(0.3505)\n",
      "19763 Training Loss: tensor(0.3501)\n",
      "19764 Training Loss: tensor(0.3521)\n",
      "19765 Training Loss: tensor(0.3499)\n",
      "19766 Training Loss: tensor(0.3494)\n",
      "19767 Training Loss: tensor(0.3540)\n",
      "19768 Training Loss: tensor(0.3519)\n",
      "19769 Training Loss: tensor(0.3532)\n",
      "19770 Training Loss: tensor(0.3498)\n",
      "19771 Training Loss: tensor(0.3520)\n",
      "19772 Training Loss: tensor(0.3525)\n",
      "19773 Training Loss: tensor(0.3558)\n",
      "19774 Training Loss: tensor(0.3495)\n",
      "19775 Training Loss: tensor(0.3546)\n",
      "19776 Training Loss: tensor(0.3538)\n",
      "19777 Training Loss: tensor(0.3511)\n",
      "19778 Training Loss: tensor(0.3513)\n",
      "19779 Training Loss: tensor(0.3598)\n",
      "19780 Training Loss: tensor(0.3510)\n",
      "19781 Training Loss: tensor(0.3501)\n",
      "19782 Training Loss: tensor(0.3550)\n",
      "19783 Training Loss: tensor(0.3495)\n",
      "19784 Training Loss: tensor(0.3495)\n",
      "19785 Training Loss: tensor(0.3511)\n",
      "19786 Training Loss: tensor(0.3553)\n",
      "19787 Training Loss: tensor(0.3514)\n",
      "19788 Training Loss: tensor(0.3512)\n",
      "19789 Training Loss: tensor(0.3509)\n",
      "19790 Training Loss: tensor(0.3504)\n",
      "19791 Training Loss: tensor(0.3518)\n",
      "19792 Training Loss: tensor(0.3522)\n",
      "19793 Training Loss: tensor(0.3503)\n",
      "19794 Training Loss: tensor(0.3516)\n",
      "19795 Training Loss: tensor(0.3498)\n",
      "19796 Training Loss: tensor(0.3499)\n",
      "19797 Training Loss: tensor(0.3582)\n",
      "19798 Training Loss: tensor(0.3538)\n",
      "19799 Training Loss: tensor(0.3515)\n",
      "19800 Training Loss: tensor(0.3508)\n",
      "19801 Training Loss: tensor(0.3507)\n",
      "19802 Training Loss: tensor(0.3496)\n",
      "19803 Training Loss: tensor(0.3513)\n",
      "19804 Training Loss: tensor(0.3509)\n",
      "19805 Training Loss: tensor(0.3490)\n",
      "19806 Training Loss: tensor(0.3566)\n",
      "19807 Training Loss: tensor(0.3534)\n",
      "19808 Training Loss: tensor(0.3490)\n",
      "19809 Training Loss: tensor(0.3552)\n",
      "19810 Training Loss: tensor(0.3532)\n",
      "19811 Training Loss: tensor(0.3496)\n",
      "19812 Training Loss: tensor(0.3485)\n",
      "19813 Training Loss: tensor(0.3491)\n",
      "19814 Training Loss: tensor(0.3490)\n",
      "19815 Training Loss: tensor(0.3515)\n",
      "19816 Training Loss: tensor(0.3584)\n",
      "19817 Training Loss: tensor(0.3571)\n",
      "19818 Training Loss: tensor(0.3527)\n",
      "19819 Training Loss: tensor(0.3510)\n",
      "19820 Training Loss: tensor(0.3505)\n",
      "19821 Training Loss: tensor(0.3545)\n",
      "19822 Training Loss: tensor(0.3549)\n",
      "19823 Training Loss: tensor(0.3496)\n",
      "19824 Training Loss: tensor(0.3509)\n",
      "19825 Training Loss: tensor(0.3525)\n",
      "19826 Training Loss: tensor(0.3519)\n",
      "19827 Training Loss: tensor(0.3511)\n",
      "19828 Training Loss: tensor(0.3568)\n",
      "19829 Training Loss: tensor(0.3496)\n",
      "19830 Training Loss: tensor(0.3584)\n",
      "19831 Training Loss: tensor(0.3490)\n",
      "19832 Training Loss: tensor(0.3502)\n",
      "19833 Training Loss: tensor(0.3504)\n",
      "19834 Training Loss: tensor(0.3575)\n",
      "19835 Training Loss: tensor(0.3501)\n",
      "19836 Training Loss: tensor(0.3503)\n",
      "19837 Training Loss: tensor(0.3507)\n",
      "19838 Training Loss: tensor(0.3540)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19839 Training Loss: tensor(0.3505)\n",
      "19840 Training Loss: tensor(0.3497)\n",
      "19841 Training Loss: tensor(0.3515)\n",
      "19842 Training Loss: tensor(0.3583)\n",
      "19843 Training Loss: tensor(0.3525)\n",
      "19844 Training Loss: tensor(0.3493)\n",
      "19845 Training Loss: tensor(0.3517)\n",
      "19846 Training Loss: tensor(0.3600)\n",
      "19847 Training Loss: tensor(0.3505)\n",
      "19848 Training Loss: tensor(0.3604)\n",
      "19849 Training Loss: tensor(0.3527)\n",
      "19850 Training Loss: tensor(0.3507)\n",
      "19851 Training Loss: tensor(0.3495)\n",
      "19852 Training Loss: tensor(0.3529)\n",
      "19853 Training Loss: tensor(0.3542)\n",
      "19854 Training Loss: tensor(0.3515)\n",
      "19855 Training Loss: tensor(0.3494)\n",
      "19856 Training Loss: tensor(0.3505)\n",
      "19857 Training Loss: tensor(0.3544)\n",
      "19858 Training Loss: tensor(0.3516)\n",
      "19859 Training Loss: tensor(0.3517)\n",
      "19860 Training Loss: tensor(0.3514)\n",
      "19861 Training Loss: tensor(0.3529)\n",
      "19862 Training Loss: tensor(0.3503)\n",
      "19863 Training Loss: tensor(0.3549)\n",
      "19864 Training Loss: tensor(0.3511)\n",
      "19865 Training Loss: tensor(0.3546)\n",
      "19866 Training Loss: tensor(0.3500)\n",
      "19867 Training Loss: tensor(0.3494)\n",
      "19868 Training Loss: tensor(0.3552)\n",
      "19869 Training Loss: tensor(0.3497)\n",
      "19870 Training Loss: tensor(0.3498)\n",
      "19871 Training Loss: tensor(0.3547)\n",
      "19872 Training Loss: tensor(0.3503)\n",
      "19873 Training Loss: tensor(0.3562)\n",
      "19874 Training Loss: tensor(0.3503)\n",
      "19875 Training Loss: tensor(0.3490)\n",
      "19876 Training Loss: tensor(0.3525)\n",
      "19877 Training Loss: tensor(0.3581)\n",
      "19878 Training Loss: tensor(0.3547)\n",
      "19879 Training Loss: tensor(0.3544)\n",
      "19880 Training Loss: tensor(0.3545)\n",
      "19881 Training Loss: tensor(0.3526)\n",
      "19882 Training Loss: tensor(0.3525)\n",
      "19883 Training Loss: tensor(0.3582)\n",
      "19884 Training Loss: tensor(0.3539)\n",
      "19885 Training Loss: tensor(0.3506)\n",
      "19886 Training Loss: tensor(0.3514)\n",
      "19887 Training Loss: tensor(0.3519)\n",
      "19888 Training Loss: tensor(0.3508)\n",
      "19889 Training Loss: tensor(0.3568)\n",
      "19890 Training Loss: tensor(0.3516)\n",
      "19891 Training Loss: tensor(0.3556)\n",
      "19892 Training Loss: tensor(0.3521)\n",
      "19893 Training Loss: tensor(0.3525)\n",
      "19894 Training Loss: tensor(0.3510)\n",
      "19895 Training Loss: tensor(0.3515)\n",
      "19896 Training Loss: tensor(0.3571)\n",
      "19897 Training Loss: tensor(0.3506)\n",
      "19898 Training Loss: tensor(0.3533)\n",
      "19899 Training Loss: tensor(0.3590)\n",
      "19900 Training Loss: tensor(0.3528)\n",
      "19901 Training Loss: tensor(0.3505)\n",
      "19902 Training Loss: tensor(0.3517)\n",
      "19903 Training Loss: tensor(0.3525)\n",
      "19904 Training Loss: tensor(0.3530)\n",
      "19905 Training Loss: tensor(0.3531)\n",
      "19906 Training Loss: tensor(0.3500)\n",
      "19907 Training Loss: tensor(0.3530)\n",
      "19908 Training Loss: tensor(0.3501)\n",
      "19909 Training Loss: tensor(0.3497)\n",
      "19910 Training Loss: tensor(0.3566)\n",
      "19911 Training Loss: tensor(0.3501)\n",
      "19912 Training Loss: tensor(0.3519)\n",
      "19913 Training Loss: tensor(0.3500)\n",
      "19914 Training Loss: tensor(0.3531)\n",
      "19915 Training Loss: tensor(0.3496)\n",
      "19916 Training Loss: tensor(0.3536)\n",
      "19917 Training Loss: tensor(0.3500)\n",
      "19918 Training Loss: tensor(0.3524)\n",
      "19919 Training Loss: tensor(0.3558)\n",
      "19920 Training Loss: tensor(0.3611)\n",
      "19921 Training Loss: tensor(0.3533)\n",
      "19922 Training Loss: tensor(0.3523)\n",
      "19923 Training Loss: tensor(0.3531)\n",
      "19924 Training Loss: tensor(0.3508)\n",
      "19925 Training Loss: tensor(0.3514)\n",
      "19926 Training Loss: tensor(0.3551)\n",
      "19927 Training Loss: tensor(0.3514)\n",
      "19928 Training Loss: tensor(0.3515)\n",
      "19929 Training Loss: tensor(0.3515)\n",
      "19930 Training Loss: tensor(0.3517)\n",
      "19931 Training Loss: tensor(0.3547)\n",
      "19932 Training Loss: tensor(0.3544)\n",
      "19933 Training Loss: tensor(0.3533)\n",
      "19934 Training Loss: tensor(0.3586)\n",
      "19935 Training Loss: tensor(0.3516)\n",
      "19936 Training Loss: tensor(0.3563)\n",
      "19937 Training Loss: tensor(0.3498)\n",
      "19938 Training Loss: tensor(0.3551)\n",
      "19939 Training Loss: tensor(0.3503)\n",
      "19940 Training Loss: tensor(0.3556)\n",
      "19941 Training Loss: tensor(0.3568)\n",
      "19942 Training Loss: tensor(0.3503)\n",
      "19943 Training Loss: tensor(0.3501)\n",
      "19944 Training Loss: tensor(0.3512)\n",
      "19945 Training Loss: tensor(0.3498)\n",
      "19946 Training Loss: tensor(0.3495)\n",
      "19947 Training Loss: tensor(0.3550)\n",
      "19948 Training Loss: tensor(0.3517)\n",
      "19949 Training Loss: tensor(0.3498)\n",
      "19950 Training Loss: tensor(0.3596)\n",
      "19951 Training Loss: tensor(0.3578)\n",
      "19952 Training Loss: tensor(0.3506)\n",
      "19953 Training Loss: tensor(0.3501)\n",
      "19954 Training Loss: tensor(0.3557)\n",
      "19955 Training Loss: tensor(0.3505)\n",
      "19956 Training Loss: tensor(0.3500)\n",
      "19957 Training Loss: tensor(0.3512)\n",
      "19958 Training Loss: tensor(0.3525)\n",
      "19959 Training Loss: tensor(0.3516)\n",
      "19960 Training Loss: tensor(0.3546)\n",
      "19961 Training Loss: tensor(0.3504)\n",
      "19962 Training Loss: tensor(0.3501)\n",
      "19963 Training Loss: tensor(0.3496)\n",
      "19964 Training Loss: tensor(0.3527)\n",
      "19965 Training Loss: tensor(0.3499)\n",
      "19966 Training Loss: tensor(0.3505)\n",
      "19967 Training Loss: tensor(0.3522)\n",
      "19968 Training Loss: tensor(0.3512)\n",
      "19969 Training Loss: tensor(0.3507)\n",
      "19970 Training Loss: tensor(0.3537)\n",
      "19971 Training Loss: tensor(0.3536)\n",
      "19972 Training Loss: tensor(0.3537)\n",
      "19973 Training Loss: tensor(0.3532)\n",
      "19974 Training Loss: tensor(0.3522)\n",
      "19975 Training Loss: tensor(0.3515)\n",
      "19976 Training Loss: tensor(0.3591)\n",
      "19977 Training Loss: tensor(0.3503)\n",
      "19978 Training Loss: tensor(0.3497)\n",
      "19979 Training Loss: tensor(0.3637)\n",
      "19980 Training Loss: tensor(0.3546)\n",
      "19981 Training Loss: tensor(0.3516)\n",
      "19982 Training Loss: tensor(0.3520)\n",
      "19983 Training Loss: tensor(0.3515)\n",
      "19984 Training Loss: tensor(0.3553)\n",
      "19985 Training Loss: tensor(0.3520)\n",
      "19986 Training Loss: tensor(0.3556)\n",
      "19987 Training Loss: tensor(0.3514)\n",
      "19988 Training Loss: tensor(0.3512)\n",
      "19989 Training Loss: tensor(0.3534)\n",
      "19990 Training Loss: tensor(0.3512)\n",
      "19991 Training Loss: tensor(0.3509)\n",
      "19992 Training Loss: tensor(0.3505)\n",
      "19993 Training Loss: tensor(0.3542)\n",
      "19994 Training Loss: tensor(0.3501)\n",
      "19995 Training Loss: tensor(0.3503)\n",
      "19996 Training Loss: tensor(0.3551)\n",
      "19997 Training Loss: tensor(0.3524)\n",
      "19998 Training Loss: tensor(0.3534)\n",
      "19999 Training Loss: tensor(0.3557)\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of iterations\n",
    "num_epochs = 20000\n",
    "#prev_val_loss = 9999999.0 \n",
    "\n",
    "# Iterating through epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Resetting gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss calculation based on boundary conditions\n",
    "    input_x_bc = Variable(x_bc.float(), requires_grad=False).to(device)\n",
    "    #input_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "    input_t_bc = Variable(t_bc.float(), requires_grad=False).to(device)\n",
    "    #input_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    \n",
    "    target_u_bc = Variable(u_bc.float(), requires_grad=False).to(device)\n",
    "    #target_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "    target_u_initial1 = Variable(u_initial1.float(), requires_grad=False).to(device)\n",
    "    target_u_initial2 = Variable(u_initial2.float(), requires_grad=False).to(device)\n",
    "\n",
    "    # Getting network output for boundary condition and initial conditions\n",
    "    output_bc = net(input_x_bc, input_t_bc)\n",
    "    output_initial1 = net(input_t_bc, target_u_initial1)\n",
    "    output_initial2 = net(input_t_bc, target_u_initial2)\n",
    "    \n",
    "    mse_u = mse_cost_function(output_bc, target_u_bc)\n",
    "    mse_initial1 = mse_cost_function(u_initial1, target_u_initial1)\n",
    "    mse_initial2 = mse_cost_function(u_initial2, target_u_initial2)\n",
    "    # Loss calculation based on partial differential equation (PDE) \n",
    "    collocation_x = np.random.uniform(low= -2.0, high=2.0, size=(500, 1)) \n",
    "    collocation_t = np.random.uniform(low=0.0, high=1.0, size=(500, 1))\n",
    "    all_zeros_target = np.zeros((500, 1))\n",
    "    \n",
    "    input_x_collocation = Variable(torch.from_numpy(collocation_x).float(), requires_grad=True).to(device)\n",
    "    input_t_collocation = Variable(torch.from_numpy(collocation_t).float(), requires_grad=True).to(device)\n",
    "    target_all_zeros = Variable(torch.from_numpy(all_zeros_target).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    # Getting network output for PDE\n",
    "    output_f = pde_loss(input_x_collocation, input_t_collocation, net)\n",
    "    mse_f = mse_cost_function(output_f, target_all_zeros)\n",
    "    \n",
    "    # Combining the loss functions\n",
    "    total_loss = mse_u + mse_f + mse_initial1 + mse_initial2\n",
    "    \n",
    "    # Backward propagation for computing gradients\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Optimizer step to update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch, \"Training Loss:\", total_loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "79be98e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the results\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Creating meshgrid for visualization\n",
    "x_visual = np.arange(-2.5 , 2.5, 0.002)\n",
    "t_visual = np.arange(0, 2, 0.002)\n",
    "ms_x_visual, ms_t_visual = np.meshgrid(x_visual, t_visual)\n",
    "## np.ravel returns a flattened and contiguous 1-D array containing the elements of the input.\n",
    "## A copy is created only if necessary.\n",
    "x_visual = np.ravel(ms_x_visual).reshape(-1, 1)\n",
    "t_visual = np.ravel(ms_t_visual).reshape(-1, 1)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "pt_x_visual = Variable(torch.from_numpy(x_visual).float(), requires_grad=True).to(device)\n",
    "pt_t_visual = Variable(torch.from_numpy(t_visual).float(), requires_grad=True).to(device)\n",
    "\n",
    "# Obtaining network predictions\n",
    "## In the function net(pt_x_visualization, pt_t_visualization), the training process updates the parameters represented by theta.\n",
    "pt_u_visual = net(pt_x_visual, pt_t_visual)\n",
    "u_visual = pt_u_visual.data.cpu().numpy()\n",
    "ms_u_visual = u_visual.reshape(ms_x_visual.shape)\n",
    "\n",
    "# Plotting the surface\n",
    "surf = ax.plot_surface(ms_x_visual, ms_t_visual, ms_u_visual, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.0003f'))\n",
    "ax.set_xlabel(\"x_axis\")\n",
    "ax.set_ylabel(\"t_axis\")\n",
    "\n",
    "\n",
    "# Adding colorbar for reference\n",
    "fig.colorbar(surf, shrink=1, aspect=5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e35f8b",
   "metadata": {},
   "source": [
    "### 3.6.1 Fitting model with training with iteration 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5f0cde5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Loss: tensor(0.3550)\n",
      "1 Training Loss: tensor(0.3537)\n",
      "2 Training Loss: tensor(0.3516)\n",
      "3 Training Loss: tensor(0.3548)\n",
      "4 Training Loss: tensor(0.3550)\n",
      "5 Training Loss: tensor(0.3535)\n",
      "6 Training Loss: tensor(0.3516)\n",
      "7 Training Loss: tensor(0.3562)\n",
      "8 Training Loss: tensor(0.3531)\n",
      "9 Training Loss: tensor(0.3503)\n",
      "10 Training Loss: tensor(0.3516)\n",
      "11 Training Loss: tensor(0.3567)\n",
      "12 Training Loss: tensor(0.3530)\n",
      "13 Training Loss: tensor(0.3510)\n",
      "14 Training Loss: tensor(0.3518)\n",
      "15 Training Loss: tensor(0.3501)\n",
      "16 Training Loss: tensor(0.3506)\n",
      "17 Training Loss: tensor(0.3589)\n",
      "18 Training Loss: tensor(0.3523)\n",
      "19 Training Loss: tensor(0.3507)\n",
      "20 Training Loss: tensor(0.3523)\n",
      "21 Training Loss: tensor(0.3550)\n",
      "22 Training Loss: tensor(0.3502)\n",
      "23 Training Loss: tensor(0.3514)\n",
      "24 Training Loss: tensor(0.3518)\n",
      "25 Training Loss: tensor(0.3537)\n",
      "26 Training Loss: tensor(0.3505)\n",
      "27 Training Loss: tensor(0.3552)\n",
      "28 Training Loss: tensor(0.3518)\n",
      "29 Training Loss: tensor(0.3493)\n",
      "30 Training Loss: tensor(0.3646)\n",
      "31 Training Loss: tensor(0.3509)\n",
      "32 Training Loss: tensor(0.3509)\n",
      "33 Training Loss: tensor(0.3502)\n",
      "34 Training Loss: tensor(0.3578)\n",
      "35 Training Loss: tensor(0.3509)\n",
      "36 Training Loss: tensor(0.3499)\n",
      "37 Training Loss: tensor(0.3500)\n",
      "38 Training Loss: tensor(0.3497)\n",
      "39 Training Loss: tensor(0.3527)\n",
      "40 Training Loss: tensor(0.3527)\n",
      "41 Training Loss: tensor(0.3533)\n",
      "42 Training Loss: tensor(0.3536)\n",
      "43 Training Loss: tensor(0.3496)\n",
      "44 Training Loss: tensor(0.3557)\n",
      "45 Training Loss: tensor(0.3500)\n",
      "46 Training Loss: tensor(0.3562)\n",
      "47 Training Loss: tensor(0.3549)\n",
      "48 Training Loss: tensor(0.3498)\n",
      "49 Training Loss: tensor(0.3510)\n",
      "50 Training Loss: tensor(0.3511)\n",
      "51 Training Loss: tensor(0.3520)\n",
      "52 Training Loss: tensor(0.3509)\n",
      "53 Training Loss: tensor(0.3602)\n",
      "54 Training Loss: tensor(0.3532)\n",
      "55 Training Loss: tensor(0.3513)\n",
      "56 Training Loss: tensor(0.3620)\n",
      "57 Training Loss: tensor(0.3528)\n",
      "58 Training Loss: tensor(0.3505)\n",
      "59 Training Loss: tensor(0.3590)\n",
      "60 Training Loss: tensor(0.3541)\n",
      "61 Training Loss: tensor(0.3547)\n",
      "62 Training Loss: tensor(0.3545)\n",
      "63 Training Loss: tensor(0.3529)\n",
      "64 Training Loss: tensor(0.3517)\n",
      "65 Training Loss: tensor(0.3535)\n",
      "66 Training Loss: tensor(0.3556)\n",
      "67 Training Loss: tensor(0.3513)\n",
      "68 Training Loss: tensor(0.3514)\n",
      "69 Training Loss: tensor(0.3504)\n",
      "70 Training Loss: tensor(0.3513)\n",
      "71 Training Loss: tensor(0.3519)\n",
      "72 Training Loss: tensor(0.3505)\n",
      "73 Training Loss: tensor(0.3577)\n",
      "74 Training Loss: tensor(0.3503)\n",
      "75 Training Loss: tensor(0.3551)\n",
      "76 Training Loss: tensor(0.3507)\n",
      "77 Training Loss: tensor(0.3494)\n",
      "78 Training Loss: tensor(0.3509)\n",
      "79 Training Loss: tensor(0.3504)\n",
      "80 Training Loss: tensor(0.3517)\n",
      "81 Training Loss: tensor(0.3507)\n",
      "82 Training Loss: tensor(0.3631)\n",
      "83 Training Loss: tensor(0.3589)\n",
      "84 Training Loss: tensor(0.3502)\n",
      "85 Training Loss: tensor(0.3496)\n",
      "86 Training Loss: tensor(0.3502)\n",
      "87 Training Loss: tensor(0.3591)\n",
      "88 Training Loss: tensor(0.3527)\n",
      "89 Training Loss: tensor(0.3540)\n",
      "90 Training Loss: tensor(0.3556)\n",
      "91 Training Loss: tensor(0.3534)\n",
      "92 Training Loss: tensor(0.3503)\n",
      "93 Training Loss: tensor(0.3505)\n",
      "94 Training Loss: tensor(0.3508)\n",
      "95 Training Loss: tensor(0.3502)\n",
      "96 Training Loss: tensor(0.3514)\n",
      "97 Training Loss: tensor(0.3522)\n",
      "98 Training Loss: tensor(0.3578)\n",
      "99 Training Loss: tensor(0.3511)\n",
      "100 Training Loss: tensor(0.3500)\n",
      "101 Training Loss: tensor(0.3530)\n",
      "102 Training Loss: tensor(0.3528)\n",
      "103 Training Loss: tensor(0.3570)\n",
      "104 Training Loss: tensor(0.3516)\n",
      "105 Training Loss: tensor(0.3515)\n",
      "106 Training Loss: tensor(0.3530)\n",
      "107 Training Loss: tensor(0.3520)\n",
      "108 Training Loss: tensor(0.3508)\n",
      "109 Training Loss: tensor(0.3534)\n",
      "110 Training Loss: tensor(0.3509)\n",
      "111 Training Loss: tensor(0.3540)\n",
      "112 Training Loss: tensor(0.3514)\n",
      "113 Training Loss: tensor(0.3499)\n",
      "114 Training Loss: tensor(0.3509)\n",
      "115 Training Loss: tensor(0.3518)\n",
      "116 Training Loss: tensor(0.3520)\n",
      "117 Training Loss: tensor(0.3545)\n",
      "118 Training Loss: tensor(0.3513)\n",
      "119 Training Loss: tensor(0.3532)\n",
      "120 Training Loss: tensor(0.3531)\n",
      "121 Training Loss: tensor(0.3553)\n",
      "122 Training Loss: tensor(0.3504)\n",
      "123 Training Loss: tensor(0.3591)\n",
      "124 Training Loss: tensor(0.3569)\n",
      "125 Training Loss: tensor(0.3522)\n",
      "126 Training Loss: tensor(0.3498)\n",
      "127 Training Loss: tensor(0.3520)\n",
      "128 Training Loss: tensor(0.3537)\n",
      "129 Training Loss: tensor(0.3501)\n",
      "130 Training Loss: tensor(0.3510)\n",
      "131 Training Loss: tensor(0.3512)\n",
      "132 Training Loss: tensor(0.3555)\n",
      "133 Training Loss: tensor(0.3555)\n",
      "134 Training Loss: tensor(0.3520)\n",
      "135 Training Loss: tensor(0.3502)\n",
      "136 Training Loss: tensor(0.3579)\n",
      "137 Training Loss: tensor(0.3527)\n",
      "138 Training Loss: tensor(0.3541)\n",
      "139 Training Loss: tensor(0.3506)\n",
      "140 Training Loss: tensor(0.3508)\n",
      "141 Training Loss: tensor(0.3504)\n",
      "142 Training Loss: tensor(0.3508)\n",
      "143 Training Loss: tensor(0.3523)\n",
      "144 Training Loss: tensor(0.3518)\n",
      "145 Training Loss: tensor(0.3501)\n",
      "146 Training Loss: tensor(0.3495)\n",
      "147 Training Loss: tensor(0.3517)\n",
      "148 Training Loss: tensor(0.3494)\n",
      "149 Training Loss: tensor(0.3511)\n",
      "150 Training Loss: tensor(0.3542)\n",
      "151 Training Loss: tensor(0.3530)\n",
      "152 Training Loss: tensor(0.3503)\n",
      "153 Training Loss: tensor(0.3542)\n",
      "154 Training Loss: tensor(0.3489)\n",
      "155 Training Loss: tensor(0.3533)\n",
      "156 Training Loss: tensor(0.3531)\n",
      "157 Training Loss: tensor(0.3489)\n",
      "158 Training Loss: tensor(0.3492)\n",
      "159 Training Loss: tensor(0.3596)\n",
      "160 Training Loss: tensor(0.3545)\n",
      "161 Training Loss: tensor(0.3522)\n",
      "162 Training Loss: tensor(0.3571)\n",
      "163 Training Loss: tensor(0.3526)\n",
      "164 Training Loss: tensor(0.3502)\n",
      "165 Training Loss: tensor(0.3527)\n",
      "166 Training Loss: tensor(0.3565)\n",
      "167 Training Loss: tensor(0.3524)\n",
      "168 Training Loss: tensor(0.3600)\n",
      "169 Training Loss: tensor(0.3508)\n",
      "170 Training Loss: tensor(0.3575)\n",
      "171 Training Loss: tensor(0.3529)\n",
      "172 Training Loss: tensor(0.3539)\n",
      "173 Training Loss: tensor(0.3545)\n",
      "174 Training Loss: tensor(0.3539)\n",
      "175 Training Loss: tensor(0.3537)\n",
      "176 Training Loss: tensor(0.3540)\n",
      "177 Training Loss: tensor(0.3536)\n",
      "178 Training Loss: tensor(0.3522)\n",
      "179 Training Loss: tensor(0.3543)\n",
      "180 Training Loss: tensor(0.3524)\n",
      "181 Training Loss: tensor(0.3526)\n",
      "182 Training Loss: tensor(0.3510)\n",
      "183 Training Loss: tensor(0.3515)\n",
      "184 Training Loss: tensor(0.3542)\n",
      "185 Training Loss: tensor(0.3547)\n",
      "186 Training Loss: tensor(0.3514)\n",
      "187 Training Loss: tensor(0.3545)\n",
      "188 Training Loss: tensor(0.3500)\n",
      "189 Training Loss: tensor(0.3517)\n",
      "190 Training Loss: tensor(0.3599)\n",
      "191 Training Loss: tensor(0.3562)\n",
      "192 Training Loss: tensor(0.3535)\n",
      "193 Training Loss: tensor(0.3563)\n",
      "194 Training Loss: tensor(0.3527)\n",
      "195 Training Loss: tensor(0.3533)\n",
      "196 Training Loss: tensor(0.3500)\n",
      "197 Training Loss: tensor(0.3509)\n",
      "198 Training Loss: tensor(0.3519)\n",
      "199 Training Loss: tensor(0.3513)\n",
      "200 Training Loss: tensor(0.3521)\n",
      "201 Training Loss: tensor(0.3509)\n",
      "202 Training Loss: tensor(0.3583)\n",
      "203 Training Loss: tensor(0.3512)\n",
      "204 Training Loss: tensor(0.3516)\n",
      "205 Training Loss: tensor(0.3535)\n",
      "206 Training Loss: tensor(0.3573)\n",
      "207 Training Loss: tensor(0.3549)\n",
      "208 Training Loss: tensor(0.3501)\n",
      "209 Training Loss: tensor(0.3534)\n",
      "210 Training Loss: tensor(0.3503)\n",
      "211 Training Loss: tensor(0.3556)\n",
      "212 Training Loss: tensor(0.3520)\n",
      "213 Training Loss: tensor(0.3523)\n",
      "214 Training Loss: tensor(0.3510)\n",
      "215 Training Loss: tensor(0.3516)\n",
      "216 Training Loss: tensor(0.3615)\n",
      "217 Training Loss: tensor(0.3520)\n",
      "218 Training Loss: tensor(0.3528)\n",
      "219 Training Loss: tensor(0.3507)\n",
      "220 Training Loss: tensor(0.3526)\n",
      "221 Training Loss: tensor(0.3503)\n",
      "222 Training Loss: tensor(0.3525)\n",
      "223 Training Loss: tensor(0.3503)\n",
      "224 Training Loss: tensor(0.3538)\n",
      "225 Training Loss: tensor(0.3513)\n",
      "226 Training Loss: tensor(0.3511)\n",
      "227 Training Loss: tensor(0.3540)\n",
      "228 Training Loss: tensor(0.3515)\n",
      "229 Training Loss: tensor(0.3508)\n",
      "230 Training Loss: tensor(0.3556)\n",
      "231 Training Loss: tensor(0.3532)\n",
      "232 Training Loss: tensor(0.3511)\n",
      "233 Training Loss: tensor(0.3501)\n",
      "234 Training Loss: tensor(0.3527)\n",
      "235 Training Loss: tensor(0.3495)\n",
      "236 Training Loss: tensor(0.3574)\n",
      "237 Training Loss: tensor(0.3511)\n",
      "238 Training Loss: tensor(0.3518)\n",
      "239 Training Loss: tensor(0.3529)\n",
      "240 Training Loss: tensor(0.3526)\n",
      "241 Training Loss: tensor(0.3509)\n",
      "242 Training Loss: tensor(0.3532)\n",
      "243 Training Loss: tensor(0.3518)\n",
      "244 Training Loss: tensor(0.3552)\n",
      "245 Training Loss: tensor(0.3528)\n",
      "246 Training Loss: tensor(0.3493)\n",
      "247 Training Loss: tensor(0.3514)\n",
      "248 Training Loss: tensor(0.3501)\n",
      "249 Training Loss: tensor(0.3514)\n",
      "250 Training Loss: tensor(0.3534)\n",
      "251 Training Loss: tensor(0.3515)\n",
      "252 Training Loss: tensor(0.3499)\n",
      "253 Training Loss: tensor(0.3498)\n",
      "254 Training Loss: tensor(0.3534)\n",
      "255 Training Loss: tensor(0.3529)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 Training Loss: tensor(0.3505)\n",
      "257 Training Loss: tensor(0.3510)\n",
      "258 Training Loss: tensor(0.3537)\n",
      "259 Training Loss: tensor(0.3497)\n",
      "260 Training Loss: tensor(0.3501)\n",
      "261 Training Loss: tensor(0.3533)\n",
      "262 Training Loss: tensor(0.3598)\n",
      "263 Training Loss: tensor(0.3506)\n",
      "264 Training Loss: tensor(0.3552)\n",
      "265 Training Loss: tensor(0.3494)\n",
      "266 Training Loss: tensor(0.3531)\n",
      "267 Training Loss: tensor(0.3538)\n",
      "268 Training Loss: tensor(0.3544)\n",
      "269 Training Loss: tensor(0.3497)\n",
      "270 Training Loss: tensor(0.3505)\n",
      "271 Training Loss: tensor(0.3534)\n",
      "272 Training Loss: tensor(0.3577)\n",
      "273 Training Loss: tensor(0.3506)\n",
      "274 Training Loss: tensor(0.3499)\n",
      "275 Training Loss: tensor(0.3531)\n",
      "276 Training Loss: tensor(0.3495)\n",
      "277 Training Loss: tensor(0.3530)\n",
      "278 Training Loss: tensor(0.3497)\n",
      "279 Training Loss: tensor(0.3498)\n",
      "280 Training Loss: tensor(0.3556)\n",
      "281 Training Loss: tensor(0.3492)\n",
      "282 Training Loss: tensor(0.3529)\n",
      "283 Training Loss: tensor(0.3545)\n",
      "284 Training Loss: tensor(0.3504)\n",
      "285 Training Loss: tensor(0.3501)\n",
      "286 Training Loss: tensor(0.3546)\n",
      "287 Training Loss: tensor(0.3547)\n",
      "288 Training Loss: tensor(0.3543)\n",
      "289 Training Loss: tensor(0.3519)\n",
      "290 Training Loss: tensor(0.3503)\n",
      "291 Training Loss: tensor(0.3502)\n",
      "292 Training Loss: tensor(0.3511)\n",
      "293 Training Loss: tensor(0.3582)\n",
      "294 Training Loss: tensor(0.3533)\n",
      "295 Training Loss: tensor(0.3513)\n",
      "296 Training Loss: tensor(0.3515)\n",
      "297 Training Loss: tensor(0.3516)\n",
      "298 Training Loss: tensor(0.3543)\n",
      "299 Training Loss: tensor(0.3517)\n",
      "300 Training Loss: tensor(0.3534)\n",
      "301 Training Loss: tensor(0.3501)\n",
      "302 Training Loss: tensor(0.3535)\n",
      "303 Training Loss: tensor(0.3497)\n",
      "304 Training Loss: tensor(0.3550)\n",
      "305 Training Loss: tensor(0.3499)\n",
      "306 Training Loss: tensor(0.3503)\n",
      "307 Training Loss: tensor(0.3527)\n",
      "308 Training Loss: tensor(0.3523)\n",
      "309 Training Loss: tensor(0.3570)\n",
      "310 Training Loss: tensor(0.3512)\n",
      "311 Training Loss: tensor(0.3506)\n",
      "312 Training Loss: tensor(0.3541)\n",
      "313 Training Loss: tensor(0.3553)\n",
      "314 Training Loss: tensor(0.3543)\n",
      "315 Training Loss: tensor(0.3556)\n",
      "316 Training Loss: tensor(0.3509)\n",
      "317 Training Loss: tensor(0.3527)\n",
      "318 Training Loss: tensor(0.3551)\n",
      "319 Training Loss: tensor(0.3505)\n",
      "320 Training Loss: tensor(0.3538)\n",
      "321 Training Loss: tensor(0.3545)\n",
      "322 Training Loss: tensor(0.3498)\n",
      "323 Training Loss: tensor(0.3511)\n",
      "324 Training Loss: tensor(0.3501)\n",
      "325 Training Loss: tensor(0.3519)\n",
      "326 Training Loss: tensor(0.3516)\n",
      "327 Training Loss: tensor(0.3501)\n",
      "328 Training Loss: tensor(0.3519)\n",
      "329 Training Loss: tensor(0.3524)\n",
      "330 Training Loss: tensor(0.3550)\n",
      "331 Training Loss: tensor(0.3525)\n",
      "332 Training Loss: tensor(0.3509)\n",
      "333 Training Loss: tensor(0.3490)\n",
      "334 Training Loss: tensor(0.3609)\n",
      "335 Training Loss: tensor(0.3497)\n",
      "336 Training Loss: tensor(0.3488)\n",
      "337 Training Loss: tensor(0.3510)\n",
      "338 Training Loss: tensor(0.3512)\n",
      "339 Training Loss: tensor(0.3558)\n",
      "340 Training Loss: tensor(0.3499)\n",
      "341 Training Loss: tensor(0.3574)\n",
      "342 Training Loss: tensor(0.3528)\n",
      "343 Training Loss: tensor(0.3492)\n",
      "344 Training Loss: tensor(0.3525)\n",
      "345 Training Loss: tensor(0.3542)\n",
      "346 Training Loss: tensor(0.3507)\n",
      "347 Training Loss: tensor(0.3501)\n",
      "348 Training Loss: tensor(0.3499)\n",
      "349 Training Loss: tensor(0.3518)\n",
      "350 Training Loss: tensor(0.3495)\n",
      "351 Training Loss: tensor(0.3516)\n",
      "352 Training Loss: tensor(0.3565)\n",
      "353 Training Loss: tensor(0.3520)\n",
      "354 Training Loss: tensor(0.3509)\n",
      "355 Training Loss: tensor(0.3505)\n",
      "356 Training Loss: tensor(0.3534)\n",
      "357 Training Loss: tensor(0.3570)\n",
      "358 Training Loss: tensor(0.3491)\n",
      "359 Training Loss: tensor(0.3531)\n",
      "360 Training Loss: tensor(0.3504)\n",
      "361 Training Loss: tensor(0.3548)\n",
      "362 Training Loss: tensor(0.3489)\n",
      "363 Training Loss: tensor(0.3528)\n",
      "364 Training Loss: tensor(0.3546)\n",
      "365 Training Loss: tensor(0.3542)\n",
      "366 Training Loss: tensor(0.3521)\n",
      "367 Training Loss: tensor(0.3521)\n",
      "368 Training Loss: tensor(0.3562)\n",
      "369 Training Loss: tensor(0.3501)\n",
      "370 Training Loss: tensor(0.3519)\n",
      "371 Training Loss: tensor(0.3522)\n",
      "372 Training Loss: tensor(0.3510)\n",
      "373 Training Loss: tensor(0.3512)\n",
      "374 Training Loss: tensor(0.3534)\n",
      "375 Training Loss: tensor(0.3523)\n",
      "376 Training Loss: tensor(0.3514)\n",
      "377 Training Loss: tensor(0.3525)\n",
      "378 Training Loss: tensor(0.3525)\n",
      "379 Training Loss: tensor(0.3500)\n",
      "380 Training Loss: tensor(0.3604)\n",
      "381 Training Loss: tensor(0.3510)\n",
      "382 Training Loss: tensor(0.3525)\n",
      "383 Training Loss: tensor(0.3522)\n",
      "384 Training Loss: tensor(0.3496)\n",
      "385 Training Loss: tensor(0.3498)\n",
      "386 Training Loss: tensor(0.3568)\n",
      "387 Training Loss: tensor(0.3509)\n",
      "388 Training Loss: tensor(0.3608)\n",
      "389 Training Loss: tensor(0.3564)\n",
      "390 Training Loss: tensor(0.3525)\n",
      "391 Training Loss: tensor(0.3501)\n",
      "392 Training Loss: tensor(0.3501)\n",
      "393 Training Loss: tensor(0.3530)\n",
      "394 Training Loss: tensor(0.3518)\n",
      "395 Training Loss: tensor(0.3518)\n",
      "396 Training Loss: tensor(0.3523)\n",
      "397 Training Loss: tensor(0.3505)\n",
      "398 Training Loss: tensor(0.3502)\n",
      "399 Training Loss: tensor(0.3512)\n",
      "400 Training Loss: tensor(0.3564)\n",
      "401 Training Loss: tensor(0.3499)\n",
      "402 Training Loss: tensor(0.3500)\n",
      "403 Training Loss: tensor(0.3494)\n",
      "404 Training Loss: tensor(0.3532)\n",
      "405 Training Loss: tensor(0.3503)\n",
      "406 Training Loss: tensor(0.3529)\n",
      "407 Training Loss: tensor(0.3521)\n",
      "408 Training Loss: tensor(0.3571)\n",
      "409 Training Loss: tensor(0.3494)\n",
      "410 Training Loss: tensor(0.3515)\n",
      "411 Training Loss: tensor(0.3501)\n",
      "412 Training Loss: tensor(0.3511)\n",
      "413 Training Loss: tensor(0.3496)\n",
      "414 Training Loss: tensor(0.3496)\n",
      "415 Training Loss: tensor(0.3549)\n",
      "416 Training Loss: tensor(0.3594)\n",
      "417 Training Loss: tensor(0.3516)\n",
      "418 Training Loss: tensor(0.3513)\n",
      "419 Training Loss: tensor(0.3517)\n",
      "420 Training Loss: tensor(0.3514)\n",
      "421 Training Loss: tensor(0.3510)\n",
      "422 Training Loss: tensor(0.3531)\n",
      "423 Training Loss: tensor(0.3520)\n",
      "424 Training Loss: tensor(0.3491)\n",
      "425 Training Loss: tensor(0.3498)\n",
      "426 Training Loss: tensor(0.3535)\n",
      "427 Training Loss: tensor(0.3506)\n",
      "428 Training Loss: tensor(0.3529)\n",
      "429 Training Loss: tensor(0.3491)\n",
      "430 Training Loss: tensor(0.3551)\n",
      "431 Training Loss: tensor(0.3521)\n",
      "432 Training Loss: tensor(0.3502)\n",
      "433 Training Loss: tensor(0.3498)\n",
      "434 Training Loss: tensor(0.3491)\n",
      "435 Training Loss: tensor(0.3502)\n",
      "436 Training Loss: tensor(0.3527)\n",
      "437 Training Loss: tensor(0.3566)\n",
      "438 Training Loss: tensor(0.3539)\n",
      "439 Training Loss: tensor(0.3503)\n",
      "440 Training Loss: tensor(0.3498)\n",
      "441 Training Loss: tensor(0.3499)\n",
      "442 Training Loss: tensor(0.3493)\n",
      "443 Training Loss: tensor(0.3526)\n",
      "444 Training Loss: tensor(0.3490)\n",
      "445 Training Loss: tensor(0.3526)\n",
      "446 Training Loss: tensor(0.3507)\n",
      "447 Training Loss: tensor(0.3497)\n",
      "448 Training Loss: tensor(0.3520)\n",
      "449 Training Loss: tensor(0.3493)\n",
      "450 Training Loss: tensor(0.3487)\n",
      "451 Training Loss: tensor(0.3545)\n",
      "452 Training Loss: tensor(0.3515)\n",
      "453 Training Loss: tensor(0.3576)\n",
      "454 Training Loss: tensor(0.3521)\n",
      "455 Training Loss: tensor(0.3498)\n",
      "456 Training Loss: tensor(0.3502)\n",
      "457 Training Loss: tensor(0.3491)\n",
      "458 Training Loss: tensor(0.3506)\n",
      "459 Training Loss: tensor(0.3514)\n",
      "460 Training Loss: tensor(0.3508)\n",
      "461 Training Loss: tensor(0.3610)\n",
      "462 Training Loss: tensor(0.3488)\n",
      "463 Training Loss: tensor(0.3497)\n",
      "464 Training Loss: tensor(0.3495)\n",
      "465 Training Loss: tensor(0.3492)\n",
      "466 Training Loss: tensor(0.3570)\n",
      "467 Training Loss: tensor(0.3517)\n",
      "468 Training Loss: tensor(0.3595)\n",
      "469 Training Loss: tensor(0.3544)\n",
      "470 Training Loss: tensor(0.3506)\n",
      "471 Training Loss: tensor(0.3514)\n",
      "472 Training Loss: tensor(0.3505)\n",
      "473 Training Loss: tensor(0.3503)\n",
      "474 Training Loss: tensor(0.3507)\n",
      "475 Training Loss: tensor(0.3511)\n",
      "476 Training Loss: tensor(0.3562)\n",
      "477 Training Loss: tensor(0.3543)\n",
      "478 Training Loss: tensor(0.3502)\n",
      "479 Training Loss: tensor(0.3502)\n",
      "480 Training Loss: tensor(0.3519)\n",
      "481 Training Loss: tensor(0.3562)\n",
      "482 Training Loss: tensor(0.3514)\n",
      "483 Training Loss: tensor(0.3514)\n",
      "484 Training Loss: tensor(0.3518)\n",
      "485 Training Loss: tensor(0.3542)\n",
      "486 Training Loss: tensor(0.3536)\n",
      "487 Training Loss: tensor(0.3502)\n",
      "488 Training Loss: tensor(0.3506)\n",
      "489 Training Loss: tensor(0.3502)\n",
      "490 Training Loss: tensor(0.3545)\n",
      "491 Training Loss: tensor(0.3514)\n",
      "492 Training Loss: tensor(0.3494)\n",
      "493 Training Loss: tensor(0.3505)\n",
      "494 Training Loss: tensor(0.3536)\n",
      "495 Training Loss: tensor(0.3494)\n",
      "496 Training Loss: tensor(0.3562)\n",
      "497 Training Loss: tensor(0.3517)\n",
      "498 Training Loss: tensor(0.3548)\n",
      "499 Training Loss: tensor(0.3489)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Loss: tensor(0.3494)\n",
      "501 Training Loss: tensor(0.3527)\n",
      "502 Training Loss: tensor(0.3500)\n",
      "503 Training Loss: tensor(0.3518)\n",
      "504 Training Loss: tensor(0.3513)\n",
      "505 Training Loss: tensor(0.3551)\n",
      "506 Training Loss: tensor(0.3491)\n",
      "507 Training Loss: tensor(0.3516)\n",
      "508 Training Loss: tensor(0.3535)\n",
      "509 Training Loss: tensor(0.3490)\n",
      "510 Training Loss: tensor(0.3499)\n",
      "511 Training Loss: tensor(0.3514)\n",
      "512 Training Loss: tensor(0.3506)\n",
      "513 Training Loss: tensor(0.3527)\n",
      "514 Training Loss: tensor(0.3498)\n",
      "515 Training Loss: tensor(0.3533)\n",
      "516 Training Loss: tensor(0.3516)\n",
      "517 Training Loss: tensor(0.3504)\n",
      "518 Training Loss: tensor(0.3516)\n",
      "519 Training Loss: tensor(0.3636)\n",
      "520 Training Loss: tensor(0.3497)\n",
      "521 Training Loss: tensor(0.3503)\n",
      "522 Training Loss: tensor(0.3513)\n",
      "523 Training Loss: tensor(0.3491)\n",
      "524 Training Loss: tensor(0.3493)\n",
      "525 Training Loss: tensor(0.3569)\n",
      "526 Training Loss: tensor(0.3574)\n",
      "527 Training Loss: tensor(0.3514)\n",
      "528 Training Loss: tensor(0.3509)\n",
      "529 Training Loss: tensor(0.3527)\n",
      "530 Training Loss: tensor(0.3498)\n",
      "531 Training Loss: tensor(0.3494)\n",
      "532 Training Loss: tensor(0.3512)\n",
      "533 Training Loss: tensor(0.3515)\n",
      "534 Training Loss: tensor(0.3549)\n",
      "535 Training Loss: tensor(0.3502)\n",
      "536 Training Loss: tensor(0.3529)\n",
      "537 Training Loss: tensor(0.3556)\n",
      "538 Training Loss: tensor(0.3507)\n",
      "539 Training Loss: tensor(0.3540)\n",
      "540 Training Loss: tensor(0.3559)\n",
      "541 Training Loss: tensor(0.3499)\n",
      "542 Training Loss: tensor(0.3523)\n",
      "543 Training Loss: tensor(0.3587)\n",
      "544 Training Loss: tensor(0.3540)\n",
      "545 Training Loss: tensor(0.3503)\n",
      "546 Training Loss: tensor(0.3547)\n",
      "547 Training Loss: tensor(0.3560)\n",
      "548 Training Loss: tensor(0.3500)\n",
      "549 Training Loss: tensor(0.3498)\n",
      "550 Training Loss: tensor(0.3508)\n",
      "551 Training Loss: tensor(0.3544)\n",
      "552 Training Loss: tensor(0.3562)\n",
      "553 Training Loss: tensor(0.3530)\n",
      "554 Training Loss: tensor(0.3513)\n",
      "555 Training Loss: tensor(0.3533)\n",
      "556 Training Loss: tensor(0.3520)\n",
      "557 Training Loss: tensor(0.3514)\n",
      "558 Training Loss: tensor(0.3564)\n",
      "559 Training Loss: tensor(0.3556)\n",
      "560 Training Loss: tensor(0.3499)\n",
      "561 Training Loss: tensor(0.3502)\n",
      "562 Training Loss: tensor(0.3553)\n",
      "563 Training Loss: tensor(0.3507)\n",
      "564 Training Loss: tensor(0.3508)\n",
      "565 Training Loss: tensor(0.3549)\n",
      "566 Training Loss: tensor(0.3524)\n",
      "567 Training Loss: tensor(0.3523)\n",
      "568 Training Loss: tensor(0.3528)\n",
      "569 Training Loss: tensor(0.3503)\n",
      "570 Training Loss: tensor(0.3507)\n",
      "571 Training Loss: tensor(0.3505)\n",
      "572 Training Loss: tensor(0.3513)\n",
      "573 Training Loss: tensor(0.3536)\n",
      "574 Training Loss: tensor(0.3504)\n",
      "575 Training Loss: tensor(0.3491)\n",
      "576 Training Loss: tensor(0.3497)\n",
      "577 Training Loss: tensor(0.3618)\n",
      "578 Training Loss: tensor(0.3506)\n",
      "579 Training Loss: tensor(0.3560)\n",
      "580 Training Loss: tensor(0.3514)\n",
      "581 Training Loss: tensor(0.3571)\n",
      "582 Training Loss: tensor(0.3499)\n",
      "583 Training Loss: tensor(0.3542)\n",
      "584 Training Loss: tensor(0.3529)\n",
      "585 Training Loss: tensor(0.3551)\n",
      "586 Training Loss: tensor(0.3512)\n",
      "587 Training Loss: tensor(0.3504)\n",
      "588 Training Loss: tensor(0.3491)\n",
      "589 Training Loss: tensor(0.3510)\n",
      "590 Training Loss: tensor(0.3514)\n",
      "591 Training Loss: tensor(0.3552)\n",
      "592 Training Loss: tensor(0.3500)\n",
      "593 Training Loss: tensor(0.3560)\n",
      "594 Training Loss: tensor(0.3515)\n",
      "595 Training Loss: tensor(0.3536)\n",
      "596 Training Loss: tensor(0.3503)\n",
      "597 Training Loss: tensor(0.3498)\n",
      "598 Training Loss: tensor(0.3508)\n",
      "599 Training Loss: tensor(0.3535)\n",
      "600 Training Loss: tensor(0.3508)\n",
      "601 Training Loss: tensor(0.3585)\n",
      "602 Training Loss: tensor(0.3507)\n",
      "603 Training Loss: tensor(0.3525)\n",
      "604 Training Loss: tensor(0.3496)\n",
      "605 Training Loss: tensor(0.3519)\n",
      "606 Training Loss: tensor(0.3521)\n",
      "607 Training Loss: tensor(0.3536)\n",
      "608 Training Loss: tensor(0.3512)\n",
      "609 Training Loss: tensor(0.3495)\n",
      "610 Training Loss: tensor(0.3540)\n",
      "611 Training Loss: tensor(0.3523)\n",
      "612 Training Loss: tensor(0.3492)\n",
      "613 Training Loss: tensor(0.3496)\n",
      "614 Training Loss: tensor(0.3501)\n",
      "615 Training Loss: tensor(0.3542)\n",
      "616 Training Loss: tensor(0.3512)\n",
      "617 Training Loss: tensor(0.3588)\n",
      "618 Training Loss: tensor(0.3501)\n",
      "619 Training Loss: tensor(0.3512)\n",
      "620 Training Loss: tensor(0.3494)\n",
      "621 Training Loss: tensor(0.3491)\n",
      "622 Training Loss: tensor(0.3502)\n",
      "623 Training Loss: tensor(0.3489)\n",
      "624 Training Loss: tensor(0.3498)\n",
      "625 Training Loss: tensor(0.3503)\n",
      "626 Training Loss: tensor(0.3488)\n",
      "627 Training Loss: tensor(0.3516)\n",
      "628 Training Loss: tensor(0.3509)\n",
      "629 Training Loss: tensor(0.3510)\n",
      "630 Training Loss: tensor(0.3612)\n",
      "631 Training Loss: tensor(0.3495)\n",
      "632 Training Loss: tensor(0.3532)\n",
      "633 Training Loss: tensor(0.3511)\n",
      "634 Training Loss: tensor(0.3504)\n",
      "635 Training Loss: tensor(0.3543)\n",
      "636 Training Loss: tensor(0.3485)\n",
      "637 Training Loss: tensor(0.3512)\n",
      "638 Training Loss: tensor(0.3546)\n",
      "639 Training Loss: tensor(0.3498)\n",
      "640 Training Loss: tensor(0.3490)\n",
      "641 Training Loss: tensor(0.3508)\n",
      "642 Training Loss: tensor(0.3566)\n",
      "643 Training Loss: tensor(0.3497)\n",
      "644 Training Loss: tensor(0.3525)\n",
      "645 Training Loss: tensor(0.3578)\n",
      "646 Training Loss: tensor(0.3509)\n",
      "647 Training Loss: tensor(0.3497)\n",
      "648 Training Loss: tensor(0.3526)\n",
      "649 Training Loss: tensor(0.3537)\n",
      "650 Training Loss: tensor(0.3531)\n",
      "651 Training Loss: tensor(0.3510)\n",
      "652 Training Loss: tensor(0.3554)\n",
      "653 Training Loss: tensor(0.3551)\n",
      "654 Training Loss: tensor(0.3520)\n",
      "655 Training Loss: tensor(0.3521)\n",
      "656 Training Loss: tensor(0.3534)\n",
      "657 Training Loss: tensor(0.3531)\n",
      "658 Training Loss: tensor(0.3500)\n",
      "659 Training Loss: tensor(0.3501)\n",
      "660 Training Loss: tensor(0.3517)\n",
      "661 Training Loss: tensor(0.3511)\n",
      "662 Training Loss: tensor(0.3537)\n",
      "663 Training Loss: tensor(0.3524)\n",
      "664 Training Loss: tensor(0.3507)\n",
      "665 Training Loss: tensor(0.3505)\n",
      "666 Training Loss: tensor(0.3488)\n",
      "667 Training Loss: tensor(0.3494)\n",
      "668 Training Loss: tensor(0.3510)\n",
      "669 Training Loss: tensor(0.3519)\n",
      "670 Training Loss: tensor(0.3528)\n",
      "671 Training Loss: tensor(0.3497)\n",
      "672 Training Loss: tensor(0.3495)\n",
      "673 Training Loss: tensor(0.3497)\n",
      "674 Training Loss: tensor(0.3624)\n",
      "675 Training Loss: tensor(0.3507)\n",
      "676 Training Loss: tensor(0.3494)\n",
      "677 Training Loss: tensor(0.3490)\n",
      "678 Training Loss: tensor(0.3574)\n",
      "679 Training Loss: tensor(0.3496)\n",
      "680 Training Loss: tensor(0.3487)\n",
      "681 Training Loss: tensor(0.3513)\n",
      "682 Training Loss: tensor(0.3615)\n",
      "683 Training Loss: tensor(0.3493)\n",
      "684 Training Loss: tensor(0.3514)\n",
      "685 Training Loss: tensor(0.3549)\n",
      "686 Training Loss: tensor(0.3494)\n",
      "687 Training Loss: tensor(0.3498)\n",
      "688 Training Loss: tensor(0.3503)\n",
      "689 Training Loss: tensor(0.3491)\n",
      "690 Training Loss: tensor(0.3501)\n",
      "691 Training Loss: tensor(0.3498)\n",
      "692 Training Loss: tensor(0.3499)\n",
      "693 Training Loss: tensor(0.3557)\n",
      "694 Training Loss: tensor(0.3501)\n",
      "695 Training Loss: tensor(0.3494)\n",
      "696 Training Loss: tensor(0.3549)\n",
      "697 Training Loss: tensor(0.3558)\n",
      "698 Training Loss: tensor(0.3533)\n",
      "699 Training Loss: tensor(0.3542)\n",
      "700 Training Loss: tensor(0.3489)\n",
      "701 Training Loss: tensor(0.3522)\n",
      "702 Training Loss: tensor(0.3530)\n",
      "703 Training Loss: tensor(0.3487)\n",
      "704 Training Loss: tensor(0.3514)\n",
      "705 Training Loss: tensor(0.3500)\n",
      "706 Training Loss: tensor(0.3517)\n",
      "707 Training Loss: tensor(0.3502)\n",
      "708 Training Loss: tensor(0.3495)\n",
      "709 Training Loss: tensor(0.3527)\n",
      "710 Training Loss: tensor(0.3502)\n",
      "711 Training Loss: tensor(0.3495)\n",
      "712 Training Loss: tensor(0.3487)\n",
      "713 Training Loss: tensor(0.3578)\n",
      "714 Training Loss: tensor(0.3582)\n",
      "715 Training Loss: tensor(0.3538)\n",
      "716 Training Loss: tensor(0.3550)\n",
      "717 Training Loss: tensor(0.3572)\n",
      "718 Training Loss: tensor(0.3504)\n",
      "719 Training Loss: tensor(0.3540)\n",
      "720 Training Loss: tensor(0.3576)\n",
      "721 Training Loss: tensor(0.3494)\n",
      "722 Training Loss: tensor(0.3516)\n",
      "723 Training Loss: tensor(0.3536)\n",
      "724 Training Loss: tensor(0.3513)\n",
      "725 Training Loss: tensor(0.3518)\n",
      "726 Training Loss: tensor(0.3507)\n",
      "727 Training Loss: tensor(0.3527)\n",
      "728 Training Loss: tensor(0.3523)\n",
      "729 Training Loss: tensor(0.3514)\n",
      "730 Training Loss: tensor(0.3518)\n",
      "731 Training Loss: tensor(0.3564)\n",
      "732 Training Loss: tensor(0.3574)\n",
      "733 Training Loss: tensor(0.3526)\n",
      "734 Training Loss: tensor(0.3525)\n",
      "735 Training Loss: tensor(0.3521)\n",
      "736 Training Loss: tensor(0.3510)\n",
      "737 Training Loss: tensor(0.3502)\n",
      "738 Training Loss: tensor(0.3530)\n",
      "739 Training Loss: tensor(0.3554)\n",
      "740 Training Loss: tensor(0.3498)\n",
      "741 Training Loss: tensor(0.3593)\n",
      "742 Training Loss: tensor(0.3497)\n",
      "743 Training Loss: tensor(0.3509)\n",
      "744 Training Loss: tensor(0.3530)\n",
      "745 Training Loss: tensor(0.3503)\n",
      "746 Training Loss: tensor(0.3563)\n",
      "747 Training Loss: tensor(0.3562)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 Training Loss: tensor(0.3535)\n",
      "749 Training Loss: tensor(0.3501)\n",
      "750 Training Loss: tensor(0.3500)\n",
      "751 Training Loss: tensor(0.3570)\n",
      "752 Training Loss: tensor(0.3527)\n",
      "753 Training Loss: tensor(0.3522)\n",
      "754 Training Loss: tensor(0.3554)\n",
      "755 Training Loss: tensor(0.3511)\n",
      "756 Training Loss: tensor(0.3545)\n",
      "757 Training Loss: tensor(0.3505)\n",
      "758 Training Loss: tensor(0.3535)\n",
      "759 Training Loss: tensor(0.3557)\n",
      "760 Training Loss: tensor(0.3515)\n",
      "761 Training Loss: tensor(0.3522)\n",
      "762 Training Loss: tensor(0.3536)\n",
      "763 Training Loss: tensor(0.3534)\n",
      "764 Training Loss: tensor(0.3508)\n",
      "765 Training Loss: tensor(0.3515)\n",
      "766 Training Loss: tensor(0.3501)\n",
      "767 Training Loss: tensor(0.3523)\n",
      "768 Training Loss: tensor(0.3508)\n",
      "769 Training Loss: tensor(0.3504)\n",
      "770 Training Loss: tensor(0.3515)\n",
      "771 Training Loss: tensor(0.3533)\n",
      "772 Training Loss: tensor(0.3507)\n",
      "773 Training Loss: tensor(0.3521)\n",
      "774 Training Loss: tensor(0.3542)\n",
      "775 Training Loss: tensor(0.3511)\n",
      "776 Training Loss: tensor(0.3500)\n",
      "777 Training Loss: tensor(0.3518)\n",
      "778 Training Loss: tensor(0.3519)\n",
      "779 Training Loss: tensor(0.3514)\n",
      "780 Training Loss: tensor(0.3550)\n",
      "781 Training Loss: tensor(0.3547)\n",
      "782 Training Loss: tensor(0.3518)\n",
      "783 Training Loss: tensor(0.3503)\n",
      "784 Training Loss: tensor(0.3566)\n",
      "785 Training Loss: tensor(0.3516)\n",
      "786 Training Loss: tensor(0.3500)\n",
      "787 Training Loss: tensor(0.3538)\n",
      "788 Training Loss: tensor(0.3539)\n",
      "789 Training Loss: tensor(0.3518)\n",
      "790 Training Loss: tensor(0.3500)\n",
      "791 Training Loss: tensor(0.3499)\n",
      "792 Training Loss: tensor(0.3493)\n",
      "793 Training Loss: tensor(0.3499)\n",
      "794 Training Loss: tensor(0.3507)\n",
      "795 Training Loss: tensor(0.3530)\n",
      "796 Training Loss: tensor(0.3511)\n",
      "797 Training Loss: tensor(0.3556)\n",
      "798 Training Loss: tensor(0.3500)\n",
      "799 Training Loss: tensor(0.3488)\n",
      "800 Training Loss: tensor(0.3509)\n",
      "801 Training Loss: tensor(0.3508)\n",
      "802 Training Loss: tensor(0.3556)\n",
      "803 Training Loss: tensor(0.3526)\n",
      "804 Training Loss: tensor(0.3495)\n",
      "805 Training Loss: tensor(0.3505)\n",
      "806 Training Loss: tensor(0.3540)\n",
      "807 Training Loss: tensor(0.3559)\n",
      "808 Training Loss: tensor(0.3518)\n",
      "809 Training Loss: tensor(0.3511)\n",
      "810 Training Loss: tensor(0.3516)\n",
      "811 Training Loss: tensor(0.3508)\n",
      "812 Training Loss: tensor(0.3555)\n",
      "813 Training Loss: tensor(0.3502)\n",
      "814 Training Loss: tensor(0.3491)\n",
      "815 Training Loss: tensor(0.3597)\n",
      "816 Training Loss: tensor(0.3494)\n",
      "817 Training Loss: tensor(0.3506)\n",
      "818 Training Loss: tensor(0.3507)\n",
      "819 Training Loss: tensor(0.3507)\n",
      "820 Training Loss: tensor(0.3535)\n",
      "821 Training Loss: tensor(0.3531)\n",
      "822 Training Loss: tensor(0.3496)\n",
      "823 Training Loss: tensor(0.3493)\n",
      "824 Training Loss: tensor(0.3581)\n",
      "825 Training Loss: tensor(0.3528)\n",
      "826 Training Loss: tensor(0.3496)\n",
      "827 Training Loss: tensor(0.3516)\n",
      "828 Training Loss: tensor(0.3498)\n",
      "829 Training Loss: tensor(0.3500)\n",
      "830 Training Loss: tensor(0.3502)\n",
      "831 Training Loss: tensor(0.3498)\n",
      "832 Training Loss: tensor(0.3498)\n",
      "833 Training Loss: tensor(0.3572)\n",
      "834 Training Loss: tensor(0.3516)\n",
      "835 Training Loss: tensor(0.3506)\n",
      "836 Training Loss: tensor(0.3507)\n",
      "837 Training Loss: tensor(0.3510)\n",
      "838 Training Loss: tensor(0.3552)\n",
      "839 Training Loss: tensor(0.3543)\n",
      "840 Training Loss: tensor(0.3542)\n",
      "841 Training Loss: tensor(0.3509)\n",
      "842 Training Loss: tensor(0.3533)\n",
      "843 Training Loss: tensor(0.3515)\n",
      "844 Training Loss: tensor(0.3532)\n",
      "845 Training Loss: tensor(0.3532)\n",
      "846 Training Loss: tensor(0.3550)\n",
      "847 Training Loss: tensor(0.3509)\n",
      "848 Training Loss: tensor(0.3502)\n",
      "849 Training Loss: tensor(0.3521)\n",
      "850 Training Loss: tensor(0.3526)\n",
      "851 Training Loss: tensor(0.3588)\n",
      "852 Training Loss: tensor(0.3494)\n",
      "853 Training Loss: tensor(0.3502)\n",
      "854 Training Loss: tensor(0.3557)\n",
      "855 Training Loss: tensor(0.3536)\n",
      "856 Training Loss: tensor(0.3514)\n",
      "857 Training Loss: tensor(0.3523)\n",
      "858 Training Loss: tensor(0.3509)\n",
      "859 Training Loss: tensor(0.3523)\n",
      "860 Training Loss: tensor(0.3500)\n",
      "861 Training Loss: tensor(0.3510)\n",
      "862 Training Loss: tensor(0.3497)\n",
      "863 Training Loss: tensor(0.3491)\n",
      "864 Training Loss: tensor(0.3531)\n",
      "865 Training Loss: tensor(0.3515)\n",
      "866 Training Loss: tensor(0.3492)\n",
      "867 Training Loss: tensor(0.3520)\n",
      "868 Training Loss: tensor(0.3508)\n",
      "869 Training Loss: tensor(0.3520)\n",
      "870 Training Loss: tensor(0.3529)\n",
      "871 Training Loss: tensor(0.3517)\n",
      "872 Training Loss: tensor(0.3540)\n",
      "873 Training Loss: tensor(0.3486)\n",
      "874 Training Loss: tensor(0.3499)\n",
      "875 Training Loss: tensor(0.3537)\n",
      "876 Training Loss: tensor(0.3510)\n",
      "877 Training Loss: tensor(0.3493)\n",
      "878 Training Loss: tensor(0.3493)\n",
      "879 Training Loss: tensor(0.3490)\n",
      "880 Training Loss: tensor(0.3481)\n",
      "881 Training Loss: tensor(0.3501)\n",
      "882 Training Loss: tensor(0.3654)\n",
      "883 Training Loss: tensor(0.3489)\n",
      "884 Training Loss: tensor(0.3491)\n",
      "885 Training Loss: tensor(0.3503)\n",
      "886 Training Loss: tensor(0.3511)\n",
      "887 Training Loss: tensor(0.3513)\n",
      "888 Training Loss: tensor(0.3528)\n",
      "889 Training Loss: tensor(0.3507)\n",
      "890 Training Loss: tensor(0.3526)\n",
      "891 Training Loss: tensor(0.3541)\n",
      "892 Training Loss: tensor(0.3554)\n",
      "893 Training Loss: tensor(0.3535)\n",
      "894 Training Loss: tensor(0.3503)\n",
      "895 Training Loss: tensor(0.3503)\n",
      "896 Training Loss: tensor(0.3530)\n",
      "897 Training Loss: tensor(0.3517)\n",
      "898 Training Loss: tensor(0.3524)\n",
      "899 Training Loss: tensor(0.3507)\n",
      "900 Training Loss: tensor(0.3503)\n",
      "901 Training Loss: tensor(0.3506)\n",
      "902 Training Loss: tensor(0.3511)\n",
      "903 Training Loss: tensor(0.3525)\n",
      "904 Training Loss: tensor(0.3488)\n",
      "905 Training Loss: tensor(0.3537)\n",
      "906 Training Loss: tensor(0.3514)\n",
      "907 Training Loss: tensor(0.3611)\n",
      "908 Training Loss: tensor(0.3505)\n",
      "909 Training Loss: tensor(0.3495)\n",
      "910 Training Loss: tensor(0.3498)\n",
      "911 Training Loss: tensor(0.3520)\n",
      "912 Training Loss: tensor(0.3535)\n",
      "913 Training Loss: tensor(0.3572)\n",
      "914 Training Loss: tensor(0.3490)\n",
      "915 Training Loss: tensor(0.3535)\n",
      "916 Training Loss: tensor(0.3510)\n",
      "917 Training Loss: tensor(0.3493)\n",
      "918 Training Loss: tensor(0.3506)\n",
      "919 Training Loss: tensor(0.3500)\n",
      "920 Training Loss: tensor(0.3504)\n",
      "921 Training Loss: tensor(0.3538)\n",
      "922 Training Loss: tensor(0.3502)\n",
      "923 Training Loss: tensor(0.3565)\n",
      "924 Training Loss: tensor(0.3497)\n",
      "925 Training Loss: tensor(0.3511)\n",
      "926 Training Loss: tensor(0.3500)\n",
      "927 Training Loss: tensor(0.3526)\n",
      "928 Training Loss: tensor(0.3492)\n",
      "929 Training Loss: tensor(0.3506)\n",
      "930 Training Loss: tensor(0.3519)\n",
      "931 Training Loss: tensor(0.3534)\n",
      "932 Training Loss: tensor(0.3554)\n",
      "933 Training Loss: tensor(0.3562)\n",
      "934 Training Loss: tensor(0.3549)\n",
      "935 Training Loss: tensor(0.3504)\n",
      "936 Training Loss: tensor(0.3504)\n",
      "937 Training Loss: tensor(0.3486)\n",
      "938 Training Loss: tensor(0.3489)\n",
      "939 Training Loss: tensor(0.3497)\n",
      "940 Training Loss: tensor(0.3582)\n",
      "941 Training Loss: tensor(0.3495)\n",
      "942 Training Loss: tensor(0.3502)\n",
      "943 Training Loss: tensor(0.3491)\n",
      "944 Training Loss: tensor(0.3602)\n",
      "945 Training Loss: tensor(0.3532)\n",
      "946 Training Loss: tensor(0.3522)\n",
      "947 Training Loss: tensor(0.3503)\n",
      "948 Training Loss: tensor(0.3489)\n",
      "949 Training Loss: tensor(0.3542)\n",
      "950 Training Loss: tensor(0.3515)\n",
      "951 Training Loss: tensor(0.3499)\n",
      "952 Training Loss: tensor(0.3538)\n",
      "953 Training Loss: tensor(0.3508)\n",
      "954 Training Loss: tensor(0.3500)\n",
      "955 Training Loss: tensor(0.3577)\n",
      "956 Training Loss: tensor(0.3549)\n",
      "957 Training Loss: tensor(0.3544)\n",
      "958 Training Loss: tensor(0.3519)\n",
      "959 Training Loss: tensor(0.3506)\n",
      "960 Training Loss: tensor(0.3547)\n",
      "961 Training Loss: tensor(0.3531)\n",
      "962 Training Loss: tensor(0.3583)\n",
      "963 Training Loss: tensor(0.3523)\n",
      "964 Training Loss: tensor(0.3518)\n",
      "965 Training Loss: tensor(0.3539)\n",
      "966 Training Loss: tensor(0.3515)\n",
      "967 Training Loss: tensor(0.3518)\n",
      "968 Training Loss: tensor(0.3532)\n",
      "969 Training Loss: tensor(0.3517)\n",
      "970 Training Loss: tensor(0.3515)\n",
      "971 Training Loss: tensor(0.3511)\n",
      "972 Training Loss: tensor(0.3518)\n",
      "973 Training Loss: tensor(0.3537)\n",
      "974 Training Loss: tensor(0.3501)\n",
      "975 Training Loss: tensor(0.3526)\n",
      "976 Training Loss: tensor(0.3512)\n",
      "977 Training Loss: tensor(0.3546)\n",
      "978 Training Loss: tensor(0.3490)\n",
      "979 Training Loss: tensor(0.3546)\n",
      "980 Training Loss: tensor(0.3509)\n",
      "981 Training Loss: tensor(0.3565)\n",
      "982 Training Loss: tensor(0.3489)\n",
      "983 Training Loss: tensor(0.3503)\n",
      "984 Training Loss: tensor(0.3539)\n",
      "985 Training Loss: tensor(0.3501)\n",
      "986 Training Loss: tensor(0.3533)\n",
      "987 Training Loss: tensor(0.3551)\n",
      "988 Training Loss: tensor(0.3492)\n",
      "989 Training Loss: tensor(0.3523)\n",
      "990 Training Loss: tensor(0.3535)\n",
      "991 Training Loss: tensor(0.3523)\n",
      "992 Training Loss: tensor(0.3507)\n",
      "993 Training Loss: tensor(0.3519)\n",
      "994 Training Loss: tensor(0.3501)\n",
      "995 Training Loss: tensor(0.3517)\n",
      "996 Training Loss: tensor(0.3493)\n",
      "997 Training Loss: tensor(0.3509)\n",
      "998 Training Loss: tensor(0.3527)\n",
      "999 Training Loss: tensor(0.3517)\n",
      "1000 Training Loss: tensor(0.3540)\n",
      "1001 Training Loss: tensor(0.3497)\n",
      "1002 Training Loss: tensor(0.3503)\n",
      "1003 Training Loss: tensor(0.3517)\n",
      "1004 Training Loss: tensor(0.3499)\n",
      "1005 Training Loss: tensor(0.3494)\n",
      "1006 Training Loss: tensor(0.3500)\n",
      "1007 Training Loss: tensor(0.3502)\n",
      "1008 Training Loss: tensor(0.3519)\n",
      "1009 Training Loss: tensor(0.3508)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010 Training Loss: tensor(0.3535)\n",
      "1011 Training Loss: tensor(0.3517)\n",
      "1012 Training Loss: tensor(0.3488)\n",
      "1013 Training Loss: tensor(0.3532)\n",
      "1014 Training Loss: tensor(0.3494)\n",
      "1015 Training Loss: tensor(0.3557)\n",
      "1016 Training Loss: tensor(0.3491)\n",
      "1017 Training Loss: tensor(0.3485)\n",
      "1018 Training Loss: tensor(0.3498)\n",
      "1019 Training Loss: tensor(0.3508)\n",
      "1020 Training Loss: tensor(0.3493)\n",
      "1021 Training Loss: tensor(0.3587)\n",
      "1022 Training Loss: tensor(0.3492)\n",
      "1023 Training Loss: tensor(0.3504)\n",
      "1024 Training Loss: tensor(0.3504)\n",
      "1025 Training Loss: tensor(0.3513)\n",
      "1026 Training Loss: tensor(0.3511)\n",
      "1027 Training Loss: tensor(0.3508)\n",
      "1028 Training Loss: tensor(0.3513)\n",
      "1029 Training Loss: tensor(0.3512)\n",
      "1030 Training Loss: tensor(0.3486)\n",
      "1031 Training Loss: tensor(0.3500)\n",
      "1032 Training Loss: tensor(0.3525)\n",
      "1033 Training Loss: tensor(0.3620)\n",
      "1034 Training Loss: tensor(0.3517)\n",
      "1035 Training Loss: tensor(0.3505)\n",
      "1036 Training Loss: tensor(0.3512)\n",
      "1037 Training Loss: tensor(0.3541)\n",
      "1038 Training Loss: tensor(0.3575)\n",
      "1039 Training Loss: tensor(0.3504)\n",
      "1040 Training Loss: tensor(0.3561)\n",
      "1041 Training Loss: tensor(0.3544)\n",
      "1042 Training Loss: tensor(0.3526)\n",
      "1043 Training Loss: tensor(0.3520)\n",
      "1044 Training Loss: tensor(0.3545)\n",
      "1045 Training Loss: tensor(0.3499)\n",
      "1046 Training Loss: tensor(0.3507)\n",
      "1047 Training Loss: tensor(0.3504)\n",
      "1048 Training Loss: tensor(0.3501)\n",
      "1049 Training Loss: tensor(0.3499)\n",
      "1050 Training Loss: tensor(0.3516)\n",
      "1051 Training Loss: tensor(0.3533)\n",
      "1052 Training Loss: tensor(0.3524)\n",
      "1053 Training Loss: tensor(0.3524)\n",
      "1054 Training Loss: tensor(0.3530)\n",
      "1055 Training Loss: tensor(0.3629)\n",
      "1056 Training Loss: tensor(0.3493)\n",
      "1057 Training Loss: tensor(0.3491)\n",
      "1058 Training Loss: tensor(0.3521)\n",
      "1059 Training Loss: tensor(0.3515)\n",
      "1060 Training Loss: tensor(0.3491)\n",
      "1061 Training Loss: tensor(0.3600)\n",
      "1062 Training Loss: tensor(0.3507)\n",
      "1063 Training Loss: tensor(0.3546)\n",
      "1064 Training Loss: tensor(0.3502)\n",
      "1065 Training Loss: tensor(0.3490)\n",
      "1066 Training Loss: tensor(0.3513)\n",
      "1067 Training Loss: tensor(0.3500)\n",
      "1068 Training Loss: tensor(0.3516)\n",
      "1069 Training Loss: tensor(0.3533)\n",
      "1070 Training Loss: tensor(0.3490)\n",
      "1071 Training Loss: tensor(0.3556)\n",
      "1072 Training Loss: tensor(0.3511)\n",
      "1073 Training Loss: tensor(0.3516)\n",
      "1074 Training Loss: tensor(0.3495)\n",
      "1075 Training Loss: tensor(0.3552)\n",
      "1076 Training Loss: tensor(0.3496)\n",
      "1077 Training Loss: tensor(0.3536)\n",
      "1078 Training Loss: tensor(0.3498)\n",
      "1079 Training Loss: tensor(0.3547)\n",
      "1080 Training Loss: tensor(0.3488)\n",
      "1081 Training Loss: tensor(0.3495)\n",
      "1082 Training Loss: tensor(0.3498)\n",
      "1083 Training Loss: tensor(0.3503)\n",
      "1084 Training Loss: tensor(0.3521)\n",
      "1085 Training Loss: tensor(0.3495)\n",
      "1086 Training Loss: tensor(0.3570)\n",
      "1087 Training Loss: tensor(0.3501)\n",
      "1088 Training Loss: tensor(0.3530)\n",
      "1089 Training Loss: tensor(0.3582)\n",
      "1090 Training Loss: tensor(0.3521)\n",
      "1091 Training Loss: tensor(0.3533)\n",
      "1092 Training Loss: tensor(0.3513)\n",
      "1093 Training Loss: tensor(0.3495)\n",
      "1094 Training Loss: tensor(0.3510)\n",
      "1095 Training Loss: tensor(0.3500)\n",
      "1096 Training Loss: tensor(0.3508)\n",
      "1097 Training Loss: tensor(0.3508)\n",
      "1098 Training Loss: tensor(0.3508)\n",
      "1099 Training Loss: tensor(0.3516)\n",
      "1100 Training Loss: tensor(0.3508)\n",
      "1101 Training Loss: tensor(0.3491)\n",
      "1102 Training Loss: tensor(0.3521)\n",
      "1103 Training Loss: tensor(0.3525)\n",
      "1104 Training Loss: tensor(0.3545)\n",
      "1105 Training Loss: tensor(0.3532)\n",
      "1106 Training Loss: tensor(0.3498)\n",
      "1107 Training Loss: tensor(0.3536)\n",
      "1108 Training Loss: tensor(0.3512)\n",
      "1109 Training Loss: tensor(0.3534)\n",
      "1110 Training Loss: tensor(0.3518)\n",
      "1111 Training Loss: tensor(0.3493)\n",
      "1112 Training Loss: tensor(0.3517)\n",
      "1113 Training Loss: tensor(0.3520)\n",
      "1114 Training Loss: tensor(0.3534)\n",
      "1115 Training Loss: tensor(0.3497)\n",
      "1116 Training Loss: tensor(0.3628)\n",
      "1117 Training Loss: tensor(0.3504)\n",
      "1118 Training Loss: tensor(0.3503)\n",
      "1119 Training Loss: tensor(0.3493)\n",
      "1120 Training Loss: tensor(0.3517)\n",
      "1121 Training Loss: tensor(0.3544)\n",
      "1122 Training Loss: tensor(0.3522)\n",
      "1123 Training Loss: tensor(0.3549)\n",
      "1124 Training Loss: tensor(0.3570)\n",
      "1125 Training Loss: tensor(0.3507)\n",
      "1126 Training Loss: tensor(0.3495)\n",
      "1127 Training Loss: tensor(0.3496)\n",
      "1128 Training Loss: tensor(0.3517)\n",
      "1129 Training Loss: tensor(0.3565)\n",
      "1130 Training Loss: tensor(0.3511)\n",
      "1131 Training Loss: tensor(0.3496)\n",
      "1132 Training Loss: tensor(0.3553)\n",
      "1133 Training Loss: tensor(0.3506)\n",
      "1134 Training Loss: tensor(0.3510)\n",
      "1135 Training Loss: tensor(0.3504)\n",
      "1136 Training Loss: tensor(0.3564)\n",
      "1137 Training Loss: tensor(0.3505)\n",
      "1138 Training Loss: tensor(0.3529)\n",
      "1139 Training Loss: tensor(0.3538)\n",
      "1140 Training Loss: tensor(0.3519)\n",
      "1141 Training Loss: tensor(0.3541)\n",
      "1142 Training Loss: tensor(0.3502)\n",
      "1143 Training Loss: tensor(0.3489)\n",
      "1144 Training Loss: tensor(0.3503)\n",
      "1145 Training Loss: tensor(0.3502)\n",
      "1146 Training Loss: tensor(0.3532)\n",
      "1147 Training Loss: tensor(0.3527)\n",
      "1148 Training Loss: tensor(0.3539)\n",
      "1149 Training Loss: tensor(0.3577)\n",
      "1150 Training Loss: tensor(0.3510)\n",
      "1151 Training Loss: tensor(0.3582)\n",
      "1152 Training Loss: tensor(0.3490)\n",
      "1153 Training Loss: tensor(0.3566)\n",
      "1154 Training Loss: tensor(0.3508)\n",
      "1155 Training Loss: tensor(0.3514)\n",
      "1156 Training Loss: tensor(0.3546)\n",
      "1157 Training Loss: tensor(0.3512)\n",
      "1158 Training Loss: tensor(0.3509)\n",
      "1159 Training Loss: tensor(0.3519)\n",
      "1160 Training Loss: tensor(0.3536)\n",
      "1161 Training Loss: tensor(0.3542)\n",
      "1162 Training Loss: tensor(0.3544)\n",
      "1163 Training Loss: tensor(0.3501)\n",
      "1164 Training Loss: tensor(0.3506)\n",
      "1165 Training Loss: tensor(0.3497)\n",
      "1166 Training Loss: tensor(0.3507)\n",
      "1167 Training Loss: tensor(0.3513)\n",
      "1168 Training Loss: tensor(0.3519)\n",
      "1169 Training Loss: tensor(0.3518)\n",
      "1170 Training Loss: tensor(0.3504)\n",
      "1171 Training Loss: tensor(0.3503)\n",
      "1172 Training Loss: tensor(0.3511)\n",
      "1173 Training Loss: tensor(0.3501)\n",
      "1174 Training Loss: tensor(0.3488)\n",
      "1175 Training Loss: tensor(0.3577)\n",
      "1176 Training Loss: tensor(0.3483)\n",
      "1177 Training Loss: tensor(0.3490)\n",
      "1178 Training Loss: tensor(0.3512)\n",
      "1179 Training Loss: tensor(0.3556)\n",
      "1180 Training Loss: tensor(0.3493)\n",
      "1181 Training Loss: tensor(0.3572)\n",
      "1182 Training Loss: tensor(0.3613)\n",
      "1183 Training Loss: tensor(0.3513)\n",
      "1184 Training Loss: tensor(0.3496)\n",
      "1185 Training Loss: tensor(0.3517)\n",
      "1186 Training Loss: tensor(0.3543)\n",
      "1187 Training Loss: tensor(0.3497)\n",
      "1188 Training Loss: tensor(0.3495)\n",
      "1189 Training Loss: tensor(0.3492)\n",
      "1190 Training Loss: tensor(0.3487)\n",
      "1191 Training Loss: tensor(0.3507)\n",
      "1192 Training Loss: tensor(0.3503)\n",
      "1193 Training Loss: tensor(0.3530)\n",
      "1194 Training Loss: tensor(0.3494)\n",
      "1195 Training Loss: tensor(0.3571)\n",
      "1196 Training Loss: tensor(0.3517)\n",
      "1197 Training Loss: tensor(0.3525)\n",
      "1198 Training Loss: tensor(0.3494)\n",
      "1199 Training Loss: tensor(0.3508)\n",
      "1200 Training Loss: tensor(0.3533)\n",
      "1201 Training Loss: tensor(0.3505)\n",
      "1202 Training Loss: tensor(0.3525)\n",
      "1203 Training Loss: tensor(0.3500)\n",
      "1204 Training Loss: tensor(0.3542)\n",
      "1205 Training Loss: tensor(0.3503)\n",
      "1206 Training Loss: tensor(0.3521)\n",
      "1207 Training Loss: tensor(0.3508)\n",
      "1208 Training Loss: tensor(0.3522)\n",
      "1209 Training Loss: tensor(0.3533)\n",
      "1210 Training Loss: tensor(0.3494)\n",
      "1211 Training Loss: tensor(0.3487)\n",
      "1212 Training Loss: tensor(0.3544)\n",
      "1213 Training Loss: tensor(0.3580)\n",
      "1214 Training Loss: tensor(0.3497)\n",
      "1215 Training Loss: tensor(0.3612)\n",
      "1216 Training Loss: tensor(0.3490)\n",
      "1217 Training Loss: tensor(0.3576)\n",
      "1218 Training Loss: tensor(0.3501)\n",
      "1219 Training Loss: tensor(0.3517)\n",
      "1220 Training Loss: tensor(0.3507)\n",
      "1221 Training Loss: tensor(0.3507)\n",
      "1222 Training Loss: tensor(0.3501)\n",
      "1223 Training Loss: tensor(0.3500)\n",
      "1224 Training Loss: tensor(0.3509)\n",
      "1225 Training Loss: tensor(0.3528)\n",
      "1226 Training Loss: tensor(0.3522)\n",
      "1227 Training Loss: tensor(0.3497)\n",
      "1228 Training Loss: tensor(0.3493)\n",
      "1229 Training Loss: tensor(0.3486)\n",
      "1230 Training Loss: tensor(0.3509)\n",
      "1231 Training Loss: tensor(0.3492)\n",
      "1232 Training Loss: tensor(0.3490)\n",
      "1233 Training Loss: tensor(0.3491)\n",
      "1234 Training Loss: tensor(0.3490)\n",
      "1235 Training Loss: tensor(0.3516)\n",
      "1236 Training Loss: tensor(0.3485)\n",
      "1237 Training Loss: tensor(0.3494)\n",
      "1238 Training Loss: tensor(0.3577)\n",
      "1239 Training Loss: tensor(0.3525)\n",
      "1240 Training Loss: tensor(0.3487)\n",
      "1241 Training Loss: tensor(0.3499)\n",
      "1242 Training Loss: tensor(0.3504)\n",
      "1243 Training Loss: tensor(0.3516)\n",
      "1244 Training Loss: tensor(0.3586)\n",
      "1245 Training Loss: tensor(0.3526)\n",
      "1246 Training Loss: tensor(0.3532)\n",
      "1247 Training Loss: tensor(0.3513)\n",
      "1248 Training Loss: tensor(0.3512)\n",
      "1249 Training Loss: tensor(0.3492)\n",
      "1250 Training Loss: tensor(0.3511)\n",
      "1251 Training Loss: tensor(0.3507)\n",
      "1252 Training Loss: tensor(0.3528)\n",
      "1253 Training Loss: tensor(0.3496)\n",
      "1254 Training Loss: tensor(0.3502)\n",
      "1255 Training Loss: tensor(0.3525)\n",
      "1256 Training Loss: tensor(0.3535)\n",
      "1257 Training Loss: tensor(0.3535)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258 Training Loss: tensor(0.3507)\n",
      "1259 Training Loss: tensor(0.3510)\n",
      "1260 Training Loss: tensor(0.3535)\n",
      "1261 Training Loss: tensor(0.3510)\n",
      "1262 Training Loss: tensor(0.3510)\n",
      "1263 Training Loss: tensor(0.3494)\n",
      "1264 Training Loss: tensor(0.3606)\n",
      "1265 Training Loss: tensor(0.3523)\n",
      "1266 Training Loss: tensor(0.3489)\n",
      "1267 Training Loss: tensor(0.3503)\n",
      "1268 Training Loss: tensor(0.3489)\n",
      "1269 Training Loss: tensor(0.3489)\n",
      "1270 Training Loss: tensor(0.3498)\n",
      "1271 Training Loss: tensor(0.3497)\n",
      "1272 Training Loss: tensor(0.3482)\n",
      "1273 Training Loss: tensor(0.3538)\n",
      "1274 Training Loss: tensor(0.3516)\n",
      "1275 Training Loss: tensor(0.3492)\n",
      "1276 Training Loss: tensor(0.3535)\n",
      "1277 Training Loss: tensor(0.3493)\n",
      "1278 Training Loss: tensor(0.3511)\n",
      "1279 Training Loss: tensor(0.3490)\n",
      "1280 Training Loss: tensor(0.3522)\n",
      "1281 Training Loss: tensor(0.3610)\n",
      "1282 Training Loss: tensor(0.3537)\n",
      "1283 Training Loss: tensor(0.3491)\n",
      "1284 Training Loss: tensor(0.3518)\n",
      "1285 Training Loss: tensor(0.3492)\n",
      "1286 Training Loss: tensor(0.3522)\n",
      "1287 Training Loss: tensor(0.3489)\n",
      "1288 Training Loss: tensor(0.3489)\n",
      "1289 Training Loss: tensor(0.3494)\n",
      "1290 Training Loss: tensor(0.3550)\n",
      "1291 Training Loss: tensor(0.3520)\n",
      "1292 Training Loss: tensor(0.3540)\n",
      "1293 Training Loss: tensor(0.3510)\n",
      "1294 Training Loss: tensor(0.3501)\n",
      "1295 Training Loss: tensor(0.3553)\n",
      "1296 Training Loss: tensor(0.3545)\n",
      "1297 Training Loss: tensor(0.3503)\n",
      "1298 Training Loss: tensor(0.3519)\n",
      "1299 Training Loss: tensor(0.3527)\n",
      "1300 Training Loss: tensor(0.3508)\n",
      "1301 Training Loss: tensor(0.3549)\n",
      "1302 Training Loss: tensor(0.3523)\n",
      "1303 Training Loss: tensor(0.3622)\n",
      "1304 Training Loss: tensor(0.3502)\n",
      "1305 Training Loss: tensor(0.3559)\n",
      "1306 Training Loss: tensor(0.3507)\n",
      "1307 Training Loss: tensor(0.3515)\n",
      "1308 Training Loss: tensor(0.3519)\n",
      "1309 Training Loss: tensor(0.3527)\n",
      "1310 Training Loss: tensor(0.3510)\n",
      "1311 Training Loss: tensor(0.3508)\n",
      "1312 Training Loss: tensor(0.3536)\n",
      "1313 Training Loss: tensor(0.3560)\n",
      "1314 Training Loss: tensor(0.3509)\n",
      "1315 Training Loss: tensor(0.3507)\n",
      "1316 Training Loss: tensor(0.3510)\n",
      "1317 Training Loss: tensor(0.3520)\n",
      "1318 Training Loss: tensor(0.3500)\n",
      "1319 Training Loss: tensor(0.3500)\n",
      "1320 Training Loss: tensor(0.3496)\n",
      "1321 Training Loss: tensor(0.3608)\n",
      "1322 Training Loss: tensor(0.3538)\n",
      "1323 Training Loss: tensor(0.3500)\n",
      "1324 Training Loss: tensor(0.3499)\n",
      "1325 Training Loss: tensor(0.3496)\n",
      "1326 Training Loss: tensor(0.3496)\n",
      "1327 Training Loss: tensor(0.3486)\n",
      "1328 Training Loss: tensor(0.3493)\n",
      "1329 Training Loss: tensor(0.3579)\n",
      "1330 Training Loss: tensor(0.3513)\n",
      "1331 Training Loss: tensor(0.3528)\n",
      "1332 Training Loss: tensor(0.3588)\n",
      "1333 Training Loss: tensor(0.3543)\n",
      "1334 Training Loss: tensor(0.3485)\n",
      "1335 Training Loss: tensor(0.3559)\n",
      "1336 Training Loss: tensor(0.3496)\n",
      "1337 Training Loss: tensor(0.3523)\n",
      "1338 Training Loss: tensor(0.3523)\n",
      "1339 Training Loss: tensor(0.3523)\n",
      "1340 Training Loss: tensor(0.3549)\n",
      "1341 Training Loss: tensor(0.3557)\n",
      "1342 Training Loss: tensor(0.3500)\n",
      "1343 Training Loss: tensor(0.3505)\n",
      "1344 Training Loss: tensor(0.3504)\n",
      "1345 Training Loss: tensor(0.3545)\n",
      "1346 Training Loss: tensor(0.3527)\n",
      "1347 Training Loss: tensor(0.3551)\n",
      "1348 Training Loss: tensor(0.3503)\n",
      "1349 Training Loss: tensor(0.3507)\n",
      "1350 Training Loss: tensor(0.3523)\n",
      "1351 Training Loss: tensor(0.3507)\n",
      "1352 Training Loss: tensor(0.3538)\n",
      "1353 Training Loss: tensor(0.3510)\n",
      "1354 Training Loss: tensor(0.3498)\n",
      "1355 Training Loss: tensor(0.3540)\n",
      "1356 Training Loss: tensor(0.3504)\n",
      "1357 Training Loss: tensor(0.3502)\n",
      "1358 Training Loss: tensor(0.3507)\n",
      "1359 Training Loss: tensor(0.3507)\n",
      "1360 Training Loss: tensor(0.3498)\n",
      "1361 Training Loss: tensor(0.3499)\n",
      "1362 Training Loss: tensor(0.3505)\n",
      "1363 Training Loss: tensor(0.3505)\n",
      "1364 Training Loss: tensor(0.3494)\n",
      "1365 Training Loss: tensor(0.3523)\n",
      "1366 Training Loss: tensor(0.3498)\n",
      "1367 Training Loss: tensor(0.3488)\n",
      "1368 Training Loss: tensor(0.3488)\n",
      "1369 Training Loss: tensor(0.3538)\n",
      "1370 Training Loss: tensor(0.3605)\n",
      "1371 Training Loss: tensor(0.3659)\n",
      "1372 Training Loss: tensor(0.3500)\n",
      "1373 Training Loss: tensor(0.3518)\n",
      "1374 Training Loss: tensor(0.3543)\n",
      "1375 Training Loss: tensor(0.3545)\n",
      "1376 Training Loss: tensor(0.3502)\n",
      "1377 Training Loss: tensor(0.3512)\n",
      "1378 Training Loss: tensor(0.3516)\n",
      "1379 Training Loss: tensor(0.3521)\n",
      "1380 Training Loss: tensor(0.3521)\n",
      "1381 Training Loss: tensor(0.3548)\n",
      "1382 Training Loss: tensor(0.3563)\n",
      "1383 Training Loss: tensor(0.3551)\n",
      "1384 Training Loss: tensor(0.3519)\n",
      "1385 Training Loss: tensor(0.3536)\n",
      "1386 Training Loss: tensor(0.3518)\n",
      "1387 Training Loss: tensor(0.3527)\n",
      "1388 Training Loss: tensor(0.3565)\n",
      "1389 Training Loss: tensor(0.3510)\n",
      "1390 Training Loss: tensor(0.3528)\n",
      "1391 Training Loss: tensor(0.3532)\n",
      "1392 Training Loss: tensor(0.3516)\n",
      "1393 Training Loss: tensor(0.3509)\n",
      "1394 Training Loss: tensor(0.3516)\n",
      "1395 Training Loss: tensor(0.3535)\n",
      "1396 Training Loss: tensor(0.3520)\n",
      "1397 Training Loss: tensor(0.3508)\n",
      "1398 Training Loss: tensor(0.3542)\n",
      "1399 Training Loss: tensor(0.3518)\n",
      "1400 Training Loss: tensor(0.3498)\n",
      "1401 Training Loss: tensor(0.3568)\n",
      "1402 Training Loss: tensor(0.3544)\n",
      "1403 Training Loss: tensor(0.3495)\n",
      "1404 Training Loss: tensor(0.3519)\n",
      "1405 Training Loss: tensor(0.3577)\n",
      "1406 Training Loss: tensor(0.3503)\n",
      "1407 Training Loss: tensor(0.3535)\n",
      "1408 Training Loss: tensor(0.3592)\n",
      "1409 Training Loss: tensor(0.3546)\n",
      "1410 Training Loss: tensor(0.3505)\n",
      "1411 Training Loss: tensor(0.3508)\n",
      "1412 Training Loss: tensor(0.3523)\n",
      "1413 Training Loss: tensor(0.3518)\n",
      "1414 Training Loss: tensor(0.3519)\n",
      "1415 Training Loss: tensor(0.3543)\n",
      "1416 Training Loss: tensor(0.3508)\n",
      "1417 Training Loss: tensor(0.3528)\n",
      "1418 Training Loss: tensor(0.3527)\n",
      "1419 Training Loss: tensor(0.3509)\n",
      "1420 Training Loss: tensor(0.3543)\n",
      "1421 Training Loss: tensor(0.3515)\n",
      "1422 Training Loss: tensor(0.3538)\n",
      "1423 Training Loss: tensor(0.3510)\n",
      "1424 Training Loss: tensor(0.3526)\n",
      "1425 Training Loss: tensor(0.3503)\n",
      "1426 Training Loss: tensor(0.3507)\n",
      "1427 Training Loss: tensor(0.3510)\n",
      "1428 Training Loss: tensor(0.3535)\n",
      "1429 Training Loss: tensor(0.3561)\n",
      "1430 Training Loss: tensor(0.3512)\n",
      "1431 Training Loss: tensor(0.3533)\n",
      "1432 Training Loss: tensor(0.3571)\n",
      "1433 Training Loss: tensor(0.3500)\n",
      "1434 Training Loss: tensor(0.3500)\n",
      "1435 Training Loss: tensor(0.3511)\n",
      "1436 Training Loss: tensor(0.3539)\n",
      "1437 Training Loss: tensor(0.3551)\n",
      "1438 Training Loss: tensor(0.3532)\n",
      "1439 Training Loss: tensor(0.3506)\n",
      "1440 Training Loss: tensor(0.3505)\n",
      "1441 Training Loss: tensor(0.3509)\n",
      "1442 Training Loss: tensor(0.3506)\n",
      "1443 Training Loss: tensor(0.3544)\n",
      "1444 Training Loss: tensor(0.3543)\n",
      "1445 Training Loss: tensor(0.3535)\n",
      "1446 Training Loss: tensor(0.3524)\n",
      "1447 Training Loss: tensor(0.3498)\n",
      "1448 Training Loss: tensor(0.3507)\n",
      "1449 Training Loss: tensor(0.3514)\n",
      "1450 Training Loss: tensor(0.3514)\n",
      "1451 Training Loss: tensor(0.3560)\n",
      "1452 Training Loss: tensor(0.3510)\n",
      "1453 Training Loss: tensor(0.3505)\n",
      "1454 Training Loss: tensor(0.3523)\n",
      "1455 Training Loss: tensor(0.3486)\n",
      "1456 Training Loss: tensor(0.3519)\n",
      "1457 Training Loss: tensor(0.3498)\n",
      "1458 Training Loss: tensor(0.3511)\n",
      "1459 Training Loss: tensor(0.3504)\n",
      "1460 Training Loss: tensor(0.3507)\n",
      "1461 Training Loss: tensor(0.3500)\n",
      "1462 Training Loss: tensor(0.3501)\n",
      "1463 Training Loss: tensor(0.3537)\n",
      "1464 Training Loss: tensor(0.3562)\n",
      "1465 Training Loss: tensor(0.3616)\n",
      "1466 Training Loss: tensor(0.3534)\n",
      "1467 Training Loss: tensor(0.3493)\n",
      "1468 Training Loss: tensor(0.3505)\n",
      "1469 Training Loss: tensor(0.3533)\n",
      "1470 Training Loss: tensor(0.3559)\n",
      "1471 Training Loss: tensor(0.3505)\n",
      "1472 Training Loss: tensor(0.3500)\n",
      "1473 Training Loss: tensor(0.3514)\n",
      "1474 Training Loss: tensor(0.3515)\n",
      "1475 Training Loss: tensor(0.3504)\n",
      "1476 Training Loss: tensor(0.3513)\n",
      "1477 Training Loss: tensor(0.3501)\n",
      "1478 Training Loss: tensor(0.3506)\n",
      "1479 Training Loss: tensor(0.3494)\n",
      "1480 Training Loss: tensor(0.3496)\n",
      "1481 Training Loss: tensor(0.3535)\n",
      "1482 Training Loss: tensor(0.3504)\n",
      "1483 Training Loss: tensor(0.3542)\n",
      "1484 Training Loss: tensor(0.3542)\n",
      "1485 Training Loss: tensor(0.3534)\n",
      "1486 Training Loss: tensor(0.3530)\n",
      "1487 Training Loss: tensor(0.3516)\n",
      "1488 Training Loss: tensor(0.3613)\n",
      "1489 Training Loss: tensor(0.3513)\n",
      "1490 Training Loss: tensor(0.3489)\n",
      "1491 Training Loss: tensor(0.3526)\n",
      "1492 Training Loss: tensor(0.3501)\n",
      "1493 Training Loss: tensor(0.3510)\n",
      "1494 Training Loss: tensor(0.3537)\n",
      "1495 Training Loss: tensor(0.3510)\n",
      "1496 Training Loss: tensor(0.3519)\n",
      "1497 Training Loss: tensor(0.3496)\n",
      "1498 Training Loss: tensor(0.3499)\n",
      "1499 Training Loss: tensor(0.3511)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 Training Loss: tensor(0.3516)\n",
      "1501 Training Loss: tensor(0.3499)\n",
      "1502 Training Loss: tensor(0.3517)\n",
      "1503 Training Loss: tensor(0.3505)\n",
      "1504 Training Loss: tensor(0.3506)\n",
      "1505 Training Loss: tensor(0.3528)\n",
      "1506 Training Loss: tensor(0.3551)\n",
      "1507 Training Loss: tensor(0.3513)\n",
      "1508 Training Loss: tensor(0.3495)\n",
      "1509 Training Loss: tensor(0.3502)\n",
      "1510 Training Loss: tensor(0.3531)\n",
      "1511 Training Loss: tensor(0.3531)\n",
      "1512 Training Loss: tensor(0.3521)\n",
      "1513 Training Loss: tensor(0.3515)\n",
      "1514 Training Loss: tensor(0.3502)\n",
      "1515 Training Loss: tensor(0.3529)\n",
      "1516 Training Loss: tensor(0.3508)\n",
      "1517 Training Loss: tensor(0.3502)\n",
      "1518 Training Loss: tensor(0.3512)\n",
      "1519 Training Loss: tensor(0.3490)\n",
      "1520 Training Loss: tensor(0.3512)\n",
      "1521 Training Loss: tensor(0.3491)\n",
      "1522 Training Loss: tensor(0.3556)\n",
      "1523 Training Loss: tensor(0.3507)\n",
      "1524 Training Loss: tensor(0.3522)\n",
      "1525 Training Loss: tensor(0.3502)\n",
      "1526 Training Loss: tensor(0.3487)\n",
      "1527 Training Loss: tensor(0.3546)\n",
      "1528 Training Loss: tensor(0.3495)\n",
      "1529 Training Loss: tensor(0.3537)\n",
      "1530 Training Loss: tensor(0.3511)\n",
      "1531 Training Loss: tensor(0.3504)\n",
      "1532 Training Loss: tensor(0.3516)\n",
      "1533 Training Loss: tensor(0.3545)\n",
      "1534 Training Loss: tensor(0.3580)\n",
      "1535 Training Loss: tensor(0.3528)\n",
      "1536 Training Loss: tensor(0.3505)\n",
      "1537 Training Loss: tensor(0.3504)\n",
      "1538 Training Loss: tensor(0.3498)\n",
      "1539 Training Loss: tensor(0.3512)\n",
      "1540 Training Loss: tensor(0.3489)\n",
      "1541 Training Loss: tensor(0.3505)\n",
      "1542 Training Loss: tensor(0.3508)\n",
      "1543 Training Loss: tensor(0.3545)\n",
      "1544 Training Loss: tensor(0.3495)\n",
      "1545 Training Loss: tensor(0.3547)\n",
      "1546 Training Loss: tensor(0.3556)\n",
      "1547 Training Loss: tensor(0.3564)\n",
      "1548 Training Loss: tensor(0.3527)\n",
      "1549 Training Loss: tensor(0.3496)\n",
      "1550 Training Loss: tensor(0.3510)\n",
      "1551 Training Loss: tensor(0.3493)\n",
      "1552 Training Loss: tensor(0.3547)\n",
      "1553 Training Loss: tensor(0.3537)\n",
      "1554 Training Loss: tensor(0.3514)\n",
      "1555 Training Loss: tensor(0.3501)\n",
      "1556 Training Loss: tensor(0.3492)\n",
      "1557 Training Loss: tensor(0.3522)\n",
      "1558 Training Loss: tensor(0.3522)\n",
      "1559 Training Loss: tensor(0.3529)\n",
      "1560 Training Loss: tensor(0.3496)\n",
      "1561 Training Loss: tensor(0.3498)\n",
      "1562 Training Loss: tensor(0.3511)\n",
      "1563 Training Loss: tensor(0.3505)\n",
      "1564 Training Loss: tensor(0.3500)\n",
      "1565 Training Loss: tensor(0.3558)\n",
      "1566 Training Loss: tensor(0.3494)\n",
      "1567 Training Loss: tensor(0.3495)\n",
      "1568 Training Loss: tensor(0.3564)\n",
      "1569 Training Loss: tensor(0.3528)\n",
      "1570 Training Loss: tensor(0.3544)\n",
      "1571 Training Loss: tensor(0.3507)\n",
      "1572 Training Loss: tensor(0.3531)\n",
      "1573 Training Loss: tensor(0.3505)\n",
      "1574 Training Loss: tensor(0.3493)\n",
      "1575 Training Loss: tensor(0.3502)\n",
      "1576 Training Loss: tensor(0.3507)\n",
      "1577 Training Loss: tensor(0.3496)\n",
      "1578 Training Loss: tensor(0.3512)\n",
      "1579 Training Loss: tensor(0.3496)\n",
      "1580 Training Loss: tensor(0.3536)\n",
      "1581 Training Loss: tensor(0.3511)\n",
      "1582 Training Loss: tensor(0.3556)\n",
      "1583 Training Loss: tensor(0.3497)\n",
      "1584 Training Loss: tensor(0.3513)\n",
      "1585 Training Loss: tensor(0.3531)\n",
      "1586 Training Loss: tensor(0.3503)\n",
      "1587 Training Loss: tensor(0.3590)\n",
      "1588 Training Loss: tensor(0.3499)\n",
      "1589 Training Loss: tensor(0.3524)\n",
      "1590 Training Loss: tensor(0.3535)\n",
      "1591 Training Loss: tensor(0.3500)\n",
      "1592 Training Loss: tensor(0.3516)\n",
      "1593 Training Loss: tensor(0.3501)\n",
      "1594 Training Loss: tensor(0.3503)\n",
      "1595 Training Loss: tensor(0.3500)\n",
      "1596 Training Loss: tensor(0.3521)\n",
      "1597 Training Loss: tensor(0.3537)\n",
      "1598 Training Loss: tensor(0.3505)\n",
      "1599 Training Loss: tensor(0.3530)\n",
      "1600 Training Loss: tensor(0.3492)\n",
      "1601 Training Loss: tensor(0.3499)\n",
      "1602 Training Loss: tensor(0.3513)\n",
      "1603 Training Loss: tensor(0.3575)\n",
      "1604 Training Loss: tensor(0.3490)\n",
      "1605 Training Loss: tensor(0.3566)\n",
      "1606 Training Loss: tensor(0.3542)\n",
      "1607 Training Loss: tensor(0.3594)\n",
      "1608 Training Loss: tensor(0.3499)\n",
      "1609 Training Loss: tensor(0.3559)\n",
      "1610 Training Loss: tensor(0.3512)\n",
      "1611 Training Loss: tensor(0.3524)\n",
      "1612 Training Loss: tensor(0.3520)\n",
      "1613 Training Loss: tensor(0.3527)\n",
      "1614 Training Loss: tensor(0.3526)\n",
      "1615 Training Loss: tensor(0.3502)\n",
      "1616 Training Loss: tensor(0.3514)\n",
      "1617 Training Loss: tensor(0.3532)\n",
      "1618 Training Loss: tensor(0.3560)\n",
      "1619 Training Loss: tensor(0.3511)\n",
      "1620 Training Loss: tensor(0.3505)\n",
      "1621 Training Loss: tensor(0.3527)\n",
      "1622 Training Loss: tensor(0.3577)\n",
      "1623 Training Loss: tensor(0.3520)\n",
      "1624 Training Loss: tensor(0.3501)\n",
      "1625 Training Loss: tensor(0.3500)\n",
      "1626 Training Loss: tensor(0.3509)\n",
      "1627 Training Loss: tensor(0.3539)\n",
      "1628 Training Loss: tensor(0.3535)\n",
      "1629 Training Loss: tensor(0.3514)\n",
      "1630 Training Loss: tensor(0.3501)\n",
      "1631 Training Loss: tensor(0.3495)\n",
      "1632 Training Loss: tensor(0.3526)\n",
      "1633 Training Loss: tensor(0.3508)\n",
      "1634 Training Loss: tensor(0.3491)\n",
      "1635 Training Loss: tensor(0.3526)\n",
      "1636 Training Loss: tensor(0.3539)\n",
      "1637 Training Loss: tensor(0.3531)\n",
      "1638 Training Loss: tensor(0.3495)\n",
      "1639 Training Loss: tensor(0.3505)\n",
      "1640 Training Loss: tensor(0.3488)\n",
      "1641 Training Loss: tensor(0.3524)\n",
      "1642 Training Loss: tensor(0.3559)\n",
      "1643 Training Loss: tensor(0.3493)\n",
      "1644 Training Loss: tensor(0.3490)\n",
      "1645 Training Loss: tensor(0.3525)\n",
      "1646 Training Loss: tensor(0.3499)\n",
      "1647 Training Loss: tensor(0.3557)\n",
      "1648 Training Loss: tensor(0.3555)\n",
      "1649 Training Loss: tensor(0.3543)\n",
      "1650 Training Loss: tensor(0.3569)\n",
      "1651 Training Loss: tensor(0.3551)\n",
      "1652 Training Loss: tensor(0.3513)\n",
      "1653 Training Loss: tensor(0.3529)\n",
      "1654 Training Loss: tensor(0.3513)\n",
      "1655 Training Loss: tensor(0.3545)\n",
      "1656 Training Loss: tensor(0.3509)\n",
      "1657 Training Loss: tensor(0.3536)\n",
      "1658 Training Loss: tensor(0.3509)\n",
      "1659 Training Loss: tensor(0.3524)\n",
      "1660 Training Loss: tensor(0.3516)\n",
      "1661 Training Loss: tensor(0.3535)\n",
      "1662 Training Loss: tensor(0.3506)\n",
      "1663 Training Loss: tensor(0.3557)\n",
      "1664 Training Loss: tensor(0.3508)\n",
      "1665 Training Loss: tensor(0.3505)\n",
      "1666 Training Loss: tensor(0.3532)\n",
      "1667 Training Loss: tensor(0.3517)\n",
      "1668 Training Loss: tensor(0.3505)\n",
      "1669 Training Loss: tensor(0.3504)\n",
      "1670 Training Loss: tensor(0.3498)\n",
      "1671 Training Loss: tensor(0.3504)\n",
      "1672 Training Loss: tensor(0.3526)\n",
      "1673 Training Loss: tensor(0.3494)\n",
      "1674 Training Loss: tensor(0.3511)\n",
      "1675 Training Loss: tensor(0.3511)\n",
      "1676 Training Loss: tensor(0.3504)\n",
      "1677 Training Loss: tensor(0.3547)\n",
      "1678 Training Loss: tensor(0.3482)\n",
      "1679 Training Loss: tensor(0.3481)\n",
      "1680 Training Loss: tensor(0.3490)\n",
      "1681 Training Loss: tensor(0.3500)\n",
      "1682 Training Loss: tensor(0.3494)\n",
      "1683 Training Loss: tensor(0.3490)\n",
      "1684 Training Loss: tensor(0.3559)\n",
      "1685 Training Loss: tensor(0.3493)\n",
      "1686 Training Loss: tensor(0.3494)\n",
      "1687 Training Loss: tensor(0.3578)\n",
      "1688 Training Loss: tensor(0.3514)\n",
      "1689 Training Loss: tensor(0.3524)\n",
      "1690 Training Loss: tensor(0.3539)\n",
      "1691 Training Loss: tensor(0.3492)\n",
      "1692 Training Loss: tensor(0.3597)\n",
      "1693 Training Loss: tensor(0.3528)\n",
      "1694 Training Loss: tensor(0.3523)\n",
      "1695 Training Loss: tensor(0.3499)\n",
      "1696 Training Loss: tensor(0.3503)\n",
      "1697 Training Loss: tensor(0.3497)\n",
      "1698 Training Loss: tensor(0.3498)\n",
      "1699 Training Loss: tensor(0.3546)\n",
      "1700 Training Loss: tensor(0.3503)\n",
      "1701 Training Loss: tensor(0.3544)\n",
      "1702 Training Loss: tensor(0.3503)\n",
      "1703 Training Loss: tensor(0.3497)\n",
      "1704 Training Loss: tensor(0.3533)\n",
      "1705 Training Loss: tensor(0.3534)\n",
      "1706 Training Loss: tensor(0.3507)\n",
      "1707 Training Loss: tensor(0.3504)\n",
      "1708 Training Loss: tensor(0.3530)\n",
      "1709 Training Loss: tensor(0.3503)\n",
      "1710 Training Loss: tensor(0.3506)\n",
      "1711 Training Loss: tensor(0.3495)\n",
      "1712 Training Loss: tensor(0.3508)\n",
      "1713 Training Loss: tensor(0.3495)\n",
      "1714 Training Loss: tensor(0.3484)\n",
      "1715 Training Loss: tensor(0.3492)\n",
      "1716 Training Loss: tensor(0.3561)\n",
      "1717 Training Loss: tensor(0.3628)\n",
      "1718 Training Loss: tensor(0.3496)\n",
      "1719 Training Loss: tensor(0.3494)\n",
      "1720 Training Loss: tensor(0.3530)\n",
      "1721 Training Loss: tensor(0.3502)\n",
      "1722 Training Loss: tensor(0.3569)\n",
      "1723 Training Loss: tensor(0.3547)\n",
      "1724 Training Loss: tensor(0.3571)\n",
      "1725 Training Loss: tensor(0.3507)\n",
      "1726 Training Loss: tensor(0.3508)\n",
      "1727 Training Loss: tensor(0.3500)\n",
      "1728 Training Loss: tensor(0.3555)\n",
      "1729 Training Loss: tensor(0.3508)\n",
      "1730 Training Loss: tensor(0.3502)\n",
      "1731 Training Loss: tensor(0.3502)\n",
      "1732 Training Loss: tensor(0.3505)\n",
      "1733 Training Loss: tensor(0.3515)\n",
      "1734 Training Loss: tensor(0.3524)\n",
      "1735 Training Loss: tensor(0.3500)\n",
      "1736 Training Loss: tensor(0.3553)\n",
      "1737 Training Loss: tensor(0.3504)\n",
      "1738 Training Loss: tensor(0.3488)\n",
      "1739 Training Loss: tensor(0.3584)\n",
      "1740 Training Loss: tensor(0.3528)\n",
      "1741 Training Loss: tensor(0.3524)\n",
      "1742 Training Loss: tensor(0.3520)\n",
      "1743 Training Loss: tensor(0.3504)\n",
      "1744 Training Loss: tensor(0.3497)\n",
      "1745 Training Loss: tensor(0.3507)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1746 Training Loss: tensor(0.3517)\n",
      "1747 Training Loss: tensor(0.3508)\n",
      "1748 Training Loss: tensor(0.3545)\n",
      "1749 Training Loss: tensor(0.3502)\n",
      "1750 Training Loss: tensor(0.3490)\n",
      "1751 Training Loss: tensor(0.3511)\n",
      "1752 Training Loss: tensor(0.3549)\n",
      "1753 Training Loss: tensor(0.3527)\n",
      "1754 Training Loss: tensor(0.3552)\n",
      "1755 Training Loss: tensor(0.3501)\n",
      "1756 Training Loss: tensor(0.3490)\n",
      "1757 Training Loss: tensor(0.3528)\n",
      "1758 Training Loss: tensor(0.3540)\n",
      "1759 Training Loss: tensor(0.3507)\n",
      "1760 Training Loss: tensor(0.3547)\n",
      "1761 Training Loss: tensor(0.3512)\n",
      "1762 Training Loss: tensor(0.3509)\n",
      "1763 Training Loss: tensor(0.3489)\n",
      "1764 Training Loss: tensor(0.3493)\n",
      "1765 Training Loss: tensor(0.3498)\n",
      "1766 Training Loss: tensor(0.3528)\n",
      "1767 Training Loss: tensor(0.3516)\n",
      "1768 Training Loss: tensor(0.3530)\n",
      "1769 Training Loss: tensor(0.3523)\n",
      "1770 Training Loss: tensor(0.3504)\n",
      "1771 Training Loss: tensor(0.3495)\n",
      "1772 Training Loss: tensor(0.3511)\n",
      "1773 Training Loss: tensor(0.3508)\n",
      "1774 Training Loss: tensor(0.3488)\n",
      "1775 Training Loss: tensor(0.3502)\n",
      "1776 Training Loss: tensor(0.3523)\n",
      "1777 Training Loss: tensor(0.3498)\n",
      "1778 Training Loss: tensor(0.3518)\n",
      "1779 Training Loss: tensor(0.3536)\n",
      "1780 Training Loss: tensor(0.3508)\n",
      "1781 Training Loss: tensor(0.3523)\n",
      "1782 Training Loss: tensor(0.3487)\n",
      "1783 Training Loss: tensor(0.3478)\n",
      "1784 Training Loss: tensor(0.3489)\n",
      "1785 Training Loss: tensor(0.3576)\n",
      "1786 Training Loss: tensor(0.3598)\n",
      "1787 Training Loss: tensor(0.3542)\n",
      "1788 Training Loss: tensor(0.3495)\n",
      "1789 Training Loss: tensor(0.3585)\n",
      "1790 Training Loss: tensor(0.3542)\n",
      "1791 Training Loss: tensor(0.3574)\n",
      "1792 Training Loss: tensor(0.3544)\n",
      "1793 Training Loss: tensor(0.3577)\n",
      "1794 Training Loss: tensor(0.3517)\n",
      "1795 Training Loss: tensor(0.3525)\n",
      "1796 Training Loss: tensor(0.3509)\n",
      "1797 Training Loss: tensor(0.3493)\n",
      "1798 Training Loss: tensor(0.3508)\n",
      "1799 Training Loss: tensor(0.3498)\n",
      "1800 Training Loss: tensor(0.3509)\n",
      "1801 Training Loss: tensor(0.3518)\n",
      "1802 Training Loss: tensor(0.3575)\n",
      "1803 Training Loss: tensor(0.3508)\n",
      "1804 Training Loss: tensor(0.3495)\n",
      "1805 Training Loss: tensor(0.3510)\n",
      "1806 Training Loss: tensor(0.3506)\n",
      "1807 Training Loss: tensor(0.3506)\n",
      "1808 Training Loss: tensor(0.3502)\n",
      "1809 Training Loss: tensor(0.3502)\n",
      "1810 Training Loss: tensor(0.3498)\n",
      "1811 Training Loss: tensor(0.3499)\n",
      "1812 Training Loss: tensor(0.3517)\n",
      "1813 Training Loss: tensor(0.3485)\n",
      "1814 Training Loss: tensor(0.3540)\n",
      "1815 Training Loss: tensor(0.3501)\n",
      "1816 Training Loss: tensor(0.3503)\n",
      "1817 Training Loss: tensor(0.3520)\n",
      "1818 Training Loss: tensor(0.3483)\n",
      "1819 Training Loss: tensor(0.3529)\n",
      "1820 Training Loss: tensor(0.3515)\n",
      "1821 Training Loss: tensor(0.3508)\n",
      "1822 Training Loss: tensor(0.3517)\n",
      "1823 Training Loss: tensor(0.3486)\n",
      "1824 Training Loss: tensor(0.3562)\n",
      "1825 Training Loss: tensor(0.3495)\n",
      "1826 Training Loss: tensor(0.3579)\n",
      "1827 Training Loss: tensor(0.3537)\n",
      "1828 Training Loss: tensor(0.3535)\n",
      "1829 Training Loss: tensor(0.3516)\n",
      "1830 Training Loss: tensor(0.3509)\n",
      "1831 Training Loss: tensor(0.3507)\n",
      "1832 Training Loss: tensor(0.3490)\n",
      "1833 Training Loss: tensor(0.3511)\n",
      "1834 Training Loss: tensor(0.3541)\n",
      "1835 Training Loss: tensor(0.3505)\n",
      "1836 Training Loss: tensor(0.3507)\n",
      "1837 Training Loss: tensor(0.3496)\n",
      "1838 Training Loss: tensor(0.3511)\n",
      "1839 Training Loss: tensor(0.3493)\n",
      "1840 Training Loss: tensor(0.3492)\n",
      "1841 Training Loss: tensor(0.3534)\n",
      "1842 Training Loss: tensor(0.3492)\n",
      "1843 Training Loss: tensor(0.3512)\n",
      "1844 Training Loss: tensor(0.3536)\n",
      "1845 Training Loss: tensor(0.3503)\n",
      "1846 Training Loss: tensor(0.3506)\n",
      "1847 Training Loss: tensor(0.3488)\n",
      "1848 Training Loss: tensor(0.3499)\n",
      "1849 Training Loss: tensor(0.3526)\n",
      "1850 Training Loss: tensor(0.3486)\n",
      "1851 Training Loss: tensor(0.3513)\n",
      "1852 Training Loss: tensor(0.3490)\n",
      "1853 Training Loss: tensor(0.3494)\n",
      "1854 Training Loss: tensor(0.3527)\n",
      "1855 Training Loss: tensor(0.3492)\n",
      "1856 Training Loss: tensor(0.3496)\n",
      "1857 Training Loss: tensor(0.3491)\n",
      "1858 Training Loss: tensor(0.3491)\n",
      "1859 Training Loss: tensor(0.3513)\n",
      "1860 Training Loss: tensor(0.3490)\n",
      "1861 Training Loss: tensor(0.3507)\n",
      "1862 Training Loss: tensor(0.3558)\n",
      "1863 Training Loss: tensor(0.3527)\n",
      "1864 Training Loss: tensor(0.3519)\n",
      "1865 Training Loss: tensor(0.3503)\n",
      "1866 Training Loss: tensor(0.3544)\n",
      "1867 Training Loss: tensor(0.3501)\n",
      "1868 Training Loss: tensor(0.3545)\n",
      "1869 Training Loss: tensor(0.3491)\n",
      "1870 Training Loss: tensor(0.3510)\n",
      "1871 Training Loss: tensor(0.3494)\n",
      "1872 Training Loss: tensor(0.3549)\n",
      "1873 Training Loss: tensor(0.3545)\n",
      "1874 Training Loss: tensor(0.3549)\n",
      "1875 Training Loss: tensor(0.3504)\n",
      "1876 Training Loss: tensor(0.3508)\n",
      "1877 Training Loss: tensor(0.3500)\n",
      "1878 Training Loss: tensor(0.3510)\n",
      "1879 Training Loss: tensor(0.3515)\n",
      "1880 Training Loss: tensor(0.3498)\n",
      "1881 Training Loss: tensor(0.3520)\n",
      "1882 Training Loss: tensor(0.3564)\n",
      "1883 Training Loss: tensor(0.3541)\n",
      "1884 Training Loss: tensor(0.3522)\n",
      "1885 Training Loss: tensor(0.3526)\n",
      "1886 Training Loss: tensor(0.3512)\n",
      "1887 Training Loss: tensor(0.3545)\n",
      "1888 Training Loss: tensor(0.3531)\n",
      "1889 Training Loss: tensor(0.3502)\n",
      "1890 Training Loss: tensor(0.3509)\n",
      "1891 Training Loss: tensor(0.3502)\n",
      "1892 Training Loss: tensor(0.3505)\n",
      "1893 Training Loss: tensor(0.3495)\n",
      "1894 Training Loss: tensor(0.3488)\n",
      "1895 Training Loss: tensor(0.3498)\n",
      "1896 Training Loss: tensor(0.3489)\n",
      "1897 Training Loss: tensor(0.3535)\n",
      "1898 Training Loss: tensor(0.3491)\n",
      "1899 Training Loss: tensor(0.3489)\n",
      "1900 Training Loss: tensor(0.3524)\n",
      "1901 Training Loss: tensor(0.3491)\n",
      "1902 Training Loss: tensor(0.3507)\n",
      "1903 Training Loss: tensor(0.3493)\n",
      "1904 Training Loss: tensor(0.3518)\n",
      "1905 Training Loss: tensor(0.3478)\n",
      "1906 Training Loss: tensor(0.3502)\n",
      "1907 Training Loss: tensor(0.3513)\n",
      "1908 Training Loss: tensor(0.3504)\n",
      "1909 Training Loss: tensor(0.3498)\n",
      "1910 Training Loss: tensor(0.3476)\n",
      "1911 Training Loss: tensor(0.3509)\n",
      "1912 Training Loss: tensor(0.3480)\n",
      "1913 Training Loss: tensor(0.3588)\n",
      "1914 Training Loss: tensor(0.3537)\n",
      "1915 Training Loss: tensor(0.3516)\n",
      "1916 Training Loss: tensor(0.3533)\n",
      "1917 Training Loss: tensor(0.3516)\n",
      "1918 Training Loss: tensor(0.3546)\n",
      "1919 Training Loss: tensor(0.3500)\n",
      "1920 Training Loss: tensor(0.3513)\n",
      "1921 Training Loss: tensor(0.3507)\n",
      "1922 Training Loss: tensor(0.3499)\n",
      "1923 Training Loss: tensor(0.3569)\n",
      "1924 Training Loss: tensor(0.3542)\n",
      "1925 Training Loss: tensor(0.3507)\n",
      "1926 Training Loss: tensor(0.3487)\n",
      "1927 Training Loss: tensor(0.3503)\n",
      "1928 Training Loss: tensor(0.3491)\n",
      "1929 Training Loss: tensor(0.3550)\n",
      "1930 Training Loss: tensor(0.3521)\n",
      "1931 Training Loss: tensor(0.3553)\n",
      "1932 Training Loss: tensor(0.3591)\n",
      "1933 Training Loss: tensor(0.3521)\n",
      "1934 Training Loss: tensor(0.3505)\n",
      "1935 Training Loss: tensor(0.3506)\n",
      "1936 Training Loss: tensor(0.3527)\n",
      "1937 Training Loss: tensor(0.3538)\n",
      "1938 Training Loss: tensor(0.3515)\n",
      "1939 Training Loss: tensor(0.3542)\n",
      "1940 Training Loss: tensor(0.3502)\n",
      "1941 Training Loss: tensor(0.3509)\n",
      "1942 Training Loss: tensor(0.3508)\n",
      "1943 Training Loss: tensor(0.3550)\n",
      "1944 Training Loss: tensor(0.3514)\n",
      "1945 Training Loss: tensor(0.3517)\n",
      "1946 Training Loss: tensor(0.3495)\n",
      "1947 Training Loss: tensor(0.3490)\n",
      "1948 Training Loss: tensor(0.3495)\n",
      "1949 Training Loss: tensor(0.3536)\n",
      "1950 Training Loss: tensor(0.3549)\n",
      "1951 Training Loss: tensor(0.3491)\n",
      "1952 Training Loss: tensor(0.3506)\n",
      "1953 Training Loss: tensor(0.3522)\n",
      "1954 Training Loss: tensor(0.3502)\n",
      "1955 Training Loss: tensor(0.3514)\n",
      "1956 Training Loss: tensor(0.3502)\n",
      "1957 Training Loss: tensor(0.3492)\n",
      "1958 Training Loss: tensor(0.3522)\n",
      "1959 Training Loss: tensor(0.3485)\n",
      "1960 Training Loss: tensor(0.3527)\n",
      "1961 Training Loss: tensor(0.3500)\n",
      "1962 Training Loss: tensor(0.3564)\n",
      "1963 Training Loss: tensor(0.3532)\n",
      "1964 Training Loss: tensor(0.3510)\n",
      "1965 Training Loss: tensor(0.3507)\n",
      "1966 Training Loss: tensor(0.3550)\n",
      "1967 Training Loss: tensor(0.3488)\n",
      "1968 Training Loss: tensor(0.3502)\n",
      "1969 Training Loss: tensor(0.3495)\n",
      "1970 Training Loss: tensor(0.3496)\n",
      "1971 Training Loss: tensor(0.3528)\n",
      "1972 Training Loss: tensor(0.3491)\n",
      "1973 Training Loss: tensor(0.3509)\n",
      "1974 Training Loss: tensor(0.3487)\n",
      "1975 Training Loss: tensor(0.3542)\n",
      "1976 Training Loss: tensor(0.3527)\n",
      "1977 Training Loss: tensor(0.3493)\n",
      "1978 Training Loss: tensor(0.3526)\n",
      "1979 Training Loss: tensor(0.3491)\n",
      "1980 Training Loss: tensor(0.3488)\n",
      "1981 Training Loss: tensor(0.3496)\n",
      "1982 Training Loss: tensor(0.3521)\n",
      "1983 Training Loss: tensor(0.3494)\n",
      "1984 Training Loss: tensor(0.3486)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985 Training Loss: tensor(0.3498)\n",
      "1986 Training Loss: tensor(0.3599)\n",
      "1987 Training Loss: tensor(0.3526)\n",
      "1988 Training Loss: tensor(0.3492)\n",
      "1989 Training Loss: tensor(0.3501)\n",
      "1990 Training Loss: tensor(0.3499)\n",
      "1991 Training Loss: tensor(0.3487)\n",
      "1992 Training Loss: tensor(0.3514)\n",
      "1993 Training Loss: tensor(0.3578)\n",
      "1994 Training Loss: tensor(0.3507)\n",
      "1995 Training Loss: tensor(0.3525)\n",
      "1996 Training Loss: tensor(0.3543)\n",
      "1997 Training Loss: tensor(0.3482)\n",
      "1998 Training Loss: tensor(0.3522)\n",
      "1999 Training Loss: tensor(0.3535)\n",
      "2000 Training Loss: tensor(0.3513)\n",
      "2001 Training Loss: tensor(0.3492)\n",
      "2002 Training Loss: tensor(0.3499)\n",
      "2003 Training Loss: tensor(0.3538)\n",
      "2004 Training Loss: tensor(0.3507)\n",
      "2005 Training Loss: tensor(0.3514)\n",
      "2006 Training Loss: tensor(0.3576)\n",
      "2007 Training Loss: tensor(0.3505)\n",
      "2008 Training Loss: tensor(0.3497)\n",
      "2009 Training Loss: tensor(0.3532)\n",
      "2010 Training Loss: tensor(0.3493)\n",
      "2011 Training Loss: tensor(0.3529)\n",
      "2012 Training Loss: tensor(0.3497)\n",
      "2013 Training Loss: tensor(0.3556)\n",
      "2014 Training Loss: tensor(0.3490)\n",
      "2015 Training Loss: tensor(0.3522)\n",
      "2016 Training Loss: tensor(0.3543)\n",
      "2017 Training Loss: tensor(0.3497)\n",
      "2018 Training Loss: tensor(0.3498)\n",
      "2019 Training Loss: tensor(0.3558)\n",
      "2020 Training Loss: tensor(0.3503)\n",
      "2021 Training Loss: tensor(0.3549)\n",
      "2022 Training Loss: tensor(0.3540)\n",
      "2023 Training Loss: tensor(0.3518)\n",
      "2024 Training Loss: tensor(0.3522)\n",
      "2025 Training Loss: tensor(0.3502)\n",
      "2026 Training Loss: tensor(0.3507)\n",
      "2027 Training Loss: tensor(0.3524)\n",
      "2028 Training Loss: tensor(0.3513)\n",
      "2029 Training Loss: tensor(0.3516)\n",
      "2030 Training Loss: tensor(0.3494)\n",
      "2031 Training Loss: tensor(0.3553)\n",
      "2032 Training Loss: tensor(0.3499)\n",
      "2033 Training Loss: tensor(0.3503)\n",
      "2034 Training Loss: tensor(0.3555)\n",
      "2035 Training Loss: tensor(0.3490)\n",
      "2036 Training Loss: tensor(0.3543)\n",
      "2037 Training Loss: tensor(0.3487)\n",
      "2038 Training Loss: tensor(0.3499)\n",
      "2039 Training Loss: tensor(0.3519)\n",
      "2040 Training Loss: tensor(0.3556)\n",
      "2041 Training Loss: tensor(0.3503)\n",
      "2042 Training Loss: tensor(0.3505)\n",
      "2043 Training Loss: tensor(0.3496)\n",
      "2044 Training Loss: tensor(0.3522)\n",
      "2045 Training Loss: tensor(0.3536)\n",
      "2046 Training Loss: tensor(0.3560)\n",
      "2047 Training Loss: tensor(0.3490)\n",
      "2048 Training Loss: tensor(0.3529)\n",
      "2049 Training Loss: tensor(0.3495)\n",
      "2050 Training Loss: tensor(0.3509)\n",
      "2051 Training Loss: tensor(0.3548)\n",
      "2052 Training Loss: tensor(0.3502)\n",
      "2053 Training Loss: tensor(0.3504)\n",
      "2054 Training Loss: tensor(0.3490)\n",
      "2055 Training Loss: tensor(0.3538)\n",
      "2056 Training Loss: tensor(0.3493)\n",
      "2057 Training Loss: tensor(0.3516)\n",
      "2058 Training Loss: tensor(0.3511)\n",
      "2059 Training Loss: tensor(0.3498)\n",
      "2060 Training Loss: tensor(0.3498)\n",
      "2061 Training Loss: tensor(0.3504)\n",
      "2062 Training Loss: tensor(0.3510)\n",
      "2063 Training Loss: tensor(0.3529)\n",
      "2064 Training Loss: tensor(0.3496)\n",
      "2065 Training Loss: tensor(0.3487)\n",
      "2066 Training Loss: tensor(0.3498)\n",
      "2067 Training Loss: tensor(0.3494)\n",
      "2068 Training Loss: tensor(0.3492)\n",
      "2069 Training Loss: tensor(0.3510)\n",
      "2070 Training Loss: tensor(0.3502)\n",
      "2071 Training Loss: tensor(0.3482)\n",
      "2072 Training Loss: tensor(0.3495)\n",
      "2073 Training Loss: tensor(0.3508)\n",
      "2074 Training Loss: tensor(0.3516)\n",
      "2075 Training Loss: tensor(0.3499)\n",
      "2076 Training Loss: tensor(0.3482)\n",
      "2077 Training Loss: tensor(0.3516)\n",
      "2078 Training Loss: tensor(0.3487)\n",
      "2079 Training Loss: tensor(0.3535)\n",
      "2080 Training Loss: tensor(0.3487)\n",
      "2081 Training Loss: tensor(0.3484)\n",
      "2082 Training Loss: tensor(0.3503)\n",
      "2083 Training Loss: tensor(0.3479)\n",
      "2084 Training Loss: tensor(0.3483)\n",
      "2085 Training Loss: tensor(0.3514)\n",
      "2086 Training Loss: tensor(0.3482)\n",
      "2087 Training Loss: tensor(0.3512)\n",
      "2088 Training Loss: tensor(0.3538)\n",
      "2089 Training Loss: tensor(0.3482)\n",
      "2090 Training Loss: tensor(0.3501)\n",
      "2091 Training Loss: tensor(0.3500)\n",
      "2092 Training Loss: tensor(0.3494)\n",
      "2093 Training Loss: tensor(0.3589)\n",
      "2094 Training Loss: tensor(0.3520)\n",
      "2095 Training Loss: tensor(0.3503)\n",
      "2096 Training Loss: tensor(0.3498)\n",
      "2097 Training Loss: tensor(0.3487)\n",
      "2098 Training Loss: tensor(0.3500)\n",
      "2099 Training Loss: tensor(0.3523)\n",
      "2100 Training Loss: tensor(0.3505)\n",
      "2101 Training Loss: tensor(0.3519)\n",
      "2102 Training Loss: tensor(0.3584)\n",
      "2103 Training Loss: tensor(0.3515)\n",
      "2104 Training Loss: tensor(0.3498)\n",
      "2105 Training Loss: tensor(0.3513)\n",
      "2106 Training Loss: tensor(0.3495)\n",
      "2107 Training Loss: tensor(0.3500)\n",
      "2108 Training Loss: tensor(0.3528)\n",
      "2109 Training Loss: tensor(0.3515)\n",
      "2110 Training Loss: tensor(0.3506)\n",
      "2111 Training Loss: tensor(0.3489)\n",
      "2112 Training Loss: tensor(0.3492)\n",
      "2113 Training Loss: tensor(0.3505)\n",
      "2114 Training Loss: tensor(0.3489)\n",
      "2115 Training Loss: tensor(0.3481)\n",
      "2116 Training Loss: tensor(0.3480)\n",
      "2117 Training Loss: tensor(0.3488)\n",
      "2118 Training Loss: tensor(0.3543)\n",
      "2119 Training Loss: tensor(0.3476)\n",
      "2120 Training Loss: tensor(0.3490)\n",
      "2121 Training Loss: tensor(0.3513)\n",
      "2122 Training Loss: tensor(0.3510)\n",
      "2123 Training Loss: tensor(0.3521)\n",
      "2124 Training Loss: tensor(0.3572)\n",
      "2125 Training Loss: tensor(0.3494)\n",
      "2126 Training Loss: tensor(0.3603)\n",
      "2127 Training Loss: tensor(0.3492)\n",
      "2128 Training Loss: tensor(0.3520)\n",
      "2129 Training Loss: tensor(0.3544)\n",
      "2130 Training Loss: tensor(0.3482)\n",
      "2131 Training Loss: tensor(0.3504)\n",
      "2132 Training Loss: tensor(0.3513)\n",
      "2133 Training Loss: tensor(0.3551)\n",
      "2134 Training Loss: tensor(0.3497)\n",
      "2135 Training Loss: tensor(0.3527)\n",
      "2136 Training Loss: tensor(0.3500)\n",
      "2137 Training Loss: tensor(0.3500)\n",
      "2138 Training Loss: tensor(0.3519)\n",
      "2139 Training Loss: tensor(0.3478)\n",
      "2140 Training Loss: tensor(0.3549)\n",
      "2141 Training Loss: tensor(0.3528)\n",
      "2142 Training Loss: tensor(0.3517)\n",
      "2143 Training Loss: tensor(0.3484)\n",
      "2144 Training Loss: tensor(0.3497)\n",
      "2145 Training Loss: tensor(0.3550)\n",
      "2146 Training Loss: tensor(0.3503)\n",
      "2147 Training Loss: tensor(0.3493)\n",
      "2148 Training Loss: tensor(0.3481)\n",
      "2149 Training Loss: tensor(0.3528)\n",
      "2150 Training Loss: tensor(0.3523)\n",
      "2151 Training Loss: tensor(0.3496)\n",
      "2152 Training Loss: tensor(0.3537)\n",
      "2153 Training Loss: tensor(0.3541)\n",
      "2154 Training Loss: tensor(0.3499)\n",
      "2155 Training Loss: tensor(0.3480)\n",
      "2156 Training Loss: tensor(0.3484)\n",
      "2157 Training Loss: tensor(0.3497)\n",
      "2158 Training Loss: tensor(0.3544)\n",
      "2159 Training Loss: tensor(0.3496)\n",
      "2160 Training Loss: tensor(0.3504)\n",
      "2161 Training Loss: tensor(0.3483)\n",
      "2162 Training Loss: tensor(0.3497)\n",
      "2163 Training Loss: tensor(0.3498)\n",
      "2164 Training Loss: tensor(0.3529)\n",
      "2165 Training Loss: tensor(0.3502)\n",
      "2166 Training Loss: tensor(0.3483)\n",
      "2167 Training Loss: tensor(0.3493)\n",
      "2168 Training Loss: tensor(0.3559)\n",
      "2169 Training Loss: tensor(0.3495)\n",
      "2170 Training Loss: tensor(0.3487)\n",
      "2171 Training Loss: tensor(0.3503)\n",
      "2172 Training Loss: tensor(0.3573)\n",
      "2173 Training Loss: tensor(0.3476)\n",
      "2174 Training Loss: tensor(0.3596)\n",
      "2175 Training Loss: tensor(0.3534)\n",
      "2176 Training Loss: tensor(0.3497)\n",
      "2177 Training Loss: tensor(0.3523)\n",
      "2178 Training Loss: tensor(0.3623)\n",
      "2179 Training Loss: tensor(0.3543)\n",
      "2180 Training Loss: tensor(0.3498)\n",
      "2181 Training Loss: tensor(0.3519)\n",
      "2182 Training Loss: tensor(0.3497)\n",
      "2183 Training Loss: tensor(0.3536)\n",
      "2184 Training Loss: tensor(0.3551)\n",
      "2185 Training Loss: tensor(0.3530)\n",
      "2186 Training Loss: tensor(0.3546)\n",
      "2187 Training Loss: tensor(0.3542)\n",
      "2188 Training Loss: tensor(0.3514)\n",
      "2189 Training Loss: tensor(0.3504)\n",
      "2190 Training Loss: tensor(0.3526)\n",
      "2191 Training Loss: tensor(0.3515)\n",
      "2192 Training Loss: tensor(0.3507)\n",
      "2193 Training Loss: tensor(0.3557)\n",
      "2194 Training Loss: tensor(0.3506)\n",
      "2195 Training Loss: tensor(0.3503)\n",
      "2196 Training Loss: tensor(0.3519)\n",
      "2197 Training Loss: tensor(0.3502)\n",
      "2198 Training Loss: tensor(0.3504)\n",
      "2199 Training Loss: tensor(0.3506)\n",
      "2200 Training Loss: tensor(0.3494)\n",
      "2201 Training Loss: tensor(0.3489)\n",
      "2202 Training Loss: tensor(0.3528)\n",
      "2203 Training Loss: tensor(0.3488)\n",
      "2204 Training Loss: tensor(0.3505)\n",
      "2205 Training Loss: tensor(0.3518)\n",
      "2206 Training Loss: tensor(0.3535)\n",
      "2207 Training Loss: tensor(0.3593)\n",
      "2208 Training Loss: tensor(0.3501)\n",
      "2209 Training Loss: tensor(0.3537)\n",
      "2210 Training Loss: tensor(0.3489)\n",
      "2211 Training Loss: tensor(0.3482)\n",
      "2212 Training Loss: tensor(0.3585)\n",
      "2213 Training Loss: tensor(0.3488)\n",
      "2214 Training Loss: tensor(0.3508)\n",
      "2215 Training Loss: tensor(0.3503)\n",
      "2216 Training Loss: tensor(0.3488)\n",
      "2217 Training Loss: tensor(0.3492)\n",
      "2218 Training Loss: tensor(0.3516)\n",
      "2219 Training Loss: tensor(0.3494)\n",
      "2220 Training Loss: tensor(0.3484)\n",
      "2221 Training Loss: tensor(0.3496)\n",
      "2222 Training Loss: tensor(0.3484)\n",
      "2223 Training Loss: tensor(0.3548)\n",
      "2224 Training Loss: tensor(0.3485)\n",
      "2225 Training Loss: tensor(0.3532)\n",
      "2226 Training Loss: tensor(0.3478)\n",
      "2227 Training Loss: tensor(0.3510)\n",
      "2228 Training Loss: tensor(0.3489)\n",
      "2229 Training Loss: tensor(0.3491)\n",
      "2230 Training Loss: tensor(0.3491)\n",
      "2231 Training Loss: tensor(0.3513)\n",
      "2232 Training Loss: tensor(0.3485)\n",
      "2233 Training Loss: tensor(0.3489)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234 Training Loss: tensor(0.3498)\n",
      "2235 Training Loss: tensor(0.3517)\n",
      "2236 Training Loss: tensor(0.3482)\n",
      "2237 Training Loss: tensor(0.3503)\n",
      "2238 Training Loss: tensor(0.3525)\n",
      "2239 Training Loss: tensor(0.3563)\n",
      "2240 Training Loss: tensor(0.3514)\n",
      "2241 Training Loss: tensor(0.3525)\n",
      "2242 Training Loss: tensor(0.3524)\n",
      "2243 Training Loss: tensor(0.3511)\n",
      "2244 Training Loss: tensor(0.3596)\n",
      "2245 Training Loss: tensor(0.3504)\n",
      "2246 Training Loss: tensor(0.3529)\n",
      "2247 Training Loss: tensor(0.3508)\n",
      "2248 Training Loss: tensor(0.3509)\n",
      "2249 Training Loss: tensor(0.3520)\n",
      "2250 Training Loss: tensor(0.3493)\n",
      "2251 Training Loss: tensor(0.3591)\n",
      "2252 Training Loss: tensor(0.3513)\n",
      "2253 Training Loss: tensor(0.3518)\n",
      "2254 Training Loss: tensor(0.3526)\n",
      "2255 Training Loss: tensor(0.3531)\n",
      "2256 Training Loss: tensor(0.3499)\n",
      "2257 Training Loss: tensor(0.3528)\n",
      "2258 Training Loss: tensor(0.3508)\n",
      "2259 Training Loss: tensor(0.3533)\n",
      "2260 Training Loss: tensor(0.3503)\n",
      "2261 Training Loss: tensor(0.3496)\n",
      "2262 Training Loss: tensor(0.3507)\n",
      "2263 Training Loss: tensor(0.3561)\n",
      "2264 Training Loss: tensor(0.3530)\n",
      "2265 Training Loss: tensor(0.3543)\n",
      "2266 Training Loss: tensor(0.3523)\n",
      "2267 Training Loss: tensor(0.3496)\n",
      "2268 Training Loss: tensor(0.3516)\n",
      "2269 Training Loss: tensor(0.3494)\n",
      "2270 Training Loss: tensor(0.3506)\n",
      "2271 Training Loss: tensor(0.3555)\n",
      "2272 Training Loss: tensor(0.3494)\n",
      "2273 Training Loss: tensor(0.3511)\n",
      "2274 Training Loss: tensor(0.3525)\n",
      "2275 Training Loss: tensor(0.3493)\n",
      "2276 Training Loss: tensor(0.3530)\n",
      "2277 Training Loss: tensor(0.3488)\n",
      "2278 Training Loss: tensor(0.3532)\n",
      "2279 Training Loss: tensor(0.3506)\n",
      "2280 Training Loss: tensor(0.3483)\n",
      "2281 Training Loss: tensor(0.3572)\n",
      "2282 Training Loss: tensor(0.3488)\n",
      "2283 Training Loss: tensor(0.3533)\n",
      "2284 Training Loss: tensor(0.3482)\n",
      "2285 Training Loss: tensor(0.3543)\n",
      "2286 Training Loss: tensor(0.3569)\n",
      "2287 Training Loss: tensor(0.3499)\n",
      "2288 Training Loss: tensor(0.3561)\n",
      "2289 Training Loss: tensor(0.3485)\n",
      "2290 Training Loss: tensor(0.3491)\n",
      "2291 Training Loss: tensor(0.3500)\n",
      "2292 Training Loss: tensor(0.3486)\n",
      "2293 Training Loss: tensor(0.3525)\n",
      "2294 Training Loss: tensor(0.3483)\n",
      "2295 Training Loss: tensor(0.3496)\n",
      "2296 Training Loss: tensor(0.3525)\n",
      "2297 Training Loss: tensor(0.3523)\n",
      "2298 Training Loss: tensor(0.3531)\n",
      "2299 Training Loss: tensor(0.3496)\n",
      "2300 Training Loss: tensor(0.3498)\n",
      "2301 Training Loss: tensor(0.3505)\n",
      "2302 Training Loss: tensor(0.3500)\n",
      "2303 Training Loss: tensor(0.3506)\n",
      "2304 Training Loss: tensor(0.3516)\n",
      "2305 Training Loss: tensor(0.3495)\n",
      "2306 Training Loss: tensor(0.3560)\n",
      "2307 Training Loss: tensor(0.3506)\n",
      "2308 Training Loss: tensor(0.3487)\n",
      "2309 Training Loss: tensor(0.3500)\n",
      "2310 Training Loss: tensor(0.3531)\n",
      "2311 Training Loss: tensor(0.3535)\n",
      "2312 Training Loss: tensor(0.3559)\n",
      "2313 Training Loss: tensor(0.3551)\n",
      "2314 Training Loss: tensor(0.3507)\n",
      "2315 Training Loss: tensor(0.3522)\n",
      "2316 Training Loss: tensor(0.3504)\n",
      "2317 Training Loss: tensor(0.3489)\n",
      "2318 Training Loss: tensor(0.3536)\n",
      "2319 Training Loss: tensor(0.3535)\n",
      "2320 Training Loss: tensor(0.3507)\n",
      "2321 Training Loss: tensor(0.3553)\n",
      "2322 Training Loss: tensor(0.3536)\n",
      "2323 Training Loss: tensor(0.3524)\n",
      "2324 Training Loss: tensor(0.3509)\n",
      "2325 Training Loss: tensor(0.3511)\n",
      "2326 Training Loss: tensor(0.3514)\n",
      "2327 Training Loss: tensor(0.3526)\n",
      "2328 Training Loss: tensor(0.3506)\n",
      "2329 Training Loss: tensor(0.3508)\n",
      "2330 Training Loss: tensor(0.3500)\n",
      "2331 Training Loss: tensor(0.3517)\n",
      "2332 Training Loss: tensor(0.3528)\n",
      "2333 Training Loss: tensor(0.3496)\n",
      "2334 Training Loss: tensor(0.3538)\n",
      "2335 Training Loss: tensor(0.3495)\n",
      "2336 Training Loss: tensor(0.3508)\n",
      "2337 Training Loss: tensor(0.3532)\n",
      "2338 Training Loss: tensor(0.3506)\n",
      "2339 Training Loss: tensor(0.3567)\n",
      "2340 Training Loss: tensor(0.3563)\n",
      "2341 Training Loss: tensor(0.3546)\n",
      "2342 Training Loss: tensor(0.3547)\n",
      "2343 Training Loss: tensor(0.3507)\n",
      "2344 Training Loss: tensor(0.3504)\n",
      "2345 Training Loss: tensor(0.3551)\n",
      "2346 Training Loss: tensor(0.3499)\n",
      "2347 Training Loss: tensor(0.3502)\n",
      "2348 Training Loss: tensor(0.3503)\n",
      "2349 Training Loss: tensor(0.3496)\n",
      "2350 Training Loss: tensor(0.3507)\n",
      "2351 Training Loss: tensor(0.3492)\n",
      "2352 Training Loss: tensor(0.3511)\n",
      "2353 Training Loss: tensor(0.3491)\n",
      "2354 Training Loss: tensor(0.3518)\n",
      "2355 Training Loss: tensor(0.3543)\n",
      "2356 Training Loss: tensor(0.3488)\n",
      "2357 Training Loss: tensor(0.3489)\n",
      "2358 Training Loss: tensor(0.3491)\n",
      "2359 Training Loss: tensor(0.3576)\n",
      "2360 Training Loss: tensor(0.3492)\n",
      "2361 Training Loss: tensor(0.3528)\n",
      "2362 Training Loss: tensor(0.3519)\n",
      "2363 Training Loss: tensor(0.3539)\n",
      "2364 Training Loss: tensor(0.3489)\n",
      "2365 Training Loss: tensor(0.3523)\n",
      "2366 Training Loss: tensor(0.3495)\n",
      "2367 Training Loss: tensor(0.3506)\n",
      "2368 Training Loss: tensor(0.3479)\n",
      "2369 Training Loss: tensor(0.3556)\n",
      "2370 Training Loss: tensor(0.3554)\n",
      "2371 Training Loss: tensor(0.3490)\n",
      "2372 Training Loss: tensor(0.3555)\n",
      "2373 Training Loss: tensor(0.3519)\n",
      "2374 Training Loss: tensor(0.3504)\n",
      "2375 Training Loss: tensor(0.3501)\n",
      "2376 Training Loss: tensor(0.3511)\n",
      "2377 Training Loss: tensor(0.3489)\n",
      "2378 Training Loss: tensor(0.3529)\n",
      "2379 Training Loss: tensor(0.3515)\n",
      "2380 Training Loss: tensor(0.3542)\n",
      "2381 Training Loss: tensor(0.3501)\n",
      "2382 Training Loss: tensor(0.3529)\n",
      "2383 Training Loss: tensor(0.3505)\n",
      "2384 Training Loss: tensor(0.3506)\n",
      "2385 Training Loss: tensor(0.3523)\n",
      "2386 Training Loss: tensor(0.3530)\n",
      "2387 Training Loss: tensor(0.3486)\n",
      "2388 Training Loss: tensor(0.3535)\n",
      "2389 Training Loss: tensor(0.3516)\n",
      "2390 Training Loss: tensor(0.3513)\n",
      "2391 Training Loss: tensor(0.3497)\n",
      "2392 Training Loss: tensor(0.3508)\n",
      "2393 Training Loss: tensor(0.3504)\n",
      "2394 Training Loss: tensor(0.3568)\n",
      "2395 Training Loss: tensor(0.3518)\n",
      "2396 Training Loss: tensor(0.3488)\n",
      "2397 Training Loss: tensor(0.3501)\n",
      "2398 Training Loss: tensor(0.3537)\n",
      "2399 Training Loss: tensor(0.3487)\n",
      "2400 Training Loss: tensor(0.3492)\n",
      "2401 Training Loss: tensor(0.3491)\n",
      "2402 Training Loss: tensor(0.3537)\n",
      "2403 Training Loss: tensor(0.3480)\n",
      "2404 Training Loss: tensor(0.3516)\n",
      "2405 Training Loss: tensor(0.3521)\n",
      "2406 Training Loss: tensor(0.3551)\n",
      "2407 Training Loss: tensor(0.3495)\n",
      "2408 Training Loss: tensor(0.3480)\n",
      "2409 Training Loss: tensor(0.3483)\n",
      "2410 Training Loss: tensor(0.3501)\n",
      "2411 Training Loss: tensor(0.3528)\n",
      "2412 Training Loss: tensor(0.3586)\n",
      "2413 Training Loss: tensor(0.3490)\n",
      "2414 Training Loss: tensor(0.3482)\n",
      "2415 Training Loss: tensor(0.3508)\n",
      "2416 Training Loss: tensor(0.3505)\n",
      "2417 Training Loss: tensor(0.3490)\n",
      "2418 Training Loss: tensor(0.3513)\n",
      "2419 Training Loss: tensor(0.3491)\n",
      "2420 Training Loss: tensor(0.3593)\n",
      "2421 Training Loss: tensor(0.3525)\n",
      "2422 Training Loss: tensor(0.3479)\n",
      "2423 Training Loss: tensor(0.3502)\n",
      "2424 Training Loss: tensor(0.3528)\n",
      "2425 Training Loss: tensor(0.3552)\n",
      "2426 Training Loss: tensor(0.3500)\n",
      "2427 Training Loss: tensor(0.3517)\n",
      "2428 Training Loss: tensor(0.3499)\n",
      "2429 Training Loss: tensor(0.3502)\n",
      "2430 Training Loss: tensor(0.3534)\n",
      "2431 Training Loss: tensor(0.3541)\n",
      "2432 Training Loss: tensor(0.3493)\n",
      "2433 Training Loss: tensor(0.3500)\n",
      "2434 Training Loss: tensor(0.3540)\n",
      "2435 Training Loss: tensor(0.3518)\n",
      "2436 Training Loss: tensor(0.3508)\n",
      "2437 Training Loss: tensor(0.3510)\n",
      "2438 Training Loss: tensor(0.3513)\n",
      "2439 Training Loss: tensor(0.3504)\n",
      "2440 Training Loss: tensor(0.3499)\n",
      "2441 Training Loss: tensor(0.3498)\n",
      "2442 Training Loss: tensor(0.3524)\n",
      "2443 Training Loss: tensor(0.3502)\n",
      "2444 Training Loss: tensor(0.3517)\n",
      "2445 Training Loss: tensor(0.3495)\n",
      "2446 Training Loss: tensor(0.3492)\n",
      "2447 Training Loss: tensor(0.3496)\n",
      "2448 Training Loss: tensor(0.3507)\n",
      "2449 Training Loss: tensor(0.3515)\n",
      "2450 Training Loss: tensor(0.3570)\n",
      "2451 Training Loss: tensor(0.3576)\n",
      "2452 Training Loss: tensor(0.3501)\n",
      "2453 Training Loss: tensor(0.3485)\n",
      "2454 Training Loss: tensor(0.3533)\n",
      "2455 Training Loss: tensor(0.3533)\n",
      "2456 Training Loss: tensor(0.3524)\n",
      "2457 Training Loss: tensor(0.3495)\n",
      "2458 Training Loss: tensor(0.3543)\n",
      "2459 Training Loss: tensor(0.3491)\n",
      "2460 Training Loss: tensor(0.3520)\n",
      "2461 Training Loss: tensor(0.3498)\n",
      "2462 Training Loss: tensor(0.3491)\n",
      "2463 Training Loss: tensor(0.3494)\n",
      "2464 Training Loss: tensor(0.3490)\n",
      "2465 Training Loss: tensor(0.3558)\n",
      "2466 Training Loss: tensor(0.3529)\n",
      "2467 Training Loss: tensor(0.3510)\n",
      "2468 Training Loss: tensor(0.3507)\n",
      "2469 Training Loss: tensor(0.3495)\n",
      "2470 Training Loss: tensor(0.3605)\n",
      "2471 Training Loss: tensor(0.3505)\n",
      "2472 Training Loss: tensor(0.3499)\n",
      "2473 Training Loss: tensor(0.3496)\n",
      "2474 Training Loss: tensor(0.3502)\n",
      "2475 Training Loss: tensor(0.3493)\n",
      "2476 Training Loss: tensor(0.3493)\n",
      "2477 Training Loss: tensor(0.3520)\n",
      "2478 Training Loss: tensor(0.3487)\n",
      "2479 Training Loss: tensor(0.3506)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480 Training Loss: tensor(0.3493)\n",
      "2481 Training Loss: tensor(0.3541)\n",
      "2482 Training Loss: tensor(0.3534)\n",
      "2483 Training Loss: tensor(0.3491)\n",
      "2484 Training Loss: tensor(0.3488)\n",
      "2485 Training Loss: tensor(0.3520)\n",
      "2486 Training Loss: tensor(0.3490)\n",
      "2487 Training Loss: tensor(0.3504)\n",
      "2488 Training Loss: tensor(0.3484)\n",
      "2489 Training Loss: tensor(0.3520)\n",
      "2490 Training Loss: tensor(0.3534)\n",
      "2491 Training Loss: tensor(0.3510)\n",
      "2492 Training Loss: tensor(0.3547)\n",
      "2493 Training Loss: tensor(0.3487)\n",
      "2494 Training Loss: tensor(0.3480)\n",
      "2495 Training Loss: tensor(0.3499)\n",
      "2496 Training Loss: tensor(0.3526)\n",
      "2497 Training Loss: tensor(0.3484)\n",
      "2498 Training Loss: tensor(0.3518)\n",
      "2499 Training Loss: tensor(0.3482)\n",
      "2500 Training Loss: tensor(0.3479)\n",
      "2501 Training Loss: tensor(0.3541)\n",
      "2502 Training Loss: tensor(0.3495)\n",
      "2503 Training Loss: tensor(0.3519)\n",
      "2504 Training Loss: tensor(0.3561)\n",
      "2505 Training Loss: tensor(0.3490)\n",
      "2506 Training Loss: tensor(0.3535)\n",
      "2507 Training Loss: tensor(0.3497)\n",
      "2508 Training Loss: tensor(0.3513)\n",
      "2509 Training Loss: tensor(0.3492)\n",
      "2510 Training Loss: tensor(0.3484)\n",
      "2511 Training Loss: tensor(0.3511)\n",
      "2512 Training Loss: tensor(0.3483)\n",
      "2513 Training Loss: tensor(0.3510)\n",
      "2514 Training Loss: tensor(0.3550)\n",
      "2515 Training Loss: tensor(0.3518)\n",
      "2516 Training Loss: tensor(0.3492)\n",
      "2517 Training Loss: tensor(0.3484)\n",
      "2518 Training Loss: tensor(0.3513)\n",
      "2519 Training Loss: tensor(0.3508)\n",
      "2520 Training Loss: tensor(0.3482)\n",
      "2521 Training Loss: tensor(0.3526)\n",
      "2522 Training Loss: tensor(0.3499)\n",
      "2523 Training Loss: tensor(0.3498)\n",
      "2524 Training Loss: tensor(0.3494)\n",
      "2525 Training Loss: tensor(0.3497)\n",
      "2526 Training Loss: tensor(0.3494)\n",
      "2527 Training Loss: tensor(0.3521)\n",
      "2528 Training Loss: tensor(0.3485)\n",
      "2529 Training Loss: tensor(0.3481)\n",
      "2530 Training Loss: tensor(0.3491)\n",
      "2531 Training Loss: tensor(0.3564)\n",
      "2532 Training Loss: tensor(0.3484)\n",
      "2533 Training Loss: tensor(0.3505)\n",
      "2534 Training Loss: tensor(0.3489)\n",
      "2535 Training Loss: tensor(0.3518)\n",
      "2536 Training Loss: tensor(0.3560)\n",
      "2537 Training Loss: tensor(0.3495)\n",
      "2538 Training Loss: tensor(0.3520)\n",
      "2539 Training Loss: tensor(0.3591)\n",
      "2540 Training Loss: tensor(0.3514)\n",
      "2541 Training Loss: tensor(0.3490)\n",
      "2542 Training Loss: tensor(0.3496)\n",
      "2543 Training Loss: tensor(0.3495)\n",
      "2544 Training Loss: tensor(0.3506)\n",
      "2545 Training Loss: tensor(0.3517)\n",
      "2546 Training Loss: tensor(0.3504)\n",
      "2547 Training Loss: tensor(0.3500)\n",
      "2548 Training Loss: tensor(0.3532)\n",
      "2549 Training Loss: tensor(0.3510)\n",
      "2550 Training Loss: tensor(0.3514)\n",
      "2551 Training Loss: tensor(0.3517)\n",
      "2552 Training Loss: tensor(0.3480)\n",
      "2553 Training Loss: tensor(0.3520)\n",
      "2554 Training Loss: tensor(0.3502)\n",
      "2555 Training Loss: tensor(0.3564)\n",
      "2556 Training Loss: tensor(0.3524)\n",
      "2557 Training Loss: tensor(0.3494)\n",
      "2558 Training Loss: tensor(0.3498)\n",
      "2559 Training Loss: tensor(0.3507)\n",
      "2560 Training Loss: tensor(0.3483)\n",
      "2561 Training Loss: tensor(0.3500)\n",
      "2562 Training Loss: tensor(0.3518)\n",
      "2563 Training Loss: tensor(0.3487)\n",
      "2564 Training Loss: tensor(0.3544)\n",
      "2565 Training Loss: tensor(0.3487)\n",
      "2566 Training Loss: tensor(0.3500)\n",
      "2567 Training Loss: tensor(0.3512)\n",
      "2568 Training Loss: tensor(0.3481)\n",
      "2569 Training Loss: tensor(0.3486)\n",
      "2570 Training Loss: tensor(0.3545)\n",
      "2571 Training Loss: tensor(0.3481)\n",
      "2572 Training Loss: tensor(0.3601)\n",
      "2573 Training Loss: tensor(0.3531)\n",
      "2574 Training Loss: tensor(0.3545)\n",
      "2575 Training Loss: tensor(0.3499)\n",
      "2576 Training Loss: tensor(0.3498)\n",
      "2577 Training Loss: tensor(0.3548)\n",
      "2578 Training Loss: tensor(0.3485)\n",
      "2579 Training Loss: tensor(0.3547)\n",
      "2580 Training Loss: tensor(0.3532)\n",
      "2581 Training Loss: tensor(0.3517)\n",
      "2582 Training Loss: tensor(0.3525)\n",
      "2583 Training Loss: tensor(0.3506)\n",
      "2584 Training Loss: tensor(0.3512)\n",
      "2585 Training Loss: tensor(0.3524)\n",
      "2586 Training Loss: tensor(0.3510)\n",
      "2587 Training Loss: tensor(0.3558)\n",
      "2588 Training Loss: tensor(0.3502)\n",
      "2589 Training Loss: tensor(0.3522)\n",
      "2590 Training Loss: tensor(0.3600)\n",
      "2591 Training Loss: tensor(0.3529)\n",
      "2592 Training Loss: tensor(0.3516)\n",
      "2593 Training Loss: tensor(0.3508)\n",
      "2594 Training Loss: tensor(0.3521)\n",
      "2595 Training Loss: tensor(0.3530)\n",
      "2596 Training Loss: tensor(0.3495)\n",
      "2597 Training Loss: tensor(0.3497)\n",
      "2598 Training Loss: tensor(0.3547)\n",
      "2599 Training Loss: tensor(0.3536)\n",
      "2600 Training Loss: tensor(0.3494)\n",
      "2601 Training Loss: tensor(0.3518)\n",
      "2602 Training Loss: tensor(0.3495)\n",
      "2603 Training Loss: tensor(0.3528)\n",
      "2604 Training Loss: tensor(0.3522)\n",
      "2605 Training Loss: tensor(0.3516)\n",
      "2606 Training Loss: tensor(0.3503)\n",
      "2607 Training Loss: tensor(0.3486)\n",
      "2608 Training Loss: tensor(0.3495)\n",
      "2609 Training Loss: tensor(0.3488)\n",
      "2610 Training Loss: tensor(0.3542)\n",
      "2611 Training Loss: tensor(0.3517)\n",
      "2612 Training Loss: tensor(0.3573)\n",
      "2613 Training Loss: tensor(0.3493)\n",
      "2614 Training Loss: tensor(0.3545)\n",
      "2615 Training Loss: tensor(0.3532)\n",
      "2616 Training Loss: tensor(0.3525)\n",
      "2617 Training Loss: tensor(0.3493)\n",
      "2618 Training Loss: tensor(0.3502)\n",
      "2619 Training Loss: tensor(0.3559)\n",
      "2620 Training Loss: tensor(0.3503)\n",
      "2621 Training Loss: tensor(0.3529)\n",
      "2622 Training Loss: tensor(0.3514)\n",
      "2623 Training Loss: tensor(0.3508)\n",
      "2624 Training Loss: tensor(0.3526)\n",
      "2625 Training Loss: tensor(0.3570)\n",
      "2626 Training Loss: tensor(0.3500)\n",
      "2627 Training Loss: tensor(0.3505)\n",
      "2628 Training Loss: tensor(0.3513)\n",
      "2629 Training Loss: tensor(0.3519)\n",
      "2630 Training Loss: tensor(0.3503)\n",
      "2631 Training Loss: tensor(0.3496)\n",
      "2632 Training Loss: tensor(0.3511)\n",
      "2633 Training Loss: tensor(0.3504)\n",
      "2634 Training Loss: tensor(0.3528)\n",
      "2635 Training Loss: tensor(0.3516)\n",
      "2636 Training Loss: tensor(0.3506)\n",
      "2637 Training Loss: tensor(0.3560)\n",
      "2638 Training Loss: tensor(0.3513)\n",
      "2639 Training Loss: tensor(0.3495)\n",
      "2640 Training Loss: tensor(0.3503)\n",
      "2641 Training Loss: tensor(0.3498)\n",
      "2642 Training Loss: tensor(0.3495)\n",
      "2643 Training Loss: tensor(0.3528)\n",
      "2644 Training Loss: tensor(0.3510)\n",
      "2645 Training Loss: tensor(0.3513)\n",
      "2646 Training Loss: tensor(0.3495)\n",
      "2647 Training Loss: tensor(0.3501)\n",
      "2648 Training Loss: tensor(0.3509)\n",
      "2649 Training Loss: tensor(0.3517)\n",
      "2650 Training Loss: tensor(0.3502)\n",
      "2651 Training Loss: tensor(0.3490)\n",
      "2652 Training Loss: tensor(0.3547)\n",
      "2653 Training Loss: tensor(0.3488)\n",
      "2654 Training Loss: tensor(0.3501)\n",
      "2655 Training Loss: tensor(0.3487)\n",
      "2656 Training Loss: tensor(0.3517)\n",
      "2657 Training Loss: tensor(0.3487)\n",
      "2658 Training Loss: tensor(0.3484)\n",
      "2659 Training Loss: tensor(0.3506)\n",
      "2660 Training Loss: tensor(0.3535)\n",
      "2661 Training Loss: tensor(0.3493)\n",
      "2662 Training Loss: tensor(0.3523)\n",
      "2663 Training Loss: tensor(0.3494)\n",
      "2664 Training Loss: tensor(0.3492)\n",
      "2665 Training Loss: tensor(0.3503)\n",
      "2666 Training Loss: tensor(0.3513)\n",
      "2667 Training Loss: tensor(0.3550)\n",
      "2668 Training Loss: tensor(0.3501)\n",
      "2669 Training Loss: tensor(0.3515)\n",
      "2670 Training Loss: tensor(0.3520)\n",
      "2671 Training Loss: tensor(0.3496)\n",
      "2672 Training Loss: tensor(0.3505)\n",
      "2673 Training Loss: tensor(0.3528)\n",
      "2674 Training Loss: tensor(0.3492)\n",
      "2675 Training Loss: tensor(0.3541)\n",
      "2676 Training Loss: tensor(0.3499)\n",
      "2677 Training Loss: tensor(0.3494)\n",
      "2678 Training Loss: tensor(0.3488)\n",
      "2679 Training Loss: tensor(0.3530)\n",
      "2680 Training Loss: tensor(0.3522)\n",
      "2681 Training Loss: tensor(0.3505)\n",
      "2682 Training Loss: tensor(0.3516)\n",
      "2683 Training Loss: tensor(0.3547)\n",
      "2684 Training Loss: tensor(0.3491)\n",
      "2685 Training Loss: tensor(0.3510)\n",
      "2686 Training Loss: tensor(0.3505)\n",
      "2687 Training Loss: tensor(0.3489)\n",
      "2688 Training Loss: tensor(0.3507)\n",
      "2689 Training Loss: tensor(0.3495)\n",
      "2690 Training Loss: tensor(0.3496)\n",
      "2691 Training Loss: tensor(0.3489)\n",
      "2692 Training Loss: tensor(0.3519)\n",
      "2693 Training Loss: tensor(0.3487)\n",
      "2694 Training Loss: tensor(0.3648)\n",
      "2695 Training Loss: tensor(0.3501)\n",
      "2696 Training Loss: tensor(0.3646)\n",
      "2697 Training Loss: tensor(0.3520)\n",
      "2698 Training Loss: tensor(0.3490)\n",
      "2699 Training Loss: tensor(0.3506)\n",
      "2700 Training Loss: tensor(0.3508)\n",
      "2701 Training Loss: tensor(0.3507)\n",
      "2702 Training Loss: tensor(0.3511)\n",
      "2703 Training Loss: tensor(0.3505)\n",
      "2704 Training Loss: tensor(0.3510)\n",
      "2705 Training Loss: tensor(0.3515)\n",
      "2706 Training Loss: tensor(0.3516)\n",
      "2707 Training Loss: tensor(0.3508)\n",
      "2708 Training Loss: tensor(0.3560)\n",
      "2709 Training Loss: tensor(0.3506)\n",
      "2710 Training Loss: tensor(0.3491)\n",
      "2711 Training Loss: tensor(0.3499)\n",
      "2712 Training Loss: tensor(0.3512)\n",
      "2713 Training Loss: tensor(0.3496)\n",
      "2714 Training Loss: tensor(0.3522)\n",
      "2715 Training Loss: tensor(0.3506)\n",
      "2716 Training Loss: tensor(0.3538)\n",
      "2717 Training Loss: tensor(0.3533)\n",
      "2718 Training Loss: tensor(0.3546)\n",
      "2719 Training Loss: tensor(0.3489)\n",
      "2720 Training Loss: tensor(0.3485)\n",
      "2721 Training Loss: tensor(0.3488)\n",
      "2722 Training Loss: tensor(0.3500)\n",
      "2723 Training Loss: tensor(0.3513)\n",
      "2724 Training Loss: tensor(0.3558)\n",
      "2725 Training Loss: tensor(0.3513)\n",
      "2726 Training Loss: tensor(0.3590)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727 Training Loss: tensor(0.3484)\n",
      "2728 Training Loss: tensor(0.3508)\n",
      "2729 Training Loss: tensor(0.3516)\n",
      "2730 Training Loss: tensor(0.3540)\n",
      "2731 Training Loss: tensor(0.3524)\n",
      "2732 Training Loss: tensor(0.3509)\n",
      "2733 Training Loss: tensor(0.3491)\n",
      "2734 Training Loss: tensor(0.3502)\n",
      "2735 Training Loss: tensor(0.3532)\n",
      "2736 Training Loss: tensor(0.3502)\n",
      "2737 Training Loss: tensor(0.3497)\n",
      "2738 Training Loss: tensor(0.3505)\n",
      "2739 Training Loss: tensor(0.3540)\n",
      "2740 Training Loss: tensor(0.3498)\n",
      "2741 Training Loss: tensor(0.3531)\n",
      "2742 Training Loss: tensor(0.3496)\n",
      "2743 Training Loss: tensor(0.3503)\n",
      "2744 Training Loss: tensor(0.3516)\n",
      "2745 Training Loss: tensor(0.3485)\n",
      "2746 Training Loss: tensor(0.3492)\n",
      "2747 Training Loss: tensor(0.3501)\n",
      "2748 Training Loss: tensor(0.3500)\n",
      "2749 Training Loss: tensor(0.3511)\n",
      "2750 Training Loss: tensor(0.3491)\n",
      "2751 Training Loss: tensor(0.3506)\n",
      "2752 Training Loss: tensor(0.3509)\n",
      "2753 Training Loss: tensor(0.3493)\n",
      "2754 Training Loss: tensor(0.3483)\n",
      "2755 Training Loss: tensor(0.3540)\n",
      "2756 Training Loss: tensor(0.3486)\n",
      "2757 Training Loss: tensor(0.3489)\n",
      "2758 Training Loss: tensor(0.3501)\n",
      "2759 Training Loss: tensor(0.3515)\n",
      "2760 Training Loss: tensor(0.3483)\n",
      "2761 Training Loss: tensor(0.3490)\n",
      "2762 Training Loss: tensor(0.3508)\n",
      "2763 Training Loss: tensor(0.3524)\n",
      "2764 Training Loss: tensor(0.3477)\n",
      "2765 Training Loss: tensor(0.3484)\n",
      "2766 Training Loss: tensor(0.3549)\n",
      "2767 Training Loss: tensor(0.3472)\n",
      "2768 Training Loss: tensor(0.3487)\n",
      "2769 Training Loss: tensor(0.3485)\n",
      "2770 Training Loss: tensor(0.3498)\n",
      "2771 Training Loss: tensor(0.3490)\n",
      "2772 Training Loss: tensor(0.3502)\n",
      "2773 Training Loss: tensor(0.3582)\n",
      "2774 Training Loss: tensor(0.3550)\n",
      "2775 Training Loss: tensor(0.3487)\n",
      "2776 Training Loss: tensor(0.3502)\n",
      "2777 Training Loss: tensor(0.3510)\n",
      "2778 Training Loss: tensor(0.3576)\n",
      "2779 Training Loss: tensor(0.3510)\n",
      "2780 Training Loss: tensor(0.3492)\n",
      "2781 Training Loss: tensor(0.3572)\n",
      "2782 Training Loss: tensor(0.3501)\n",
      "2783 Training Loss: tensor(0.3521)\n",
      "2784 Training Loss: tensor(0.3518)\n",
      "2785 Training Loss: tensor(0.3504)\n",
      "2786 Training Loss: tensor(0.3513)\n",
      "2787 Training Loss: tensor(0.3496)\n",
      "2788 Training Loss: tensor(0.3492)\n",
      "2789 Training Loss: tensor(0.3503)\n",
      "2790 Training Loss: tensor(0.3547)\n",
      "2791 Training Loss: tensor(0.3503)\n",
      "2792 Training Loss: tensor(0.3536)\n",
      "2793 Training Loss: tensor(0.3547)\n",
      "2794 Training Loss: tensor(0.3584)\n",
      "2795 Training Loss: tensor(0.3497)\n",
      "2796 Training Loss: tensor(0.3503)\n",
      "2797 Training Loss: tensor(0.3526)\n",
      "2798 Training Loss: tensor(0.3525)\n",
      "2799 Training Loss: tensor(0.3500)\n",
      "2800 Training Loss: tensor(0.3563)\n",
      "2801 Training Loss: tensor(0.3492)\n",
      "2802 Training Loss: tensor(0.3517)\n",
      "2803 Training Loss: tensor(0.3503)\n",
      "2804 Training Loss: tensor(0.3558)\n",
      "2805 Training Loss: tensor(0.3492)\n",
      "2806 Training Loss: tensor(0.3506)\n",
      "2807 Training Loss: tensor(0.3491)\n",
      "2808 Training Loss: tensor(0.3642)\n",
      "2809 Training Loss: tensor(0.3529)\n",
      "2810 Training Loss: tensor(0.3488)\n",
      "2811 Training Loss: tensor(0.3489)\n",
      "2812 Training Loss: tensor(0.3490)\n",
      "2813 Training Loss: tensor(0.3522)\n",
      "2814 Training Loss: tensor(0.3506)\n",
      "2815 Training Loss: tensor(0.3537)\n",
      "2816 Training Loss: tensor(0.3504)\n",
      "2817 Training Loss: tensor(0.3538)\n",
      "2818 Training Loss: tensor(0.3500)\n",
      "2819 Training Loss: tensor(0.3490)\n",
      "2820 Training Loss: tensor(0.3502)\n",
      "2821 Training Loss: tensor(0.3500)\n",
      "2822 Training Loss: tensor(0.3515)\n",
      "2823 Training Loss: tensor(0.3508)\n",
      "2824 Training Loss: tensor(0.3497)\n",
      "2825 Training Loss: tensor(0.3483)\n",
      "2826 Training Loss: tensor(0.3498)\n",
      "2827 Training Loss: tensor(0.3490)\n",
      "2828 Training Loss: tensor(0.3486)\n",
      "2829 Training Loss: tensor(0.3500)\n",
      "2830 Training Loss: tensor(0.3552)\n",
      "2831 Training Loss: tensor(0.3526)\n",
      "2832 Training Loss: tensor(0.3492)\n",
      "2833 Training Loss: tensor(0.3547)\n",
      "2834 Training Loss: tensor(0.3491)\n",
      "2835 Training Loss: tensor(0.3481)\n",
      "2836 Training Loss: tensor(0.3549)\n",
      "2837 Training Loss: tensor(0.3506)\n",
      "2838 Training Loss: tensor(0.3478)\n",
      "2839 Training Loss: tensor(0.3487)\n",
      "2840 Training Loss: tensor(0.3481)\n",
      "2841 Training Loss: tensor(0.3491)\n",
      "2842 Training Loss: tensor(0.3486)\n",
      "2843 Training Loss: tensor(0.3535)\n",
      "2844 Training Loss: tensor(0.3513)\n",
      "2845 Training Loss: tensor(0.3493)\n",
      "2846 Training Loss: tensor(0.3500)\n",
      "2847 Training Loss: tensor(0.3538)\n",
      "2848 Training Loss: tensor(0.3502)\n",
      "2849 Training Loss: tensor(0.3488)\n",
      "2850 Training Loss: tensor(0.3537)\n",
      "2851 Training Loss: tensor(0.3482)\n",
      "2852 Training Loss: tensor(0.3487)\n",
      "2853 Training Loss: tensor(0.3494)\n",
      "2854 Training Loss: tensor(0.3494)\n",
      "2855 Training Loss: tensor(0.3512)\n",
      "2856 Training Loss: tensor(0.3487)\n",
      "2857 Training Loss: tensor(0.3502)\n",
      "2858 Training Loss: tensor(0.3547)\n",
      "2859 Training Loss: tensor(0.3535)\n",
      "2860 Training Loss: tensor(0.3531)\n",
      "2861 Training Loss: tensor(0.3604)\n",
      "2862 Training Loss: tensor(0.3489)\n",
      "2863 Training Loss: tensor(0.3490)\n",
      "2864 Training Loss: tensor(0.3488)\n",
      "2865 Training Loss: tensor(0.3498)\n",
      "2866 Training Loss: tensor(0.3496)\n",
      "2867 Training Loss: tensor(0.3518)\n",
      "2868 Training Loss: tensor(0.3482)\n",
      "2869 Training Loss: tensor(0.3523)\n",
      "2870 Training Loss: tensor(0.3508)\n",
      "2871 Training Loss: tensor(0.3496)\n",
      "2872 Training Loss: tensor(0.3485)\n",
      "2873 Training Loss: tensor(0.3474)\n",
      "2874 Training Loss: tensor(0.3489)\n",
      "2875 Training Loss: tensor(0.3485)\n",
      "2876 Training Loss: tensor(0.3576)\n",
      "2877 Training Loss: tensor(0.3542)\n",
      "2878 Training Loss: tensor(0.3542)\n",
      "2879 Training Loss: tensor(0.3518)\n",
      "2880 Training Loss: tensor(0.3494)\n",
      "2881 Training Loss: tensor(0.3510)\n",
      "2882 Training Loss: tensor(0.3539)\n",
      "2883 Training Loss: tensor(0.3536)\n",
      "2884 Training Loss: tensor(0.3498)\n",
      "2885 Training Loss: tensor(0.3522)\n",
      "2886 Training Loss: tensor(0.3500)\n",
      "2887 Training Loss: tensor(0.3489)\n",
      "2888 Training Loss: tensor(0.3510)\n",
      "2889 Training Loss: tensor(0.3515)\n",
      "2890 Training Loss: tensor(0.3518)\n",
      "2891 Training Loss: tensor(0.3493)\n",
      "2892 Training Loss: tensor(0.3514)\n",
      "2893 Training Loss: tensor(0.3489)\n",
      "2894 Training Loss: tensor(0.3490)\n",
      "2895 Training Loss: tensor(0.3496)\n",
      "2896 Training Loss: tensor(0.3492)\n",
      "2897 Training Loss: tensor(0.3491)\n",
      "2898 Training Loss: tensor(0.3513)\n",
      "2899 Training Loss: tensor(0.3523)\n",
      "2900 Training Loss: tensor(0.3514)\n",
      "2901 Training Loss: tensor(0.3514)\n",
      "2902 Training Loss: tensor(0.3487)\n",
      "2903 Training Loss: tensor(0.3498)\n",
      "2904 Training Loss: tensor(0.3545)\n",
      "2905 Training Loss: tensor(0.3480)\n",
      "2906 Training Loss: tensor(0.3494)\n",
      "2907 Training Loss: tensor(0.3497)\n",
      "2908 Training Loss: tensor(0.3496)\n",
      "2909 Training Loss: tensor(0.3501)\n",
      "2910 Training Loss: tensor(0.3487)\n",
      "2911 Training Loss: tensor(0.3491)\n",
      "2912 Training Loss: tensor(0.3542)\n",
      "2913 Training Loss: tensor(0.3493)\n",
      "2914 Training Loss: tensor(0.3474)\n",
      "2915 Training Loss: tensor(0.3494)\n",
      "2916 Training Loss: tensor(0.3528)\n",
      "2917 Training Loss: tensor(0.3542)\n",
      "2918 Training Loss: tensor(0.3544)\n",
      "2919 Training Loss: tensor(0.3492)\n",
      "2920 Training Loss: tensor(0.3495)\n",
      "2921 Training Loss: tensor(0.3516)\n",
      "2922 Training Loss: tensor(0.3503)\n",
      "2923 Training Loss: tensor(0.3540)\n",
      "2924 Training Loss: tensor(0.3530)\n",
      "2925 Training Loss: tensor(0.3482)\n",
      "2926 Training Loss: tensor(0.3533)\n",
      "2927 Training Loss: tensor(0.3494)\n",
      "2928 Training Loss: tensor(0.3503)\n",
      "2929 Training Loss: tensor(0.3548)\n",
      "2930 Training Loss: tensor(0.3490)\n",
      "2931 Training Loss: tensor(0.3530)\n",
      "2932 Training Loss: tensor(0.3499)\n",
      "2933 Training Loss: tensor(0.3494)\n",
      "2934 Training Loss: tensor(0.3506)\n",
      "2935 Training Loss: tensor(0.3502)\n",
      "2936 Training Loss: tensor(0.3492)\n",
      "2937 Training Loss: tensor(0.3570)\n",
      "2938 Training Loss: tensor(0.3491)\n",
      "2939 Training Loss: tensor(0.3519)\n",
      "2940 Training Loss: tensor(0.3604)\n",
      "2941 Training Loss: tensor(0.3498)\n",
      "2942 Training Loss: tensor(0.3538)\n",
      "2943 Training Loss: tensor(0.3493)\n",
      "2944 Training Loss: tensor(0.3501)\n",
      "2945 Training Loss: tensor(0.3486)\n",
      "2946 Training Loss: tensor(0.3512)\n",
      "2947 Training Loss: tensor(0.3497)\n",
      "2948 Training Loss: tensor(0.3503)\n",
      "2949 Training Loss: tensor(0.3486)\n",
      "2950 Training Loss: tensor(0.3489)\n",
      "2951 Training Loss: tensor(0.3479)\n",
      "2952 Training Loss: tensor(0.3536)\n",
      "2953 Training Loss: tensor(0.3490)\n",
      "2954 Training Loss: tensor(0.3534)\n",
      "2955 Training Loss: tensor(0.3477)\n",
      "2956 Training Loss: tensor(0.3482)\n",
      "2957 Training Loss: tensor(0.3491)\n",
      "2958 Training Loss: tensor(0.3545)\n",
      "2959 Training Loss: tensor(0.3481)\n",
      "2960 Training Loss: tensor(0.3507)\n",
      "2961 Training Loss: tensor(0.3518)\n",
      "2962 Training Loss: tensor(0.3476)\n",
      "2963 Training Loss: tensor(0.3485)\n",
      "2964 Training Loss: tensor(0.3480)\n",
      "2965 Training Loss: tensor(0.3553)\n",
      "2966 Training Loss: tensor(0.3490)\n",
      "2967 Training Loss: tensor(0.3487)\n",
      "2968 Training Loss: tensor(0.3567)\n",
      "2969 Training Loss: tensor(0.3521)\n",
      "2970 Training Loss: tensor(0.3495)\n",
      "2971 Training Loss: tensor(0.3537)\n",
      "2972 Training Loss: tensor(0.3491)\n",
      "2973 Training Loss: tensor(0.3495)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2974 Training Loss: tensor(0.3512)\n",
      "2975 Training Loss: tensor(0.3505)\n",
      "2976 Training Loss: tensor(0.3489)\n",
      "2977 Training Loss: tensor(0.3538)\n",
      "2978 Training Loss: tensor(0.3484)\n",
      "2979 Training Loss: tensor(0.3556)\n",
      "2980 Training Loss: tensor(0.3487)\n",
      "2981 Training Loss: tensor(0.3496)\n",
      "2982 Training Loss: tensor(0.3556)\n",
      "2983 Training Loss: tensor(0.3510)\n",
      "2984 Training Loss: tensor(0.3514)\n",
      "2985 Training Loss: tensor(0.3493)\n",
      "2986 Training Loss: tensor(0.3550)\n",
      "2987 Training Loss: tensor(0.3527)\n",
      "2988 Training Loss: tensor(0.3488)\n",
      "2989 Training Loss: tensor(0.3492)\n",
      "2990 Training Loss: tensor(0.3506)\n",
      "2991 Training Loss: tensor(0.3505)\n",
      "2992 Training Loss: tensor(0.3509)\n",
      "2993 Training Loss: tensor(0.3505)\n",
      "2994 Training Loss: tensor(0.3503)\n",
      "2995 Training Loss: tensor(0.3486)\n",
      "2996 Training Loss: tensor(0.3492)\n",
      "2997 Training Loss: tensor(0.3591)\n",
      "2998 Training Loss: tensor(0.3483)\n",
      "2999 Training Loss: tensor(0.3494)\n",
      "3000 Training Loss: tensor(0.3500)\n",
      "3001 Training Loss: tensor(0.3487)\n",
      "3002 Training Loss: tensor(0.3479)\n",
      "3003 Training Loss: tensor(0.3507)\n",
      "3004 Training Loss: tensor(0.3485)\n",
      "3005 Training Loss: tensor(0.3491)\n",
      "3006 Training Loss: tensor(0.3513)\n",
      "3007 Training Loss: tensor(0.3474)\n",
      "3008 Training Loss: tensor(0.3494)\n",
      "3009 Training Loss: tensor(0.3496)\n",
      "3010 Training Loss: tensor(0.3532)\n",
      "3011 Training Loss: tensor(0.3486)\n",
      "3012 Training Loss: tensor(0.3599)\n",
      "3013 Training Loss: tensor(0.3603)\n",
      "3014 Training Loss: tensor(0.3537)\n",
      "3015 Training Loss: tensor(0.3487)\n",
      "3016 Training Loss: tensor(0.3508)\n",
      "3017 Training Loss: tensor(0.3538)\n",
      "3018 Training Loss: tensor(0.3517)\n",
      "3019 Training Loss: tensor(0.3497)\n",
      "3020 Training Loss: tensor(0.3537)\n",
      "3021 Training Loss: tensor(0.3491)\n",
      "3022 Training Loss: tensor(0.3505)\n",
      "3023 Training Loss: tensor(0.3501)\n",
      "3024 Training Loss: tensor(0.3502)\n",
      "3025 Training Loss: tensor(0.3506)\n",
      "3026 Training Loss: tensor(0.3496)\n",
      "3027 Training Loss: tensor(0.3549)\n",
      "3028 Training Loss: tensor(0.3532)\n",
      "3029 Training Loss: tensor(0.3536)\n",
      "3030 Training Loss: tensor(0.3502)\n",
      "3031 Training Loss: tensor(0.3482)\n",
      "3032 Training Loss: tensor(0.3489)\n",
      "3033 Training Loss: tensor(0.3487)\n",
      "3034 Training Loss: tensor(0.3539)\n",
      "3035 Training Loss: tensor(0.3484)\n",
      "3036 Training Loss: tensor(0.3517)\n",
      "3037 Training Loss: tensor(0.3494)\n",
      "3038 Training Loss: tensor(0.3514)\n",
      "3039 Training Loss: tensor(0.3506)\n",
      "3040 Training Loss: tensor(0.3487)\n",
      "3041 Training Loss: tensor(0.3520)\n",
      "3042 Training Loss: tensor(0.3484)\n",
      "3043 Training Loss: tensor(0.3507)\n",
      "3044 Training Loss: tensor(0.3477)\n",
      "3045 Training Loss: tensor(0.3488)\n",
      "3046 Training Loss: tensor(0.3497)\n",
      "3047 Training Loss: tensor(0.3497)\n",
      "3048 Training Loss: tensor(0.3579)\n",
      "3049 Training Loss: tensor(0.3480)\n",
      "3050 Training Loss: tensor(0.3494)\n",
      "3051 Training Loss: tensor(0.3493)\n",
      "3052 Training Loss: tensor(0.3508)\n",
      "3053 Training Loss: tensor(0.3471)\n",
      "3054 Training Loss: tensor(0.3481)\n",
      "3055 Training Loss: tensor(0.3543)\n",
      "3056 Training Loss: tensor(0.3477)\n",
      "3057 Training Loss: tensor(0.3501)\n",
      "3058 Training Loss: tensor(0.3497)\n",
      "3059 Training Loss: tensor(0.3514)\n",
      "3060 Training Loss: tensor(0.3486)\n",
      "3061 Training Loss: tensor(0.3498)\n",
      "3062 Training Loss: tensor(0.3486)\n",
      "3063 Training Loss: tensor(0.3577)\n",
      "3064 Training Loss: tensor(0.3565)\n",
      "3065 Training Loss: tensor(0.3493)\n",
      "3066 Training Loss: tensor(0.3544)\n",
      "3067 Training Loss: tensor(0.3495)\n",
      "3068 Training Loss: tensor(0.3569)\n",
      "3069 Training Loss: tensor(0.3524)\n",
      "3070 Training Loss: tensor(0.3482)\n",
      "3071 Training Loss: tensor(0.3487)\n",
      "3072 Training Loss: tensor(0.3492)\n",
      "3073 Training Loss: tensor(0.3515)\n",
      "3074 Training Loss: tensor(0.3490)\n",
      "3075 Training Loss: tensor(0.3528)\n",
      "3076 Training Loss: tensor(0.3504)\n",
      "3077 Training Loss: tensor(0.3491)\n",
      "3078 Training Loss: tensor(0.3549)\n",
      "3079 Training Loss: tensor(0.3483)\n",
      "3080 Training Loss: tensor(0.3492)\n",
      "3081 Training Loss: tensor(0.3578)\n",
      "3082 Training Loss: tensor(0.3503)\n",
      "3083 Training Loss: tensor(0.3540)\n",
      "3084 Training Loss: tensor(0.3513)\n",
      "3085 Training Loss: tensor(0.3502)\n",
      "3086 Training Loss: tensor(0.3495)\n",
      "3087 Training Loss: tensor(0.3484)\n",
      "3088 Training Loss: tensor(0.3502)\n",
      "3089 Training Loss: tensor(0.3517)\n",
      "3090 Training Loss: tensor(0.3486)\n",
      "3091 Training Loss: tensor(0.3562)\n",
      "3092 Training Loss: tensor(0.3548)\n",
      "3093 Training Loss: tensor(0.3486)\n",
      "3094 Training Loss: tensor(0.3489)\n",
      "3095 Training Loss: tensor(0.3620)\n",
      "3096 Training Loss: tensor(0.3525)\n",
      "3097 Training Loss: tensor(0.3498)\n",
      "3098 Training Loss: tensor(0.3481)\n",
      "3099 Training Loss: tensor(0.3499)\n",
      "3100 Training Loss: tensor(0.3510)\n",
      "3101 Training Loss: tensor(0.3493)\n",
      "3102 Training Loss: tensor(0.3506)\n",
      "3103 Training Loss: tensor(0.3487)\n",
      "3104 Training Loss: tensor(0.3565)\n",
      "3105 Training Loss: tensor(0.3505)\n",
      "3106 Training Loss: tensor(0.3494)\n",
      "3107 Training Loss: tensor(0.3495)\n",
      "3108 Training Loss: tensor(0.3568)\n",
      "3109 Training Loss: tensor(0.3496)\n",
      "3110 Training Loss: tensor(0.3500)\n",
      "3111 Training Loss: tensor(0.3500)\n",
      "3112 Training Loss: tensor(0.3493)\n",
      "3113 Training Loss: tensor(0.3490)\n",
      "3114 Training Loss: tensor(0.3570)\n",
      "3115 Training Loss: tensor(0.3506)\n",
      "3116 Training Loss: tensor(0.3532)\n",
      "3117 Training Loss: tensor(0.3500)\n",
      "3118 Training Loss: tensor(0.3543)\n",
      "3119 Training Loss: tensor(0.3488)\n",
      "3120 Training Loss: tensor(0.3503)\n",
      "3121 Training Loss: tensor(0.3490)\n",
      "3122 Training Loss: tensor(0.3489)\n",
      "3123 Training Loss: tensor(0.3502)\n",
      "3124 Training Loss: tensor(0.3535)\n",
      "3125 Training Loss: tensor(0.3493)\n",
      "3126 Training Loss: tensor(0.3506)\n",
      "3127 Training Loss: tensor(0.3481)\n",
      "3128 Training Loss: tensor(0.3499)\n",
      "3129 Training Loss: tensor(0.3492)\n",
      "3130 Training Loss: tensor(0.3527)\n",
      "3131 Training Loss: tensor(0.3493)\n",
      "3132 Training Loss: tensor(0.3517)\n",
      "3133 Training Loss: tensor(0.3484)\n",
      "3134 Training Loss: tensor(0.3485)\n",
      "3135 Training Loss: tensor(0.3501)\n",
      "3136 Training Loss: tensor(0.3482)\n",
      "3137 Training Loss: tensor(0.3571)\n",
      "3138 Training Loss: tensor(0.3496)\n",
      "3139 Training Loss: tensor(0.3494)\n",
      "3140 Training Loss: tensor(0.3494)\n",
      "3141 Training Loss: tensor(0.3494)\n",
      "3142 Training Loss: tensor(0.3528)\n",
      "3143 Training Loss: tensor(0.3490)\n",
      "3144 Training Loss: tensor(0.3488)\n",
      "3145 Training Loss: tensor(0.3521)\n",
      "3146 Training Loss: tensor(0.3550)\n",
      "3147 Training Loss: tensor(0.3521)\n",
      "3148 Training Loss: tensor(0.3476)\n",
      "3149 Training Loss: tensor(0.3521)\n",
      "3150 Training Loss: tensor(0.3483)\n",
      "3151 Training Loss: tensor(0.3485)\n",
      "3152 Training Loss: tensor(0.3502)\n",
      "3153 Training Loss: tensor(0.3501)\n",
      "3154 Training Loss: tensor(0.3488)\n",
      "3155 Training Loss: tensor(0.3488)\n",
      "3156 Training Loss: tensor(0.3505)\n",
      "3157 Training Loss: tensor(0.3550)\n",
      "3158 Training Loss: tensor(0.3486)\n",
      "3159 Training Loss: tensor(0.3530)\n",
      "3160 Training Loss: tensor(0.3562)\n",
      "3161 Training Loss: tensor(0.3491)\n",
      "3162 Training Loss: tensor(0.3528)\n",
      "3163 Training Loss: tensor(0.3525)\n",
      "3164 Training Loss: tensor(0.3496)\n",
      "3165 Training Loss: tensor(0.3488)\n",
      "3166 Training Loss: tensor(0.3522)\n",
      "3167 Training Loss: tensor(0.3496)\n",
      "3168 Training Loss: tensor(0.3492)\n",
      "3169 Training Loss: tensor(0.3553)\n",
      "3170 Training Loss: tensor(0.3490)\n",
      "3171 Training Loss: tensor(0.3509)\n",
      "3172 Training Loss: tensor(0.3502)\n",
      "3173 Training Loss: tensor(0.3568)\n",
      "3174 Training Loss: tensor(0.3528)\n",
      "3175 Training Loss: tensor(0.3490)\n",
      "3176 Training Loss: tensor(0.3497)\n",
      "3177 Training Loss: tensor(0.3538)\n",
      "3178 Training Loss: tensor(0.3560)\n",
      "3179 Training Loss: tensor(0.3512)\n",
      "3180 Training Loss: tensor(0.3523)\n",
      "3181 Training Loss: tensor(0.3493)\n",
      "3182 Training Loss: tensor(0.3494)\n",
      "3183 Training Loss: tensor(0.3510)\n",
      "3184 Training Loss: tensor(0.3508)\n",
      "3185 Training Loss: tensor(0.3514)\n",
      "3186 Training Loss: tensor(0.3513)\n",
      "3187 Training Loss: tensor(0.3519)\n",
      "3188 Training Loss: tensor(0.3518)\n",
      "3189 Training Loss: tensor(0.3531)\n",
      "3190 Training Loss: tensor(0.3538)\n",
      "3191 Training Loss: tensor(0.3512)\n",
      "3192 Training Loss: tensor(0.3507)\n",
      "3193 Training Loss: tensor(0.3566)\n",
      "3194 Training Loss: tensor(0.3524)\n",
      "3195 Training Loss: tensor(0.3514)\n",
      "3196 Training Loss: tensor(0.3508)\n",
      "3197 Training Loss: tensor(0.3519)\n",
      "3198 Training Loss: tensor(0.3535)\n",
      "3199 Training Loss: tensor(0.3505)\n",
      "3200 Training Loss: tensor(0.3513)\n",
      "3201 Training Loss: tensor(0.3491)\n",
      "3202 Training Loss: tensor(0.3494)\n",
      "3203 Training Loss: tensor(0.3490)\n",
      "3204 Training Loss: tensor(0.3502)\n",
      "3205 Training Loss: tensor(0.3498)\n",
      "3206 Training Loss: tensor(0.3499)\n",
      "3207 Training Loss: tensor(0.3545)\n",
      "3208 Training Loss: tensor(0.3495)\n",
      "3209 Training Loss: tensor(0.3500)\n",
      "3210 Training Loss: tensor(0.3514)\n",
      "3211 Training Loss: tensor(0.3486)\n",
      "3212 Training Loss: tensor(0.3499)\n",
      "3213 Training Loss: tensor(0.3493)\n",
      "3214 Training Loss: tensor(0.3508)\n",
      "3215 Training Loss: tensor(0.3508)\n",
      "3216 Training Loss: tensor(0.3496)\n",
      "3217 Training Loss: tensor(0.3477)\n",
      "3218 Training Loss: tensor(0.3483)\n",
      "3219 Training Loss: tensor(0.3525)\n",
      "3220 Training Loss: tensor(0.3478)\n",
      "3221 Training Loss: tensor(0.3476)\n",
      "3222 Training Loss: tensor(0.3508)\n",
      "3223 Training Loss: tensor(0.3498)\n",
      "3224 Training Loss: tensor(0.3481)\n",
      "3225 Training Loss: tensor(0.3552)\n",
      "3226 Training Loss: tensor(0.3494)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3227 Training Loss: tensor(0.3577)\n",
      "3228 Training Loss: tensor(0.3478)\n",
      "3229 Training Loss: tensor(0.3543)\n",
      "3230 Training Loss: tensor(0.3492)\n",
      "3231 Training Loss: tensor(0.3504)\n",
      "3232 Training Loss: tensor(0.3486)\n",
      "3233 Training Loss: tensor(0.3483)\n",
      "3234 Training Loss: tensor(0.3530)\n",
      "3235 Training Loss: tensor(0.3498)\n",
      "3236 Training Loss: tensor(0.3489)\n",
      "3237 Training Loss: tensor(0.3496)\n",
      "3238 Training Loss: tensor(0.3491)\n",
      "3239 Training Loss: tensor(0.3551)\n",
      "3240 Training Loss: tensor(0.3496)\n",
      "3241 Training Loss: tensor(0.3488)\n",
      "3242 Training Loss: tensor(0.3495)\n",
      "3243 Training Loss: tensor(0.3490)\n",
      "3244 Training Loss: tensor(0.3542)\n",
      "3245 Training Loss: tensor(0.3511)\n",
      "3246 Training Loss: tensor(0.3492)\n",
      "3247 Training Loss: tensor(0.3492)\n",
      "3248 Training Loss: tensor(0.3490)\n",
      "3249 Training Loss: tensor(0.3480)\n",
      "3250 Training Loss: tensor(0.3492)\n",
      "3251 Training Loss: tensor(0.3515)\n",
      "3252 Training Loss: tensor(0.3471)\n",
      "3253 Training Loss: tensor(0.3531)\n",
      "3254 Training Loss: tensor(0.3481)\n",
      "3255 Training Loss: tensor(0.3499)\n",
      "3256 Training Loss: tensor(0.3497)\n",
      "3257 Training Loss: tensor(0.3479)\n",
      "3258 Training Loss: tensor(0.3533)\n",
      "3259 Training Loss: tensor(0.3497)\n",
      "3260 Training Loss: tensor(0.3605)\n",
      "3261 Training Loss: tensor(0.3492)\n",
      "3262 Training Loss: tensor(0.3538)\n",
      "3263 Training Loss: tensor(0.3483)\n",
      "3264 Training Loss: tensor(0.3549)\n",
      "3265 Training Loss: tensor(0.3555)\n",
      "3266 Training Loss: tensor(0.3491)\n",
      "3267 Training Loss: tensor(0.3506)\n",
      "3268 Training Loss: tensor(0.3502)\n",
      "3269 Training Loss: tensor(0.3505)\n",
      "3270 Training Loss: tensor(0.3488)\n",
      "3271 Training Loss: tensor(0.3503)\n",
      "3272 Training Loss: tensor(0.3508)\n",
      "3273 Training Loss: tensor(0.3502)\n",
      "3274 Training Loss: tensor(0.3495)\n",
      "3275 Training Loss: tensor(0.3489)\n",
      "3276 Training Loss: tensor(0.3503)\n",
      "3277 Training Loss: tensor(0.3485)\n",
      "3278 Training Loss: tensor(0.3502)\n",
      "3279 Training Loss: tensor(0.3524)\n",
      "3280 Training Loss: tensor(0.3514)\n",
      "3281 Training Loss: tensor(0.3598)\n",
      "3282 Training Loss: tensor(0.3518)\n",
      "3283 Training Loss: tensor(0.3494)\n",
      "3284 Training Loss: tensor(0.3523)\n",
      "3285 Training Loss: tensor(0.3522)\n",
      "3286 Training Loss: tensor(0.3512)\n",
      "3287 Training Loss: tensor(0.3488)\n",
      "3288 Training Loss: tensor(0.3492)\n",
      "3289 Training Loss: tensor(0.3553)\n",
      "3290 Training Loss: tensor(0.3499)\n",
      "3291 Training Loss: tensor(0.3477)\n",
      "3292 Training Loss: tensor(0.3512)\n",
      "3293 Training Loss: tensor(0.3496)\n",
      "3294 Training Loss: tensor(0.3487)\n",
      "3295 Training Loss: tensor(0.3488)\n",
      "3296 Training Loss: tensor(0.3504)\n",
      "3297 Training Loss: tensor(0.3498)\n",
      "3298 Training Loss: tensor(0.3488)\n",
      "3299 Training Loss: tensor(0.3630)\n",
      "3300 Training Loss: tensor(0.3484)\n",
      "3301 Training Loss: tensor(0.3497)\n",
      "3302 Training Loss: tensor(0.3485)\n",
      "3303 Training Loss: tensor(0.3508)\n",
      "3304 Training Loss: tensor(0.3517)\n",
      "3305 Training Loss: tensor(0.3497)\n",
      "3306 Training Loss: tensor(0.3504)\n",
      "3307 Training Loss: tensor(0.3500)\n",
      "3308 Training Loss: tensor(0.3498)\n",
      "3309 Training Loss: tensor(0.3500)\n",
      "3310 Training Loss: tensor(0.3541)\n",
      "3311 Training Loss: tensor(0.3503)\n",
      "3312 Training Loss: tensor(0.3480)\n",
      "3313 Training Loss: tensor(0.3520)\n",
      "3314 Training Loss: tensor(0.3490)\n",
      "3315 Training Loss: tensor(0.3574)\n",
      "3316 Training Loss: tensor(0.3522)\n",
      "3317 Training Loss: tensor(0.3476)\n",
      "3318 Training Loss: tensor(0.3561)\n",
      "3319 Training Loss: tensor(0.3499)\n",
      "3320 Training Loss: tensor(0.3486)\n",
      "3321 Training Loss: tensor(0.3548)\n",
      "3322 Training Loss: tensor(0.3517)\n",
      "3323 Training Loss: tensor(0.3519)\n",
      "3324 Training Loss: tensor(0.3541)\n",
      "3325 Training Loss: tensor(0.3493)\n",
      "3326 Training Loss: tensor(0.3502)\n",
      "3327 Training Loss: tensor(0.3513)\n",
      "3328 Training Loss: tensor(0.3511)\n",
      "3329 Training Loss: tensor(0.3505)\n",
      "3330 Training Loss: tensor(0.3498)\n",
      "3331 Training Loss: tensor(0.3520)\n",
      "3332 Training Loss: tensor(0.3505)\n",
      "3333 Training Loss: tensor(0.3528)\n",
      "3334 Training Loss: tensor(0.3517)\n",
      "3335 Training Loss: tensor(0.3527)\n",
      "3336 Training Loss: tensor(0.3485)\n",
      "3337 Training Loss: tensor(0.3496)\n",
      "3338 Training Loss: tensor(0.3550)\n",
      "3339 Training Loss: tensor(0.3495)\n",
      "3340 Training Loss: tensor(0.3493)\n",
      "3341 Training Loss: tensor(0.3507)\n",
      "3342 Training Loss: tensor(0.3494)\n",
      "3343 Training Loss: tensor(0.3512)\n",
      "3344 Training Loss: tensor(0.3535)\n",
      "3345 Training Loss: tensor(0.3500)\n",
      "3346 Training Loss: tensor(0.3519)\n",
      "3347 Training Loss: tensor(0.3503)\n",
      "3348 Training Loss: tensor(0.3489)\n",
      "3349 Training Loss: tensor(0.3492)\n",
      "3350 Training Loss: tensor(0.3490)\n",
      "3351 Training Loss: tensor(0.3482)\n",
      "3352 Training Loss: tensor(0.3567)\n",
      "3353 Training Loss: tensor(0.3526)\n",
      "3354 Training Loss: tensor(0.3496)\n",
      "3355 Training Loss: tensor(0.3496)\n",
      "3356 Training Loss: tensor(0.3485)\n",
      "3357 Training Loss: tensor(0.3532)\n",
      "3358 Training Loss: tensor(0.3514)\n",
      "3359 Training Loss: tensor(0.3491)\n",
      "3360 Training Loss: tensor(0.3494)\n",
      "3361 Training Loss: tensor(0.3505)\n",
      "3362 Training Loss: tensor(0.3537)\n",
      "3363 Training Loss: tensor(0.3491)\n",
      "3364 Training Loss: tensor(0.3498)\n",
      "3365 Training Loss: tensor(0.3488)\n",
      "3366 Training Loss: tensor(0.3489)\n",
      "3367 Training Loss: tensor(0.3499)\n",
      "3368 Training Loss: tensor(0.3483)\n",
      "3369 Training Loss: tensor(0.3512)\n",
      "3370 Training Loss: tensor(0.3556)\n",
      "3371 Training Loss: tensor(0.3480)\n",
      "3372 Training Loss: tensor(0.3480)\n",
      "3373 Training Loss: tensor(0.3483)\n",
      "3374 Training Loss: tensor(0.3500)\n",
      "3375 Training Loss: tensor(0.3572)\n",
      "3376 Training Loss: tensor(0.3496)\n",
      "3377 Training Loss: tensor(0.3523)\n",
      "3378 Training Loss: tensor(0.3491)\n",
      "3379 Training Loss: tensor(0.3494)\n",
      "3380 Training Loss: tensor(0.3486)\n",
      "3381 Training Loss: tensor(0.3486)\n",
      "3382 Training Loss: tensor(0.3498)\n",
      "3383 Training Loss: tensor(0.3499)\n",
      "3384 Training Loss: tensor(0.3526)\n",
      "3385 Training Loss: tensor(0.3488)\n",
      "3386 Training Loss: tensor(0.3512)\n",
      "3387 Training Loss: tensor(0.3500)\n",
      "3388 Training Loss: tensor(0.3482)\n",
      "3389 Training Loss: tensor(0.3530)\n",
      "3390 Training Loss: tensor(0.3485)\n",
      "3391 Training Loss: tensor(0.3508)\n",
      "3392 Training Loss: tensor(0.3482)\n",
      "3393 Training Loss: tensor(0.3578)\n",
      "3394 Training Loss: tensor(0.3600)\n",
      "3395 Training Loss: tensor(0.3554)\n",
      "3396 Training Loss: tensor(0.3496)\n",
      "3397 Training Loss: tensor(0.3522)\n",
      "3398 Training Loss: tensor(0.3485)\n",
      "3399 Training Loss: tensor(0.3490)\n",
      "3400 Training Loss: tensor(0.3502)\n",
      "3401 Training Loss: tensor(0.3485)\n",
      "3402 Training Loss: tensor(0.3485)\n",
      "3403 Training Loss: tensor(0.3492)\n",
      "3404 Training Loss: tensor(0.3506)\n",
      "3405 Training Loss: tensor(0.3524)\n",
      "3406 Training Loss: tensor(0.3493)\n",
      "3407 Training Loss: tensor(0.3498)\n",
      "3408 Training Loss: tensor(0.3488)\n",
      "3409 Training Loss: tensor(0.3508)\n",
      "3410 Training Loss: tensor(0.3499)\n",
      "3411 Training Loss: tensor(0.3481)\n",
      "3412 Training Loss: tensor(0.3545)\n",
      "3413 Training Loss: tensor(0.3482)\n",
      "3414 Training Loss: tensor(0.3510)\n",
      "3415 Training Loss: tensor(0.3481)\n",
      "3416 Training Loss: tensor(0.3532)\n",
      "3417 Training Loss: tensor(0.3508)\n",
      "3418 Training Loss: tensor(0.3521)\n",
      "3419 Training Loss: tensor(0.3530)\n",
      "3420 Training Loss: tensor(0.3531)\n",
      "3421 Training Loss: tensor(0.3493)\n",
      "3422 Training Loss: tensor(0.3476)\n",
      "3423 Training Loss: tensor(0.3498)\n",
      "3424 Training Loss: tensor(0.3486)\n",
      "3425 Training Loss: tensor(0.3503)\n",
      "3426 Training Loss: tensor(0.3509)\n",
      "3427 Training Loss: tensor(0.3522)\n",
      "3428 Training Loss: tensor(0.3485)\n",
      "3429 Training Loss: tensor(0.3561)\n",
      "3430 Training Loss: tensor(0.3473)\n",
      "3431 Training Loss: tensor(0.3483)\n",
      "3432 Training Loss: tensor(0.3499)\n",
      "3433 Training Loss: tensor(0.3539)\n",
      "3434 Training Loss: tensor(0.3506)\n",
      "3435 Training Loss: tensor(0.3494)\n",
      "3436 Training Loss: tensor(0.3478)\n",
      "3437 Training Loss: tensor(0.3490)\n",
      "3438 Training Loss: tensor(0.3584)\n",
      "3439 Training Loss: tensor(0.3475)\n",
      "3440 Training Loss: tensor(0.3496)\n",
      "3441 Training Loss: tensor(0.3524)\n",
      "3442 Training Loss: tensor(0.3538)\n",
      "3443 Training Loss: tensor(0.3493)\n",
      "3444 Training Loss: tensor(0.3488)\n",
      "3445 Training Loss: tensor(0.3500)\n",
      "3446 Training Loss: tensor(0.3511)\n",
      "3447 Training Loss: tensor(0.3488)\n",
      "3448 Training Loss: tensor(0.3479)\n",
      "3449 Training Loss: tensor(0.3478)\n",
      "3450 Training Loss: tensor(0.3497)\n",
      "3451 Training Loss: tensor(0.3481)\n",
      "3452 Training Loss: tensor(0.3512)\n",
      "3453 Training Loss: tensor(0.3493)\n",
      "3454 Training Loss: tensor(0.3489)\n",
      "3455 Training Loss: tensor(0.3483)\n",
      "3456 Training Loss: tensor(0.3503)\n",
      "3457 Training Loss: tensor(0.3533)\n",
      "3458 Training Loss: tensor(0.3493)\n",
      "3459 Training Loss: tensor(0.3494)\n",
      "3460 Training Loss: tensor(0.3507)\n",
      "3461 Training Loss: tensor(0.3532)\n",
      "3462 Training Loss: tensor(0.3582)\n",
      "3463 Training Loss: tensor(0.3511)\n",
      "3464 Training Loss: tensor(0.3490)\n",
      "3465 Training Loss: tensor(0.3485)\n",
      "3466 Training Loss: tensor(0.3485)\n",
      "3467 Training Loss: tensor(0.3487)\n",
      "3468 Training Loss: tensor(0.3480)\n",
      "3469 Training Loss: tensor(0.3524)\n",
      "3470 Training Loss: tensor(0.3481)\n",
      "3471 Training Loss: tensor(0.3506)\n",
      "3472 Training Loss: tensor(0.3525)\n",
      "3473 Training Loss: tensor(0.3552)\n",
      "3474 Training Loss: tensor(0.3479)\n",
      "3475 Training Loss: tensor(0.3484)\n",
      "3476 Training Loss: tensor(0.3486)\n",
      "3477 Training Loss: tensor(0.3499)\n",
      "3478 Training Loss: tensor(0.3529)\n",
      "3479 Training Loss: tensor(0.3493)\n",
      "3480 Training Loss: tensor(0.3504)\n",
      "3481 Training Loss: tensor(0.3484)\n",
      "3482 Training Loss: tensor(0.3491)\n",
      "3483 Training Loss: tensor(0.3478)\n",
      "3484 Training Loss: tensor(0.3536)\n",
      "3485 Training Loss: tensor(0.3481)\n",
      "3486 Training Loss: tensor(0.3498)\n",
      "3487 Training Loss: tensor(0.3503)\n",
      "3488 Training Loss: tensor(0.3475)\n",
      "3489 Training Loss: tensor(0.3478)\n",
      "3490 Training Loss: tensor(0.3478)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3491 Training Loss: tensor(0.3527)\n",
      "3492 Training Loss: tensor(0.3480)\n",
      "3493 Training Loss: tensor(0.3470)\n",
      "3494 Training Loss: tensor(0.3526)\n",
      "3495 Training Loss: tensor(0.3473)\n",
      "3496 Training Loss: tensor(0.3519)\n",
      "3497 Training Loss: tensor(0.3564)\n",
      "3498 Training Loss: tensor(0.3500)\n",
      "3499 Training Loss: tensor(0.3477)\n",
      "3500 Training Loss: tensor(0.3497)\n",
      "3501 Training Loss: tensor(0.3488)\n",
      "3502 Training Loss: tensor(0.3587)\n",
      "3503 Training Loss: tensor(0.3487)\n",
      "3504 Training Loss: tensor(0.3505)\n",
      "3505 Training Loss: tensor(0.3502)\n",
      "3506 Training Loss: tensor(0.3484)\n",
      "3507 Training Loss: tensor(0.3515)\n",
      "3508 Training Loss: tensor(0.3483)\n",
      "3509 Training Loss: tensor(0.3509)\n",
      "3510 Training Loss: tensor(0.3516)\n",
      "3511 Training Loss: tensor(0.3495)\n",
      "3512 Training Loss: tensor(0.3528)\n",
      "3513 Training Loss: tensor(0.3509)\n",
      "3514 Training Loss: tensor(0.3491)\n",
      "3515 Training Loss: tensor(0.3501)\n",
      "3516 Training Loss: tensor(0.3530)\n",
      "3517 Training Loss: tensor(0.3543)\n",
      "3518 Training Loss: tensor(0.3508)\n",
      "3519 Training Loss: tensor(0.3481)\n",
      "3520 Training Loss: tensor(0.3517)\n",
      "3521 Training Loss: tensor(0.3487)\n",
      "3522 Training Loss: tensor(0.3504)\n",
      "3523 Training Loss: tensor(0.3521)\n",
      "3524 Training Loss: tensor(0.3550)\n",
      "3525 Training Loss: tensor(0.3509)\n",
      "3526 Training Loss: tensor(0.3541)\n",
      "3527 Training Loss: tensor(0.3539)\n",
      "3528 Training Loss: tensor(0.3538)\n",
      "3529 Training Loss: tensor(0.3495)\n",
      "3530 Training Loss: tensor(0.3485)\n",
      "3531 Training Loss: tensor(0.3495)\n",
      "3532 Training Loss: tensor(0.3486)\n",
      "3533 Training Loss: tensor(0.3502)\n",
      "3534 Training Loss: tensor(0.3614)\n",
      "3535 Training Loss: tensor(0.3539)\n",
      "3536 Training Loss: tensor(0.3543)\n",
      "3537 Training Loss: tensor(0.3501)\n",
      "3538 Training Loss: tensor(0.3511)\n",
      "3539 Training Loss: tensor(0.3491)\n",
      "3540 Training Loss: tensor(0.3525)\n",
      "3541 Training Loss: tensor(0.3498)\n",
      "3542 Training Loss: tensor(0.3519)\n",
      "3543 Training Loss: tensor(0.3537)\n",
      "3544 Training Loss: tensor(0.3505)\n",
      "3545 Training Loss: tensor(0.3489)\n",
      "3546 Training Loss: tensor(0.3517)\n",
      "3547 Training Loss: tensor(0.3488)\n",
      "3548 Training Loss: tensor(0.3494)\n",
      "3549 Training Loss: tensor(0.3517)\n",
      "3550 Training Loss: tensor(0.3492)\n",
      "3551 Training Loss: tensor(0.3495)\n",
      "3552 Training Loss: tensor(0.3477)\n",
      "3553 Training Loss: tensor(0.3530)\n",
      "3554 Training Loss: tensor(0.3525)\n",
      "3555 Training Loss: tensor(0.3534)\n",
      "3556 Training Loss: tensor(0.3527)\n",
      "3557 Training Loss: tensor(0.3492)\n",
      "3558 Training Loss: tensor(0.3552)\n",
      "3559 Training Loss: tensor(0.3479)\n",
      "3560 Training Loss: tensor(0.3490)\n",
      "3561 Training Loss: tensor(0.3508)\n",
      "3562 Training Loss: tensor(0.3526)\n",
      "3563 Training Loss: tensor(0.3480)\n",
      "3564 Training Loss: tensor(0.3516)\n",
      "3565 Training Loss: tensor(0.3477)\n",
      "3566 Training Loss: tensor(0.3499)\n",
      "3567 Training Loss: tensor(0.3508)\n",
      "3568 Training Loss: tensor(0.3476)\n",
      "3569 Training Loss: tensor(0.3493)\n",
      "3570 Training Loss: tensor(0.3499)\n",
      "3571 Training Loss: tensor(0.3488)\n",
      "3572 Training Loss: tensor(0.3475)\n",
      "3573 Training Loss: tensor(0.3486)\n",
      "3574 Training Loss: tensor(0.3482)\n",
      "3575 Training Loss: tensor(0.3477)\n",
      "3576 Training Loss: tensor(0.3497)\n",
      "3577 Training Loss: tensor(0.3477)\n",
      "3578 Training Loss: tensor(0.3546)\n",
      "3579 Training Loss: tensor(0.3556)\n",
      "3580 Training Loss: tensor(0.3513)\n",
      "3581 Training Loss: tensor(0.3516)\n",
      "3582 Training Loss: tensor(0.3531)\n",
      "3583 Training Loss: tensor(0.3486)\n",
      "3584 Training Loss: tensor(0.3492)\n",
      "3585 Training Loss: tensor(0.3487)\n",
      "3586 Training Loss: tensor(0.3558)\n",
      "3587 Training Loss: tensor(0.3504)\n",
      "3588 Training Loss: tensor(0.3522)\n",
      "3589 Training Loss: tensor(0.3482)\n",
      "3590 Training Loss: tensor(0.3577)\n",
      "3591 Training Loss: tensor(0.3505)\n",
      "3592 Training Loss: tensor(0.3542)\n",
      "3593 Training Loss: tensor(0.3501)\n",
      "3594 Training Loss: tensor(0.3502)\n",
      "3595 Training Loss: tensor(0.3502)\n",
      "3596 Training Loss: tensor(0.3495)\n",
      "3597 Training Loss: tensor(0.3495)\n",
      "3598 Training Loss: tensor(0.3494)\n",
      "3599 Training Loss: tensor(0.3497)\n",
      "3600 Training Loss: tensor(0.3489)\n",
      "3601 Training Loss: tensor(0.3533)\n",
      "3602 Training Loss: tensor(0.3489)\n",
      "3603 Training Loss: tensor(0.3486)\n",
      "3604 Training Loss: tensor(0.3475)\n",
      "3605 Training Loss: tensor(0.3491)\n",
      "3606 Training Loss: tensor(0.3485)\n",
      "3607 Training Loss: tensor(0.3491)\n",
      "3608 Training Loss: tensor(0.3504)\n",
      "3609 Training Loss: tensor(0.3476)\n",
      "3610 Training Loss: tensor(0.3511)\n",
      "3611 Training Loss: tensor(0.3479)\n",
      "3612 Training Loss: tensor(0.3477)\n",
      "3613 Training Loss: tensor(0.3525)\n",
      "3614 Training Loss: tensor(0.3506)\n",
      "3615 Training Loss: tensor(0.3479)\n",
      "3616 Training Loss: tensor(0.3513)\n",
      "3617 Training Loss: tensor(0.3483)\n",
      "3618 Training Loss: tensor(0.3487)\n",
      "3619 Training Loss: tensor(0.3526)\n",
      "3620 Training Loss: tensor(0.3558)\n",
      "3621 Training Loss: tensor(0.3503)\n",
      "3622 Training Loss: tensor(0.3533)\n",
      "3623 Training Loss: tensor(0.3507)\n",
      "3624 Training Loss: tensor(0.3489)\n",
      "3625 Training Loss: tensor(0.3497)\n",
      "3626 Training Loss: tensor(0.3528)\n",
      "3627 Training Loss: tensor(0.3527)\n",
      "3628 Training Loss: tensor(0.3517)\n",
      "3629 Training Loss: tensor(0.3498)\n",
      "3630 Training Loss: tensor(0.3497)\n",
      "3631 Training Loss: tensor(0.3521)\n",
      "3632 Training Loss: tensor(0.3492)\n",
      "3633 Training Loss: tensor(0.3512)\n",
      "3634 Training Loss: tensor(0.3539)\n",
      "3635 Training Loss: tensor(0.3507)\n",
      "3636 Training Loss: tensor(0.3507)\n",
      "3637 Training Loss: tensor(0.3487)\n",
      "3638 Training Loss: tensor(0.3498)\n",
      "3639 Training Loss: tensor(0.3504)\n",
      "3640 Training Loss: tensor(0.3486)\n",
      "3641 Training Loss: tensor(0.3484)\n",
      "3642 Training Loss: tensor(0.3535)\n",
      "3643 Training Loss: tensor(0.3508)\n",
      "3644 Training Loss: tensor(0.3487)\n",
      "3645 Training Loss: tensor(0.3477)\n",
      "3646 Training Loss: tensor(0.3492)\n",
      "3647 Training Loss: tensor(0.3495)\n",
      "3648 Training Loss: tensor(0.3524)\n",
      "3649 Training Loss: tensor(0.3498)\n",
      "3650 Training Loss: tensor(0.3497)\n",
      "3651 Training Loss: tensor(0.3487)\n",
      "3652 Training Loss: tensor(0.3492)\n",
      "3653 Training Loss: tensor(0.3498)\n",
      "3654 Training Loss: tensor(0.3468)\n",
      "3655 Training Loss: tensor(0.3475)\n",
      "3656 Training Loss: tensor(0.3488)\n",
      "3657 Training Loss: tensor(0.3559)\n",
      "3658 Training Loss: tensor(0.3512)\n",
      "3659 Training Loss: tensor(0.3469)\n",
      "3660 Training Loss: tensor(0.3529)\n",
      "3661 Training Loss: tensor(0.3609)\n",
      "3662 Training Loss: tensor(0.3510)\n",
      "3663 Training Loss: tensor(0.3491)\n",
      "3664 Training Loss: tensor(0.3483)\n",
      "3665 Training Loss: tensor(0.3510)\n",
      "3666 Training Loss: tensor(0.3485)\n",
      "3667 Training Loss: tensor(0.3567)\n",
      "3668 Training Loss: tensor(0.3507)\n",
      "3669 Training Loss: tensor(0.3519)\n",
      "3670 Training Loss: tensor(0.3497)\n",
      "3671 Training Loss: tensor(0.3523)\n",
      "3672 Training Loss: tensor(0.3494)\n",
      "3673 Training Loss: tensor(0.3498)\n",
      "3674 Training Loss: tensor(0.3521)\n",
      "3675 Training Loss: tensor(0.3529)\n",
      "3676 Training Loss: tensor(0.3498)\n",
      "3677 Training Loss: tensor(0.3540)\n",
      "3678 Training Loss: tensor(0.3513)\n",
      "3679 Training Loss: tensor(0.3489)\n",
      "3680 Training Loss: tensor(0.3522)\n",
      "3681 Training Loss: tensor(0.3504)\n",
      "3682 Training Loss: tensor(0.3493)\n",
      "3683 Training Loss: tensor(0.3498)\n",
      "3684 Training Loss: tensor(0.3497)\n",
      "3685 Training Loss: tensor(0.3502)\n",
      "3686 Training Loss: tensor(0.3529)\n",
      "3687 Training Loss: tensor(0.3523)\n",
      "3688 Training Loss: tensor(0.3499)\n",
      "3689 Training Loss: tensor(0.3512)\n",
      "3690 Training Loss: tensor(0.3487)\n",
      "3691 Training Loss: tensor(0.3533)\n",
      "3692 Training Loss: tensor(0.3485)\n",
      "3693 Training Loss: tensor(0.3498)\n",
      "3694 Training Loss: tensor(0.3514)\n",
      "3695 Training Loss: tensor(0.3538)\n",
      "3696 Training Loss: tensor(0.3480)\n",
      "3697 Training Loss: tensor(0.3509)\n",
      "3698 Training Loss: tensor(0.3509)\n",
      "3699 Training Loss: tensor(0.3510)\n",
      "3700 Training Loss: tensor(0.3490)\n",
      "3701 Training Loss: tensor(0.3483)\n",
      "3702 Training Loss: tensor(0.3509)\n",
      "3703 Training Loss: tensor(0.3564)\n",
      "3704 Training Loss: tensor(0.3560)\n",
      "3705 Training Loss: tensor(0.3484)\n",
      "3706 Training Loss: tensor(0.3486)\n",
      "3707 Training Loss: tensor(0.3517)\n",
      "3708 Training Loss: tensor(0.3487)\n",
      "3709 Training Loss: tensor(0.3521)\n",
      "3710 Training Loss: tensor(0.3488)\n",
      "3711 Training Loss: tensor(0.3487)\n",
      "3712 Training Loss: tensor(0.3483)\n",
      "3713 Training Loss: tensor(0.3484)\n",
      "3714 Training Loss: tensor(0.3484)\n",
      "3715 Training Loss: tensor(0.3517)\n",
      "3716 Training Loss: tensor(0.3486)\n",
      "3717 Training Loss: tensor(0.3503)\n",
      "3718 Training Loss: tensor(0.3488)\n",
      "3719 Training Loss: tensor(0.3494)\n",
      "3720 Training Loss: tensor(0.3565)\n",
      "3721 Training Loss: tensor(0.3492)\n",
      "3722 Training Loss: tensor(0.3504)\n",
      "3723 Training Loss: tensor(0.3476)\n",
      "3724 Training Loss: tensor(0.3516)\n",
      "3725 Training Loss: tensor(0.3482)\n",
      "3726 Training Loss: tensor(0.3553)\n",
      "3727 Training Loss: tensor(0.3566)\n",
      "3728 Training Loss: tensor(0.3492)\n",
      "3729 Training Loss: tensor(0.3511)\n",
      "3730 Training Loss: tensor(0.3486)\n",
      "3731 Training Loss: tensor(0.3486)\n",
      "3732 Training Loss: tensor(0.3529)\n",
      "3733 Training Loss: tensor(0.3491)\n",
      "3734 Training Loss: tensor(0.3495)\n",
      "3735 Training Loss: tensor(0.3519)\n",
      "3736 Training Loss: tensor(0.3486)\n",
      "3737 Training Loss: tensor(0.3506)\n",
      "3738 Training Loss: tensor(0.3481)\n",
      "3739 Training Loss: tensor(0.3501)\n",
      "3740 Training Loss: tensor(0.3507)\n",
      "3741 Training Loss: tensor(0.3527)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742 Training Loss: tensor(0.3492)\n",
      "3743 Training Loss: tensor(0.3539)\n",
      "3744 Training Loss: tensor(0.3492)\n",
      "3745 Training Loss: tensor(0.3489)\n",
      "3746 Training Loss: tensor(0.3508)\n",
      "3747 Training Loss: tensor(0.3491)\n",
      "3748 Training Loss: tensor(0.3513)\n",
      "3749 Training Loss: tensor(0.3507)\n",
      "3750 Training Loss: tensor(0.3489)\n",
      "3751 Training Loss: tensor(0.3497)\n",
      "3752 Training Loss: tensor(0.3497)\n",
      "3753 Training Loss: tensor(0.3526)\n",
      "3754 Training Loss: tensor(0.3506)\n",
      "3755 Training Loss: tensor(0.3530)\n",
      "3756 Training Loss: tensor(0.3493)\n",
      "3757 Training Loss: tensor(0.3492)\n",
      "3758 Training Loss: tensor(0.3496)\n",
      "3759 Training Loss: tensor(0.3501)\n",
      "3760 Training Loss: tensor(0.3499)\n",
      "3761 Training Loss: tensor(0.3502)\n",
      "3762 Training Loss: tensor(0.3492)\n",
      "3763 Training Loss: tensor(0.3531)\n",
      "3764 Training Loss: tensor(0.3489)\n",
      "3765 Training Loss: tensor(0.3479)\n",
      "3766 Training Loss: tensor(0.3496)\n",
      "3767 Training Loss: tensor(0.3518)\n",
      "3768 Training Loss: tensor(0.3503)\n",
      "3769 Training Loss: tensor(0.3502)\n",
      "3770 Training Loss: tensor(0.3566)\n",
      "3771 Training Loss: tensor(0.3479)\n",
      "3772 Training Loss: tensor(0.3504)\n",
      "3773 Training Loss: tensor(0.3487)\n",
      "3774 Training Loss: tensor(0.3506)\n",
      "3775 Training Loss: tensor(0.3545)\n",
      "3776 Training Loss: tensor(0.3485)\n",
      "3777 Training Loss: tensor(0.3532)\n",
      "3778 Training Loss: tensor(0.3487)\n",
      "3779 Training Loss: tensor(0.3478)\n",
      "3780 Training Loss: tensor(0.3483)\n",
      "3781 Training Loss: tensor(0.3494)\n",
      "3782 Training Loss: tensor(0.3478)\n",
      "3783 Training Loss: tensor(0.3473)\n",
      "3784 Training Loss: tensor(0.3509)\n",
      "3785 Training Loss: tensor(0.3483)\n",
      "3786 Training Loss: tensor(0.3512)\n",
      "3787 Training Loss: tensor(0.3479)\n",
      "3788 Training Loss: tensor(0.3509)\n",
      "3789 Training Loss: tensor(0.3490)\n",
      "3790 Training Loss: tensor(0.3546)\n",
      "3791 Training Loss: tensor(0.3504)\n",
      "3792 Training Loss: tensor(0.3493)\n",
      "3793 Training Loss: tensor(0.3490)\n",
      "3794 Training Loss: tensor(0.3483)\n",
      "3795 Training Loss: tensor(0.3489)\n",
      "3796 Training Loss: tensor(0.3532)\n",
      "3797 Training Loss: tensor(0.3484)\n",
      "3798 Training Loss: tensor(0.3548)\n",
      "3799 Training Loss: tensor(0.3526)\n",
      "3800 Training Loss: tensor(0.3495)\n",
      "3801 Training Loss: tensor(0.3528)\n",
      "3802 Training Loss: tensor(0.3473)\n",
      "3803 Training Loss: tensor(0.3482)\n",
      "3804 Training Loss: tensor(0.3488)\n",
      "3805 Training Loss: tensor(0.3495)\n",
      "3806 Training Loss: tensor(0.3491)\n",
      "3807 Training Loss: tensor(0.3539)\n",
      "3808 Training Loss: tensor(0.3496)\n",
      "3809 Training Loss: tensor(0.3487)\n",
      "3810 Training Loss: tensor(0.3472)\n",
      "3811 Training Loss: tensor(0.3514)\n",
      "3812 Training Loss: tensor(0.3486)\n",
      "3813 Training Loss: tensor(0.3495)\n",
      "3814 Training Loss: tensor(0.3519)\n",
      "3815 Training Loss: tensor(0.3546)\n",
      "3816 Training Loss: tensor(0.3480)\n",
      "3817 Training Loss: tensor(0.3531)\n",
      "3818 Training Loss: tensor(0.3472)\n",
      "3819 Training Loss: tensor(0.3503)\n",
      "3820 Training Loss: tensor(0.3476)\n",
      "3821 Training Loss: tensor(0.3480)\n",
      "3822 Training Loss: tensor(0.3496)\n",
      "3823 Training Loss: tensor(0.3515)\n",
      "3824 Training Loss: tensor(0.3487)\n",
      "3825 Training Loss: tensor(0.3487)\n",
      "3826 Training Loss: tensor(0.3537)\n",
      "3827 Training Loss: tensor(0.3473)\n",
      "3828 Training Loss: tensor(0.3477)\n",
      "3829 Training Loss: tensor(0.3526)\n",
      "3830 Training Loss: tensor(0.3524)\n",
      "3831 Training Loss: tensor(0.3476)\n",
      "3832 Training Loss: tensor(0.3489)\n",
      "3833 Training Loss: tensor(0.3491)\n",
      "3834 Training Loss: tensor(0.3512)\n",
      "3835 Training Loss: tensor(0.3479)\n",
      "3836 Training Loss: tensor(0.3540)\n",
      "3837 Training Loss: tensor(0.3530)\n",
      "3838 Training Loss: tensor(0.3481)\n",
      "3839 Training Loss: tensor(0.3522)\n",
      "3840 Training Loss: tensor(0.3521)\n",
      "3841 Training Loss: tensor(0.3481)\n",
      "3842 Training Loss: tensor(0.3507)\n",
      "3843 Training Loss: tensor(0.3511)\n",
      "3844 Training Loss: tensor(0.3474)\n",
      "3845 Training Loss: tensor(0.3488)\n",
      "3846 Training Loss: tensor(0.3552)\n",
      "3847 Training Loss: tensor(0.3469)\n",
      "3848 Training Loss: tensor(0.3490)\n",
      "3849 Training Loss: tensor(0.3469)\n",
      "3850 Training Loss: tensor(0.3555)\n",
      "3851 Training Loss: tensor(0.3490)\n",
      "3852 Training Loss: tensor(0.3476)\n",
      "3853 Training Loss: tensor(0.3491)\n",
      "3854 Training Loss: tensor(0.3490)\n",
      "3855 Training Loss: tensor(0.3487)\n",
      "3856 Training Loss: tensor(0.3476)\n",
      "3857 Training Loss: tensor(0.3506)\n",
      "3858 Training Loss: tensor(0.3479)\n",
      "3859 Training Loss: tensor(0.3514)\n",
      "3860 Training Loss: tensor(0.3536)\n",
      "3861 Training Loss: tensor(0.3483)\n",
      "3862 Training Loss: tensor(0.3507)\n",
      "3863 Training Loss: tensor(0.3517)\n",
      "3864 Training Loss: tensor(0.3469)\n",
      "3865 Training Loss: tensor(0.3487)\n",
      "3866 Training Loss: tensor(0.3515)\n",
      "3867 Training Loss: tensor(0.3503)\n",
      "3868 Training Loss: tensor(0.3535)\n",
      "3869 Training Loss: tensor(0.3514)\n",
      "3870 Training Loss: tensor(0.3483)\n",
      "3871 Training Loss: tensor(0.3486)\n",
      "3872 Training Loss: tensor(0.3481)\n",
      "3873 Training Loss: tensor(0.3523)\n",
      "3874 Training Loss: tensor(0.3570)\n",
      "3875 Training Loss: tensor(0.3520)\n",
      "3876 Training Loss: tensor(0.3478)\n",
      "3877 Training Loss: tensor(0.3553)\n",
      "3878 Training Loss: tensor(0.3517)\n",
      "3879 Training Loss: tensor(0.3483)\n",
      "3880 Training Loss: tensor(0.3520)\n",
      "3881 Training Loss: tensor(0.3531)\n",
      "3882 Training Loss: tensor(0.3504)\n",
      "3883 Training Loss: tensor(0.3487)\n",
      "3884 Training Loss: tensor(0.3480)\n",
      "3885 Training Loss: tensor(0.3485)\n",
      "3886 Training Loss: tensor(0.3520)\n",
      "3887 Training Loss: tensor(0.3551)\n",
      "3888 Training Loss: tensor(0.3515)\n",
      "3889 Training Loss: tensor(0.3502)\n",
      "3890 Training Loss: tensor(0.3514)\n",
      "3891 Training Loss: tensor(0.3485)\n",
      "3892 Training Loss: tensor(0.3479)\n",
      "3893 Training Loss: tensor(0.3489)\n",
      "3894 Training Loss: tensor(0.3490)\n",
      "3895 Training Loss: tensor(0.3484)\n",
      "3896 Training Loss: tensor(0.3480)\n",
      "3897 Training Loss: tensor(0.3491)\n",
      "3898 Training Loss: tensor(0.3518)\n",
      "3899 Training Loss: tensor(0.3515)\n",
      "3900 Training Loss: tensor(0.3531)\n",
      "3901 Training Loss: tensor(0.3474)\n",
      "3902 Training Loss: tensor(0.3478)\n",
      "3903 Training Loss: tensor(0.3479)\n",
      "3904 Training Loss: tensor(0.3490)\n",
      "3905 Training Loss: tensor(0.3525)\n",
      "3906 Training Loss: tensor(0.3492)\n",
      "3907 Training Loss: tensor(0.3496)\n",
      "3908 Training Loss: tensor(0.3474)\n",
      "3909 Training Loss: tensor(0.3536)\n",
      "3910 Training Loss: tensor(0.3506)\n",
      "3911 Training Loss: tensor(0.3477)\n",
      "3912 Training Loss: tensor(0.3481)\n",
      "3913 Training Loss: tensor(0.3476)\n",
      "3914 Training Loss: tensor(0.3467)\n",
      "3915 Training Loss: tensor(0.3637)\n",
      "3916 Training Loss: tensor(0.3491)\n",
      "3917 Training Loss: tensor(0.3489)\n",
      "3918 Training Loss: tensor(0.3528)\n",
      "3919 Training Loss: tensor(0.3506)\n",
      "3920 Training Loss: tensor(0.3502)\n",
      "3921 Training Loss: tensor(0.3486)\n",
      "3922 Training Loss: tensor(0.3519)\n",
      "3923 Training Loss: tensor(0.3497)\n",
      "3924 Training Loss: tensor(0.3506)\n",
      "3925 Training Loss: tensor(0.3491)\n",
      "3926 Training Loss: tensor(0.3498)\n",
      "3927 Training Loss: tensor(0.3516)\n",
      "3928 Training Loss: tensor(0.3484)\n",
      "3929 Training Loss: tensor(0.3505)\n",
      "3930 Training Loss: tensor(0.3554)\n",
      "3931 Training Loss: tensor(0.3491)\n",
      "3932 Training Loss: tensor(0.3488)\n",
      "3933 Training Loss: tensor(0.3502)\n",
      "3934 Training Loss: tensor(0.3477)\n",
      "3935 Training Loss: tensor(0.3493)\n",
      "3936 Training Loss: tensor(0.3486)\n",
      "3937 Training Loss: tensor(0.3503)\n",
      "3938 Training Loss: tensor(0.3479)\n",
      "3939 Training Loss: tensor(0.3500)\n",
      "3940 Training Loss: tensor(0.3494)\n",
      "3941 Training Loss: tensor(0.3486)\n",
      "3942 Training Loss: tensor(0.3494)\n",
      "3943 Training Loss: tensor(0.3484)\n",
      "3944 Training Loss: tensor(0.3502)\n",
      "3945 Training Loss: tensor(0.3516)\n",
      "3946 Training Loss: tensor(0.3526)\n",
      "3947 Training Loss: tensor(0.3494)\n",
      "3948 Training Loss: tensor(0.3547)\n",
      "3949 Training Loss: tensor(0.3483)\n",
      "3950 Training Loss: tensor(0.3477)\n",
      "3951 Training Loss: tensor(0.3472)\n",
      "3952 Training Loss: tensor(0.3483)\n",
      "3953 Training Loss: tensor(0.3468)\n",
      "3954 Training Loss: tensor(0.3601)\n",
      "3955 Training Loss: tensor(0.3493)\n",
      "3956 Training Loss: tensor(0.3508)\n",
      "3957 Training Loss: tensor(0.3526)\n",
      "3958 Training Loss: tensor(0.3537)\n",
      "3959 Training Loss: tensor(0.3501)\n",
      "3960 Training Loss: tensor(0.3477)\n",
      "3961 Training Loss: tensor(0.3477)\n",
      "3962 Training Loss: tensor(0.3500)\n",
      "3963 Training Loss: tensor(0.3521)\n",
      "3964 Training Loss: tensor(0.3545)\n",
      "3965 Training Loss: tensor(0.3513)\n",
      "3966 Training Loss: tensor(0.3516)\n",
      "3967 Training Loss: tensor(0.3565)\n",
      "3968 Training Loss: tensor(0.3524)\n",
      "3969 Training Loss: tensor(0.3506)\n",
      "3970 Training Loss: tensor(0.3503)\n",
      "3971 Training Loss: tensor(0.3493)\n",
      "3972 Training Loss: tensor(0.3541)\n",
      "3973 Training Loss: tensor(0.3493)\n",
      "3974 Training Loss: tensor(0.3499)\n",
      "3975 Training Loss: tensor(0.3489)\n",
      "3976 Training Loss: tensor(0.3504)\n",
      "3977 Training Loss: tensor(0.3489)\n",
      "3978 Training Loss: tensor(0.3525)\n",
      "3979 Training Loss: tensor(0.3483)\n",
      "3980 Training Loss: tensor(0.3498)\n",
      "3981 Training Loss: tensor(0.3499)\n",
      "3982 Training Loss: tensor(0.3495)\n",
      "3983 Training Loss: tensor(0.3485)\n",
      "3984 Training Loss: tensor(0.3510)\n",
      "3985 Training Loss: tensor(0.3476)\n",
      "3986 Training Loss: tensor(0.3474)\n",
      "3987 Training Loss: tensor(0.3536)\n",
      "3988 Training Loss: tensor(0.3469)\n",
      "3989 Training Loss: tensor(0.3480)\n",
      "3990 Training Loss: tensor(0.3489)\n",
      "3991 Training Loss: tensor(0.3539)\n",
      "3992 Training Loss: tensor(0.3474)\n",
      "3993 Training Loss: tensor(0.3473)\n",
      "3994 Training Loss: tensor(0.3483)\n",
      "3995 Training Loss: tensor(0.3513)\n",
      "3996 Training Loss: tensor(0.3538)\n",
      "3997 Training Loss: tensor(0.3514)\n",
      "3998 Training Loss: tensor(0.3487)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999 Training Loss: tensor(0.3530)\n",
      "4000 Training Loss: tensor(0.3503)\n",
      "4001 Training Loss: tensor(0.3505)\n",
      "4002 Training Loss: tensor(0.3538)\n",
      "4003 Training Loss: tensor(0.3494)\n",
      "4004 Training Loss: tensor(0.3482)\n",
      "4005 Training Loss: tensor(0.3480)\n",
      "4006 Training Loss: tensor(0.3477)\n",
      "4007 Training Loss: tensor(0.3478)\n",
      "4008 Training Loss: tensor(0.3477)\n",
      "4009 Training Loss: tensor(0.3490)\n",
      "4010 Training Loss: tensor(0.3486)\n",
      "4011 Training Loss: tensor(0.3502)\n",
      "4012 Training Loss: tensor(0.3491)\n",
      "4013 Training Loss: tensor(0.3502)\n",
      "4014 Training Loss: tensor(0.3476)\n",
      "4015 Training Loss: tensor(0.3488)\n",
      "4016 Training Loss: tensor(0.3497)\n",
      "4017 Training Loss: tensor(0.3480)\n",
      "4018 Training Loss: tensor(0.3479)\n",
      "4019 Training Loss: tensor(0.3518)\n",
      "4020 Training Loss: tensor(0.3482)\n",
      "4021 Training Loss: tensor(0.3468)\n",
      "4022 Training Loss: tensor(0.3474)\n",
      "4023 Training Loss: tensor(0.3512)\n",
      "4024 Training Loss: tensor(0.3483)\n",
      "4025 Training Loss: tensor(0.3601)\n",
      "4026 Training Loss: tensor(0.3516)\n",
      "4027 Training Loss: tensor(0.3468)\n",
      "4028 Training Loss: tensor(0.3478)\n",
      "4029 Training Loss: tensor(0.3505)\n",
      "4030 Training Loss: tensor(0.3491)\n",
      "4031 Training Loss: tensor(0.3512)\n",
      "4032 Training Loss: tensor(0.3597)\n",
      "4033 Training Loss: tensor(0.3552)\n",
      "4034 Training Loss: tensor(0.3495)\n",
      "4035 Training Loss: tensor(0.3564)\n",
      "4036 Training Loss: tensor(0.3495)\n",
      "4037 Training Loss: tensor(0.3485)\n",
      "4038 Training Loss: tensor(0.3492)\n",
      "4039 Training Loss: tensor(0.3487)\n",
      "4040 Training Loss: tensor(0.3479)\n",
      "4041 Training Loss: tensor(0.3531)\n",
      "4042 Training Loss: tensor(0.3577)\n",
      "4043 Training Loss: tensor(0.3506)\n",
      "4044 Training Loss: tensor(0.3533)\n",
      "4045 Training Loss: tensor(0.3495)\n",
      "4046 Training Loss: tensor(0.3523)\n",
      "4047 Training Loss: tensor(0.3522)\n",
      "4048 Training Loss: tensor(0.3545)\n",
      "4049 Training Loss: tensor(0.3492)\n",
      "4050 Training Loss: tensor(0.3541)\n",
      "4051 Training Loss: tensor(0.3504)\n",
      "4052 Training Loss: tensor(0.3496)\n",
      "4053 Training Loss: tensor(0.3507)\n",
      "4054 Training Loss: tensor(0.3495)\n",
      "4055 Training Loss: tensor(0.3499)\n",
      "4056 Training Loss: tensor(0.3495)\n",
      "4057 Training Loss: tensor(0.3509)\n",
      "4058 Training Loss: tensor(0.3476)\n",
      "4059 Training Loss: tensor(0.3537)\n",
      "4060 Training Loss: tensor(0.3476)\n",
      "4061 Training Loss: tensor(0.3493)\n",
      "4062 Training Loss: tensor(0.3527)\n",
      "4063 Training Loss: tensor(0.3604)\n",
      "4064 Training Loss: tensor(0.3495)\n",
      "4065 Training Loss: tensor(0.3490)\n",
      "4066 Training Loss: tensor(0.3482)\n",
      "4067 Training Loss: tensor(0.3486)\n",
      "4068 Training Loss: tensor(0.3499)\n",
      "4069 Training Loss: tensor(0.3506)\n",
      "4070 Training Loss: tensor(0.3498)\n",
      "4071 Training Loss: tensor(0.3480)\n",
      "4072 Training Loss: tensor(0.3493)\n",
      "4073 Training Loss: tensor(0.3598)\n",
      "4074 Training Loss: tensor(0.3480)\n",
      "4075 Training Loss: tensor(0.3502)\n",
      "4076 Training Loss: tensor(0.3513)\n",
      "4077 Training Loss: tensor(0.3544)\n",
      "4078 Training Loss: tensor(0.3478)\n",
      "4079 Training Loss: tensor(0.3524)\n",
      "4080 Training Loss: tensor(0.3506)\n",
      "4081 Training Loss: tensor(0.3487)\n",
      "4082 Training Loss: tensor(0.3494)\n",
      "4083 Training Loss: tensor(0.3477)\n",
      "4084 Training Loss: tensor(0.3494)\n",
      "4085 Training Loss: tensor(0.3494)\n",
      "4086 Training Loss: tensor(0.3512)\n",
      "4087 Training Loss: tensor(0.3507)\n",
      "4088 Training Loss: tensor(0.3492)\n",
      "4089 Training Loss: tensor(0.3514)\n",
      "4090 Training Loss: tensor(0.3508)\n",
      "4091 Training Loss: tensor(0.3517)\n",
      "4092 Training Loss: tensor(0.3506)\n",
      "4093 Training Loss: tensor(0.3526)\n",
      "4094 Training Loss: tensor(0.3486)\n",
      "4095 Training Loss: tensor(0.3509)\n",
      "4096 Training Loss: tensor(0.3486)\n",
      "4097 Training Loss: tensor(0.3510)\n",
      "4098 Training Loss: tensor(0.3509)\n",
      "4099 Training Loss: tensor(0.3483)\n",
      "4100 Training Loss: tensor(0.3541)\n",
      "4101 Training Loss: tensor(0.3480)\n",
      "4102 Training Loss: tensor(0.3484)\n",
      "4103 Training Loss: tensor(0.3519)\n",
      "4104 Training Loss: tensor(0.3520)\n",
      "4105 Training Loss: tensor(0.3516)\n",
      "4106 Training Loss: tensor(0.3480)\n",
      "4107 Training Loss: tensor(0.3517)\n",
      "4108 Training Loss: tensor(0.3487)\n",
      "4109 Training Loss: tensor(0.3519)\n",
      "4110 Training Loss: tensor(0.3532)\n",
      "4111 Training Loss: tensor(0.3481)\n",
      "4112 Training Loss: tensor(0.3492)\n",
      "4113 Training Loss: tensor(0.3501)\n",
      "4114 Training Loss: tensor(0.3489)\n",
      "4115 Training Loss: tensor(0.3496)\n",
      "4116 Training Loss: tensor(0.3518)\n",
      "4117 Training Loss: tensor(0.3478)\n",
      "4118 Training Loss: tensor(0.3481)\n",
      "4119 Training Loss: tensor(0.3526)\n",
      "4120 Training Loss: tensor(0.3485)\n",
      "4121 Training Loss: tensor(0.3483)\n",
      "4122 Training Loss: tensor(0.3487)\n",
      "4123 Training Loss: tensor(0.3483)\n",
      "4124 Training Loss: tensor(0.3498)\n",
      "4125 Training Loss: tensor(0.3486)\n",
      "4126 Training Loss: tensor(0.3477)\n",
      "4127 Training Loss: tensor(0.3529)\n",
      "4128 Training Loss: tensor(0.3524)\n",
      "4129 Training Loss: tensor(0.3495)\n",
      "4130 Training Loss: tensor(0.3478)\n",
      "4131 Training Loss: tensor(0.3580)\n",
      "4132 Training Loss: tensor(0.3468)\n",
      "4133 Training Loss: tensor(0.3479)\n",
      "4134 Training Loss: tensor(0.3490)\n",
      "4135 Training Loss: tensor(0.3511)\n",
      "4136 Training Loss: tensor(0.3483)\n",
      "4137 Training Loss: tensor(0.3526)\n",
      "4138 Training Loss: tensor(0.3496)\n",
      "4139 Training Loss: tensor(0.3483)\n",
      "4140 Training Loss: tensor(0.3486)\n",
      "4141 Training Loss: tensor(0.3503)\n",
      "4142 Training Loss: tensor(0.3495)\n",
      "4143 Training Loss: tensor(0.3519)\n",
      "4144 Training Loss: tensor(0.3483)\n",
      "4145 Training Loss: tensor(0.3486)\n",
      "4146 Training Loss: tensor(0.3497)\n",
      "4147 Training Loss: tensor(0.3487)\n",
      "4148 Training Loss: tensor(0.3501)\n",
      "4149 Training Loss: tensor(0.3507)\n",
      "4150 Training Loss: tensor(0.3485)\n",
      "4151 Training Loss: tensor(0.3475)\n",
      "4152 Training Loss: tensor(0.3488)\n",
      "4153 Training Loss: tensor(0.3502)\n",
      "4154 Training Loss: tensor(0.3499)\n",
      "4155 Training Loss: tensor(0.3514)\n",
      "4156 Training Loss: tensor(0.3528)\n",
      "4157 Training Loss: tensor(0.3515)\n",
      "4158 Training Loss: tensor(0.3571)\n",
      "4159 Training Loss: tensor(0.3499)\n",
      "4160 Training Loss: tensor(0.3488)\n",
      "4161 Training Loss: tensor(0.3484)\n",
      "4162 Training Loss: tensor(0.3474)\n",
      "4163 Training Loss: tensor(0.3512)\n",
      "4164 Training Loss: tensor(0.3501)\n",
      "4165 Training Loss: tensor(0.3534)\n",
      "4166 Training Loss: tensor(0.3505)\n",
      "4167 Training Loss: tensor(0.3482)\n",
      "4168 Training Loss: tensor(0.3505)\n",
      "4169 Training Loss: tensor(0.3493)\n",
      "4170 Training Loss: tensor(0.3510)\n",
      "4171 Training Loss: tensor(0.3498)\n",
      "4172 Training Loss: tensor(0.3495)\n",
      "4173 Training Loss: tensor(0.3486)\n",
      "4174 Training Loss: tensor(0.3481)\n",
      "4175 Training Loss: tensor(0.3531)\n",
      "4176 Training Loss: tensor(0.3492)\n",
      "4177 Training Loss: tensor(0.3507)\n",
      "4178 Training Loss: tensor(0.3505)\n",
      "4179 Training Loss: tensor(0.3486)\n",
      "4180 Training Loss: tensor(0.3479)\n",
      "4181 Training Loss: tensor(0.3500)\n",
      "4182 Training Loss: tensor(0.3491)\n",
      "4183 Training Loss: tensor(0.3485)\n",
      "4184 Training Loss: tensor(0.3480)\n",
      "4185 Training Loss: tensor(0.3484)\n",
      "4186 Training Loss: tensor(0.3469)\n",
      "4187 Training Loss: tensor(0.3501)\n",
      "4188 Training Loss: tensor(0.3467)\n",
      "4189 Training Loss: tensor(0.3515)\n",
      "4190 Training Loss: tensor(0.3491)\n",
      "4191 Training Loss: tensor(0.3483)\n",
      "4192 Training Loss: tensor(0.3539)\n",
      "4193 Training Loss: tensor(0.3471)\n",
      "4194 Training Loss: tensor(0.3492)\n",
      "4195 Training Loss: tensor(0.3584)\n",
      "4196 Training Loss: tensor(0.3548)\n",
      "4197 Training Loss: tensor(0.3486)\n",
      "4198 Training Loss: tensor(0.3544)\n",
      "4199 Training Loss: tensor(0.3504)\n",
      "4200 Training Loss: tensor(0.3572)\n",
      "4201 Training Loss: tensor(0.3526)\n",
      "4202 Training Loss: tensor(0.3525)\n",
      "4203 Training Loss: tensor(0.3503)\n",
      "4204 Training Loss: tensor(0.3519)\n",
      "4205 Training Loss: tensor(0.3492)\n",
      "4206 Training Loss: tensor(0.3509)\n",
      "4207 Training Loss: tensor(0.3555)\n",
      "4208 Training Loss: tensor(0.3502)\n",
      "4209 Training Loss: tensor(0.3496)\n",
      "4210 Training Loss: tensor(0.3511)\n",
      "4211 Training Loss: tensor(0.3498)\n",
      "4212 Training Loss: tensor(0.3505)\n",
      "4213 Training Loss: tensor(0.3487)\n",
      "4214 Training Loss: tensor(0.3536)\n",
      "4215 Training Loss: tensor(0.3495)\n",
      "4216 Training Loss: tensor(0.3496)\n",
      "4217 Training Loss: tensor(0.3561)\n",
      "4218 Training Loss: tensor(0.3505)\n",
      "4219 Training Loss: tensor(0.3489)\n",
      "4220 Training Loss: tensor(0.3521)\n",
      "4221 Training Loss: tensor(0.3510)\n",
      "4222 Training Loss: tensor(0.3489)\n",
      "4223 Training Loss: tensor(0.3518)\n",
      "4224 Training Loss: tensor(0.3537)\n",
      "4225 Training Loss: tensor(0.3491)\n",
      "4226 Training Loss: tensor(0.3488)\n",
      "4227 Training Loss: tensor(0.3578)\n",
      "4228 Training Loss: tensor(0.3508)\n",
      "4229 Training Loss: tensor(0.3521)\n",
      "4230 Training Loss: tensor(0.3487)\n",
      "4231 Training Loss: tensor(0.3480)\n",
      "4232 Training Loss: tensor(0.3503)\n",
      "4233 Training Loss: tensor(0.3487)\n",
      "4234 Training Loss: tensor(0.3543)\n",
      "4235 Training Loss: tensor(0.3494)\n",
      "4236 Training Loss: tensor(0.3482)\n",
      "4237 Training Loss: tensor(0.3530)\n",
      "4238 Training Loss: tensor(0.3509)\n",
      "4239 Training Loss: tensor(0.3493)\n",
      "4240 Training Loss: tensor(0.3480)\n",
      "4241 Training Loss: tensor(0.3497)\n",
      "4242 Training Loss: tensor(0.3489)\n",
      "4243 Training Loss: tensor(0.3482)\n",
      "4244 Training Loss: tensor(0.3512)\n",
      "4245 Training Loss: tensor(0.3525)\n",
      "4246 Training Loss: tensor(0.3488)\n",
      "4247 Training Loss: tensor(0.3494)\n",
      "4248 Training Loss: tensor(0.3500)\n",
      "4249 Training Loss: tensor(0.3531)\n",
      "4250 Training Loss: tensor(0.3483)\n",
      "4251 Training Loss: tensor(0.3519)\n",
      "4252 Training Loss: tensor(0.3486)\n",
      "4253 Training Loss: tensor(0.3504)\n",
      "4254 Training Loss: tensor(0.3525)\n",
      "4255 Training Loss: tensor(0.3503)\n",
      "4256 Training Loss: tensor(0.3505)\n",
      "4257 Training Loss: tensor(0.3479)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258 Training Loss: tensor(0.3542)\n",
      "4259 Training Loss: tensor(0.3502)\n",
      "4260 Training Loss: tensor(0.3477)\n",
      "4261 Training Loss: tensor(0.3531)\n",
      "4262 Training Loss: tensor(0.3482)\n",
      "4263 Training Loss: tensor(0.3495)\n",
      "4264 Training Loss: tensor(0.3517)\n",
      "4265 Training Loss: tensor(0.3478)\n",
      "4266 Training Loss: tensor(0.3508)\n",
      "4267 Training Loss: tensor(0.3487)\n",
      "4268 Training Loss: tensor(0.3479)\n",
      "4269 Training Loss: tensor(0.3500)\n",
      "4270 Training Loss: tensor(0.3531)\n",
      "4271 Training Loss: tensor(0.3515)\n",
      "4272 Training Loss: tensor(0.3503)\n",
      "4273 Training Loss: tensor(0.3521)\n",
      "4274 Training Loss: tensor(0.3475)\n",
      "4275 Training Loss: tensor(0.3523)\n",
      "4276 Training Loss: tensor(0.3484)\n",
      "4277 Training Loss: tensor(0.3483)\n",
      "4278 Training Loss: tensor(0.3519)\n",
      "4279 Training Loss: tensor(0.3517)\n",
      "4280 Training Loss: tensor(0.3525)\n",
      "4281 Training Loss: tensor(0.3508)\n",
      "4282 Training Loss: tensor(0.3480)\n",
      "4283 Training Loss: tensor(0.3592)\n",
      "4284 Training Loss: tensor(0.3488)\n",
      "4285 Training Loss: tensor(0.3491)\n",
      "4286 Training Loss: tensor(0.3514)\n",
      "4287 Training Loss: tensor(0.3496)\n",
      "4288 Training Loss: tensor(0.3514)\n",
      "4289 Training Loss: tensor(0.3514)\n",
      "4290 Training Loss: tensor(0.3490)\n",
      "4291 Training Loss: tensor(0.3505)\n",
      "4292 Training Loss: tensor(0.3496)\n",
      "4293 Training Loss: tensor(0.3511)\n",
      "4294 Training Loss: tensor(0.3490)\n",
      "4295 Training Loss: tensor(0.3494)\n",
      "4296 Training Loss: tensor(0.3500)\n",
      "4297 Training Loss: tensor(0.3499)\n",
      "4298 Training Loss: tensor(0.3499)\n",
      "4299 Training Loss: tensor(0.3478)\n",
      "4300 Training Loss: tensor(0.3514)\n",
      "4301 Training Loss: tensor(0.3537)\n",
      "4302 Training Loss: tensor(0.3517)\n",
      "4303 Training Loss: tensor(0.3483)\n",
      "4304 Training Loss: tensor(0.3486)\n",
      "4305 Training Loss: tensor(0.3503)\n",
      "4306 Training Loss: tensor(0.3513)\n",
      "4307 Training Loss: tensor(0.3527)\n",
      "4308 Training Loss: tensor(0.3476)\n",
      "4309 Training Loss: tensor(0.3486)\n",
      "4310 Training Loss: tensor(0.3488)\n",
      "4311 Training Loss: tensor(0.3477)\n",
      "4312 Training Loss: tensor(0.3528)\n",
      "4313 Training Loss: tensor(0.3500)\n",
      "4314 Training Loss: tensor(0.3475)\n",
      "4315 Training Loss: tensor(0.3480)\n",
      "4316 Training Loss: tensor(0.3517)\n",
      "4317 Training Loss: tensor(0.3498)\n",
      "4318 Training Loss: tensor(0.3478)\n",
      "4319 Training Loss: tensor(0.3494)\n",
      "4320 Training Loss: tensor(0.3559)\n",
      "4321 Training Loss: tensor(0.3494)\n",
      "4322 Training Loss: tensor(0.3508)\n",
      "4323 Training Loss: tensor(0.3478)\n",
      "4324 Training Loss: tensor(0.3555)\n",
      "4325 Training Loss: tensor(0.3486)\n",
      "4326 Training Loss: tensor(0.3528)\n",
      "4327 Training Loss: tensor(0.3492)\n",
      "4328 Training Loss: tensor(0.3531)\n",
      "4329 Training Loss: tensor(0.3510)\n",
      "4330 Training Loss: tensor(0.3535)\n",
      "4331 Training Loss: tensor(0.3526)\n",
      "4332 Training Loss: tensor(0.3504)\n",
      "4333 Training Loss: tensor(0.3492)\n",
      "4334 Training Loss: tensor(0.3485)\n",
      "4335 Training Loss: tensor(0.3496)\n",
      "4336 Training Loss: tensor(0.3499)\n",
      "4337 Training Loss: tensor(0.3493)\n",
      "4338 Training Loss: tensor(0.3486)\n",
      "4339 Training Loss: tensor(0.3480)\n",
      "4340 Training Loss: tensor(0.3503)\n",
      "4341 Training Loss: tensor(0.3485)\n",
      "4342 Training Loss: tensor(0.3549)\n",
      "4343 Training Loss: tensor(0.3485)\n",
      "4344 Training Loss: tensor(0.3542)\n",
      "4345 Training Loss: tensor(0.3501)\n",
      "4346 Training Loss: tensor(0.3488)\n",
      "4347 Training Loss: tensor(0.3474)\n",
      "4348 Training Loss: tensor(0.3510)\n",
      "4349 Training Loss: tensor(0.3488)\n",
      "4350 Training Loss: tensor(0.3500)\n",
      "4351 Training Loss: tensor(0.3495)\n",
      "4352 Training Loss: tensor(0.3494)\n",
      "4353 Training Loss: tensor(0.3510)\n",
      "4354 Training Loss: tensor(0.3502)\n",
      "4355 Training Loss: tensor(0.3478)\n",
      "4356 Training Loss: tensor(0.3511)\n",
      "4357 Training Loss: tensor(0.3512)\n",
      "4358 Training Loss: tensor(0.3500)\n",
      "4359 Training Loss: tensor(0.3475)\n",
      "4360 Training Loss: tensor(0.3481)\n",
      "4361 Training Loss: tensor(0.3477)\n",
      "4362 Training Loss: tensor(0.3482)\n",
      "4363 Training Loss: tensor(0.3481)\n",
      "4364 Training Loss: tensor(0.3479)\n",
      "4365 Training Loss: tensor(0.3482)\n",
      "4366 Training Loss: tensor(0.3487)\n",
      "4367 Training Loss: tensor(0.3478)\n",
      "4368 Training Loss: tensor(0.3488)\n",
      "4369 Training Loss: tensor(0.3503)\n",
      "4370 Training Loss: tensor(0.3472)\n",
      "4371 Training Loss: tensor(0.3469)\n",
      "4372 Training Loss: tensor(0.3527)\n",
      "4373 Training Loss: tensor(0.3528)\n",
      "4374 Training Loss: tensor(0.3471)\n",
      "4375 Training Loss: tensor(0.3498)\n",
      "4376 Training Loss: tensor(0.3562)\n",
      "4377 Training Loss: tensor(0.3501)\n",
      "4378 Training Loss: tensor(0.3523)\n",
      "4379 Training Loss: tensor(0.3488)\n",
      "4380 Training Loss: tensor(0.3478)\n",
      "4381 Training Loss: tensor(0.3475)\n",
      "4382 Training Loss: tensor(0.3484)\n",
      "4383 Training Loss: tensor(0.3480)\n",
      "4384 Training Loss: tensor(0.3512)\n",
      "4385 Training Loss: tensor(0.3472)\n",
      "4386 Training Loss: tensor(0.3528)\n",
      "4387 Training Loss: tensor(0.3514)\n",
      "4388 Training Loss: tensor(0.3477)\n",
      "4389 Training Loss: tensor(0.3505)\n",
      "4390 Training Loss: tensor(0.3492)\n",
      "4391 Training Loss: tensor(0.3488)\n",
      "4392 Training Loss: tensor(0.3484)\n",
      "4393 Training Loss: tensor(0.3486)\n",
      "4394 Training Loss: tensor(0.3509)\n",
      "4395 Training Loss: tensor(0.3502)\n",
      "4396 Training Loss: tensor(0.3499)\n",
      "4397 Training Loss: tensor(0.3563)\n",
      "4398 Training Loss: tensor(0.3492)\n",
      "4399 Training Loss: tensor(0.3498)\n",
      "4400 Training Loss: tensor(0.3508)\n",
      "4401 Training Loss: tensor(0.3500)\n",
      "4402 Training Loss: tensor(0.3499)\n",
      "4403 Training Loss: tensor(0.3513)\n",
      "4404 Training Loss: tensor(0.3504)\n",
      "4405 Training Loss: tensor(0.3487)\n",
      "4406 Training Loss: tensor(0.3534)\n",
      "4407 Training Loss: tensor(0.3526)\n",
      "4408 Training Loss: tensor(0.3481)\n",
      "4409 Training Loss: tensor(0.3483)\n",
      "4410 Training Loss: tensor(0.3491)\n",
      "4411 Training Loss: tensor(0.3505)\n",
      "4412 Training Loss: tensor(0.3491)\n",
      "4413 Training Loss: tensor(0.3533)\n",
      "4414 Training Loss: tensor(0.3504)\n",
      "4415 Training Loss: tensor(0.3530)\n",
      "4416 Training Loss: tensor(0.3497)\n",
      "4417 Training Loss: tensor(0.3541)\n",
      "4418 Training Loss: tensor(0.3479)\n",
      "4419 Training Loss: tensor(0.3490)\n",
      "4420 Training Loss: tensor(0.3498)\n",
      "4421 Training Loss: tensor(0.3491)\n",
      "4422 Training Loss: tensor(0.3533)\n",
      "4423 Training Loss: tensor(0.3482)\n",
      "4424 Training Loss: tensor(0.3518)\n",
      "4425 Training Loss: tensor(0.3573)\n",
      "4426 Training Loss: tensor(0.3493)\n",
      "4427 Training Loss: tensor(0.3496)\n",
      "4428 Training Loss: tensor(0.3553)\n",
      "4429 Training Loss: tensor(0.3531)\n",
      "4430 Training Loss: tensor(0.3539)\n",
      "4431 Training Loss: tensor(0.3513)\n",
      "4432 Training Loss: tensor(0.3520)\n",
      "4433 Training Loss: tensor(0.3483)\n",
      "4434 Training Loss: tensor(0.3557)\n",
      "4435 Training Loss: tensor(0.3488)\n",
      "4436 Training Loss: tensor(0.3524)\n",
      "4437 Training Loss: tensor(0.3501)\n",
      "4438 Training Loss: tensor(0.3512)\n",
      "4439 Training Loss: tensor(0.3489)\n",
      "4440 Training Loss: tensor(0.3492)\n",
      "4441 Training Loss: tensor(0.3516)\n",
      "4442 Training Loss: tensor(0.3522)\n",
      "4443 Training Loss: tensor(0.3491)\n",
      "4444 Training Loss: tensor(0.3502)\n",
      "4445 Training Loss: tensor(0.3554)\n",
      "4446 Training Loss: tensor(0.3524)\n",
      "4447 Training Loss: tensor(0.3480)\n",
      "4448 Training Loss: tensor(0.3501)\n",
      "4449 Training Loss: tensor(0.3489)\n",
      "4450 Training Loss: tensor(0.3494)\n",
      "4451 Training Loss: tensor(0.3489)\n",
      "4452 Training Loss: tensor(0.3480)\n",
      "4453 Training Loss: tensor(0.3550)\n",
      "4454 Training Loss: tensor(0.3505)\n",
      "4455 Training Loss: tensor(0.3485)\n",
      "4456 Training Loss: tensor(0.3497)\n",
      "4457 Training Loss: tensor(0.3502)\n",
      "4458 Training Loss: tensor(0.3495)\n",
      "4459 Training Loss: tensor(0.3483)\n",
      "4460 Training Loss: tensor(0.3486)\n",
      "4461 Training Loss: tensor(0.3527)\n",
      "4462 Training Loss: tensor(0.3490)\n",
      "4463 Training Loss: tensor(0.3508)\n",
      "4464 Training Loss: tensor(0.3504)\n",
      "4465 Training Loss: tensor(0.3494)\n",
      "4466 Training Loss: tensor(0.3485)\n",
      "4467 Training Loss: tensor(0.3489)\n",
      "4468 Training Loss: tensor(0.3543)\n",
      "4469 Training Loss: tensor(0.3492)\n",
      "4470 Training Loss: tensor(0.3490)\n",
      "4471 Training Loss: tensor(0.3478)\n",
      "4472 Training Loss: tensor(0.3539)\n",
      "4473 Training Loss: tensor(0.3481)\n",
      "4474 Training Loss: tensor(0.3549)\n",
      "4475 Training Loss: tensor(0.3525)\n",
      "4476 Training Loss: tensor(0.3488)\n",
      "4477 Training Loss: tensor(0.3487)\n",
      "4478 Training Loss: tensor(0.3520)\n",
      "4479 Training Loss: tensor(0.3494)\n",
      "4480 Training Loss: tensor(0.3631)\n",
      "4481 Training Loss: tensor(0.3489)\n",
      "4482 Training Loss: tensor(0.3498)\n",
      "4483 Training Loss: tensor(0.3505)\n",
      "4484 Training Loss: tensor(0.3484)\n",
      "4485 Training Loss: tensor(0.3507)\n",
      "4486 Training Loss: tensor(0.3493)\n",
      "4487 Training Loss: tensor(0.3489)\n",
      "4488 Training Loss: tensor(0.3488)\n",
      "4489 Training Loss: tensor(0.3481)\n",
      "4490 Training Loss: tensor(0.3490)\n",
      "4491 Training Loss: tensor(0.3486)\n",
      "4492 Training Loss: tensor(0.3496)\n",
      "4493 Training Loss: tensor(0.3482)\n",
      "4494 Training Loss: tensor(0.3484)\n",
      "4495 Training Loss: tensor(0.3471)\n",
      "4496 Training Loss: tensor(0.3564)\n",
      "4497 Training Loss: tensor(0.3498)\n",
      "4498 Training Loss: tensor(0.3472)\n",
      "4499 Training Loss: tensor(0.3536)\n",
      "4500 Training Loss: tensor(0.3548)\n",
      "4501 Training Loss: tensor(0.3532)\n",
      "4502 Training Loss: tensor(0.3506)\n",
      "4503 Training Loss: tensor(0.3501)\n",
      "4504 Training Loss: tensor(0.3538)\n",
      "4505 Training Loss: tensor(0.3486)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4506 Training Loss: tensor(0.3488)\n",
      "4507 Training Loss: tensor(0.3477)\n",
      "4508 Training Loss: tensor(0.3486)\n",
      "4509 Training Loss: tensor(0.3498)\n",
      "4510 Training Loss: tensor(0.3515)\n",
      "4511 Training Loss: tensor(0.3488)\n",
      "4512 Training Loss: tensor(0.3483)\n",
      "4513 Training Loss: tensor(0.3484)\n",
      "4514 Training Loss: tensor(0.3481)\n",
      "4515 Training Loss: tensor(0.3492)\n",
      "4516 Training Loss: tensor(0.3500)\n",
      "4517 Training Loss: tensor(0.3482)\n",
      "4518 Training Loss: tensor(0.3545)\n",
      "4519 Training Loss: tensor(0.3523)\n",
      "4520 Training Loss: tensor(0.3490)\n",
      "4521 Training Loss: tensor(0.3475)\n",
      "4522 Training Loss: tensor(0.3485)\n",
      "4523 Training Loss: tensor(0.3502)\n",
      "4524 Training Loss: tensor(0.3481)\n",
      "4525 Training Loss: tensor(0.3541)\n",
      "4526 Training Loss: tensor(0.3496)\n",
      "4527 Training Loss: tensor(0.3481)\n",
      "4528 Training Loss: tensor(0.3542)\n",
      "4529 Training Loss: tensor(0.3537)\n",
      "4530 Training Loss: tensor(0.3494)\n",
      "4531 Training Loss: tensor(0.3499)\n",
      "4532 Training Loss: tensor(0.3473)\n",
      "4533 Training Loss: tensor(0.3472)\n",
      "4534 Training Loss: tensor(0.3531)\n",
      "4535 Training Loss: tensor(0.3497)\n",
      "4536 Training Loss: tensor(0.3493)\n",
      "4537 Training Loss: tensor(0.3477)\n",
      "4538 Training Loss: tensor(0.3490)\n",
      "4539 Training Loss: tensor(0.3502)\n",
      "4540 Training Loss: tensor(0.3482)\n",
      "4541 Training Loss: tensor(0.3504)\n",
      "4542 Training Loss: tensor(0.3485)\n",
      "4543 Training Loss: tensor(0.3488)\n",
      "4544 Training Loss: tensor(0.3488)\n",
      "4545 Training Loss: tensor(0.3475)\n",
      "4546 Training Loss: tensor(0.3480)\n",
      "4547 Training Loss: tensor(0.3499)\n",
      "4548 Training Loss: tensor(0.3468)\n",
      "4549 Training Loss: tensor(0.3517)\n",
      "4550 Training Loss: tensor(0.3533)\n",
      "4551 Training Loss: tensor(0.3467)\n",
      "4552 Training Loss: tensor(0.3473)\n",
      "4553 Training Loss: tensor(0.3540)\n",
      "4554 Training Loss: tensor(0.3467)\n",
      "4555 Training Loss: tensor(0.3482)\n",
      "4556 Training Loss: tensor(0.3498)\n",
      "4557 Training Loss: tensor(0.3475)\n",
      "4558 Training Loss: tensor(0.3490)\n",
      "4559 Training Loss: tensor(0.3470)\n",
      "4560 Training Loss: tensor(0.3491)\n",
      "4561 Training Loss: tensor(0.3502)\n",
      "4562 Training Loss: tensor(0.3496)\n",
      "4563 Training Loss: tensor(0.3474)\n",
      "4564 Training Loss: tensor(0.3550)\n",
      "4565 Training Loss: tensor(0.3572)\n",
      "4566 Training Loss: tensor(0.3480)\n",
      "4567 Training Loss: tensor(0.3541)\n",
      "4568 Training Loss: tensor(0.3483)\n",
      "4569 Training Loss: tensor(0.3485)\n",
      "4570 Training Loss: tensor(0.3489)\n",
      "4571 Training Loss: tensor(0.3494)\n",
      "4572 Training Loss: tensor(0.3487)\n",
      "4573 Training Loss: tensor(0.3483)\n",
      "4574 Training Loss: tensor(0.3469)\n",
      "4575 Training Loss: tensor(0.3500)\n",
      "4576 Training Loss: tensor(0.3521)\n",
      "4577 Training Loss: tensor(0.3477)\n",
      "4578 Training Loss: tensor(0.3487)\n",
      "4579 Training Loss: tensor(0.3515)\n",
      "4580 Training Loss: tensor(0.3525)\n",
      "4581 Training Loss: tensor(0.3479)\n",
      "4582 Training Loss: tensor(0.3510)\n",
      "4583 Training Loss: tensor(0.3482)\n",
      "4584 Training Loss: tensor(0.3476)\n",
      "4585 Training Loss: tensor(0.3472)\n",
      "4586 Training Loss: tensor(0.3475)\n",
      "4587 Training Loss: tensor(0.3578)\n",
      "4588 Training Loss: tensor(0.3557)\n",
      "4589 Training Loss: tensor(0.3471)\n",
      "4590 Training Loss: tensor(0.3547)\n",
      "4591 Training Loss: tensor(0.3475)\n",
      "4592 Training Loss: tensor(0.3500)\n",
      "4593 Training Loss: tensor(0.3484)\n",
      "4594 Training Loss: tensor(0.3509)\n",
      "4595 Training Loss: tensor(0.3504)\n",
      "4596 Training Loss: tensor(0.3503)\n",
      "4597 Training Loss: tensor(0.3487)\n",
      "4598 Training Loss: tensor(0.3529)\n",
      "4599 Training Loss: tensor(0.3480)\n",
      "4600 Training Loss: tensor(0.3523)\n",
      "4601 Training Loss: tensor(0.3489)\n",
      "4602 Training Loss: tensor(0.3481)\n",
      "4603 Training Loss: tensor(0.3495)\n",
      "4604 Training Loss: tensor(0.3509)\n",
      "4605 Training Loss: tensor(0.3496)\n",
      "4606 Training Loss: tensor(0.3560)\n",
      "4607 Training Loss: tensor(0.3497)\n",
      "4608 Training Loss: tensor(0.3484)\n",
      "4609 Training Loss: tensor(0.3476)\n",
      "4610 Training Loss: tensor(0.3490)\n",
      "4611 Training Loss: tensor(0.3537)\n",
      "4612 Training Loss: tensor(0.3516)\n",
      "4613 Training Loss: tensor(0.3479)\n",
      "4614 Training Loss: tensor(0.3494)\n",
      "4615 Training Loss: tensor(0.3512)\n",
      "4616 Training Loss: tensor(0.3485)\n",
      "4617 Training Loss: tensor(0.3485)\n",
      "4618 Training Loss: tensor(0.3468)\n",
      "4619 Training Loss: tensor(0.3497)\n",
      "4620 Training Loss: tensor(0.3475)\n",
      "4621 Training Loss: tensor(0.3472)\n",
      "4622 Training Loss: tensor(0.3552)\n",
      "4623 Training Loss: tensor(0.3585)\n",
      "4624 Training Loss: tensor(0.3495)\n",
      "4625 Training Loss: tensor(0.3488)\n",
      "4626 Training Loss: tensor(0.3513)\n",
      "4627 Training Loss: tensor(0.3475)\n",
      "4628 Training Loss: tensor(0.3501)\n",
      "4629 Training Loss: tensor(0.3534)\n",
      "4630 Training Loss: tensor(0.3481)\n",
      "4631 Training Loss: tensor(0.3505)\n",
      "4632 Training Loss: tensor(0.3488)\n",
      "4633 Training Loss: tensor(0.3475)\n",
      "4634 Training Loss: tensor(0.3515)\n",
      "4635 Training Loss: tensor(0.3530)\n",
      "4636 Training Loss: tensor(0.3492)\n",
      "4637 Training Loss: tensor(0.3485)\n",
      "4638 Training Loss: tensor(0.3522)\n",
      "4639 Training Loss: tensor(0.3483)\n",
      "4640 Training Loss: tensor(0.3485)\n",
      "4641 Training Loss: tensor(0.3513)\n",
      "4642 Training Loss: tensor(0.3499)\n",
      "4643 Training Loss: tensor(0.3504)\n",
      "4644 Training Loss: tensor(0.3487)\n",
      "4645 Training Loss: tensor(0.3498)\n",
      "4646 Training Loss: tensor(0.3474)\n",
      "4647 Training Loss: tensor(0.3495)\n",
      "4648 Training Loss: tensor(0.3517)\n",
      "4649 Training Loss: tensor(0.3490)\n",
      "4650 Training Loss: tensor(0.3474)\n",
      "4651 Training Loss: tensor(0.3487)\n",
      "4652 Training Loss: tensor(0.3485)\n",
      "4653 Training Loss: tensor(0.3551)\n",
      "4654 Training Loss: tensor(0.3470)\n",
      "4655 Training Loss: tensor(0.3504)\n",
      "4656 Training Loss: tensor(0.3509)\n",
      "4657 Training Loss: tensor(0.3497)\n",
      "4658 Training Loss: tensor(0.3520)\n",
      "4659 Training Loss: tensor(0.3518)\n",
      "4660 Training Loss: tensor(0.3477)\n",
      "4661 Training Loss: tensor(0.3465)\n",
      "4662 Training Loss: tensor(0.3564)\n",
      "4663 Training Loss: tensor(0.3469)\n",
      "4664 Training Loss: tensor(0.3475)\n",
      "4665 Training Loss: tensor(0.3476)\n",
      "4666 Training Loss: tensor(0.3554)\n",
      "4667 Training Loss: tensor(0.3505)\n",
      "4668 Training Loss: tensor(0.3513)\n",
      "4669 Training Loss: tensor(0.3491)\n",
      "4670 Training Loss: tensor(0.3497)\n",
      "4671 Training Loss: tensor(0.3493)\n",
      "4672 Training Loss: tensor(0.3487)\n",
      "4673 Training Loss: tensor(0.3503)\n",
      "4674 Training Loss: tensor(0.3490)\n",
      "4675 Training Loss: tensor(0.3548)\n",
      "4676 Training Loss: tensor(0.3481)\n",
      "4677 Training Loss: tensor(0.3486)\n",
      "4678 Training Loss: tensor(0.3485)\n",
      "4679 Training Loss: tensor(0.3543)\n",
      "4680 Training Loss: tensor(0.3492)\n",
      "4681 Training Loss: tensor(0.3519)\n",
      "4682 Training Loss: tensor(0.3493)\n",
      "4683 Training Loss: tensor(0.3482)\n",
      "4684 Training Loss: tensor(0.3516)\n",
      "4685 Training Loss: tensor(0.3533)\n",
      "4686 Training Loss: tensor(0.3478)\n",
      "4687 Training Loss: tensor(0.3482)\n",
      "4688 Training Loss: tensor(0.3519)\n",
      "4689 Training Loss: tensor(0.3503)\n",
      "4690 Training Loss: tensor(0.3536)\n",
      "4691 Training Loss: tensor(0.3570)\n",
      "4692 Training Loss: tensor(0.3490)\n",
      "4693 Training Loss: tensor(0.3505)\n",
      "4694 Training Loss: tensor(0.3485)\n",
      "4695 Training Loss: tensor(0.3540)\n",
      "4696 Training Loss: tensor(0.3513)\n",
      "4697 Training Loss: tensor(0.3501)\n",
      "4698 Training Loss: tensor(0.3481)\n",
      "4699 Training Loss: tensor(0.3486)\n",
      "4700 Training Loss: tensor(0.3494)\n",
      "4701 Training Loss: tensor(0.3500)\n",
      "4702 Training Loss: tensor(0.3487)\n",
      "4703 Training Loss: tensor(0.3568)\n",
      "4704 Training Loss: tensor(0.3495)\n",
      "4705 Training Loss: tensor(0.3501)\n",
      "4706 Training Loss: tensor(0.3505)\n",
      "4707 Training Loss: tensor(0.3484)\n",
      "4708 Training Loss: tensor(0.3512)\n",
      "4709 Training Loss: tensor(0.3479)\n",
      "4710 Training Loss: tensor(0.3499)\n",
      "4711 Training Loss: tensor(0.3479)\n",
      "4712 Training Loss: tensor(0.3484)\n",
      "4713 Training Loss: tensor(0.3484)\n",
      "4714 Training Loss: tensor(0.3495)\n",
      "4715 Training Loss: tensor(0.3540)\n",
      "4716 Training Loss: tensor(0.3518)\n",
      "4717 Training Loss: tensor(0.3501)\n",
      "4718 Training Loss: tensor(0.3479)\n",
      "4719 Training Loss: tensor(0.3479)\n",
      "4720 Training Loss: tensor(0.3498)\n",
      "4721 Training Loss: tensor(0.3497)\n",
      "4722 Training Loss: tensor(0.3543)\n",
      "4723 Training Loss: tensor(0.3505)\n",
      "4724 Training Loss: tensor(0.3509)\n",
      "4725 Training Loss: tensor(0.3505)\n",
      "4726 Training Loss: tensor(0.3468)\n",
      "4727 Training Loss: tensor(0.3481)\n",
      "4728 Training Loss: tensor(0.3499)\n",
      "4729 Training Loss: tensor(0.3474)\n",
      "4730 Training Loss: tensor(0.3473)\n",
      "4731 Training Loss: tensor(0.3480)\n",
      "4732 Training Loss: tensor(0.3483)\n",
      "4733 Training Loss: tensor(0.3480)\n",
      "4734 Training Loss: tensor(0.3490)\n",
      "4735 Training Loss: tensor(0.3491)\n",
      "4736 Training Loss: tensor(0.3523)\n",
      "4737 Training Loss: tensor(0.3508)\n",
      "4738 Training Loss: tensor(0.3483)\n",
      "4739 Training Loss: tensor(0.3483)\n",
      "4740 Training Loss: tensor(0.3488)\n",
      "4741 Training Loss: tensor(0.3476)\n",
      "4742 Training Loss: tensor(0.3473)\n",
      "4743 Training Loss: tensor(0.3502)\n",
      "4744 Training Loss: tensor(0.3471)\n",
      "4745 Training Loss: tensor(0.3484)\n",
      "4746 Training Loss: tensor(0.3498)\n",
      "4747 Training Loss: tensor(0.3485)\n",
      "4748 Training Loss: tensor(0.3480)\n",
      "4749 Training Loss: tensor(0.3528)\n",
      "4750 Training Loss: tensor(0.3475)\n",
      "4751 Training Loss: tensor(0.3533)\n",
      "4752 Training Loss: tensor(0.3548)\n",
      "4753 Training Loss: tensor(0.3484)\n",
      "4754 Training Loss: tensor(0.3484)\n",
      "4755 Training Loss: tensor(0.3488)\n",
      "4756 Training Loss: tensor(0.3497)\n",
      "4757 Training Loss: tensor(0.3478)\n",
      "4758 Training Loss: tensor(0.3475)\n",
      "4759 Training Loss: tensor(0.3482)\n",
      "4760 Training Loss: tensor(0.3471)\n",
      "4761 Training Loss: tensor(0.3521)\n",
      "4762 Training Loss: tensor(0.3560)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4763 Training Loss: tensor(0.3538)\n",
      "4764 Training Loss: tensor(0.3498)\n",
      "4765 Training Loss: tensor(0.3496)\n",
      "4766 Training Loss: tensor(0.3516)\n",
      "4767 Training Loss: tensor(0.3528)\n",
      "4768 Training Loss: tensor(0.3504)\n",
      "4769 Training Loss: tensor(0.3475)\n",
      "4770 Training Loss: tensor(0.3488)\n",
      "4771 Training Loss: tensor(0.3502)\n",
      "4772 Training Loss: tensor(0.3477)\n",
      "4773 Training Loss: tensor(0.3492)\n",
      "4774 Training Loss: tensor(0.3480)\n",
      "4775 Training Loss: tensor(0.3473)\n",
      "4776 Training Loss: tensor(0.3515)\n",
      "4777 Training Loss: tensor(0.3500)\n",
      "4778 Training Loss: tensor(0.3536)\n",
      "4779 Training Loss: tensor(0.3520)\n",
      "4780 Training Loss: tensor(0.3488)\n",
      "4781 Training Loss: tensor(0.3484)\n",
      "4782 Training Loss: tensor(0.3496)\n",
      "4783 Training Loss: tensor(0.3516)\n",
      "4784 Training Loss: tensor(0.3470)\n",
      "4785 Training Loss: tensor(0.3496)\n",
      "4786 Training Loss: tensor(0.3482)\n",
      "4787 Training Loss: tensor(0.3476)\n",
      "4788 Training Loss: tensor(0.3547)\n",
      "4789 Training Loss: tensor(0.3470)\n",
      "4790 Training Loss: tensor(0.3484)\n",
      "4791 Training Loss: tensor(0.3483)\n",
      "4792 Training Loss: tensor(0.3480)\n",
      "4793 Training Loss: tensor(0.3489)\n",
      "4794 Training Loss: tensor(0.3495)\n",
      "4795 Training Loss: tensor(0.3471)\n",
      "4796 Training Loss: tensor(0.3503)\n",
      "4797 Training Loss: tensor(0.3490)\n",
      "4798 Training Loss: tensor(0.3485)\n",
      "4799 Training Loss: tensor(0.3500)\n",
      "4800 Training Loss: tensor(0.3500)\n",
      "4801 Training Loss: tensor(0.3500)\n",
      "4802 Training Loss: tensor(0.3469)\n",
      "4803 Training Loss: tensor(0.3496)\n",
      "4804 Training Loss: tensor(0.3488)\n",
      "4805 Training Loss: tensor(0.3470)\n",
      "4806 Training Loss: tensor(0.3487)\n",
      "4807 Training Loss: tensor(0.3521)\n",
      "4808 Training Loss: tensor(0.3487)\n",
      "4809 Training Loss: tensor(0.3470)\n",
      "4810 Training Loss: tensor(0.3530)\n",
      "4811 Training Loss: tensor(0.3479)\n",
      "4812 Training Loss: tensor(0.3493)\n",
      "4813 Training Loss: tensor(0.3501)\n",
      "4814 Training Loss: tensor(0.3554)\n",
      "4815 Training Loss: tensor(0.3485)\n",
      "4816 Training Loss: tensor(0.3476)\n",
      "4817 Training Loss: tensor(0.3493)\n",
      "4818 Training Loss: tensor(0.3484)\n",
      "4819 Training Loss: tensor(0.3526)\n",
      "4820 Training Loss: tensor(0.3493)\n",
      "4821 Training Loss: tensor(0.3536)\n",
      "4822 Training Loss: tensor(0.3495)\n",
      "4823 Training Loss: tensor(0.3510)\n",
      "4824 Training Loss: tensor(0.3556)\n",
      "4825 Training Loss: tensor(0.3482)\n",
      "4826 Training Loss: tensor(0.3506)\n",
      "4827 Training Loss: tensor(0.3522)\n",
      "4828 Training Loss: tensor(0.3508)\n",
      "4829 Training Loss: tensor(0.3506)\n",
      "4830 Training Loss: tensor(0.3506)\n",
      "4831 Training Loss: tensor(0.3489)\n",
      "4832 Training Loss: tensor(0.3561)\n",
      "4833 Training Loss: tensor(0.3505)\n",
      "4834 Training Loss: tensor(0.3538)\n",
      "4835 Training Loss: tensor(0.3491)\n",
      "4836 Training Loss: tensor(0.3527)\n",
      "4837 Training Loss: tensor(0.3499)\n",
      "4838 Training Loss: tensor(0.3486)\n",
      "4839 Training Loss: tensor(0.3491)\n",
      "4840 Training Loss: tensor(0.3496)\n",
      "4841 Training Loss: tensor(0.3486)\n",
      "4842 Training Loss: tensor(0.3490)\n",
      "4843 Training Loss: tensor(0.3475)\n",
      "4844 Training Loss: tensor(0.3493)\n",
      "4845 Training Loss: tensor(0.3496)\n",
      "4846 Training Loss: tensor(0.3476)\n",
      "4847 Training Loss: tensor(0.3477)\n",
      "4848 Training Loss: tensor(0.3482)\n",
      "4849 Training Loss: tensor(0.3493)\n",
      "4850 Training Loss: tensor(0.3474)\n",
      "4851 Training Loss: tensor(0.3474)\n",
      "4852 Training Loss: tensor(0.3495)\n",
      "4853 Training Loss: tensor(0.3480)\n",
      "4854 Training Loss: tensor(0.3479)\n",
      "4855 Training Loss: tensor(0.3490)\n",
      "4856 Training Loss: tensor(0.3506)\n",
      "4857 Training Loss: tensor(0.3537)\n",
      "4858 Training Loss: tensor(0.3618)\n",
      "4859 Training Loss: tensor(0.3492)\n",
      "4860 Training Loss: tensor(0.3490)\n",
      "4861 Training Loss: tensor(0.3515)\n",
      "4862 Training Loss: tensor(0.3492)\n",
      "4863 Training Loss: tensor(0.3531)\n",
      "4864 Training Loss: tensor(0.3524)\n",
      "4865 Training Loss: tensor(0.3493)\n",
      "4866 Training Loss: tensor(0.3500)\n",
      "4867 Training Loss: tensor(0.3488)\n",
      "4868 Training Loss: tensor(0.3546)\n",
      "4869 Training Loss: tensor(0.3477)\n",
      "4870 Training Loss: tensor(0.3509)\n",
      "4871 Training Loss: tensor(0.3487)\n",
      "4872 Training Loss: tensor(0.3493)\n",
      "4873 Training Loss: tensor(0.3488)\n",
      "4874 Training Loss: tensor(0.3486)\n",
      "4875 Training Loss: tensor(0.3471)\n",
      "4876 Training Loss: tensor(0.3517)\n",
      "4877 Training Loss: tensor(0.3489)\n",
      "4878 Training Loss: tensor(0.3478)\n",
      "4879 Training Loss: tensor(0.3529)\n",
      "4880 Training Loss: tensor(0.3478)\n",
      "4881 Training Loss: tensor(0.3472)\n",
      "4882 Training Loss: tensor(0.3493)\n",
      "4883 Training Loss: tensor(0.3474)\n",
      "4884 Training Loss: tensor(0.3542)\n",
      "4885 Training Loss: tensor(0.3505)\n",
      "4886 Training Loss: tensor(0.3500)\n",
      "4887 Training Loss: tensor(0.3478)\n",
      "4888 Training Loss: tensor(0.3486)\n",
      "4889 Training Loss: tensor(0.3641)\n",
      "4890 Training Loss: tensor(0.3508)\n",
      "4891 Training Loss: tensor(0.3471)\n",
      "4892 Training Loss: tensor(0.3504)\n",
      "4893 Training Loss: tensor(0.3529)\n",
      "4894 Training Loss: tensor(0.3484)\n",
      "4895 Training Loss: tensor(0.3494)\n",
      "4896 Training Loss: tensor(0.3543)\n",
      "4897 Training Loss: tensor(0.3528)\n",
      "4898 Training Loss: tensor(0.3497)\n",
      "4899 Training Loss: tensor(0.3487)\n",
      "4900 Training Loss: tensor(0.3527)\n",
      "4901 Training Loss: tensor(0.3522)\n",
      "4902 Training Loss: tensor(0.3503)\n",
      "4903 Training Loss: tensor(0.3497)\n",
      "4904 Training Loss: tensor(0.3515)\n",
      "4905 Training Loss: tensor(0.3506)\n",
      "4906 Training Loss: tensor(0.3479)\n",
      "4907 Training Loss: tensor(0.3477)\n",
      "4908 Training Loss: tensor(0.3612)\n",
      "4909 Training Loss: tensor(0.3487)\n",
      "4910 Training Loss: tensor(0.3502)\n",
      "4911 Training Loss: tensor(0.3499)\n",
      "4912 Training Loss: tensor(0.3533)\n",
      "4913 Training Loss: tensor(0.3486)\n",
      "4914 Training Loss: tensor(0.3487)\n",
      "4915 Training Loss: tensor(0.3544)\n",
      "4916 Training Loss: tensor(0.3494)\n",
      "4917 Training Loss: tensor(0.3489)\n",
      "4918 Training Loss: tensor(0.3508)\n",
      "4919 Training Loss: tensor(0.3531)\n",
      "4920 Training Loss: tensor(0.3497)\n",
      "4921 Training Loss: tensor(0.3518)\n",
      "4922 Training Loss: tensor(0.3497)\n",
      "4923 Training Loss: tensor(0.3478)\n",
      "4924 Training Loss: tensor(0.3542)\n",
      "4925 Training Loss: tensor(0.3492)\n",
      "4926 Training Loss: tensor(0.3499)\n",
      "4927 Training Loss: tensor(0.3501)\n",
      "4928 Training Loss: tensor(0.3522)\n",
      "4929 Training Loss: tensor(0.3502)\n",
      "4930 Training Loss: tensor(0.3512)\n",
      "4931 Training Loss: tensor(0.3494)\n",
      "4932 Training Loss: tensor(0.3493)\n",
      "4933 Training Loss: tensor(0.3497)\n",
      "4934 Training Loss: tensor(0.3506)\n",
      "4935 Training Loss: tensor(0.3522)\n",
      "4936 Training Loss: tensor(0.3517)\n",
      "4937 Training Loss: tensor(0.3492)\n",
      "4938 Training Loss: tensor(0.3497)\n",
      "4939 Training Loss: tensor(0.3505)\n",
      "4940 Training Loss: tensor(0.3520)\n",
      "4941 Training Loss: tensor(0.3496)\n",
      "4942 Training Loss: tensor(0.3498)\n",
      "4943 Training Loss: tensor(0.3503)\n",
      "4944 Training Loss: tensor(0.3513)\n",
      "4945 Training Loss: tensor(0.3493)\n",
      "4946 Training Loss: tensor(0.3558)\n",
      "4947 Training Loss: tensor(0.3495)\n",
      "4948 Training Loss: tensor(0.3491)\n",
      "4949 Training Loss: tensor(0.3481)\n",
      "4950 Training Loss: tensor(0.3513)\n",
      "4951 Training Loss: tensor(0.3481)\n",
      "4952 Training Loss: tensor(0.3483)\n",
      "4953 Training Loss: tensor(0.3482)\n",
      "4954 Training Loss: tensor(0.3509)\n",
      "4955 Training Loss: tensor(0.3505)\n",
      "4956 Training Loss: tensor(0.3529)\n",
      "4957 Training Loss: tensor(0.3493)\n",
      "4958 Training Loss: tensor(0.3493)\n",
      "4959 Training Loss: tensor(0.3543)\n",
      "4960 Training Loss: tensor(0.3499)\n",
      "4961 Training Loss: tensor(0.3498)\n",
      "4962 Training Loss: tensor(0.3489)\n",
      "4963 Training Loss: tensor(0.3485)\n",
      "4964 Training Loss: tensor(0.3480)\n",
      "4965 Training Loss: tensor(0.3514)\n",
      "4966 Training Loss: tensor(0.3506)\n",
      "4967 Training Loss: tensor(0.3533)\n",
      "4968 Training Loss: tensor(0.3539)\n",
      "4969 Training Loss: tensor(0.3504)\n",
      "4970 Training Loss: tensor(0.3492)\n",
      "4971 Training Loss: tensor(0.3482)\n",
      "4972 Training Loss: tensor(0.3500)\n",
      "4973 Training Loss: tensor(0.3490)\n",
      "4974 Training Loss: tensor(0.3510)\n",
      "4975 Training Loss: tensor(0.3478)\n",
      "4976 Training Loss: tensor(0.3492)\n",
      "4977 Training Loss: tensor(0.3564)\n",
      "4978 Training Loss: tensor(0.3521)\n",
      "4979 Training Loss: tensor(0.3492)\n",
      "4980 Training Loss: tensor(0.3533)\n",
      "4981 Training Loss: tensor(0.3490)\n",
      "4982 Training Loss: tensor(0.3511)\n",
      "4983 Training Loss: tensor(0.3493)\n",
      "4984 Training Loss: tensor(0.3485)\n",
      "4985 Training Loss: tensor(0.3483)\n",
      "4986 Training Loss: tensor(0.3480)\n",
      "4987 Training Loss: tensor(0.3494)\n",
      "4988 Training Loss: tensor(0.3498)\n",
      "4989 Training Loss: tensor(0.3521)\n",
      "4990 Training Loss: tensor(0.3477)\n",
      "4991 Training Loss: tensor(0.3487)\n",
      "4992 Training Loss: tensor(0.3491)\n",
      "4993 Training Loss: tensor(0.3475)\n",
      "4994 Training Loss: tensor(0.3597)\n",
      "4995 Training Loss: tensor(0.3525)\n",
      "4996 Training Loss: tensor(0.3469)\n",
      "4997 Training Loss: tensor(0.3522)\n",
      "4998 Training Loss: tensor(0.3552)\n",
      "4999 Training Loss: tensor(0.3491)\n",
      "5000 Training Loss: tensor(0.3517)\n",
      "5001 Training Loss: tensor(0.3505)\n",
      "5002 Training Loss: tensor(0.3495)\n",
      "5003 Training Loss: tensor(0.3485)\n",
      "5004 Training Loss: tensor(0.3491)\n",
      "5005 Training Loss: tensor(0.3504)\n",
      "5006 Training Loss: tensor(0.3494)\n",
      "5007 Training Loss: tensor(0.3500)\n",
      "5008 Training Loss: tensor(0.3507)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5009 Training Loss: tensor(0.3490)\n",
      "5010 Training Loss: tensor(0.3484)\n",
      "5011 Training Loss: tensor(0.3480)\n",
      "5012 Training Loss: tensor(0.3499)\n",
      "5013 Training Loss: tensor(0.3489)\n",
      "5014 Training Loss: tensor(0.3476)\n",
      "5015 Training Loss: tensor(0.3515)\n",
      "5016 Training Loss: tensor(0.3502)\n",
      "5017 Training Loss: tensor(0.3520)\n",
      "5018 Training Loss: tensor(0.3481)\n",
      "5019 Training Loss: tensor(0.3480)\n",
      "5020 Training Loss: tensor(0.3475)\n",
      "5021 Training Loss: tensor(0.3487)\n",
      "5022 Training Loss: tensor(0.3513)\n",
      "5023 Training Loss: tensor(0.3471)\n",
      "5024 Training Loss: tensor(0.3492)\n",
      "5025 Training Loss: tensor(0.3478)\n",
      "5026 Training Loss: tensor(0.3475)\n",
      "5027 Training Loss: tensor(0.3475)\n",
      "5028 Training Loss: tensor(0.3470)\n",
      "5029 Training Loss: tensor(0.3500)\n",
      "5030 Training Loss: tensor(0.3529)\n",
      "5031 Training Loss: tensor(0.3473)\n",
      "5032 Training Loss: tensor(0.3538)\n",
      "5033 Training Loss: tensor(0.3473)\n",
      "5034 Training Loss: tensor(0.3493)\n",
      "5035 Training Loss: tensor(0.3495)\n",
      "5036 Training Loss: tensor(0.3500)\n",
      "5037 Training Loss: tensor(0.3524)\n",
      "5038 Training Loss: tensor(0.3489)\n",
      "5039 Training Loss: tensor(0.3500)\n",
      "5040 Training Loss: tensor(0.3469)\n",
      "5041 Training Loss: tensor(0.3484)\n",
      "5042 Training Loss: tensor(0.3470)\n",
      "5043 Training Loss: tensor(0.3480)\n",
      "5044 Training Loss: tensor(0.3535)\n",
      "5045 Training Loss: tensor(0.3494)\n",
      "5046 Training Loss: tensor(0.3506)\n",
      "5047 Training Loss: tensor(0.3481)\n",
      "5048 Training Loss: tensor(0.3517)\n",
      "5049 Training Loss: tensor(0.3487)\n",
      "5050 Training Loss: tensor(0.3477)\n",
      "5051 Training Loss: tensor(0.3478)\n",
      "5052 Training Loss: tensor(0.3511)\n",
      "5053 Training Loss: tensor(0.3468)\n",
      "5054 Training Loss: tensor(0.3475)\n",
      "5055 Training Loss: tensor(0.3486)\n",
      "5056 Training Loss: tensor(0.3490)\n",
      "5057 Training Loss: tensor(0.3490)\n",
      "5058 Training Loss: tensor(0.3488)\n",
      "5059 Training Loss: tensor(0.3479)\n",
      "5060 Training Loss: tensor(0.3475)\n",
      "5061 Training Loss: tensor(0.3468)\n",
      "5062 Training Loss: tensor(0.3470)\n",
      "5063 Training Loss: tensor(0.3503)\n",
      "5064 Training Loss: tensor(0.3470)\n",
      "5065 Training Loss: tensor(0.3472)\n",
      "5066 Training Loss: tensor(0.3580)\n",
      "5067 Training Loss: tensor(0.3478)\n",
      "5068 Training Loss: tensor(0.3482)\n",
      "5069 Training Loss: tensor(0.3501)\n",
      "5070 Training Loss: tensor(0.3552)\n",
      "5071 Training Loss: tensor(0.3516)\n",
      "5072 Training Loss: tensor(0.3565)\n",
      "5073 Training Loss: tensor(0.3490)\n",
      "5074 Training Loss: tensor(0.3503)\n",
      "5075 Training Loss: tensor(0.3492)\n",
      "5076 Training Loss: tensor(0.3486)\n",
      "5077 Training Loss: tensor(0.3499)\n",
      "5078 Training Loss: tensor(0.3500)\n",
      "5079 Training Loss: tensor(0.3533)\n",
      "5080 Training Loss: tensor(0.3551)\n",
      "5081 Training Loss: tensor(0.3495)\n",
      "5082 Training Loss: tensor(0.3512)\n",
      "5083 Training Loss: tensor(0.3497)\n",
      "5084 Training Loss: tensor(0.3474)\n",
      "5085 Training Loss: tensor(0.3514)\n",
      "5086 Training Loss: tensor(0.3490)\n",
      "5087 Training Loss: tensor(0.3496)\n",
      "5088 Training Loss: tensor(0.3482)\n",
      "5089 Training Loss: tensor(0.3506)\n",
      "5090 Training Loss: tensor(0.3482)\n",
      "5091 Training Loss: tensor(0.3489)\n",
      "5092 Training Loss: tensor(0.3483)\n",
      "5093 Training Loss: tensor(0.3483)\n",
      "5094 Training Loss: tensor(0.3486)\n",
      "5095 Training Loss: tensor(0.3480)\n",
      "5096 Training Loss: tensor(0.3497)\n",
      "5097 Training Loss: tensor(0.3475)\n",
      "5098 Training Loss: tensor(0.3479)\n",
      "5099 Training Loss: tensor(0.3495)\n",
      "5100 Training Loss: tensor(0.3485)\n",
      "5101 Training Loss: tensor(0.3478)\n",
      "5102 Training Loss: tensor(0.3474)\n",
      "5103 Training Loss: tensor(0.3482)\n",
      "5104 Training Loss: tensor(0.3479)\n",
      "5105 Training Loss: tensor(0.3516)\n",
      "5106 Training Loss: tensor(0.3523)\n",
      "5107 Training Loss: tensor(0.3471)\n",
      "5108 Training Loss: tensor(0.3497)\n",
      "5109 Training Loss: tensor(0.3493)\n",
      "5110 Training Loss: tensor(0.3489)\n",
      "5111 Training Loss: tensor(0.3475)\n",
      "5112 Training Loss: tensor(0.3478)\n",
      "5113 Training Loss: tensor(0.3469)\n",
      "5114 Training Loss: tensor(0.3561)\n",
      "5115 Training Loss: tensor(0.3475)\n",
      "5116 Training Loss: tensor(0.3497)\n",
      "5117 Training Loss: tensor(0.3504)\n",
      "5118 Training Loss: tensor(0.3474)\n",
      "5119 Training Loss: tensor(0.3579)\n",
      "5120 Training Loss: tensor(0.3485)\n",
      "5121 Training Loss: tensor(0.3481)\n",
      "5122 Training Loss: tensor(0.3492)\n",
      "5123 Training Loss: tensor(0.3523)\n",
      "5124 Training Loss: tensor(0.3481)\n",
      "5125 Training Loss: tensor(0.3511)\n",
      "5126 Training Loss: tensor(0.3520)\n",
      "5127 Training Loss: tensor(0.3499)\n",
      "5128 Training Loss: tensor(0.3489)\n",
      "5129 Training Loss: tensor(0.3512)\n",
      "5130 Training Loss: tensor(0.3486)\n",
      "5131 Training Loss: tensor(0.3481)\n",
      "5132 Training Loss: tensor(0.3525)\n",
      "5133 Training Loss: tensor(0.3494)\n",
      "5134 Training Loss: tensor(0.3496)\n",
      "5135 Training Loss: tensor(0.3502)\n",
      "5136 Training Loss: tensor(0.3485)\n",
      "5137 Training Loss: tensor(0.3476)\n",
      "5138 Training Loss: tensor(0.3492)\n",
      "5139 Training Loss: tensor(0.3488)\n",
      "5140 Training Loss: tensor(0.3508)\n",
      "5141 Training Loss: tensor(0.3476)\n",
      "5142 Training Loss: tensor(0.3474)\n",
      "5143 Training Loss: tensor(0.3562)\n",
      "5144 Training Loss: tensor(0.3496)\n",
      "5145 Training Loss: tensor(0.3481)\n",
      "5146 Training Loss: tensor(0.3477)\n",
      "5147 Training Loss: tensor(0.3474)\n",
      "5148 Training Loss: tensor(0.3484)\n",
      "5149 Training Loss: tensor(0.3491)\n",
      "5150 Training Loss: tensor(0.3483)\n",
      "5151 Training Loss: tensor(0.3496)\n",
      "5152 Training Loss: tensor(0.3481)\n",
      "5153 Training Loss: tensor(0.3527)\n",
      "5154 Training Loss: tensor(0.3520)\n",
      "5155 Training Loss: tensor(0.3482)\n",
      "5156 Training Loss: tensor(0.3483)\n",
      "5157 Training Loss: tensor(0.3514)\n",
      "5158 Training Loss: tensor(0.3485)\n",
      "5159 Training Loss: tensor(0.3478)\n",
      "5160 Training Loss: tensor(0.3520)\n",
      "5161 Training Loss: tensor(0.3541)\n",
      "5162 Training Loss: tensor(0.3545)\n",
      "5163 Training Loss: tensor(0.3497)\n",
      "5164 Training Loss: tensor(0.3485)\n",
      "5165 Training Loss: tensor(0.3487)\n",
      "5166 Training Loss: tensor(0.3514)\n",
      "5167 Training Loss: tensor(0.3498)\n",
      "5168 Training Loss: tensor(0.3487)\n",
      "5169 Training Loss: tensor(0.3483)\n",
      "5170 Training Loss: tensor(0.3486)\n",
      "5171 Training Loss: tensor(0.3570)\n",
      "5172 Training Loss: tensor(0.3477)\n",
      "5173 Training Loss: tensor(0.3476)\n",
      "5174 Training Loss: tensor(0.3489)\n",
      "5175 Training Loss: tensor(0.3490)\n",
      "5176 Training Loss: tensor(0.3477)\n",
      "5177 Training Loss: tensor(0.3490)\n",
      "5178 Training Loss: tensor(0.3492)\n",
      "5179 Training Loss: tensor(0.3474)\n",
      "5180 Training Loss: tensor(0.3502)\n",
      "5181 Training Loss: tensor(0.3483)\n",
      "5182 Training Loss: tensor(0.3476)\n",
      "5183 Training Loss: tensor(0.3479)\n",
      "5184 Training Loss: tensor(0.3501)\n",
      "5185 Training Loss: tensor(0.3527)\n",
      "5186 Training Loss: tensor(0.3478)\n",
      "5187 Training Loss: tensor(0.3517)\n",
      "5188 Training Loss: tensor(0.3465)\n",
      "5189 Training Loss: tensor(0.3486)\n",
      "5190 Training Loss: tensor(0.3483)\n",
      "5191 Training Loss: tensor(0.3474)\n",
      "5192 Training Loss: tensor(0.3604)\n",
      "5193 Training Loss: tensor(0.3488)\n",
      "5194 Training Loss: tensor(0.3487)\n",
      "5195 Training Loss: tensor(0.3495)\n",
      "5196 Training Loss: tensor(0.3475)\n",
      "5197 Training Loss: tensor(0.3483)\n",
      "5198 Training Loss: tensor(0.3491)\n",
      "5199 Training Loss: tensor(0.3492)\n",
      "5200 Training Loss: tensor(0.3509)\n",
      "5201 Training Loss: tensor(0.3474)\n",
      "5202 Training Loss: tensor(0.3489)\n",
      "5203 Training Loss: tensor(0.3477)\n",
      "5204 Training Loss: tensor(0.3480)\n",
      "5205 Training Loss: tensor(0.3475)\n",
      "5206 Training Loss: tensor(0.3509)\n",
      "5207 Training Loss: tensor(0.3483)\n",
      "5208 Training Loss: tensor(0.3499)\n",
      "5209 Training Loss: tensor(0.3476)\n",
      "5210 Training Loss: tensor(0.3529)\n",
      "5211 Training Loss: tensor(0.3518)\n",
      "5212 Training Loss: tensor(0.3477)\n",
      "5213 Training Loss: tensor(0.3588)\n",
      "5214 Training Loss: tensor(0.3514)\n",
      "5215 Training Loss: tensor(0.3508)\n",
      "5216 Training Loss: tensor(0.3484)\n",
      "5217 Training Loss: tensor(0.3534)\n",
      "5218 Training Loss: tensor(0.3493)\n",
      "5219 Training Loss: tensor(0.3504)\n",
      "5220 Training Loss: tensor(0.3479)\n",
      "5221 Training Loss: tensor(0.3517)\n",
      "5222 Training Loss: tensor(0.3514)\n",
      "5223 Training Loss: tensor(0.3494)\n",
      "5224 Training Loss: tensor(0.3536)\n",
      "5225 Training Loss: tensor(0.3496)\n",
      "5226 Training Loss: tensor(0.3508)\n",
      "5227 Training Loss: tensor(0.3485)\n",
      "5228 Training Loss: tensor(0.3541)\n",
      "5229 Training Loss: tensor(0.3515)\n",
      "5230 Training Loss: tensor(0.3511)\n",
      "5231 Training Loss: tensor(0.3499)\n",
      "5232 Training Loss: tensor(0.3502)\n",
      "5233 Training Loss: tensor(0.3501)\n",
      "5234 Training Loss: tensor(0.3488)\n",
      "5235 Training Loss: tensor(0.3515)\n",
      "5236 Training Loss: tensor(0.3526)\n",
      "5237 Training Loss: tensor(0.3495)\n",
      "5238 Training Loss: tensor(0.3505)\n",
      "5239 Training Loss: tensor(0.3482)\n",
      "5240 Training Loss: tensor(0.3555)\n",
      "5241 Training Loss: tensor(0.3509)\n",
      "5242 Training Loss: tensor(0.3499)\n",
      "5243 Training Loss: tensor(0.3481)\n",
      "5244 Training Loss: tensor(0.3499)\n",
      "5245 Training Loss: tensor(0.3579)\n",
      "5246 Training Loss: tensor(0.3484)\n",
      "5247 Training Loss: tensor(0.3547)\n",
      "5248 Training Loss: tensor(0.3490)\n",
      "5249 Training Loss: tensor(0.3500)\n",
      "5250 Training Loss: tensor(0.3512)\n",
      "5251 Training Loss: tensor(0.3486)\n",
      "5252 Training Loss: tensor(0.3524)\n",
      "5253 Training Loss: tensor(0.3493)\n",
      "5254 Training Loss: tensor(0.3503)\n",
      "5255 Training Loss: tensor(0.3493)\n",
      "5256 Training Loss: tensor(0.3482)\n",
      "5257 Training Loss: tensor(0.3485)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5258 Training Loss: tensor(0.3492)\n",
      "5259 Training Loss: tensor(0.3515)\n",
      "5260 Training Loss: tensor(0.3489)\n",
      "5261 Training Loss: tensor(0.3483)\n",
      "5262 Training Loss: tensor(0.3529)\n",
      "5263 Training Loss: tensor(0.3495)\n",
      "5264 Training Loss: tensor(0.3478)\n",
      "5265 Training Loss: tensor(0.3510)\n",
      "5266 Training Loss: tensor(0.3473)\n",
      "5267 Training Loss: tensor(0.3484)\n",
      "5268 Training Loss: tensor(0.3492)\n",
      "5269 Training Loss: tensor(0.3485)\n",
      "5270 Training Loss: tensor(0.3578)\n",
      "5271 Training Loss: tensor(0.3509)\n",
      "5272 Training Loss: tensor(0.3482)\n",
      "5273 Training Loss: tensor(0.3476)\n",
      "5274 Training Loss: tensor(0.3506)\n",
      "5275 Training Loss: tensor(0.3492)\n",
      "5276 Training Loss: tensor(0.3520)\n",
      "5277 Training Loss: tensor(0.3539)\n",
      "5278 Training Loss: tensor(0.3486)\n",
      "5279 Training Loss: tensor(0.3506)\n",
      "5280 Training Loss: tensor(0.3488)\n",
      "5281 Training Loss: tensor(0.3496)\n",
      "5282 Training Loss: tensor(0.3481)\n",
      "5283 Training Loss: tensor(0.3477)\n",
      "5284 Training Loss: tensor(0.3491)\n",
      "5285 Training Loss: tensor(0.3482)\n",
      "5286 Training Loss: tensor(0.3488)\n",
      "5287 Training Loss: tensor(0.3474)\n",
      "5288 Training Loss: tensor(0.3526)\n",
      "5289 Training Loss: tensor(0.3493)\n",
      "5290 Training Loss: tensor(0.3479)\n",
      "5291 Training Loss: tensor(0.3500)\n",
      "5292 Training Loss: tensor(0.3510)\n",
      "5293 Training Loss: tensor(0.3522)\n",
      "5294 Training Loss: tensor(0.3486)\n",
      "5295 Training Loss: tensor(0.3495)\n",
      "5296 Training Loss: tensor(0.3519)\n",
      "5297 Training Loss: tensor(0.3476)\n",
      "5298 Training Loss: tensor(0.3509)\n",
      "5299 Training Loss: tensor(0.3489)\n",
      "5300 Training Loss: tensor(0.3470)\n",
      "5301 Training Loss: tensor(0.3480)\n",
      "5302 Training Loss: tensor(0.3516)\n",
      "5303 Training Loss: tensor(0.3494)\n",
      "5304 Training Loss: tensor(0.3474)\n",
      "5305 Training Loss: tensor(0.3485)\n",
      "5306 Training Loss: tensor(0.3479)\n",
      "5307 Training Loss: tensor(0.3508)\n",
      "5308 Training Loss: tensor(0.3480)\n",
      "5309 Training Loss: tensor(0.3500)\n",
      "5310 Training Loss: tensor(0.3479)\n",
      "5311 Training Loss: tensor(0.3473)\n",
      "5312 Training Loss: tensor(0.3494)\n",
      "5313 Training Loss: tensor(0.3469)\n",
      "5314 Training Loss: tensor(0.3467)\n",
      "5315 Training Loss: tensor(0.3476)\n",
      "5316 Training Loss: tensor(0.3485)\n",
      "5317 Training Loss: tensor(0.3515)\n",
      "5318 Training Loss: tensor(0.3475)\n",
      "5319 Training Loss: tensor(0.3477)\n",
      "5320 Training Loss: tensor(0.3467)\n",
      "5321 Training Loss: tensor(0.3610)\n",
      "5322 Training Loss: tensor(0.3554)\n",
      "5323 Training Loss: tensor(0.3480)\n",
      "5324 Training Loss: tensor(0.3520)\n",
      "5325 Training Loss: tensor(0.3509)\n",
      "5326 Training Loss: tensor(0.3486)\n",
      "5327 Training Loss: tensor(0.3504)\n",
      "5328 Training Loss: tensor(0.3488)\n",
      "5329 Training Loss: tensor(0.3476)\n",
      "5330 Training Loss: tensor(0.3478)\n",
      "5331 Training Loss: tensor(0.3491)\n",
      "5332 Training Loss: tensor(0.3478)\n",
      "5333 Training Loss: tensor(0.3483)\n",
      "5334 Training Loss: tensor(0.3503)\n",
      "5335 Training Loss: tensor(0.3482)\n",
      "5336 Training Loss: tensor(0.3515)\n",
      "5337 Training Loss: tensor(0.3505)\n",
      "5338 Training Loss: tensor(0.3474)\n",
      "5339 Training Loss: tensor(0.3475)\n",
      "5340 Training Loss: tensor(0.3495)\n",
      "5341 Training Loss: tensor(0.3482)\n",
      "5342 Training Loss: tensor(0.3480)\n",
      "5343 Training Loss: tensor(0.3500)\n",
      "5344 Training Loss: tensor(0.3468)\n",
      "5345 Training Loss: tensor(0.3498)\n",
      "5346 Training Loss: tensor(0.3524)\n",
      "5347 Training Loss: tensor(0.3470)\n",
      "5348 Training Loss: tensor(0.3475)\n",
      "5349 Training Loss: tensor(0.3551)\n",
      "5350 Training Loss: tensor(0.3478)\n",
      "5351 Training Loss: tensor(0.3492)\n",
      "5352 Training Loss: tensor(0.3469)\n",
      "5353 Training Loss: tensor(0.3481)\n",
      "5354 Training Loss: tensor(0.3523)\n",
      "5355 Training Loss: tensor(0.3473)\n",
      "5356 Training Loss: tensor(0.3489)\n",
      "5357 Training Loss: tensor(0.3468)\n",
      "5358 Training Loss: tensor(0.3538)\n",
      "5359 Training Loss: tensor(0.3473)\n",
      "5360 Training Loss: tensor(0.3491)\n",
      "5361 Training Loss: tensor(0.3494)\n",
      "5362 Training Loss: tensor(0.3468)\n",
      "5363 Training Loss: tensor(0.3521)\n",
      "5364 Training Loss: tensor(0.3488)\n",
      "5365 Training Loss: tensor(0.3490)\n",
      "5366 Training Loss: tensor(0.3468)\n",
      "5367 Training Loss: tensor(0.3483)\n",
      "5368 Training Loss: tensor(0.3485)\n",
      "5369 Training Loss: tensor(0.3480)\n",
      "5370 Training Loss: tensor(0.3481)\n",
      "5371 Training Loss: tensor(0.3532)\n",
      "5372 Training Loss: tensor(0.3499)\n",
      "5373 Training Loss: tensor(0.3496)\n",
      "5374 Training Loss: tensor(0.3595)\n",
      "5375 Training Loss: tensor(0.3495)\n",
      "5376 Training Loss: tensor(0.3488)\n",
      "5377 Training Loss: tensor(0.3496)\n",
      "5378 Training Loss: tensor(0.3488)\n",
      "5379 Training Loss: tensor(0.3496)\n",
      "5380 Training Loss: tensor(0.3469)\n",
      "5381 Training Loss: tensor(0.3478)\n",
      "5382 Training Loss: tensor(0.3471)\n",
      "5383 Training Loss: tensor(0.3522)\n",
      "5384 Training Loss: tensor(0.3478)\n",
      "5385 Training Loss: tensor(0.3532)\n",
      "5386 Training Loss: tensor(0.3474)\n",
      "5387 Training Loss: tensor(0.3484)\n",
      "5388 Training Loss: tensor(0.3498)\n",
      "5389 Training Loss: tensor(0.3474)\n",
      "5390 Training Loss: tensor(0.3495)\n",
      "5391 Training Loss: tensor(0.3483)\n",
      "5392 Training Loss: tensor(0.3466)\n",
      "5393 Training Loss: tensor(0.3463)\n",
      "5394 Training Loss: tensor(0.3477)\n",
      "5395 Training Loss: tensor(0.3481)\n",
      "5396 Training Loss: tensor(0.3471)\n",
      "5397 Training Loss: tensor(0.3501)\n",
      "5398 Training Loss: tensor(0.3469)\n",
      "5399 Training Loss: tensor(0.3511)\n",
      "5400 Training Loss: tensor(0.3488)\n",
      "5401 Training Loss: tensor(0.3579)\n",
      "5402 Training Loss: tensor(0.3533)\n",
      "5403 Training Loss: tensor(0.3486)\n",
      "5404 Training Loss: tensor(0.3485)\n",
      "5405 Training Loss: tensor(0.3570)\n",
      "5406 Training Loss: tensor(0.3483)\n",
      "5407 Training Loss: tensor(0.3496)\n",
      "5408 Training Loss: tensor(0.3483)\n",
      "5409 Training Loss: tensor(0.3484)\n",
      "5410 Training Loss: tensor(0.3496)\n",
      "5411 Training Loss: tensor(0.3480)\n",
      "5412 Training Loss: tensor(0.3504)\n",
      "5413 Training Loss: tensor(0.3557)\n",
      "5414 Training Loss: tensor(0.3482)\n",
      "5415 Training Loss: tensor(0.3470)\n",
      "5416 Training Loss: tensor(0.3475)\n",
      "5417 Training Loss: tensor(0.3484)\n",
      "5418 Training Loss: tensor(0.3584)\n",
      "5419 Training Loss: tensor(0.3482)\n",
      "5420 Training Loss: tensor(0.3483)\n",
      "5421 Training Loss: tensor(0.3561)\n",
      "5422 Training Loss: tensor(0.3475)\n",
      "5423 Training Loss: tensor(0.3515)\n",
      "5424 Training Loss: tensor(0.3499)\n",
      "5425 Training Loss: tensor(0.3477)\n",
      "5426 Training Loss: tensor(0.3547)\n",
      "5427 Training Loss: tensor(0.3553)\n",
      "5428 Training Loss: tensor(0.3507)\n",
      "5429 Training Loss: tensor(0.3569)\n",
      "5430 Training Loss: tensor(0.3470)\n",
      "5431 Training Loss: tensor(0.3494)\n",
      "5432 Training Loss: tensor(0.3499)\n",
      "5433 Training Loss: tensor(0.3498)\n",
      "5434 Training Loss: tensor(0.3616)\n",
      "5435 Training Loss: tensor(0.3494)\n",
      "5436 Training Loss: tensor(0.3478)\n",
      "5437 Training Loss: tensor(0.3489)\n",
      "5438 Training Loss: tensor(0.3488)\n",
      "5439 Training Loss: tensor(0.3535)\n",
      "5440 Training Loss: tensor(0.3511)\n",
      "5441 Training Loss: tensor(0.3502)\n",
      "5442 Training Loss: tensor(0.3479)\n",
      "5443 Training Loss: tensor(0.3492)\n",
      "5444 Training Loss: tensor(0.3493)\n",
      "5445 Training Loss: tensor(0.3491)\n",
      "5446 Training Loss: tensor(0.3493)\n",
      "5447 Training Loss: tensor(0.3486)\n",
      "5448 Training Loss: tensor(0.3489)\n",
      "5449 Training Loss: tensor(0.3482)\n",
      "5450 Training Loss: tensor(0.3560)\n",
      "5451 Training Loss: tensor(0.3490)\n",
      "5452 Training Loss: tensor(0.3509)\n",
      "5453 Training Loss: tensor(0.3535)\n",
      "5454 Training Loss: tensor(0.3482)\n",
      "5455 Training Loss: tensor(0.3479)\n",
      "5456 Training Loss: tensor(0.3502)\n",
      "5457 Training Loss: tensor(0.3483)\n",
      "5458 Training Loss: tensor(0.3486)\n",
      "5459 Training Loss: tensor(0.3492)\n",
      "5460 Training Loss: tensor(0.3507)\n",
      "5461 Training Loss: tensor(0.3485)\n",
      "5462 Training Loss: tensor(0.3481)\n",
      "5463 Training Loss: tensor(0.3477)\n",
      "5464 Training Loss: tensor(0.3471)\n",
      "5465 Training Loss: tensor(0.3591)\n",
      "5466 Training Loss: tensor(0.3542)\n",
      "5467 Training Loss: tensor(0.3504)\n",
      "5468 Training Loss: tensor(0.3488)\n",
      "5469 Training Loss: tensor(0.3470)\n",
      "5470 Training Loss: tensor(0.3478)\n",
      "5471 Training Loss: tensor(0.3518)\n",
      "5472 Training Loss: tensor(0.3494)\n",
      "5473 Training Loss: tensor(0.3550)\n",
      "5474 Training Loss: tensor(0.3486)\n",
      "5475 Training Loss: tensor(0.3491)\n",
      "5476 Training Loss: tensor(0.3479)\n",
      "5477 Training Loss: tensor(0.3487)\n",
      "5478 Training Loss: tensor(0.3483)\n",
      "5479 Training Loss: tensor(0.3511)\n",
      "5480 Training Loss: tensor(0.3543)\n",
      "5481 Training Loss: tensor(0.3503)\n",
      "5482 Training Loss: tensor(0.3487)\n",
      "5483 Training Loss: tensor(0.3500)\n",
      "5484 Training Loss: tensor(0.3479)\n",
      "5485 Training Loss: tensor(0.3484)\n",
      "5486 Training Loss: tensor(0.3537)\n",
      "5487 Training Loss: tensor(0.3491)\n",
      "5488 Training Loss: tensor(0.3548)\n",
      "5489 Training Loss: tensor(0.3483)\n",
      "5490 Training Loss: tensor(0.3484)\n",
      "5491 Training Loss: tensor(0.3477)\n",
      "5492 Training Loss: tensor(0.3490)\n",
      "5493 Training Loss: tensor(0.3514)\n",
      "5494 Training Loss: tensor(0.3603)\n",
      "5495 Training Loss: tensor(0.3478)\n",
      "5496 Training Loss: tensor(0.3500)\n",
      "5497 Training Loss: tensor(0.3528)\n",
      "5498 Training Loss: tensor(0.3488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5499 Training Loss: tensor(0.3472)\n",
      "5500 Training Loss: tensor(0.3516)\n",
      "5501 Training Loss: tensor(0.3515)\n",
      "5502 Training Loss: tensor(0.3501)\n",
      "5503 Training Loss: tensor(0.3497)\n",
      "5504 Training Loss: tensor(0.3487)\n",
      "5505 Training Loss: tensor(0.3495)\n",
      "5506 Training Loss: tensor(0.3503)\n",
      "5507 Training Loss: tensor(0.3494)\n",
      "5508 Training Loss: tensor(0.3485)\n",
      "5509 Training Loss: tensor(0.3495)\n",
      "5510 Training Loss: tensor(0.3514)\n",
      "5511 Training Loss: tensor(0.3526)\n",
      "5512 Training Loss: tensor(0.3517)\n",
      "5513 Training Loss: tensor(0.3474)\n",
      "5514 Training Loss: tensor(0.3503)\n",
      "5515 Training Loss: tensor(0.3484)\n",
      "5516 Training Loss: tensor(0.3532)\n",
      "5517 Training Loss: tensor(0.3556)\n",
      "5518 Training Loss: tensor(0.3482)\n",
      "5519 Training Loss: tensor(0.3506)\n",
      "5520 Training Loss: tensor(0.3484)\n",
      "5521 Training Loss: tensor(0.3516)\n",
      "5522 Training Loss: tensor(0.3484)\n",
      "5523 Training Loss: tensor(0.3488)\n",
      "5524 Training Loss: tensor(0.3478)\n",
      "5525 Training Loss: tensor(0.3476)\n",
      "5526 Training Loss: tensor(0.3503)\n",
      "5527 Training Loss: tensor(0.3539)\n",
      "5528 Training Loss: tensor(0.3549)\n",
      "5529 Training Loss: tensor(0.3486)\n",
      "5530 Training Loss: tensor(0.3488)\n",
      "5531 Training Loss: tensor(0.3510)\n",
      "5532 Training Loss: tensor(0.3495)\n",
      "5533 Training Loss: tensor(0.3477)\n",
      "5534 Training Loss: tensor(0.3540)\n",
      "5535 Training Loss: tensor(0.3480)\n",
      "5536 Training Loss: tensor(0.3489)\n",
      "5537 Training Loss: tensor(0.3472)\n",
      "5538 Training Loss: tensor(0.3480)\n",
      "5539 Training Loss: tensor(0.3487)\n",
      "5540 Training Loss: tensor(0.3540)\n",
      "5541 Training Loss: tensor(0.3481)\n",
      "5542 Training Loss: tensor(0.3489)\n",
      "5543 Training Loss: tensor(0.3491)\n",
      "5544 Training Loss: tensor(0.3480)\n",
      "5545 Training Loss: tensor(0.3476)\n",
      "5546 Training Loss: tensor(0.3484)\n",
      "5547 Training Loss: tensor(0.3498)\n",
      "5548 Training Loss: tensor(0.3488)\n",
      "5549 Training Loss: tensor(0.3477)\n",
      "5550 Training Loss: tensor(0.3530)\n",
      "5551 Training Loss: tensor(0.3482)\n",
      "5552 Training Loss: tensor(0.3511)\n",
      "5553 Training Loss: tensor(0.3478)\n",
      "5554 Training Loss: tensor(0.3486)\n",
      "5555 Training Loss: tensor(0.3475)\n",
      "5556 Training Loss: tensor(0.3472)\n",
      "5557 Training Loss: tensor(0.3526)\n",
      "5558 Training Loss: tensor(0.3582)\n",
      "5559 Training Loss: tensor(0.3475)\n",
      "5560 Training Loss: tensor(0.3479)\n",
      "5561 Training Loss: tensor(0.3484)\n",
      "5562 Training Loss: tensor(0.3474)\n",
      "5563 Training Loss: tensor(0.3473)\n",
      "5564 Training Loss: tensor(0.3482)\n",
      "5565 Training Loss: tensor(0.3474)\n",
      "5566 Training Loss: tensor(0.3474)\n",
      "5567 Training Loss: tensor(0.3481)\n",
      "5568 Training Loss: tensor(0.3524)\n",
      "5569 Training Loss: tensor(0.3486)\n",
      "5570 Training Loss: tensor(0.3474)\n",
      "5571 Training Loss: tensor(0.3524)\n",
      "5572 Training Loss: tensor(0.3485)\n",
      "5573 Training Loss: tensor(0.3525)\n",
      "5574 Training Loss: tensor(0.3473)\n",
      "5575 Training Loss: tensor(0.3506)\n",
      "5576 Training Loss: tensor(0.3482)\n",
      "5577 Training Loss: tensor(0.3474)\n",
      "5578 Training Loss: tensor(0.3481)\n",
      "5579 Training Loss: tensor(0.3480)\n",
      "5580 Training Loss: tensor(0.3478)\n",
      "5581 Training Loss: tensor(0.3485)\n",
      "5582 Training Loss: tensor(0.3488)\n",
      "5583 Training Loss: tensor(0.3467)\n",
      "5584 Training Loss: tensor(0.3506)\n",
      "5585 Training Loss: tensor(0.3504)\n",
      "5586 Training Loss: tensor(0.3538)\n",
      "5587 Training Loss: tensor(0.3523)\n",
      "5588 Training Loss: tensor(0.3491)\n",
      "5589 Training Loss: tensor(0.3525)\n",
      "5590 Training Loss: tensor(0.3478)\n",
      "5591 Training Loss: tensor(0.3519)\n",
      "5592 Training Loss: tensor(0.3547)\n",
      "5593 Training Loss: tensor(0.3483)\n",
      "5594 Training Loss: tensor(0.3482)\n",
      "5595 Training Loss: tensor(0.3475)\n",
      "5596 Training Loss: tensor(0.3496)\n",
      "5597 Training Loss: tensor(0.3478)\n",
      "5598 Training Loss: tensor(0.3479)\n",
      "5599 Training Loss: tensor(0.3512)\n",
      "5600 Training Loss: tensor(0.3505)\n",
      "5601 Training Loss: tensor(0.3486)\n",
      "5602 Training Loss: tensor(0.3500)\n",
      "5603 Training Loss: tensor(0.3543)\n",
      "5604 Training Loss: tensor(0.3504)\n",
      "5605 Training Loss: tensor(0.3497)\n",
      "5606 Training Loss: tensor(0.3510)\n",
      "5607 Training Loss: tensor(0.3487)\n",
      "5608 Training Loss: tensor(0.3484)\n",
      "5609 Training Loss: tensor(0.3535)\n",
      "5610 Training Loss: tensor(0.3483)\n",
      "5611 Training Loss: tensor(0.3502)\n",
      "5612 Training Loss: tensor(0.3482)\n",
      "5613 Training Loss: tensor(0.3476)\n",
      "5614 Training Loss: tensor(0.3475)\n",
      "5615 Training Loss: tensor(0.3502)\n",
      "5616 Training Loss: tensor(0.3476)\n",
      "5617 Training Loss: tensor(0.3471)\n",
      "5618 Training Loss: tensor(0.3494)\n",
      "5619 Training Loss: tensor(0.3492)\n",
      "5620 Training Loss: tensor(0.3486)\n",
      "5621 Training Loss: tensor(0.3517)\n",
      "5622 Training Loss: tensor(0.3470)\n",
      "5623 Training Loss: tensor(0.3488)\n",
      "5624 Training Loss: tensor(0.3502)\n",
      "5625 Training Loss: tensor(0.3474)\n",
      "5626 Training Loss: tensor(0.3482)\n",
      "5627 Training Loss: tensor(0.3519)\n",
      "5628 Training Loss: tensor(0.3607)\n",
      "5629 Training Loss: tensor(0.3466)\n",
      "5630 Training Loss: tensor(0.3559)\n",
      "5631 Training Loss: tensor(0.3466)\n",
      "5632 Training Loss: tensor(0.3492)\n",
      "5633 Training Loss: tensor(0.3525)\n",
      "5634 Training Loss: tensor(0.3515)\n",
      "5635 Training Loss: tensor(0.3482)\n",
      "5636 Training Loss: tensor(0.3534)\n",
      "5637 Training Loss: tensor(0.3480)\n",
      "5638 Training Loss: tensor(0.3502)\n",
      "5639 Training Loss: tensor(0.3513)\n",
      "5640 Training Loss: tensor(0.3538)\n",
      "5641 Training Loss: tensor(0.3477)\n",
      "5642 Training Loss: tensor(0.3509)\n",
      "5643 Training Loss: tensor(0.3537)\n",
      "5644 Training Loss: tensor(0.3496)\n",
      "5645 Training Loss: tensor(0.3511)\n",
      "5646 Training Loss: tensor(0.3502)\n",
      "5647 Training Loss: tensor(0.3483)\n",
      "5648 Training Loss: tensor(0.3487)\n",
      "5649 Training Loss: tensor(0.3490)\n",
      "5650 Training Loss: tensor(0.3479)\n",
      "5651 Training Loss: tensor(0.3486)\n",
      "5652 Training Loss: tensor(0.3547)\n",
      "5653 Training Loss: tensor(0.3489)\n",
      "5654 Training Loss: tensor(0.3492)\n",
      "5655 Training Loss: tensor(0.3537)\n",
      "5656 Training Loss: tensor(0.3509)\n",
      "5657 Training Loss: tensor(0.3515)\n",
      "5658 Training Loss: tensor(0.3496)\n",
      "5659 Training Loss: tensor(0.3481)\n",
      "5660 Training Loss: tensor(0.3506)\n",
      "5661 Training Loss: tensor(0.3500)\n",
      "5662 Training Loss: tensor(0.3485)\n",
      "5663 Training Loss: tensor(0.3499)\n",
      "5664 Training Loss: tensor(0.3521)\n",
      "5665 Training Loss: tensor(0.3492)\n",
      "5666 Training Loss: tensor(0.3552)\n",
      "5667 Training Loss: tensor(0.3478)\n",
      "5668 Training Loss: tensor(0.3511)\n",
      "5669 Training Loss: tensor(0.3500)\n",
      "5670 Training Loss: tensor(0.3497)\n",
      "5671 Training Loss: tensor(0.3477)\n",
      "5672 Training Loss: tensor(0.3506)\n",
      "5673 Training Loss: tensor(0.3480)\n",
      "5674 Training Loss: tensor(0.3544)\n",
      "5675 Training Loss: tensor(0.3493)\n",
      "5676 Training Loss: tensor(0.3486)\n",
      "5677 Training Loss: tensor(0.3476)\n",
      "5678 Training Loss: tensor(0.3495)\n",
      "5679 Training Loss: tensor(0.3490)\n",
      "5680 Training Loss: tensor(0.3489)\n",
      "5681 Training Loss: tensor(0.3480)\n",
      "5682 Training Loss: tensor(0.3476)\n",
      "5683 Training Loss: tensor(0.3480)\n",
      "5684 Training Loss: tensor(0.3478)\n",
      "5685 Training Loss: tensor(0.3477)\n",
      "5686 Training Loss: tensor(0.3545)\n",
      "5687 Training Loss: tensor(0.3563)\n",
      "5688 Training Loss: tensor(0.3499)\n",
      "5689 Training Loss: tensor(0.3497)\n",
      "5690 Training Loss: tensor(0.3485)\n",
      "5691 Training Loss: tensor(0.3480)\n",
      "5692 Training Loss: tensor(0.3471)\n",
      "5693 Training Loss: tensor(0.3501)\n",
      "5694 Training Loss: tensor(0.3521)\n",
      "5695 Training Loss: tensor(0.3484)\n",
      "5696 Training Loss: tensor(0.3493)\n",
      "5697 Training Loss: tensor(0.3501)\n",
      "5698 Training Loss: tensor(0.3491)\n",
      "5699 Training Loss: tensor(0.3471)\n",
      "5700 Training Loss: tensor(0.3474)\n",
      "5701 Training Loss: tensor(0.3472)\n",
      "5702 Training Loss: tensor(0.3470)\n",
      "5703 Training Loss: tensor(0.3510)\n",
      "5704 Training Loss: tensor(0.3515)\n",
      "5705 Training Loss: tensor(0.3487)\n",
      "5706 Training Loss: tensor(0.3497)\n",
      "5707 Training Loss: tensor(0.3511)\n",
      "5708 Training Loss: tensor(0.3523)\n",
      "5709 Training Loss: tensor(0.3481)\n",
      "5710 Training Loss: tensor(0.3482)\n",
      "5711 Training Loss: tensor(0.3514)\n",
      "5712 Training Loss: tensor(0.3498)\n",
      "5713 Training Loss: tensor(0.3473)\n",
      "5714 Training Loss: tensor(0.3509)\n",
      "5715 Training Loss: tensor(0.3481)\n",
      "5716 Training Loss: tensor(0.3473)\n",
      "5717 Training Loss: tensor(0.3474)\n",
      "5718 Training Loss: tensor(0.3493)\n",
      "5719 Training Loss: tensor(0.3562)\n",
      "5720 Training Loss: tensor(0.3483)\n",
      "5721 Training Loss: tensor(0.3496)\n",
      "5722 Training Loss: tensor(0.3494)\n",
      "5723 Training Loss: tensor(0.3485)\n",
      "5724 Training Loss: tensor(0.3476)\n",
      "5725 Training Loss: tensor(0.3468)\n",
      "5726 Training Loss: tensor(0.3539)\n",
      "5727 Training Loss: tensor(0.3484)\n",
      "5728 Training Loss: tensor(0.3530)\n",
      "5729 Training Loss: tensor(0.3522)\n",
      "5730 Training Loss: tensor(0.3476)\n",
      "5731 Training Loss: tensor(0.3480)\n",
      "5732 Training Loss: tensor(0.3497)\n",
      "5733 Training Loss: tensor(0.3481)\n",
      "5734 Training Loss: tensor(0.3523)\n",
      "5735 Training Loss: tensor(0.3522)\n",
      "5736 Training Loss: tensor(0.3481)\n",
      "5737 Training Loss: tensor(0.3483)\n",
      "5738 Training Loss: tensor(0.3485)\n",
      "5739 Training Loss: tensor(0.3531)\n",
      "5740 Training Loss: tensor(0.3474)\n",
      "5741 Training Loss: tensor(0.3479)\n",
      "5742 Training Loss: tensor(0.3490)\n",
      "5743 Training Loss: tensor(0.3510)\n",
      "5744 Training Loss: tensor(0.3479)\n",
      "5745 Training Loss: tensor(0.3491)\n",
      "5746 Training Loss: tensor(0.3507)\n",
      "5747 Training Loss: tensor(0.3484)\n",
      "5748 Training Loss: tensor(0.3481)\n",
      "5749 Training Loss: tensor(0.3488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5750 Training Loss: tensor(0.3547)\n",
      "5751 Training Loss: tensor(0.3473)\n",
      "5752 Training Loss: tensor(0.3492)\n",
      "5753 Training Loss: tensor(0.3475)\n",
      "5754 Training Loss: tensor(0.3475)\n",
      "5755 Training Loss: tensor(0.3495)\n",
      "5756 Training Loss: tensor(0.3477)\n",
      "5757 Training Loss: tensor(0.3469)\n",
      "5758 Training Loss: tensor(0.3472)\n",
      "5759 Training Loss: tensor(0.3476)\n",
      "5760 Training Loss: tensor(0.3471)\n",
      "5761 Training Loss: tensor(0.3472)\n",
      "5762 Training Loss: tensor(0.3521)\n",
      "5763 Training Loss: tensor(0.3497)\n",
      "5764 Training Loss: tensor(0.3469)\n",
      "5765 Training Loss: tensor(0.3557)\n",
      "5766 Training Loss: tensor(0.3550)\n",
      "5767 Training Loss: tensor(0.3564)\n",
      "5768 Training Loss: tensor(0.3522)\n",
      "5769 Training Loss: tensor(0.3500)\n",
      "5770 Training Loss: tensor(0.3518)\n",
      "5771 Training Loss: tensor(0.3479)\n",
      "5772 Training Loss: tensor(0.3478)\n",
      "5773 Training Loss: tensor(0.3513)\n",
      "5774 Training Loss: tensor(0.3480)\n",
      "5775 Training Loss: tensor(0.3492)\n",
      "5776 Training Loss: tensor(0.3506)\n",
      "5777 Training Loss: tensor(0.3493)\n",
      "5778 Training Loss: tensor(0.3480)\n",
      "5779 Training Loss: tensor(0.3481)\n",
      "5780 Training Loss: tensor(0.3495)\n",
      "5781 Training Loss: tensor(0.3508)\n",
      "5782 Training Loss: tensor(0.3488)\n",
      "5783 Training Loss: tensor(0.3503)\n",
      "5784 Training Loss: tensor(0.3494)\n",
      "5785 Training Loss: tensor(0.3485)\n",
      "5786 Training Loss: tensor(0.3477)\n",
      "5787 Training Loss: tensor(0.3514)\n",
      "5788 Training Loss: tensor(0.3499)\n",
      "5789 Training Loss: tensor(0.3490)\n",
      "5790 Training Loss: tensor(0.3525)\n",
      "5791 Training Loss: tensor(0.3482)\n",
      "5792 Training Loss: tensor(0.3557)\n",
      "5793 Training Loss: tensor(0.3489)\n",
      "5794 Training Loss: tensor(0.3491)\n",
      "5795 Training Loss: tensor(0.3484)\n",
      "5796 Training Loss: tensor(0.3497)\n",
      "5797 Training Loss: tensor(0.3489)\n",
      "5798 Training Loss: tensor(0.3476)\n",
      "5799 Training Loss: tensor(0.3498)\n",
      "5800 Training Loss: tensor(0.3463)\n",
      "5801 Training Loss: tensor(0.3516)\n",
      "5802 Training Loss: tensor(0.3478)\n",
      "5803 Training Loss: tensor(0.3480)\n",
      "5804 Training Loss: tensor(0.3592)\n",
      "5805 Training Loss: tensor(0.3496)\n",
      "5806 Training Loss: tensor(0.3557)\n",
      "5807 Training Loss: tensor(0.3492)\n",
      "5808 Training Loss: tensor(0.3487)\n",
      "5809 Training Loss: tensor(0.3475)\n",
      "5810 Training Loss: tensor(0.3478)\n",
      "5811 Training Loss: tensor(0.3493)\n",
      "5812 Training Loss: tensor(0.3493)\n",
      "5813 Training Loss: tensor(0.3474)\n",
      "5814 Training Loss: tensor(0.3495)\n",
      "5815 Training Loss: tensor(0.3506)\n",
      "5816 Training Loss: tensor(0.3483)\n",
      "5817 Training Loss: tensor(0.3574)\n",
      "5818 Training Loss: tensor(0.3515)\n",
      "5819 Training Loss: tensor(0.3488)\n",
      "5820 Training Loss: tensor(0.3496)\n",
      "5821 Training Loss: tensor(0.3506)\n",
      "5822 Training Loss: tensor(0.3471)\n",
      "5823 Training Loss: tensor(0.3498)\n",
      "5824 Training Loss: tensor(0.3476)\n",
      "5825 Training Loss: tensor(0.3588)\n",
      "5826 Training Loss: tensor(0.3479)\n",
      "5827 Training Loss: tensor(0.3484)\n",
      "5828 Training Loss: tensor(0.3479)\n",
      "5829 Training Loss: tensor(0.3481)\n",
      "5830 Training Loss: tensor(0.3474)\n",
      "5831 Training Loss: tensor(0.3493)\n",
      "5832 Training Loss: tensor(0.3471)\n",
      "5833 Training Loss: tensor(0.3499)\n",
      "5834 Training Loss: tensor(0.3469)\n",
      "5835 Training Loss: tensor(0.3473)\n",
      "5836 Training Loss: tensor(0.3489)\n",
      "5837 Training Loss: tensor(0.3489)\n",
      "5838 Training Loss: tensor(0.3503)\n",
      "5839 Training Loss: tensor(0.3499)\n",
      "5840 Training Loss: tensor(0.3507)\n",
      "5841 Training Loss: tensor(0.3475)\n",
      "5842 Training Loss: tensor(0.3473)\n",
      "5843 Training Loss: tensor(0.3472)\n",
      "5844 Training Loss: tensor(0.3506)\n",
      "5845 Training Loss: tensor(0.3469)\n",
      "5846 Training Loss: tensor(0.3488)\n",
      "5847 Training Loss: tensor(0.3496)\n",
      "5848 Training Loss: tensor(0.3536)\n",
      "5849 Training Loss: tensor(0.3466)\n",
      "5850 Training Loss: tensor(0.3506)\n",
      "5851 Training Loss: tensor(0.3518)\n",
      "5852 Training Loss: tensor(0.3476)\n",
      "5853 Training Loss: tensor(0.3493)\n",
      "5854 Training Loss: tensor(0.3567)\n",
      "5855 Training Loss: tensor(0.3554)\n",
      "5856 Training Loss: tensor(0.3491)\n",
      "5857 Training Loss: tensor(0.3505)\n",
      "5858 Training Loss: tensor(0.3485)\n",
      "5859 Training Loss: tensor(0.3501)\n",
      "5860 Training Loss: tensor(0.3485)\n",
      "5861 Training Loss: tensor(0.3489)\n",
      "5862 Training Loss: tensor(0.3517)\n",
      "5863 Training Loss: tensor(0.3534)\n",
      "5864 Training Loss: tensor(0.3512)\n",
      "5865 Training Loss: tensor(0.3480)\n",
      "5866 Training Loss: tensor(0.3495)\n",
      "5867 Training Loss: tensor(0.3485)\n",
      "5868 Training Loss: tensor(0.3504)\n",
      "5869 Training Loss: tensor(0.3489)\n",
      "5870 Training Loss: tensor(0.3486)\n",
      "5871 Training Loss: tensor(0.3480)\n",
      "5872 Training Loss: tensor(0.3500)\n",
      "5873 Training Loss: tensor(0.3488)\n",
      "5874 Training Loss: tensor(0.3491)\n",
      "5875 Training Loss: tensor(0.3479)\n",
      "5876 Training Loss: tensor(0.3474)\n",
      "5877 Training Loss: tensor(0.3467)\n",
      "5878 Training Loss: tensor(0.3501)\n",
      "5879 Training Loss: tensor(0.3470)\n",
      "5880 Training Loss: tensor(0.3499)\n",
      "5881 Training Loss: tensor(0.3494)\n",
      "5882 Training Loss: tensor(0.3468)\n",
      "5883 Training Loss: tensor(0.3469)\n",
      "5884 Training Loss: tensor(0.3485)\n",
      "5885 Training Loss: tensor(0.3469)\n",
      "5886 Training Loss: tensor(0.3473)\n",
      "5887 Training Loss: tensor(0.3485)\n",
      "5888 Training Loss: tensor(0.3476)\n",
      "5889 Training Loss: tensor(0.3461)\n",
      "5890 Training Loss: tensor(0.3566)\n",
      "5891 Training Loss: tensor(0.3549)\n",
      "5892 Training Loss: tensor(0.3476)\n",
      "5893 Training Loss: tensor(0.3566)\n",
      "5894 Training Loss: tensor(0.3541)\n",
      "5895 Training Loss: tensor(0.3466)\n",
      "5896 Training Loss: tensor(0.3474)\n",
      "5897 Training Loss: tensor(0.3479)\n",
      "5898 Training Loss: tensor(0.3475)\n",
      "5899 Training Loss: tensor(0.3499)\n",
      "5900 Training Loss: tensor(0.3466)\n",
      "5901 Training Loss: tensor(0.3584)\n",
      "5902 Training Loss: tensor(0.3505)\n",
      "5903 Training Loss: tensor(0.3480)\n",
      "5904 Training Loss: tensor(0.3548)\n",
      "5905 Training Loss: tensor(0.3480)\n",
      "5906 Training Loss: tensor(0.3490)\n",
      "5907 Training Loss: tensor(0.3500)\n",
      "5908 Training Loss: tensor(0.3504)\n",
      "5909 Training Loss: tensor(0.3488)\n",
      "5910 Training Loss: tensor(0.3484)\n",
      "5911 Training Loss: tensor(0.3501)\n",
      "5912 Training Loss: tensor(0.3525)\n",
      "5913 Training Loss: tensor(0.3526)\n",
      "5914 Training Loss: tensor(0.3484)\n",
      "5915 Training Loss: tensor(0.3505)\n",
      "5916 Training Loss: tensor(0.3489)\n",
      "5917 Training Loss: tensor(0.3472)\n",
      "5918 Training Loss: tensor(0.3496)\n",
      "5919 Training Loss: tensor(0.3474)\n",
      "5920 Training Loss: tensor(0.3482)\n",
      "5921 Training Loss: tensor(0.3471)\n",
      "5922 Training Loss: tensor(0.3490)\n",
      "5923 Training Loss: tensor(0.3504)\n",
      "5924 Training Loss: tensor(0.3470)\n",
      "5925 Training Loss: tensor(0.3497)\n",
      "5926 Training Loss: tensor(0.3462)\n",
      "5927 Training Loss: tensor(0.3467)\n",
      "5928 Training Loss: tensor(0.3508)\n",
      "5929 Training Loss: tensor(0.3519)\n",
      "5930 Training Loss: tensor(0.3488)\n",
      "5931 Training Loss: tensor(0.3505)\n",
      "5932 Training Loss: tensor(0.3466)\n",
      "5933 Training Loss: tensor(0.3501)\n",
      "5934 Training Loss: tensor(0.3482)\n",
      "5935 Training Loss: tensor(0.3526)\n",
      "5936 Training Loss: tensor(0.3512)\n",
      "5937 Training Loss: tensor(0.3497)\n",
      "5938 Training Loss: tensor(0.3465)\n",
      "5939 Training Loss: tensor(0.3473)\n",
      "5940 Training Loss: tensor(0.3488)\n",
      "5941 Training Loss: tensor(0.3507)\n",
      "5942 Training Loss: tensor(0.3513)\n",
      "5943 Training Loss: tensor(0.3474)\n",
      "5944 Training Loss: tensor(0.3478)\n",
      "5945 Training Loss: tensor(0.3476)\n",
      "5946 Training Loss: tensor(0.3469)\n",
      "5947 Training Loss: tensor(0.3478)\n",
      "5948 Training Loss: tensor(0.3566)\n",
      "5949 Training Loss: tensor(0.3497)\n",
      "5950 Training Loss: tensor(0.3480)\n",
      "5951 Training Loss: tensor(0.3472)\n",
      "5952 Training Loss: tensor(0.3485)\n",
      "5953 Training Loss: tensor(0.3476)\n",
      "5954 Training Loss: tensor(0.3504)\n",
      "5955 Training Loss: tensor(0.3475)\n",
      "5956 Training Loss: tensor(0.3534)\n",
      "5957 Training Loss: tensor(0.3471)\n",
      "5958 Training Loss: tensor(0.3515)\n",
      "5959 Training Loss: tensor(0.3468)\n",
      "5960 Training Loss: tensor(0.3464)\n",
      "5961 Training Loss: tensor(0.3465)\n",
      "5962 Training Loss: tensor(0.3633)\n",
      "5963 Training Loss: tensor(0.3486)\n",
      "5964 Training Loss: tensor(0.3529)\n",
      "5965 Training Loss: tensor(0.3462)\n",
      "5966 Training Loss: tensor(0.3486)\n",
      "5967 Training Loss: tensor(0.3475)\n",
      "5968 Training Loss: tensor(0.3478)\n",
      "5969 Training Loss: tensor(0.3478)\n",
      "5970 Training Loss: tensor(0.3484)\n",
      "5971 Training Loss: tensor(0.3480)\n",
      "5972 Training Loss: tensor(0.3475)\n",
      "5973 Training Loss: tensor(0.3464)\n",
      "5974 Training Loss: tensor(0.3594)\n",
      "5975 Training Loss: tensor(0.3469)\n",
      "5976 Training Loss: tensor(0.3500)\n",
      "5977 Training Loss: tensor(0.3466)\n",
      "5978 Training Loss: tensor(0.3482)\n",
      "5979 Training Loss: tensor(0.3545)\n",
      "5980 Training Loss: tensor(0.3484)\n",
      "5981 Training Loss: tensor(0.3513)\n",
      "5982 Training Loss: tensor(0.3474)\n",
      "5983 Training Loss: tensor(0.3479)\n",
      "5984 Training Loss: tensor(0.3470)\n",
      "5985 Training Loss: tensor(0.3510)\n",
      "5986 Training Loss: tensor(0.3485)\n",
      "5987 Training Loss: tensor(0.3563)\n",
      "5988 Training Loss: tensor(0.3470)\n",
      "5989 Training Loss: tensor(0.3516)\n",
      "5990 Training Loss: tensor(0.3483)\n",
      "5991 Training Loss: tensor(0.3485)\n",
      "5992 Training Loss: tensor(0.3514)\n",
      "5993 Training Loss: tensor(0.3475)\n",
      "5994 Training Loss: tensor(0.3521)\n",
      "5995 Training Loss: tensor(0.3496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996 Training Loss: tensor(0.3480)\n",
      "5997 Training Loss: tensor(0.3472)\n",
      "5998 Training Loss: tensor(0.3472)\n",
      "5999 Training Loss: tensor(0.3499)\n",
      "6000 Training Loss: tensor(0.3469)\n",
      "6001 Training Loss: tensor(0.3475)\n",
      "6002 Training Loss: tensor(0.3468)\n",
      "6003 Training Loss: tensor(0.3473)\n",
      "6004 Training Loss: tensor(0.3473)\n",
      "6005 Training Loss: tensor(0.3515)\n",
      "6006 Training Loss: tensor(0.3464)\n",
      "6007 Training Loss: tensor(0.3490)\n",
      "6008 Training Loss: tensor(0.3502)\n",
      "6009 Training Loss: tensor(0.3554)\n",
      "6010 Training Loss: tensor(0.3512)\n",
      "6011 Training Loss: tensor(0.3457)\n",
      "6012 Training Loss: tensor(0.3499)\n",
      "6013 Training Loss: tensor(0.3485)\n",
      "6014 Training Loss: tensor(0.3476)\n",
      "6015 Training Loss: tensor(0.3498)\n",
      "6016 Training Loss: tensor(0.3490)\n",
      "6017 Training Loss: tensor(0.3473)\n",
      "6018 Training Loss: tensor(0.3493)\n",
      "6019 Training Loss: tensor(0.3468)\n",
      "6020 Training Loss: tensor(0.3488)\n",
      "6021 Training Loss: tensor(0.3482)\n",
      "6022 Training Loss: tensor(0.3464)\n",
      "6023 Training Loss: tensor(0.3468)\n",
      "6024 Training Loss: tensor(0.3481)\n",
      "6025 Training Loss: tensor(0.3496)\n",
      "6026 Training Loss: tensor(0.3522)\n",
      "6027 Training Loss: tensor(0.3510)\n",
      "6028 Training Loss: tensor(0.3470)\n",
      "6029 Training Loss: tensor(0.3480)\n",
      "6030 Training Loss: tensor(0.3487)\n",
      "6031 Training Loss: tensor(0.3470)\n",
      "6032 Training Loss: tensor(0.3550)\n",
      "6033 Training Loss: tensor(0.3495)\n",
      "6034 Training Loss: tensor(0.3545)\n",
      "6035 Training Loss: tensor(0.3487)\n",
      "6036 Training Loss: tensor(0.3541)\n",
      "6037 Training Loss: tensor(0.3474)\n",
      "6038 Training Loss: tensor(0.3491)\n",
      "6039 Training Loss: tensor(0.3491)\n",
      "6040 Training Loss: tensor(0.3496)\n",
      "6041 Training Loss: tensor(0.3475)\n",
      "6042 Training Loss: tensor(0.3511)\n",
      "6043 Training Loss: tensor(0.3516)\n",
      "6044 Training Loss: tensor(0.3490)\n",
      "6045 Training Loss: tensor(0.3496)\n",
      "6046 Training Loss: tensor(0.3500)\n",
      "6047 Training Loss: tensor(0.3477)\n",
      "6048 Training Loss: tensor(0.3506)\n",
      "6049 Training Loss: tensor(0.3478)\n",
      "6050 Training Loss: tensor(0.3478)\n",
      "6051 Training Loss: tensor(0.3473)\n",
      "6052 Training Loss: tensor(0.3496)\n",
      "6053 Training Loss: tensor(0.3477)\n",
      "6054 Training Loss: tensor(0.3504)\n",
      "6055 Training Loss: tensor(0.3482)\n",
      "6056 Training Loss: tensor(0.3468)\n",
      "6057 Training Loss: tensor(0.3513)\n",
      "6058 Training Loss: tensor(0.3540)\n",
      "6059 Training Loss: tensor(0.3591)\n",
      "6060 Training Loss: tensor(0.3485)\n",
      "6061 Training Loss: tensor(0.3501)\n",
      "6062 Training Loss: tensor(0.3531)\n",
      "6063 Training Loss: tensor(0.3486)\n",
      "6064 Training Loss: tensor(0.3488)\n",
      "6065 Training Loss: tensor(0.3474)\n",
      "6066 Training Loss: tensor(0.3472)\n",
      "6067 Training Loss: tensor(0.3487)\n",
      "6068 Training Loss: tensor(0.3487)\n",
      "6069 Training Loss: tensor(0.3483)\n",
      "6070 Training Loss: tensor(0.3494)\n",
      "6071 Training Loss: tensor(0.3481)\n",
      "6072 Training Loss: tensor(0.3478)\n",
      "6073 Training Loss: tensor(0.3500)\n",
      "6074 Training Loss: tensor(0.3479)\n",
      "6075 Training Loss: tensor(0.3474)\n",
      "6076 Training Loss: tensor(0.3499)\n",
      "6077 Training Loss: tensor(0.3485)\n",
      "6078 Training Loss: tensor(0.3498)\n",
      "6079 Training Loss: tensor(0.3505)\n",
      "6080 Training Loss: tensor(0.3514)\n",
      "6081 Training Loss: tensor(0.3492)\n",
      "6082 Training Loss: tensor(0.3467)\n",
      "6083 Training Loss: tensor(0.3515)\n",
      "6084 Training Loss: tensor(0.3467)\n",
      "6085 Training Loss: tensor(0.3525)\n",
      "6086 Training Loss: tensor(0.3493)\n",
      "6087 Training Loss: tensor(0.3574)\n",
      "6088 Training Loss: tensor(0.3478)\n",
      "6089 Training Loss: tensor(0.3469)\n",
      "6090 Training Loss: tensor(0.3489)\n",
      "6091 Training Loss: tensor(0.3503)\n",
      "6092 Training Loss: tensor(0.3489)\n",
      "6093 Training Loss: tensor(0.3488)\n",
      "6094 Training Loss: tensor(0.3485)\n",
      "6095 Training Loss: tensor(0.3507)\n",
      "6096 Training Loss: tensor(0.3477)\n",
      "6097 Training Loss: tensor(0.3561)\n",
      "6098 Training Loss: tensor(0.3478)\n",
      "6099 Training Loss: tensor(0.3539)\n",
      "6100 Training Loss: tensor(0.3497)\n",
      "6101 Training Loss: tensor(0.3509)\n",
      "6102 Training Loss: tensor(0.3501)\n",
      "6103 Training Loss: tensor(0.3483)\n",
      "6104 Training Loss: tensor(0.3496)\n",
      "6105 Training Loss: tensor(0.3498)\n",
      "6106 Training Loss: tensor(0.3485)\n",
      "6107 Training Loss: tensor(0.3494)\n",
      "6108 Training Loss: tensor(0.3501)\n",
      "6109 Training Loss: tensor(0.3507)\n",
      "6110 Training Loss: tensor(0.3510)\n",
      "6111 Training Loss: tensor(0.3481)\n",
      "6112 Training Loss: tensor(0.3477)\n",
      "6113 Training Loss: tensor(0.3490)\n",
      "6114 Training Loss: tensor(0.3484)\n",
      "6115 Training Loss: tensor(0.3480)\n",
      "6116 Training Loss: tensor(0.3495)\n",
      "6117 Training Loss: tensor(0.3481)\n",
      "6118 Training Loss: tensor(0.3503)\n",
      "6119 Training Loss: tensor(0.3593)\n",
      "6120 Training Loss: tensor(0.3501)\n",
      "6121 Training Loss: tensor(0.3473)\n",
      "6122 Training Loss: tensor(0.3468)\n",
      "6123 Training Loss: tensor(0.3473)\n",
      "6124 Training Loss: tensor(0.3476)\n",
      "6125 Training Loss: tensor(0.3540)\n",
      "6126 Training Loss: tensor(0.3467)\n",
      "6127 Training Loss: tensor(0.3489)\n",
      "6128 Training Loss: tensor(0.3482)\n",
      "6129 Training Loss: tensor(0.3483)\n",
      "6130 Training Loss: tensor(0.3525)\n",
      "6131 Training Loss: tensor(0.3481)\n",
      "6132 Training Loss: tensor(0.3475)\n",
      "6133 Training Loss: tensor(0.3486)\n",
      "6134 Training Loss: tensor(0.3494)\n",
      "6135 Training Loss: tensor(0.3504)\n",
      "6136 Training Loss: tensor(0.3477)\n",
      "6137 Training Loss: tensor(0.3494)\n",
      "6138 Training Loss: tensor(0.3514)\n",
      "6139 Training Loss: tensor(0.3476)\n",
      "6140 Training Loss: tensor(0.3505)\n",
      "6141 Training Loss: tensor(0.3502)\n",
      "6142 Training Loss: tensor(0.3529)\n",
      "6143 Training Loss: tensor(0.3532)\n",
      "6144 Training Loss: tensor(0.3492)\n",
      "6145 Training Loss: tensor(0.3504)\n",
      "6146 Training Loss: tensor(0.3487)\n",
      "6147 Training Loss: tensor(0.3504)\n",
      "6148 Training Loss: tensor(0.3546)\n",
      "6149 Training Loss: tensor(0.3482)\n",
      "6150 Training Loss: tensor(0.3519)\n",
      "6151 Training Loss: tensor(0.3483)\n",
      "6152 Training Loss: tensor(0.3550)\n",
      "6153 Training Loss: tensor(0.3494)\n",
      "6154 Training Loss: tensor(0.3482)\n",
      "6155 Training Loss: tensor(0.3513)\n",
      "6156 Training Loss: tensor(0.3481)\n",
      "6157 Training Loss: tensor(0.3499)\n",
      "6158 Training Loss: tensor(0.3484)\n",
      "6159 Training Loss: tensor(0.3500)\n",
      "6160 Training Loss: tensor(0.3470)\n",
      "6161 Training Loss: tensor(0.3569)\n",
      "6162 Training Loss: tensor(0.3485)\n",
      "6163 Training Loss: tensor(0.3465)\n",
      "6164 Training Loss: tensor(0.3474)\n",
      "6165 Training Loss: tensor(0.3476)\n",
      "6166 Training Loss: tensor(0.3489)\n",
      "6167 Training Loss: tensor(0.3560)\n",
      "6168 Training Loss: tensor(0.3471)\n",
      "6169 Training Loss: tensor(0.3473)\n",
      "6170 Training Loss: tensor(0.3486)\n",
      "6171 Training Loss: tensor(0.3470)\n",
      "6172 Training Loss: tensor(0.3533)\n",
      "6173 Training Loss: tensor(0.3473)\n",
      "6174 Training Loss: tensor(0.3465)\n",
      "6175 Training Loss: tensor(0.3469)\n",
      "6176 Training Loss: tensor(0.3513)\n",
      "6177 Training Loss: tensor(0.3507)\n",
      "6178 Training Loss: tensor(0.3525)\n",
      "6179 Training Loss: tensor(0.3553)\n",
      "6180 Training Loss: tensor(0.3527)\n",
      "6181 Training Loss: tensor(0.3605)\n",
      "6182 Training Loss: tensor(0.3506)\n",
      "6183 Training Loss: tensor(0.3487)\n",
      "6184 Training Loss: tensor(0.3498)\n",
      "6185 Training Loss: tensor(0.3495)\n",
      "6186 Training Loss: tensor(0.3488)\n",
      "6187 Training Loss: tensor(0.3549)\n",
      "6188 Training Loss: tensor(0.3506)\n",
      "6189 Training Loss: tensor(0.3498)\n",
      "6190 Training Loss: tensor(0.3522)\n",
      "6191 Training Loss: tensor(0.3490)\n",
      "6192 Training Loss: tensor(0.3507)\n",
      "6193 Training Loss: tensor(0.3508)\n",
      "6194 Training Loss: tensor(0.3510)\n",
      "6195 Training Loss: tensor(0.3496)\n",
      "6196 Training Loss: tensor(0.3495)\n",
      "6197 Training Loss: tensor(0.3494)\n",
      "6198 Training Loss: tensor(0.3505)\n",
      "6199 Training Loss: tensor(0.3484)\n",
      "6200 Training Loss: tensor(0.3485)\n",
      "6201 Training Loss: tensor(0.3476)\n",
      "6202 Training Loss: tensor(0.3485)\n",
      "6203 Training Loss: tensor(0.3503)\n",
      "6204 Training Loss: tensor(0.3501)\n",
      "6205 Training Loss: tensor(0.3470)\n",
      "6206 Training Loss: tensor(0.3467)\n",
      "6207 Training Loss: tensor(0.3537)\n",
      "6208 Training Loss: tensor(0.3466)\n",
      "6209 Training Loss: tensor(0.3507)\n",
      "6210 Training Loss: tensor(0.3493)\n",
      "6211 Training Loss: tensor(0.3492)\n",
      "6212 Training Loss: tensor(0.3465)\n",
      "6213 Training Loss: tensor(0.3485)\n",
      "6214 Training Loss: tensor(0.3478)\n",
      "6215 Training Loss: tensor(0.3476)\n",
      "6216 Training Loss: tensor(0.3495)\n",
      "6217 Training Loss: tensor(0.3495)\n",
      "6218 Training Loss: tensor(0.3500)\n",
      "6219 Training Loss: tensor(0.3484)\n",
      "6220 Training Loss: tensor(0.3471)\n",
      "6221 Training Loss: tensor(0.3507)\n",
      "6222 Training Loss: tensor(0.3499)\n",
      "6223 Training Loss: tensor(0.3490)\n",
      "6224 Training Loss: tensor(0.3496)\n",
      "6225 Training Loss: tensor(0.3468)\n",
      "6226 Training Loss: tensor(0.3465)\n",
      "6227 Training Loss: tensor(0.3470)\n",
      "6228 Training Loss: tensor(0.3472)\n",
      "6229 Training Loss: tensor(0.3484)\n",
      "6230 Training Loss: tensor(0.3475)\n",
      "6231 Training Loss: tensor(0.3470)\n",
      "6232 Training Loss: tensor(0.3507)\n",
      "6233 Training Loss: tensor(0.3471)\n",
      "6234 Training Loss: tensor(0.3535)\n",
      "6235 Training Loss: tensor(0.3494)\n",
      "6236 Training Loss: tensor(0.3473)\n",
      "6237 Training Loss: tensor(0.3478)\n",
      "6238 Training Loss: tensor(0.3482)\n",
      "6239 Training Loss: tensor(0.3530)\n",
      "6240 Training Loss: tensor(0.3461)\n",
      "6241 Training Loss: tensor(0.3477)\n",
      "6242 Training Loss: tensor(0.3513)\n",
      "6243 Training Loss: tensor(0.3464)\n",
      "6244 Training Loss: tensor(0.3481)\n",
      "6245 Training Loss: tensor(0.3503)\n",
      "6246 Training Loss: tensor(0.3477)\n",
      "6247 Training Loss: tensor(0.3615)\n",
      "6248 Training Loss: tensor(0.3486)\n",
      "6249 Training Loss: tensor(0.3490)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250 Training Loss: tensor(0.3497)\n",
      "6251 Training Loss: tensor(0.3474)\n",
      "6252 Training Loss: tensor(0.3525)\n",
      "6253 Training Loss: tensor(0.3481)\n",
      "6254 Training Loss: tensor(0.3481)\n",
      "6255 Training Loss: tensor(0.3488)\n",
      "6256 Training Loss: tensor(0.3511)\n",
      "6257 Training Loss: tensor(0.3468)\n",
      "6258 Training Loss: tensor(0.3476)\n",
      "6259 Training Loss: tensor(0.3474)\n",
      "6260 Training Loss: tensor(0.3491)\n",
      "6261 Training Loss: tensor(0.3473)\n",
      "6262 Training Loss: tensor(0.3472)\n",
      "6263 Training Loss: tensor(0.3468)\n",
      "6264 Training Loss: tensor(0.3500)\n",
      "6265 Training Loss: tensor(0.3551)\n",
      "6266 Training Loss: tensor(0.3464)\n",
      "6267 Training Loss: tensor(0.3471)\n",
      "6268 Training Loss: tensor(0.3496)\n",
      "6269 Training Loss: tensor(0.3528)\n",
      "6270 Training Loss: tensor(0.3504)\n",
      "6271 Training Loss: tensor(0.3492)\n",
      "6272 Training Loss: tensor(0.3523)\n",
      "6273 Training Loss: tensor(0.3488)\n",
      "6274 Training Loss: tensor(0.3487)\n",
      "6275 Training Loss: tensor(0.3523)\n",
      "6276 Training Loss: tensor(0.3487)\n",
      "6277 Training Loss: tensor(0.3522)\n",
      "6278 Training Loss: tensor(0.3490)\n",
      "6279 Training Loss: tensor(0.3500)\n",
      "6280 Training Loss: tensor(0.3499)\n",
      "6281 Training Loss: tensor(0.3489)\n",
      "6282 Training Loss: tensor(0.3464)\n",
      "6283 Training Loss: tensor(0.3496)\n",
      "6284 Training Loss: tensor(0.3504)\n",
      "6285 Training Loss: tensor(0.3498)\n",
      "6286 Training Loss: tensor(0.3500)\n",
      "6287 Training Loss: tensor(0.3512)\n",
      "6288 Training Loss: tensor(0.3642)\n",
      "6289 Training Loss: tensor(0.3550)\n",
      "6290 Training Loss: tensor(0.3500)\n",
      "6291 Training Loss: tensor(0.3500)\n",
      "6292 Training Loss: tensor(0.3527)\n",
      "6293 Training Loss: tensor(0.3482)\n",
      "6294 Training Loss: tensor(0.3513)\n",
      "6295 Training Loss: tensor(0.3502)\n",
      "6296 Training Loss: tensor(0.3517)\n",
      "6297 Training Loss: tensor(0.3523)\n",
      "6298 Training Loss: tensor(0.3523)\n",
      "6299 Training Loss: tensor(0.3518)\n",
      "6300 Training Loss: tensor(0.3497)\n",
      "6301 Training Loss: tensor(0.3481)\n",
      "6302 Training Loss: tensor(0.3487)\n",
      "6303 Training Loss: tensor(0.3480)\n",
      "6304 Training Loss: tensor(0.3496)\n",
      "6305 Training Loss: tensor(0.3508)\n",
      "6306 Training Loss: tensor(0.3499)\n",
      "6307 Training Loss: tensor(0.3501)\n",
      "6308 Training Loss: tensor(0.3486)\n",
      "6309 Training Loss: tensor(0.3583)\n",
      "6310 Training Loss: tensor(0.3525)\n",
      "6311 Training Loss: tensor(0.3477)\n",
      "6312 Training Loss: tensor(0.3476)\n",
      "6313 Training Loss: tensor(0.3492)\n",
      "6314 Training Loss: tensor(0.3479)\n",
      "6315 Training Loss: tensor(0.3503)\n",
      "6316 Training Loss: tensor(0.3507)\n",
      "6317 Training Loss: tensor(0.3486)\n",
      "6318 Training Loss: tensor(0.3484)\n",
      "6319 Training Loss: tensor(0.3480)\n",
      "6320 Training Loss: tensor(0.3487)\n",
      "6321 Training Loss: tensor(0.3490)\n",
      "6322 Training Loss: tensor(0.3483)\n",
      "6323 Training Loss: tensor(0.3505)\n",
      "6324 Training Loss: tensor(0.3475)\n",
      "6325 Training Loss: tensor(0.3505)\n",
      "6326 Training Loss: tensor(0.3489)\n",
      "6327 Training Loss: tensor(0.3473)\n",
      "6328 Training Loss: tensor(0.3472)\n",
      "6329 Training Loss: tensor(0.3475)\n",
      "6330 Training Loss: tensor(0.3500)\n",
      "6331 Training Loss: tensor(0.3480)\n",
      "6332 Training Loss: tensor(0.3521)\n",
      "6333 Training Loss: tensor(0.3485)\n",
      "6334 Training Loss: tensor(0.3481)\n",
      "6335 Training Loss: tensor(0.3507)\n",
      "6336 Training Loss: tensor(0.3486)\n",
      "6337 Training Loss: tensor(0.3467)\n",
      "6338 Training Loss: tensor(0.3465)\n",
      "6339 Training Loss: tensor(0.3510)\n",
      "6340 Training Loss: tensor(0.3487)\n",
      "6341 Training Loss: tensor(0.3466)\n",
      "6342 Training Loss: tensor(0.3469)\n",
      "6343 Training Loss: tensor(0.3475)\n",
      "6344 Training Loss: tensor(0.3514)\n",
      "6345 Training Loss: tensor(0.3482)\n",
      "6346 Training Loss: tensor(0.3504)\n",
      "6347 Training Loss: tensor(0.3468)\n",
      "6348 Training Loss: tensor(0.3503)\n",
      "6349 Training Loss: tensor(0.3472)\n",
      "6350 Training Loss: tensor(0.3466)\n",
      "6351 Training Loss: tensor(0.3466)\n",
      "6352 Training Loss: tensor(0.3515)\n",
      "6353 Training Loss: tensor(0.3468)\n",
      "6354 Training Loss: tensor(0.3475)\n",
      "6355 Training Loss: tensor(0.3471)\n",
      "6356 Training Loss: tensor(0.3517)\n",
      "6357 Training Loss: tensor(0.3496)\n",
      "6358 Training Loss: tensor(0.3545)\n",
      "6359 Training Loss: tensor(0.3473)\n",
      "6360 Training Loss: tensor(0.3469)\n",
      "6361 Training Loss: tensor(0.3482)\n",
      "6362 Training Loss: tensor(0.3477)\n",
      "6363 Training Loss: tensor(0.3470)\n",
      "6364 Training Loss: tensor(0.3500)\n",
      "6365 Training Loss: tensor(0.3489)\n",
      "6366 Training Loss: tensor(0.3562)\n",
      "6367 Training Loss: tensor(0.3480)\n",
      "6368 Training Loss: tensor(0.3522)\n",
      "6369 Training Loss: tensor(0.3475)\n",
      "6370 Training Loss: tensor(0.3529)\n",
      "6371 Training Loss: tensor(0.3499)\n",
      "6372 Training Loss: tensor(0.3478)\n",
      "6373 Training Loss: tensor(0.3478)\n",
      "6374 Training Loss: tensor(0.3510)\n",
      "6375 Training Loss: tensor(0.3532)\n",
      "6376 Training Loss: tensor(0.3474)\n",
      "6377 Training Loss: tensor(0.3518)\n",
      "6378 Training Loss: tensor(0.3498)\n",
      "6379 Training Loss: tensor(0.3478)\n",
      "6380 Training Loss: tensor(0.3488)\n",
      "6381 Training Loss: tensor(0.3489)\n",
      "6382 Training Loss: tensor(0.3473)\n",
      "6383 Training Loss: tensor(0.3484)\n",
      "6384 Training Loss: tensor(0.3530)\n",
      "6385 Training Loss: tensor(0.3481)\n",
      "6386 Training Loss: tensor(0.3476)\n",
      "6387 Training Loss: tensor(0.3474)\n",
      "6388 Training Loss: tensor(0.3497)\n",
      "6389 Training Loss: tensor(0.3476)\n",
      "6390 Training Loss: tensor(0.3543)\n",
      "6391 Training Loss: tensor(0.3487)\n",
      "6392 Training Loss: tensor(0.3480)\n",
      "6393 Training Loss: tensor(0.3550)\n",
      "6394 Training Loss: tensor(0.3481)\n",
      "6395 Training Loss: tensor(0.3468)\n",
      "6396 Training Loss: tensor(0.3512)\n",
      "6397 Training Loss: tensor(0.3469)\n",
      "6398 Training Loss: tensor(0.3499)\n",
      "6399 Training Loss: tensor(0.3478)\n",
      "6400 Training Loss: tensor(0.3482)\n",
      "6401 Training Loss: tensor(0.3482)\n",
      "6402 Training Loss: tensor(0.3488)\n",
      "6403 Training Loss: tensor(0.3473)\n",
      "6404 Training Loss: tensor(0.3479)\n",
      "6405 Training Loss: tensor(0.3472)\n",
      "6406 Training Loss: tensor(0.3508)\n",
      "6407 Training Loss: tensor(0.3508)\n",
      "6408 Training Loss: tensor(0.3486)\n",
      "6409 Training Loss: tensor(0.3474)\n",
      "6410 Training Loss: tensor(0.3479)\n",
      "6411 Training Loss: tensor(0.3484)\n",
      "6412 Training Loss: tensor(0.3469)\n",
      "6413 Training Loss: tensor(0.3474)\n",
      "6414 Training Loss: tensor(0.3474)\n",
      "6415 Training Loss: tensor(0.3506)\n",
      "6416 Training Loss: tensor(0.3472)\n",
      "6417 Training Loss: tensor(0.3479)\n",
      "6418 Training Loss: tensor(0.3536)\n",
      "6419 Training Loss: tensor(0.3462)\n",
      "6420 Training Loss: tensor(0.3471)\n",
      "6421 Training Loss: tensor(0.3579)\n",
      "6422 Training Loss: tensor(0.3470)\n",
      "6423 Training Loss: tensor(0.3485)\n",
      "6424 Training Loss: tensor(0.3466)\n",
      "6425 Training Loss: tensor(0.3529)\n",
      "6426 Training Loss: tensor(0.3536)\n",
      "6427 Training Loss: tensor(0.3467)\n",
      "6428 Training Loss: tensor(0.3487)\n",
      "6429 Training Loss: tensor(0.3469)\n",
      "6430 Training Loss: tensor(0.3484)\n",
      "6431 Training Loss: tensor(0.3480)\n",
      "6432 Training Loss: tensor(0.3509)\n",
      "6433 Training Loss: tensor(0.3477)\n",
      "6434 Training Loss: tensor(0.3469)\n",
      "6435 Training Loss: tensor(0.3463)\n",
      "6436 Training Loss: tensor(0.3461)\n",
      "6437 Training Loss: tensor(0.3489)\n",
      "6438 Training Loss: tensor(0.3475)\n",
      "6439 Training Loss: tensor(0.3487)\n",
      "6440 Training Loss: tensor(0.3477)\n",
      "6441 Training Loss: tensor(0.3499)\n",
      "6442 Training Loss: tensor(0.3537)\n",
      "6443 Training Loss: tensor(0.3515)\n",
      "6444 Training Loss: tensor(0.3468)\n",
      "6445 Training Loss: tensor(0.3507)\n",
      "6446 Training Loss: tensor(0.3475)\n",
      "6447 Training Loss: tensor(0.3473)\n",
      "6448 Training Loss: tensor(0.3492)\n",
      "6449 Training Loss: tensor(0.3508)\n",
      "6450 Training Loss: tensor(0.3527)\n",
      "6451 Training Loss: tensor(0.3472)\n",
      "6452 Training Loss: tensor(0.3499)\n",
      "6453 Training Loss: tensor(0.3513)\n",
      "6454 Training Loss: tensor(0.3470)\n",
      "6455 Training Loss: tensor(0.3469)\n",
      "6456 Training Loss: tensor(0.3472)\n",
      "6457 Training Loss: tensor(0.3483)\n",
      "6458 Training Loss: tensor(0.3525)\n",
      "6459 Training Loss: tensor(0.3495)\n",
      "6460 Training Loss: tensor(0.3474)\n",
      "6461 Training Loss: tensor(0.3491)\n",
      "6462 Training Loss: tensor(0.3482)\n",
      "6463 Training Loss: tensor(0.3480)\n",
      "6464 Training Loss: tensor(0.3554)\n",
      "6465 Training Loss: tensor(0.3527)\n",
      "6466 Training Loss: tensor(0.3516)\n",
      "6467 Training Loss: tensor(0.3468)\n",
      "6468 Training Loss: tensor(0.3473)\n",
      "6469 Training Loss: tensor(0.3477)\n",
      "6470 Training Loss: tensor(0.3512)\n",
      "6471 Training Loss: tensor(0.3489)\n",
      "6472 Training Loss: tensor(0.3481)\n",
      "6473 Training Loss: tensor(0.3488)\n",
      "6474 Training Loss: tensor(0.3509)\n",
      "6475 Training Loss: tensor(0.3477)\n",
      "6476 Training Loss: tensor(0.3491)\n",
      "6477 Training Loss: tensor(0.3468)\n",
      "6478 Training Loss: tensor(0.3488)\n",
      "6479 Training Loss: tensor(0.3514)\n",
      "6480 Training Loss: tensor(0.3475)\n",
      "6481 Training Loss: tensor(0.3482)\n",
      "6482 Training Loss: tensor(0.3477)\n",
      "6483 Training Loss: tensor(0.3483)\n",
      "6484 Training Loss: tensor(0.3475)\n",
      "6485 Training Loss: tensor(0.3479)\n",
      "6486 Training Loss: tensor(0.3518)\n",
      "6487 Training Loss: tensor(0.3491)\n",
      "6488 Training Loss: tensor(0.3474)\n",
      "6489 Training Loss: tensor(0.3468)\n",
      "6490 Training Loss: tensor(0.3465)\n",
      "6491 Training Loss: tensor(0.3549)\n",
      "6492 Training Loss: tensor(0.3509)\n",
      "6493 Training Loss: tensor(0.3479)\n",
      "6494 Training Loss: tensor(0.3589)\n",
      "6495 Training Loss: tensor(0.3509)\n",
      "6496 Training Loss: tensor(0.3475)\n",
      "6497 Training Loss: tensor(0.3464)\n",
      "6498 Training Loss: tensor(0.3466)\n",
      "6499 Training Loss: tensor(0.3479)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500 Training Loss: tensor(0.3470)\n",
      "6501 Training Loss: tensor(0.3482)\n",
      "6502 Training Loss: tensor(0.3487)\n",
      "6503 Training Loss: tensor(0.3481)\n",
      "6504 Training Loss: tensor(0.3471)\n",
      "6505 Training Loss: tensor(0.3592)\n",
      "6506 Training Loss: tensor(0.3483)\n",
      "6507 Training Loss: tensor(0.3479)\n",
      "6508 Training Loss: tensor(0.3503)\n",
      "6509 Training Loss: tensor(0.3529)\n",
      "6510 Training Loss: tensor(0.3509)\n",
      "6511 Training Loss: tensor(0.3477)\n",
      "6512 Training Loss: tensor(0.3507)\n",
      "6513 Training Loss: tensor(0.3487)\n",
      "6514 Training Loss: tensor(0.3495)\n",
      "6515 Training Loss: tensor(0.3476)\n",
      "6516 Training Loss: tensor(0.3506)\n",
      "6517 Training Loss: tensor(0.3472)\n",
      "6518 Training Loss: tensor(0.3474)\n",
      "6519 Training Loss: tensor(0.3496)\n",
      "6520 Training Loss: tensor(0.3553)\n",
      "6521 Training Loss: tensor(0.3480)\n",
      "6522 Training Loss: tensor(0.3474)\n",
      "6523 Training Loss: tensor(0.3474)\n",
      "6524 Training Loss: tensor(0.3483)\n",
      "6525 Training Loss: tensor(0.3471)\n",
      "6526 Training Loss: tensor(0.3472)\n",
      "6527 Training Loss: tensor(0.3467)\n",
      "6528 Training Loss: tensor(0.3474)\n",
      "6529 Training Loss: tensor(0.3489)\n",
      "6530 Training Loss: tensor(0.3526)\n",
      "6531 Training Loss: tensor(0.3463)\n",
      "6532 Training Loss: tensor(0.3496)\n",
      "6533 Training Loss: tensor(0.3529)\n",
      "6534 Training Loss: tensor(0.3473)\n",
      "6535 Training Loss: tensor(0.3535)\n",
      "6536 Training Loss: tensor(0.3484)\n",
      "6537 Training Loss: tensor(0.3552)\n",
      "6538 Training Loss: tensor(0.3504)\n",
      "6539 Training Loss: tensor(0.3536)\n",
      "6540 Training Loss: tensor(0.3490)\n",
      "6541 Training Loss: tensor(0.3469)\n",
      "6542 Training Loss: tensor(0.3478)\n",
      "6543 Training Loss: tensor(0.3507)\n",
      "6544 Training Loss: tensor(0.3484)\n",
      "6545 Training Loss: tensor(0.3481)\n",
      "6546 Training Loss: tensor(0.3481)\n",
      "6547 Training Loss: tensor(0.3478)\n",
      "6548 Training Loss: tensor(0.3526)\n",
      "6549 Training Loss: tensor(0.3464)\n",
      "6550 Training Loss: tensor(0.3512)\n",
      "6551 Training Loss: tensor(0.3516)\n",
      "6552 Training Loss: tensor(0.3489)\n",
      "6553 Training Loss: tensor(0.3464)\n",
      "6554 Training Loss: tensor(0.3467)\n",
      "6555 Training Loss: tensor(0.3466)\n",
      "6556 Training Loss: tensor(0.3469)\n",
      "6557 Training Loss: tensor(0.3476)\n",
      "6558 Training Loss: tensor(0.3483)\n",
      "6559 Training Loss: tensor(0.3484)\n",
      "6560 Training Loss: tensor(0.3532)\n",
      "6561 Training Loss: tensor(0.3468)\n",
      "6562 Training Loss: tensor(0.3495)\n",
      "6563 Training Loss: tensor(0.3468)\n",
      "6564 Training Loss: tensor(0.3529)\n",
      "6565 Training Loss: tensor(0.3474)\n",
      "6566 Training Loss: tensor(0.3471)\n",
      "6567 Training Loss: tensor(0.3476)\n",
      "6568 Training Loss: tensor(0.3487)\n",
      "6569 Training Loss: tensor(0.3462)\n",
      "6570 Training Loss: tensor(0.3476)\n",
      "6571 Training Loss: tensor(0.3537)\n",
      "6572 Training Loss: tensor(0.3472)\n",
      "6573 Training Loss: tensor(0.3468)\n",
      "6574 Training Loss: tensor(0.3488)\n",
      "6575 Training Loss: tensor(0.3484)\n",
      "6576 Training Loss: tensor(0.3574)\n",
      "6577 Training Loss: tensor(0.3510)\n",
      "6578 Training Loss: tensor(0.3486)\n",
      "6579 Training Loss: tensor(0.3469)\n",
      "6580 Training Loss: tensor(0.3500)\n",
      "6581 Training Loss: tensor(0.3492)\n",
      "6582 Training Loss: tensor(0.3486)\n",
      "6583 Training Loss: tensor(0.3473)\n",
      "6584 Training Loss: tensor(0.3466)\n",
      "6585 Training Loss: tensor(0.3515)\n",
      "6586 Training Loss: tensor(0.3467)\n",
      "6587 Training Loss: tensor(0.3480)\n",
      "6588 Training Loss: tensor(0.3489)\n",
      "6589 Training Loss: tensor(0.3477)\n",
      "6590 Training Loss: tensor(0.3515)\n",
      "6591 Training Loss: tensor(0.3483)\n",
      "6592 Training Loss: tensor(0.3503)\n",
      "6593 Training Loss: tensor(0.3643)\n",
      "6594 Training Loss: tensor(0.3497)\n",
      "6595 Training Loss: tensor(0.3481)\n",
      "6596 Training Loss: tensor(0.3484)\n",
      "6597 Training Loss: tensor(0.3500)\n",
      "6598 Training Loss: tensor(0.3523)\n",
      "6599 Training Loss: tensor(0.3480)\n",
      "6600 Training Loss: tensor(0.3484)\n",
      "6601 Training Loss: tensor(0.3484)\n",
      "6602 Training Loss: tensor(0.3490)\n",
      "6603 Training Loss: tensor(0.3505)\n",
      "6604 Training Loss: tensor(0.3495)\n",
      "6605 Training Loss: tensor(0.3487)\n",
      "6606 Training Loss: tensor(0.3474)\n",
      "6607 Training Loss: tensor(0.3474)\n",
      "6608 Training Loss: tensor(0.3493)\n",
      "6609 Training Loss: tensor(0.3471)\n",
      "6610 Training Loss: tensor(0.3498)\n",
      "6611 Training Loss: tensor(0.3476)\n",
      "6612 Training Loss: tensor(0.3475)\n",
      "6613 Training Loss: tensor(0.3529)\n",
      "6614 Training Loss: tensor(0.3468)\n",
      "6615 Training Loss: tensor(0.3471)\n",
      "6616 Training Loss: tensor(0.3496)\n",
      "6617 Training Loss: tensor(0.3472)\n",
      "6618 Training Loss: tensor(0.3464)\n",
      "6619 Training Loss: tensor(0.3471)\n",
      "6620 Training Loss: tensor(0.3502)\n",
      "6621 Training Loss: tensor(0.3485)\n",
      "6622 Training Loss: tensor(0.3478)\n",
      "6623 Training Loss: tensor(0.3477)\n",
      "6624 Training Loss: tensor(0.3510)\n",
      "6625 Training Loss: tensor(0.3550)\n",
      "6626 Training Loss: tensor(0.3512)\n",
      "6627 Training Loss: tensor(0.3491)\n",
      "6628 Training Loss: tensor(0.3528)\n",
      "6629 Training Loss: tensor(0.3489)\n",
      "6630 Training Loss: tensor(0.3516)\n",
      "6631 Training Loss: tensor(0.3499)\n",
      "6632 Training Loss: tensor(0.3487)\n",
      "6633 Training Loss: tensor(0.3542)\n",
      "6634 Training Loss: tensor(0.3500)\n",
      "6635 Training Loss: tensor(0.3483)\n",
      "6636 Training Loss: tensor(0.3493)\n",
      "6637 Training Loss: tensor(0.3520)\n",
      "6638 Training Loss: tensor(0.3492)\n",
      "6639 Training Loss: tensor(0.3489)\n",
      "6640 Training Loss: tensor(0.3505)\n",
      "6641 Training Loss: tensor(0.3490)\n",
      "6642 Training Loss: tensor(0.3521)\n",
      "6643 Training Loss: tensor(0.3479)\n",
      "6644 Training Loss: tensor(0.3488)\n",
      "6645 Training Loss: tensor(0.3503)\n",
      "6646 Training Loss: tensor(0.3506)\n",
      "6647 Training Loss: tensor(0.3499)\n",
      "6648 Training Loss: tensor(0.3478)\n",
      "6649 Training Loss: tensor(0.3484)\n",
      "6650 Training Loss: tensor(0.3468)\n",
      "6651 Training Loss: tensor(0.3481)\n",
      "6652 Training Loss: tensor(0.3467)\n",
      "6653 Training Loss: tensor(0.3465)\n",
      "6654 Training Loss: tensor(0.3479)\n",
      "6655 Training Loss: tensor(0.3515)\n",
      "6656 Training Loss: tensor(0.3482)\n",
      "6657 Training Loss: tensor(0.3506)\n",
      "6658 Training Loss: tensor(0.3495)\n",
      "6659 Training Loss: tensor(0.3464)\n",
      "6660 Training Loss: tensor(0.3471)\n",
      "6661 Training Loss: tensor(0.3708)\n",
      "6662 Training Loss: tensor(0.3506)\n",
      "6663 Training Loss: tensor(0.3496)\n",
      "6664 Training Loss: tensor(0.3511)\n",
      "6665 Training Loss: tensor(0.3530)\n",
      "6666 Training Loss: tensor(0.3483)\n",
      "6667 Training Loss: tensor(0.3540)\n",
      "6668 Training Loss: tensor(0.3539)\n",
      "6669 Training Loss: tensor(0.3511)\n",
      "6670 Training Loss: tensor(0.3522)\n",
      "6671 Training Loss: tensor(0.3510)\n",
      "6672 Training Loss: tensor(0.3501)\n",
      "6673 Training Loss: tensor(0.3486)\n",
      "6674 Training Loss: tensor(0.3503)\n",
      "6675 Training Loss: tensor(0.3479)\n",
      "6676 Training Loss: tensor(0.3486)\n",
      "6677 Training Loss: tensor(0.3511)\n",
      "6678 Training Loss: tensor(0.3488)\n",
      "6679 Training Loss: tensor(0.3487)\n",
      "6680 Training Loss: tensor(0.3503)\n",
      "6681 Training Loss: tensor(0.3489)\n",
      "6682 Training Loss: tensor(0.3471)\n",
      "6683 Training Loss: tensor(0.3466)\n",
      "6684 Training Loss: tensor(0.3478)\n",
      "6685 Training Loss: tensor(0.3461)\n",
      "6686 Training Loss: tensor(0.3476)\n",
      "6687 Training Loss: tensor(0.3502)\n",
      "6688 Training Loss: tensor(0.3622)\n",
      "6689 Training Loss: tensor(0.3499)\n",
      "6690 Training Loss: tensor(0.3524)\n",
      "6691 Training Loss: tensor(0.3490)\n",
      "6692 Training Loss: tensor(0.3487)\n",
      "6693 Training Loss: tensor(0.3503)\n",
      "6694 Training Loss: tensor(0.3471)\n",
      "6695 Training Loss: tensor(0.3475)\n",
      "6696 Training Loss: tensor(0.3507)\n",
      "6697 Training Loss: tensor(0.3485)\n",
      "6698 Training Loss: tensor(0.3533)\n",
      "6699 Training Loss: tensor(0.3477)\n",
      "6700 Training Loss: tensor(0.3493)\n",
      "6701 Training Loss: tensor(0.3498)\n",
      "6702 Training Loss: tensor(0.3488)\n",
      "6703 Training Loss: tensor(0.3477)\n",
      "6704 Training Loss: tensor(0.3489)\n",
      "6705 Training Loss: tensor(0.3486)\n",
      "6706 Training Loss: tensor(0.3487)\n",
      "6707 Training Loss: tensor(0.3479)\n",
      "6708 Training Loss: tensor(0.3496)\n",
      "6709 Training Loss: tensor(0.3468)\n",
      "6710 Training Loss: tensor(0.3471)\n",
      "6711 Training Loss: tensor(0.3487)\n",
      "6712 Training Loss: tensor(0.3475)\n",
      "6713 Training Loss: tensor(0.3479)\n",
      "6714 Training Loss: tensor(0.3484)\n",
      "6715 Training Loss: tensor(0.3466)\n",
      "6716 Training Loss: tensor(0.3491)\n",
      "6717 Training Loss: tensor(0.3473)\n",
      "6718 Training Loss: tensor(0.3569)\n",
      "6719 Training Loss: tensor(0.3462)\n",
      "6720 Training Loss: tensor(0.3492)\n",
      "6721 Training Loss: tensor(0.3530)\n",
      "6722 Training Loss: tensor(0.3469)\n",
      "6723 Training Loss: tensor(0.3473)\n",
      "6724 Training Loss: tensor(0.3478)\n",
      "6725 Training Loss: tensor(0.3468)\n",
      "6726 Training Loss: tensor(0.3475)\n",
      "6727 Training Loss: tensor(0.3481)\n",
      "6728 Training Loss: tensor(0.3480)\n",
      "6729 Training Loss: tensor(0.3476)\n",
      "6730 Training Loss: tensor(0.3472)\n",
      "6731 Training Loss: tensor(0.3507)\n",
      "6732 Training Loss: tensor(0.3489)\n",
      "6733 Training Loss: tensor(0.3554)\n",
      "6734 Training Loss: tensor(0.3487)\n",
      "6735 Training Loss: tensor(0.3557)\n",
      "6736 Training Loss: tensor(0.3488)\n",
      "6737 Training Loss: tensor(0.3478)\n",
      "6738 Training Loss: tensor(0.3514)\n",
      "6739 Training Loss: tensor(0.3489)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6740 Training Loss: tensor(0.3488)\n",
      "6741 Training Loss: tensor(0.3479)\n",
      "6742 Training Loss: tensor(0.3493)\n",
      "6743 Training Loss: tensor(0.3477)\n",
      "6744 Training Loss: tensor(0.3476)\n",
      "6745 Training Loss: tensor(0.3479)\n",
      "6746 Training Loss: tensor(0.3491)\n",
      "6747 Training Loss: tensor(0.3482)\n",
      "6748 Training Loss: tensor(0.3504)\n",
      "6749 Training Loss: tensor(0.3522)\n",
      "6750 Training Loss: tensor(0.3500)\n",
      "6751 Training Loss: tensor(0.3531)\n",
      "6752 Training Loss: tensor(0.3481)\n",
      "6753 Training Loss: tensor(0.3487)\n",
      "6754 Training Loss: tensor(0.3563)\n",
      "6755 Training Loss: tensor(0.3479)\n",
      "6756 Training Loss: tensor(0.3512)\n",
      "6757 Training Loss: tensor(0.3486)\n",
      "6758 Training Loss: tensor(0.3485)\n",
      "6759 Training Loss: tensor(0.3490)\n",
      "6760 Training Loss: tensor(0.3489)\n",
      "6761 Training Loss: tensor(0.3469)\n",
      "6762 Training Loss: tensor(0.3535)\n",
      "6763 Training Loss: tensor(0.3465)\n",
      "6764 Training Loss: tensor(0.3529)\n",
      "6765 Training Loss: tensor(0.3468)\n",
      "6766 Training Loss: tensor(0.3468)\n",
      "6767 Training Loss: tensor(0.3469)\n",
      "6768 Training Loss: tensor(0.3476)\n",
      "6769 Training Loss: tensor(0.3474)\n",
      "6770 Training Loss: tensor(0.3468)\n",
      "6771 Training Loss: tensor(0.3484)\n",
      "6772 Training Loss: tensor(0.3478)\n",
      "6773 Training Loss: tensor(0.3503)\n",
      "6774 Training Loss: tensor(0.3464)\n",
      "6775 Training Loss: tensor(0.3497)\n",
      "6776 Training Loss: tensor(0.3520)\n",
      "6777 Training Loss: tensor(0.3467)\n",
      "6778 Training Loss: tensor(0.3532)\n",
      "6779 Training Loss: tensor(0.3490)\n",
      "6780 Training Loss: tensor(0.3475)\n",
      "6781 Training Loss: tensor(0.3471)\n",
      "6782 Training Loss: tensor(0.3544)\n",
      "6783 Training Loss: tensor(0.3491)\n",
      "6784 Training Loss: tensor(0.3488)\n",
      "6785 Training Loss: tensor(0.3494)\n",
      "6786 Training Loss: tensor(0.3505)\n",
      "6787 Training Loss: tensor(0.3493)\n",
      "6788 Training Loss: tensor(0.3468)\n",
      "6789 Training Loss: tensor(0.3485)\n",
      "6790 Training Loss: tensor(0.3489)\n",
      "6791 Training Loss: tensor(0.3485)\n",
      "6792 Training Loss: tensor(0.3480)\n",
      "6793 Training Loss: tensor(0.3483)\n",
      "6794 Training Loss: tensor(0.3488)\n",
      "6795 Training Loss: tensor(0.3461)\n",
      "6796 Training Loss: tensor(0.3473)\n",
      "6797 Training Loss: tensor(0.3464)\n",
      "6798 Training Loss: tensor(0.3466)\n",
      "6799 Training Loss: tensor(0.3480)\n",
      "6800 Training Loss: tensor(0.3472)\n",
      "6801 Training Loss: tensor(0.3518)\n",
      "6802 Training Loss: tensor(0.3470)\n",
      "6803 Training Loss: tensor(0.3471)\n",
      "6804 Training Loss: tensor(0.3471)\n",
      "6805 Training Loss: tensor(0.3474)\n",
      "6806 Training Loss: tensor(0.3520)\n",
      "6807 Training Loss: tensor(0.3470)\n",
      "6808 Training Loss: tensor(0.3464)\n",
      "6809 Training Loss: tensor(0.3503)\n",
      "6810 Training Loss: tensor(0.3499)\n",
      "6811 Training Loss: tensor(0.3473)\n",
      "6812 Training Loss: tensor(0.3519)\n",
      "6813 Training Loss: tensor(0.3514)\n",
      "6814 Training Loss: tensor(0.3483)\n",
      "6815 Training Loss: tensor(0.3492)\n",
      "6816 Training Loss: tensor(0.3498)\n",
      "6817 Training Loss: tensor(0.3479)\n",
      "6818 Training Loss: tensor(0.3465)\n",
      "6819 Training Loss: tensor(0.3487)\n",
      "6820 Training Loss: tensor(0.3514)\n",
      "6821 Training Loss: tensor(0.3467)\n",
      "6822 Training Loss: tensor(0.3490)\n",
      "6823 Training Loss: tensor(0.3469)\n",
      "6824 Training Loss: tensor(0.3485)\n",
      "6825 Training Loss: tensor(0.3479)\n",
      "6826 Training Loss: tensor(0.3468)\n",
      "6827 Training Loss: tensor(0.3479)\n",
      "6828 Training Loss: tensor(0.3480)\n",
      "6829 Training Loss: tensor(0.3504)\n",
      "6830 Training Loss: tensor(0.3477)\n",
      "6831 Training Loss: tensor(0.3485)\n",
      "6832 Training Loss: tensor(0.3484)\n",
      "6833 Training Loss: tensor(0.3470)\n",
      "6834 Training Loss: tensor(0.3468)\n",
      "6835 Training Loss: tensor(0.3520)\n",
      "6836 Training Loss: tensor(0.3480)\n",
      "6837 Training Loss: tensor(0.3474)\n",
      "6838 Training Loss: tensor(0.3484)\n",
      "6839 Training Loss: tensor(0.3479)\n",
      "6840 Training Loss: tensor(0.3535)\n",
      "6841 Training Loss: tensor(0.3454)\n",
      "6842 Training Loss: tensor(0.3526)\n",
      "6843 Training Loss: tensor(0.3478)\n",
      "6844 Training Loss: tensor(0.3467)\n",
      "6845 Training Loss: tensor(0.3511)\n",
      "6846 Training Loss: tensor(0.3458)\n",
      "6847 Training Loss: tensor(0.3460)\n",
      "6848 Training Loss: tensor(0.3509)\n",
      "6849 Training Loss: tensor(0.3480)\n",
      "6850 Training Loss: tensor(0.3477)\n",
      "6851 Training Loss: tensor(0.3541)\n",
      "6852 Training Loss: tensor(0.3479)\n",
      "6853 Training Loss: tensor(0.3499)\n",
      "6854 Training Loss: tensor(0.3478)\n",
      "6855 Training Loss: tensor(0.3495)\n",
      "6856 Training Loss: tensor(0.3495)\n",
      "6857 Training Loss: tensor(0.3519)\n",
      "6858 Training Loss: tensor(0.3534)\n",
      "6859 Training Loss: tensor(0.3497)\n",
      "6860 Training Loss: tensor(0.3480)\n",
      "6861 Training Loss: tensor(0.3522)\n",
      "6862 Training Loss: tensor(0.3509)\n",
      "6863 Training Loss: tensor(0.3574)\n",
      "6864 Training Loss: tensor(0.3547)\n",
      "6865 Training Loss: tensor(0.3485)\n",
      "6866 Training Loss: tensor(0.3486)\n",
      "6867 Training Loss: tensor(0.3489)\n",
      "6868 Training Loss: tensor(0.3543)\n",
      "6869 Training Loss: tensor(0.3506)\n",
      "6870 Training Loss: tensor(0.3484)\n",
      "6871 Training Loss: tensor(0.3494)\n",
      "6872 Training Loss: tensor(0.3527)\n",
      "6873 Training Loss: tensor(0.3497)\n",
      "6874 Training Loss: tensor(0.3478)\n",
      "6875 Training Loss: tensor(0.3520)\n",
      "6876 Training Loss: tensor(0.3495)\n",
      "6877 Training Loss: tensor(0.3493)\n",
      "6878 Training Loss: tensor(0.3480)\n",
      "6879 Training Loss: tensor(0.3481)\n",
      "6880 Training Loss: tensor(0.3468)\n",
      "6881 Training Loss: tensor(0.3503)\n",
      "6882 Training Loss: tensor(0.3493)\n",
      "6883 Training Loss: tensor(0.3488)\n",
      "6884 Training Loss: tensor(0.3487)\n",
      "6885 Training Loss: tensor(0.3494)\n",
      "6886 Training Loss: tensor(0.3470)\n",
      "6887 Training Loss: tensor(0.3467)\n",
      "6888 Training Loss: tensor(0.3473)\n",
      "6889 Training Loss: tensor(0.3477)\n",
      "6890 Training Loss: tensor(0.3473)\n",
      "6891 Training Loss: tensor(0.3464)\n",
      "6892 Training Loss: tensor(0.3533)\n",
      "6893 Training Loss: tensor(0.3464)\n",
      "6894 Training Loss: tensor(0.3465)\n",
      "6895 Training Loss: tensor(0.3471)\n",
      "6896 Training Loss: tensor(0.3470)\n",
      "6897 Training Loss: tensor(0.3492)\n",
      "6898 Training Loss: tensor(0.3463)\n",
      "6899 Training Loss: tensor(0.3463)\n",
      "6900 Training Loss: tensor(0.3470)\n",
      "6901 Training Loss: tensor(0.3478)\n",
      "6902 Training Loss: tensor(0.3458)\n",
      "6903 Training Loss: tensor(0.3507)\n",
      "6904 Training Loss: tensor(0.3522)\n",
      "6905 Training Loss: tensor(0.3479)\n",
      "6906 Training Loss: tensor(0.3523)\n",
      "6907 Training Loss: tensor(0.3517)\n",
      "6908 Training Loss: tensor(0.3544)\n",
      "6909 Training Loss: tensor(0.3470)\n",
      "6910 Training Loss: tensor(0.3492)\n",
      "6911 Training Loss: tensor(0.3555)\n",
      "6912 Training Loss: tensor(0.3468)\n",
      "6913 Training Loss: tensor(0.3532)\n",
      "6914 Training Loss: tensor(0.3506)\n",
      "6915 Training Loss: tensor(0.3577)\n",
      "6916 Training Loss: tensor(0.3511)\n",
      "6917 Training Loss: tensor(0.3499)\n",
      "6918 Training Loss: tensor(0.3514)\n",
      "6919 Training Loss: tensor(0.3516)\n",
      "6920 Training Loss: tensor(0.3524)\n",
      "6921 Training Loss: tensor(0.3519)\n",
      "6922 Training Loss: tensor(0.3499)\n",
      "6923 Training Loss: tensor(0.3515)\n",
      "6924 Training Loss: tensor(0.3501)\n",
      "6925 Training Loss: tensor(0.3496)\n",
      "6926 Training Loss: tensor(0.3513)\n",
      "6927 Training Loss: tensor(0.3506)\n",
      "6928 Training Loss: tensor(0.3510)\n",
      "6929 Training Loss: tensor(0.3544)\n",
      "6930 Training Loss: tensor(0.3487)\n",
      "6931 Training Loss: tensor(0.3482)\n",
      "6932 Training Loss: tensor(0.3490)\n",
      "6933 Training Loss: tensor(0.3558)\n",
      "6934 Training Loss: tensor(0.3487)\n",
      "6935 Training Loss: tensor(0.3487)\n",
      "6936 Training Loss: tensor(0.3506)\n",
      "6937 Training Loss: tensor(0.3477)\n",
      "6938 Training Loss: tensor(0.3471)\n",
      "6939 Training Loss: tensor(0.3474)\n",
      "6940 Training Loss: tensor(0.3479)\n",
      "6941 Training Loss: tensor(0.3504)\n",
      "6942 Training Loss: tensor(0.3487)\n",
      "6943 Training Loss: tensor(0.3473)\n",
      "6944 Training Loss: tensor(0.3463)\n",
      "6945 Training Loss: tensor(0.3481)\n",
      "6946 Training Loss: tensor(0.3484)\n",
      "6947 Training Loss: tensor(0.3515)\n",
      "6948 Training Loss: tensor(0.3469)\n",
      "6949 Training Loss: tensor(0.3470)\n",
      "6950 Training Loss: tensor(0.3488)\n",
      "6951 Training Loss: tensor(0.3492)\n",
      "6952 Training Loss: tensor(0.3472)\n",
      "6953 Training Loss: tensor(0.3468)\n",
      "6954 Training Loss: tensor(0.3486)\n",
      "6955 Training Loss: tensor(0.3507)\n",
      "6956 Training Loss: tensor(0.3520)\n",
      "6957 Training Loss: tensor(0.3499)\n",
      "6958 Training Loss: tensor(0.3507)\n",
      "6959 Training Loss: tensor(0.3462)\n",
      "6960 Training Loss: tensor(0.3468)\n",
      "6961 Training Loss: tensor(0.3479)\n",
      "6962 Training Loss: tensor(0.3469)\n",
      "6963 Training Loss: tensor(0.3486)\n",
      "6964 Training Loss: tensor(0.3493)\n",
      "6965 Training Loss: tensor(0.3476)\n",
      "6966 Training Loss: tensor(0.3510)\n",
      "6967 Training Loss: tensor(0.3478)\n",
      "6968 Training Loss: tensor(0.3479)\n",
      "6969 Training Loss: tensor(0.3500)\n",
      "6970 Training Loss: tensor(0.3478)\n",
      "6971 Training Loss: tensor(0.3507)\n",
      "6972 Training Loss: tensor(0.3497)\n",
      "6973 Training Loss: tensor(0.3468)\n",
      "6974 Training Loss: tensor(0.3546)\n",
      "6975 Training Loss: tensor(0.3497)\n",
      "6976 Training Loss: tensor(0.3479)\n",
      "6977 Training Loss: tensor(0.3478)\n",
      "6978 Training Loss: tensor(0.3469)\n",
      "6979 Training Loss: tensor(0.3492)\n",
      "6980 Training Loss: tensor(0.3489)\n",
      "6981 Training Loss: tensor(0.3475)\n",
      "6982 Training Loss: tensor(0.3499)\n",
      "6983 Training Loss: tensor(0.3482)\n",
      "6984 Training Loss: tensor(0.3475)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6985 Training Loss: tensor(0.3505)\n",
      "6986 Training Loss: tensor(0.3467)\n",
      "6987 Training Loss: tensor(0.3520)\n",
      "6988 Training Loss: tensor(0.3493)\n",
      "6989 Training Loss: tensor(0.3479)\n",
      "6990 Training Loss: tensor(0.3500)\n",
      "6991 Training Loss: tensor(0.3499)\n",
      "6992 Training Loss: tensor(0.3502)\n",
      "6993 Training Loss: tensor(0.3474)\n",
      "6994 Training Loss: tensor(0.3495)\n",
      "6995 Training Loss: tensor(0.3527)\n",
      "6996 Training Loss: tensor(0.3471)\n",
      "6997 Training Loss: tensor(0.3494)\n",
      "6998 Training Loss: tensor(0.3470)\n",
      "6999 Training Loss: tensor(0.3499)\n",
      "7000 Training Loss: tensor(0.3489)\n",
      "7001 Training Loss: tensor(0.3514)\n",
      "7002 Training Loss: tensor(0.3501)\n",
      "7003 Training Loss: tensor(0.3481)\n",
      "7004 Training Loss: tensor(0.3488)\n",
      "7005 Training Loss: tensor(0.3470)\n",
      "7006 Training Loss: tensor(0.3529)\n",
      "7007 Training Loss: tensor(0.3481)\n",
      "7008 Training Loss: tensor(0.3480)\n",
      "7009 Training Loss: tensor(0.3469)\n",
      "7010 Training Loss: tensor(0.3521)\n",
      "7011 Training Loss: tensor(0.3490)\n",
      "7012 Training Loss: tensor(0.3490)\n",
      "7013 Training Loss: tensor(0.3542)\n",
      "7014 Training Loss: tensor(0.3472)\n",
      "7015 Training Loss: tensor(0.3489)\n",
      "7016 Training Loss: tensor(0.3466)\n",
      "7017 Training Loss: tensor(0.3476)\n",
      "7018 Training Loss: tensor(0.3498)\n",
      "7019 Training Loss: tensor(0.3502)\n",
      "7020 Training Loss: tensor(0.3474)\n",
      "7021 Training Loss: tensor(0.3591)\n",
      "7022 Training Loss: tensor(0.3488)\n",
      "7023 Training Loss: tensor(0.3495)\n",
      "7024 Training Loss: tensor(0.3498)\n",
      "7025 Training Loss: tensor(0.3474)\n",
      "7026 Training Loss: tensor(0.3478)\n",
      "7027 Training Loss: tensor(0.3483)\n",
      "7028 Training Loss: tensor(0.3487)\n",
      "7029 Training Loss: tensor(0.3466)\n",
      "7030 Training Loss: tensor(0.3464)\n",
      "7031 Training Loss: tensor(0.3488)\n",
      "7032 Training Loss: tensor(0.3504)\n",
      "7033 Training Loss: tensor(0.3472)\n",
      "7034 Training Loss: tensor(0.3503)\n",
      "7035 Training Loss: tensor(0.3479)\n",
      "7036 Training Loss: tensor(0.3491)\n",
      "7037 Training Loss: tensor(0.3486)\n",
      "7038 Training Loss: tensor(0.3514)\n",
      "7039 Training Loss: tensor(0.3473)\n",
      "7040 Training Loss: tensor(0.3461)\n",
      "7041 Training Loss: tensor(0.3486)\n",
      "7042 Training Loss: tensor(0.3500)\n",
      "7043 Training Loss: tensor(0.3504)\n",
      "7044 Training Loss: tensor(0.3485)\n",
      "7045 Training Loss: tensor(0.3468)\n",
      "7046 Training Loss: tensor(0.3470)\n",
      "7047 Training Loss: tensor(0.3494)\n",
      "7048 Training Loss: tensor(0.3506)\n",
      "7049 Training Loss: tensor(0.3486)\n",
      "7050 Training Loss: tensor(0.3490)\n",
      "7051 Training Loss: tensor(0.3502)\n",
      "7052 Training Loss: tensor(0.3466)\n",
      "7053 Training Loss: tensor(0.3465)\n",
      "7054 Training Loss: tensor(0.3499)\n",
      "7055 Training Loss: tensor(0.3515)\n",
      "7056 Training Loss: tensor(0.3519)\n",
      "7057 Training Loss: tensor(0.3465)\n",
      "7058 Training Loss: tensor(0.3477)\n",
      "7059 Training Loss: tensor(0.3500)\n",
      "7060 Training Loss: tensor(0.3485)\n",
      "7061 Training Loss: tensor(0.3470)\n",
      "7062 Training Loss: tensor(0.3507)\n",
      "7063 Training Loss: tensor(0.3566)\n",
      "7064 Training Loss: tensor(0.3463)\n",
      "7065 Training Loss: tensor(0.3491)\n",
      "7066 Training Loss: tensor(0.3485)\n",
      "7067 Training Loss: tensor(0.3493)\n",
      "7068 Training Loss: tensor(0.3477)\n",
      "7069 Training Loss: tensor(0.3500)\n",
      "7070 Training Loss: tensor(0.3502)\n",
      "7071 Training Loss: tensor(0.3482)\n",
      "7072 Training Loss: tensor(0.3491)\n",
      "7073 Training Loss: tensor(0.3509)\n",
      "7074 Training Loss: tensor(0.3472)\n",
      "7075 Training Loss: tensor(0.3529)\n",
      "7076 Training Loss: tensor(0.3481)\n",
      "7077 Training Loss: tensor(0.3472)\n",
      "7078 Training Loss: tensor(0.3481)\n",
      "7079 Training Loss: tensor(0.3476)\n",
      "7080 Training Loss: tensor(0.3469)\n",
      "7081 Training Loss: tensor(0.3544)\n",
      "7082 Training Loss: tensor(0.3476)\n",
      "7083 Training Loss: tensor(0.3477)\n",
      "7084 Training Loss: tensor(0.3465)\n",
      "7085 Training Loss: tensor(0.3487)\n",
      "7086 Training Loss: tensor(0.3499)\n",
      "7087 Training Loss: tensor(0.3487)\n",
      "7088 Training Loss: tensor(0.3472)\n",
      "7089 Training Loss: tensor(0.3543)\n",
      "7090 Training Loss: tensor(0.3475)\n",
      "7091 Training Loss: tensor(0.3511)\n",
      "7092 Training Loss: tensor(0.3512)\n",
      "7093 Training Loss: tensor(0.3492)\n",
      "7094 Training Loss: tensor(0.3542)\n",
      "7095 Training Loss: tensor(0.3522)\n",
      "7096 Training Loss: tensor(0.3485)\n",
      "7097 Training Loss: tensor(0.3463)\n",
      "7098 Training Loss: tensor(0.3533)\n",
      "7099 Training Loss: tensor(0.3476)\n",
      "7100 Training Loss: tensor(0.3489)\n",
      "7101 Training Loss: tensor(0.3493)\n",
      "7102 Training Loss: tensor(0.3485)\n",
      "7103 Training Loss: tensor(0.3487)\n",
      "7104 Training Loss: tensor(0.3490)\n",
      "7105 Training Loss: tensor(0.3496)\n",
      "7106 Training Loss: tensor(0.3489)\n",
      "7107 Training Loss: tensor(0.3470)\n",
      "7108 Training Loss: tensor(0.3475)\n",
      "7109 Training Loss: tensor(0.3480)\n",
      "7110 Training Loss: tensor(0.3490)\n",
      "7111 Training Loss: tensor(0.3496)\n",
      "7112 Training Loss: tensor(0.3507)\n",
      "7113 Training Loss: tensor(0.3503)\n",
      "7114 Training Loss: tensor(0.3521)\n",
      "7115 Training Loss: tensor(0.3469)\n",
      "7116 Training Loss: tensor(0.3485)\n",
      "7117 Training Loss: tensor(0.3481)\n",
      "7118 Training Loss: tensor(0.3505)\n",
      "7119 Training Loss: tensor(0.3534)\n",
      "7120 Training Loss: tensor(0.3506)\n",
      "7121 Training Loss: tensor(0.3486)\n",
      "7122 Training Loss: tensor(0.3468)\n",
      "7123 Training Loss: tensor(0.3476)\n",
      "7124 Training Loss: tensor(0.3475)\n",
      "7125 Training Loss: tensor(0.3472)\n",
      "7126 Training Loss: tensor(0.3477)\n",
      "7127 Training Loss: tensor(0.3482)\n",
      "7128 Training Loss: tensor(0.3463)\n",
      "7129 Training Loss: tensor(0.3513)\n",
      "7130 Training Loss: tensor(0.3529)\n",
      "7131 Training Loss: tensor(0.3478)\n",
      "7132 Training Loss: tensor(0.3504)\n",
      "7133 Training Loss: tensor(0.3552)\n",
      "7134 Training Loss: tensor(0.3473)\n",
      "7135 Training Loss: tensor(0.3519)\n",
      "7136 Training Loss: tensor(0.3474)\n",
      "7137 Training Loss: tensor(0.3494)\n",
      "7138 Training Loss: tensor(0.3483)\n",
      "7139 Training Loss: tensor(0.3484)\n",
      "7140 Training Loss: tensor(0.3482)\n",
      "7141 Training Loss: tensor(0.3484)\n",
      "7142 Training Loss: tensor(0.3486)\n",
      "7143 Training Loss: tensor(0.3471)\n",
      "7144 Training Loss: tensor(0.3491)\n",
      "7145 Training Loss: tensor(0.3470)\n",
      "7146 Training Loss: tensor(0.3500)\n",
      "7147 Training Loss: tensor(0.3468)\n",
      "7148 Training Loss: tensor(0.3467)\n",
      "7149 Training Loss: tensor(0.3485)\n",
      "7150 Training Loss: tensor(0.3474)\n",
      "7151 Training Loss: tensor(0.3545)\n",
      "7152 Training Loss: tensor(0.3528)\n",
      "7153 Training Loss: tensor(0.3481)\n",
      "7154 Training Loss: tensor(0.3463)\n",
      "7155 Training Loss: tensor(0.3494)\n",
      "7156 Training Loss: tensor(0.3515)\n",
      "7157 Training Loss: tensor(0.3477)\n",
      "7158 Training Loss: tensor(0.3542)\n",
      "7159 Training Loss: tensor(0.3485)\n",
      "7160 Training Loss: tensor(0.3478)\n",
      "7161 Training Loss: tensor(0.3480)\n",
      "7162 Training Loss: tensor(0.3484)\n",
      "7163 Training Loss: tensor(0.3465)\n",
      "7164 Training Loss: tensor(0.3464)\n",
      "7165 Training Loss: tensor(0.3507)\n",
      "7166 Training Loss: tensor(0.3481)\n",
      "7167 Training Loss: tensor(0.3482)\n",
      "7168 Training Loss: tensor(0.3538)\n",
      "7169 Training Loss: tensor(0.3490)\n",
      "7170 Training Loss: tensor(0.3479)\n",
      "7171 Training Loss: tensor(0.3510)\n",
      "7172 Training Loss: tensor(0.3534)\n",
      "7173 Training Loss: tensor(0.3483)\n",
      "7174 Training Loss: tensor(0.3487)\n",
      "7175 Training Loss: tensor(0.3508)\n",
      "7176 Training Loss: tensor(0.3475)\n",
      "7177 Training Loss: tensor(0.3494)\n",
      "7178 Training Loss: tensor(0.3479)\n",
      "7179 Training Loss: tensor(0.3483)\n",
      "7180 Training Loss: tensor(0.3487)\n",
      "7181 Training Loss: tensor(0.3482)\n",
      "7182 Training Loss: tensor(0.3488)\n",
      "7183 Training Loss: tensor(0.3475)\n",
      "7184 Training Loss: tensor(0.3482)\n",
      "7185 Training Loss: tensor(0.3508)\n",
      "7186 Training Loss: tensor(0.3463)\n",
      "7187 Training Loss: tensor(0.3513)\n",
      "7188 Training Loss: tensor(0.3469)\n",
      "7189 Training Loss: tensor(0.3472)\n",
      "7190 Training Loss: tensor(0.3482)\n",
      "7191 Training Loss: tensor(0.3475)\n",
      "7192 Training Loss: tensor(0.3512)\n",
      "7193 Training Loss: tensor(0.3465)\n",
      "7194 Training Loss: tensor(0.3475)\n",
      "7195 Training Loss: tensor(0.3490)\n",
      "7196 Training Loss: tensor(0.3482)\n",
      "7197 Training Loss: tensor(0.3473)\n",
      "7198 Training Loss: tensor(0.3467)\n",
      "7199 Training Loss: tensor(0.3475)\n",
      "7200 Training Loss: tensor(0.3478)\n",
      "7201 Training Loss: tensor(0.3477)\n",
      "7202 Training Loss: tensor(0.3503)\n",
      "7203 Training Loss: tensor(0.3551)\n",
      "7204 Training Loss: tensor(0.3515)\n",
      "7205 Training Loss: tensor(0.3505)\n",
      "7206 Training Loss: tensor(0.3486)\n",
      "7207 Training Loss: tensor(0.3471)\n",
      "7208 Training Loss: tensor(0.3470)\n",
      "7209 Training Loss: tensor(0.3475)\n",
      "7210 Training Loss: tensor(0.3472)\n",
      "7211 Training Loss: tensor(0.3540)\n",
      "7212 Training Loss: tensor(0.3488)\n",
      "7213 Training Loss: tensor(0.3483)\n",
      "7214 Training Loss: tensor(0.3477)\n",
      "7215 Training Loss: tensor(0.3572)\n",
      "7216 Training Loss: tensor(0.3477)\n",
      "7217 Training Loss: tensor(0.3489)\n",
      "7218 Training Loss: tensor(0.3480)\n",
      "7219 Training Loss: tensor(0.3477)\n",
      "7220 Training Loss: tensor(0.3498)\n",
      "7221 Training Loss: tensor(0.3471)\n",
      "7222 Training Loss: tensor(0.3491)\n",
      "7223 Training Loss: tensor(0.3472)\n",
      "7224 Training Loss: tensor(0.3476)\n",
      "7225 Training Loss: tensor(0.3489)\n",
      "7226 Training Loss: tensor(0.3490)\n",
      "7227 Training Loss: tensor(0.3476)\n",
      "7228 Training Loss: tensor(0.3526)\n",
      "7229 Training Loss: tensor(0.3466)\n",
      "7230 Training Loss: tensor(0.3519)\n",
      "7231 Training Loss: tensor(0.3500)\n",
      "7232 Training Loss: tensor(0.3465)\n",
      "7233 Training Loss: tensor(0.3562)\n",
      "7234 Training Loss: tensor(0.3482)\n",
      "7235 Training Loss: tensor(0.3475)\n",
      "7236 Training Loss: tensor(0.3509)\n",
      "7237 Training Loss: tensor(0.3479)\n",
      "7238 Training Loss: tensor(0.3485)\n",
      "7239 Training Loss: tensor(0.3477)\n",
      "7240 Training Loss: tensor(0.3472)\n",
      "7241 Training Loss: tensor(0.3524)\n",
      "7242 Training Loss: tensor(0.3507)\n",
      "7243 Training Loss: tensor(0.3483)\n",
      "7244 Training Loss: tensor(0.3473)\n",
      "7245 Training Loss: tensor(0.3486)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7246 Training Loss: tensor(0.3464)\n",
      "7247 Training Loss: tensor(0.3506)\n",
      "7248 Training Loss: tensor(0.3497)\n",
      "7249 Training Loss: tensor(0.3487)\n",
      "7250 Training Loss: tensor(0.3477)\n",
      "7251 Training Loss: tensor(0.3476)\n",
      "7252 Training Loss: tensor(0.3488)\n",
      "7253 Training Loss: tensor(0.3481)\n",
      "7254 Training Loss: tensor(0.3467)\n",
      "7255 Training Loss: tensor(0.3509)\n",
      "7256 Training Loss: tensor(0.3482)\n",
      "7257 Training Loss: tensor(0.3468)\n",
      "7258 Training Loss: tensor(0.3471)\n",
      "7259 Training Loss: tensor(0.3507)\n",
      "7260 Training Loss: tensor(0.3500)\n",
      "7261 Training Loss: tensor(0.3468)\n",
      "7262 Training Loss: tensor(0.3482)\n",
      "7263 Training Loss: tensor(0.3498)\n",
      "7264 Training Loss: tensor(0.3484)\n",
      "7265 Training Loss: tensor(0.3471)\n",
      "7266 Training Loss: tensor(0.3534)\n",
      "7267 Training Loss: tensor(0.3471)\n",
      "7268 Training Loss: tensor(0.3496)\n",
      "7269 Training Loss: tensor(0.3473)\n",
      "7270 Training Loss: tensor(0.3500)\n",
      "7271 Training Loss: tensor(0.3461)\n",
      "7272 Training Loss: tensor(0.3472)\n",
      "7273 Training Loss: tensor(0.3484)\n",
      "7274 Training Loss: tensor(0.3479)\n",
      "7275 Training Loss: tensor(0.3464)\n",
      "7276 Training Loss: tensor(0.3476)\n",
      "7277 Training Loss: tensor(0.3471)\n",
      "7278 Training Loss: tensor(0.3487)\n",
      "7279 Training Loss: tensor(0.3461)\n",
      "7280 Training Loss: tensor(0.3471)\n",
      "7281 Training Loss: tensor(0.3471)\n",
      "7282 Training Loss: tensor(0.3476)\n",
      "7283 Training Loss: tensor(0.3485)\n",
      "7284 Training Loss: tensor(0.3468)\n",
      "7285 Training Loss: tensor(0.3617)\n",
      "7286 Training Loss: tensor(0.3527)\n",
      "7287 Training Loss: tensor(0.3496)\n",
      "7288 Training Loss: tensor(0.3532)\n",
      "7289 Training Loss: tensor(0.3476)\n",
      "7290 Training Loss: tensor(0.3466)\n",
      "7291 Training Loss: tensor(0.3510)\n",
      "7292 Training Loss: tensor(0.3467)\n",
      "7293 Training Loss: tensor(0.3504)\n",
      "7294 Training Loss: tensor(0.3497)\n",
      "7295 Training Loss: tensor(0.3470)\n",
      "7296 Training Loss: tensor(0.3482)\n",
      "7297 Training Loss: tensor(0.3509)\n",
      "7298 Training Loss: tensor(0.3484)\n",
      "7299 Training Loss: tensor(0.3485)\n",
      "7300 Training Loss: tensor(0.3487)\n",
      "7301 Training Loss: tensor(0.3524)\n",
      "7302 Training Loss: tensor(0.3469)\n",
      "7303 Training Loss: tensor(0.3506)\n",
      "7304 Training Loss: tensor(0.3481)\n",
      "7305 Training Loss: tensor(0.3482)\n",
      "7306 Training Loss: tensor(0.3475)\n",
      "7307 Training Loss: tensor(0.3542)\n",
      "7308 Training Loss: tensor(0.3509)\n",
      "7309 Training Loss: tensor(0.3474)\n",
      "7310 Training Loss: tensor(0.3470)\n",
      "7311 Training Loss: tensor(0.3480)\n",
      "7312 Training Loss: tensor(0.3486)\n",
      "7313 Training Loss: tensor(0.3481)\n",
      "7314 Training Loss: tensor(0.3501)\n",
      "7315 Training Loss: tensor(0.3474)\n",
      "7316 Training Loss: tensor(0.3554)\n",
      "7317 Training Loss: tensor(0.3514)\n",
      "7318 Training Loss: tensor(0.3513)\n",
      "7319 Training Loss: tensor(0.3462)\n",
      "7320 Training Loss: tensor(0.3530)\n",
      "7321 Training Loss: tensor(0.3481)\n",
      "7322 Training Loss: tensor(0.3464)\n",
      "7323 Training Loss: tensor(0.3505)\n",
      "7324 Training Loss: tensor(0.3476)\n",
      "7325 Training Loss: tensor(0.3502)\n",
      "7326 Training Loss: tensor(0.3473)\n",
      "7327 Training Loss: tensor(0.3484)\n",
      "7328 Training Loss: tensor(0.3489)\n",
      "7329 Training Loss: tensor(0.3546)\n",
      "7330 Training Loss: tensor(0.3496)\n",
      "7331 Training Loss: tensor(0.3465)\n",
      "7332 Training Loss: tensor(0.3482)\n",
      "7333 Training Loss: tensor(0.3486)\n",
      "7334 Training Loss: tensor(0.3477)\n",
      "7335 Training Loss: tensor(0.3527)\n",
      "7336 Training Loss: tensor(0.3474)\n",
      "7337 Training Loss: tensor(0.3474)\n",
      "7338 Training Loss: tensor(0.3474)\n",
      "7339 Training Loss: tensor(0.3470)\n",
      "7340 Training Loss: tensor(0.3517)\n",
      "7341 Training Loss: tensor(0.3472)\n",
      "7342 Training Loss: tensor(0.3485)\n",
      "7343 Training Loss: tensor(0.3469)\n",
      "7344 Training Loss: tensor(0.3477)\n",
      "7345 Training Loss: tensor(0.3484)\n",
      "7346 Training Loss: tensor(0.3463)\n",
      "7347 Training Loss: tensor(0.3462)\n",
      "7348 Training Loss: tensor(0.3469)\n",
      "7349 Training Loss: tensor(0.3475)\n",
      "7350 Training Loss: tensor(0.3474)\n",
      "7351 Training Loss: tensor(0.3490)\n",
      "7352 Training Loss: tensor(0.3472)\n",
      "7353 Training Loss: tensor(0.3462)\n",
      "7354 Training Loss: tensor(0.3472)\n",
      "7355 Training Loss: tensor(0.3463)\n",
      "7356 Training Loss: tensor(0.3548)\n",
      "7357 Training Loss: tensor(0.3460)\n",
      "7358 Training Loss: tensor(0.3526)\n",
      "7359 Training Loss: tensor(0.3465)\n",
      "7360 Training Loss: tensor(0.3496)\n",
      "7361 Training Loss: tensor(0.3469)\n",
      "7362 Training Loss: tensor(0.3461)\n",
      "7363 Training Loss: tensor(0.3469)\n",
      "7364 Training Loss: tensor(0.3536)\n",
      "7365 Training Loss: tensor(0.3477)\n",
      "7366 Training Loss: tensor(0.3467)\n",
      "7367 Training Loss: tensor(0.3487)\n",
      "7368 Training Loss: tensor(0.3478)\n",
      "7369 Training Loss: tensor(0.3491)\n",
      "7370 Training Loss: tensor(0.3472)\n",
      "7371 Training Loss: tensor(0.3465)\n",
      "7372 Training Loss: tensor(0.3469)\n",
      "7373 Training Loss: tensor(0.3477)\n",
      "7374 Training Loss: tensor(0.3466)\n",
      "7375 Training Loss: tensor(0.3570)\n",
      "7376 Training Loss: tensor(0.3488)\n",
      "7377 Training Loss: tensor(0.3526)\n",
      "7378 Training Loss: tensor(0.3482)\n",
      "7379 Training Loss: tensor(0.3477)\n",
      "7380 Training Loss: tensor(0.3474)\n",
      "7381 Training Loss: tensor(0.3488)\n",
      "7382 Training Loss: tensor(0.3480)\n",
      "7383 Training Loss: tensor(0.3492)\n",
      "7384 Training Loss: tensor(0.3469)\n",
      "7385 Training Loss: tensor(0.3479)\n",
      "7386 Training Loss: tensor(0.3476)\n",
      "7387 Training Loss: tensor(0.3485)\n",
      "7388 Training Loss: tensor(0.3465)\n",
      "7389 Training Loss: tensor(0.3469)\n",
      "7390 Training Loss: tensor(0.3479)\n",
      "7391 Training Loss: tensor(0.3496)\n",
      "7392 Training Loss: tensor(0.3535)\n",
      "7393 Training Loss: tensor(0.3478)\n",
      "7394 Training Loss: tensor(0.3457)\n",
      "7395 Training Loss: tensor(0.3471)\n",
      "7396 Training Loss: tensor(0.3479)\n",
      "7397 Training Loss: tensor(0.3510)\n",
      "7398 Training Loss: tensor(0.3498)\n",
      "7399 Training Loss: tensor(0.3464)\n",
      "7400 Training Loss: tensor(0.3474)\n",
      "7401 Training Loss: tensor(0.3463)\n",
      "7402 Training Loss: tensor(0.3478)\n",
      "7403 Training Loss: tensor(0.3470)\n",
      "7404 Training Loss: tensor(0.3458)\n",
      "7405 Training Loss: tensor(0.3501)\n",
      "7406 Training Loss: tensor(0.3474)\n",
      "7407 Training Loss: tensor(0.3489)\n",
      "7408 Training Loss: tensor(0.3552)\n",
      "7409 Training Loss: tensor(0.3559)\n",
      "7410 Training Loss: tensor(0.3486)\n",
      "7411 Training Loss: tensor(0.3545)\n",
      "7412 Training Loss: tensor(0.3463)\n",
      "7413 Training Loss: tensor(0.3507)\n",
      "7414 Training Loss: tensor(0.3490)\n",
      "7415 Training Loss: tensor(0.3477)\n",
      "7416 Training Loss: tensor(0.3477)\n",
      "7417 Training Loss: tensor(0.3486)\n",
      "7418 Training Loss: tensor(0.3481)\n",
      "7419 Training Loss: tensor(0.3491)\n",
      "7420 Training Loss: tensor(0.3492)\n",
      "7421 Training Loss: tensor(0.3491)\n",
      "7422 Training Loss: tensor(0.3485)\n",
      "7423 Training Loss: tensor(0.3485)\n",
      "7424 Training Loss: tensor(0.3475)\n",
      "7425 Training Loss: tensor(0.3480)\n",
      "7426 Training Loss: tensor(0.3476)\n",
      "7427 Training Loss: tensor(0.3459)\n",
      "7428 Training Loss: tensor(0.3484)\n",
      "7429 Training Loss: tensor(0.3467)\n",
      "7430 Training Loss: tensor(0.3469)\n",
      "7431 Training Loss: tensor(0.3466)\n",
      "7432 Training Loss: tensor(0.3519)\n",
      "7433 Training Loss: tensor(0.3489)\n",
      "7434 Training Loss: tensor(0.3457)\n",
      "7435 Training Loss: tensor(0.3458)\n",
      "7436 Training Loss: tensor(0.3467)\n",
      "7437 Training Loss: tensor(0.3478)\n",
      "7438 Training Loss: tensor(0.3468)\n",
      "7439 Training Loss: tensor(0.3518)\n",
      "7440 Training Loss: tensor(0.3474)\n",
      "7441 Training Loss: tensor(0.3468)\n",
      "7442 Training Loss: tensor(0.3474)\n",
      "7443 Training Loss: tensor(0.3468)\n",
      "7444 Training Loss: tensor(0.3477)\n",
      "7445 Training Loss: tensor(0.3475)\n",
      "7446 Training Loss: tensor(0.3506)\n",
      "7447 Training Loss: tensor(0.3521)\n",
      "7448 Training Loss: tensor(0.3522)\n",
      "7449 Training Loss: tensor(0.3501)\n",
      "7450 Training Loss: tensor(0.3497)\n",
      "7451 Training Loss: tensor(0.3504)\n",
      "7452 Training Loss: tensor(0.3476)\n",
      "7453 Training Loss: tensor(0.3467)\n",
      "7454 Training Loss: tensor(0.3536)\n",
      "7455 Training Loss: tensor(0.3477)\n",
      "7456 Training Loss: tensor(0.3532)\n",
      "7457 Training Loss: tensor(0.3515)\n",
      "7458 Training Loss: tensor(0.3467)\n",
      "7459 Training Loss: tensor(0.3474)\n",
      "7460 Training Loss: tensor(0.3528)\n",
      "7461 Training Loss: tensor(0.3484)\n",
      "7462 Training Loss: tensor(0.3490)\n",
      "7463 Training Loss: tensor(0.3480)\n",
      "7464 Training Loss: tensor(0.3484)\n",
      "7465 Training Loss: tensor(0.3488)\n",
      "7466 Training Loss: tensor(0.3481)\n",
      "7467 Training Loss: tensor(0.3514)\n",
      "7468 Training Loss: tensor(0.3471)\n",
      "7469 Training Loss: tensor(0.3482)\n",
      "7470 Training Loss: tensor(0.3477)\n",
      "7471 Training Loss: tensor(0.3471)\n",
      "7472 Training Loss: tensor(0.3472)\n",
      "7473 Training Loss: tensor(0.3466)\n",
      "7474 Training Loss: tensor(0.3474)\n",
      "7475 Training Loss: tensor(0.3471)\n",
      "7476 Training Loss: tensor(0.3468)\n",
      "7477 Training Loss: tensor(0.3514)\n",
      "7478 Training Loss: tensor(0.3466)\n",
      "7479 Training Loss: tensor(0.3467)\n",
      "7480 Training Loss: tensor(0.3516)\n",
      "7481 Training Loss: tensor(0.3528)\n",
      "7482 Training Loss: tensor(0.3490)\n",
      "7483 Training Loss: tensor(0.3471)\n",
      "7484 Training Loss: tensor(0.3561)\n",
      "7485 Training Loss: tensor(0.3539)\n",
      "7486 Training Loss: tensor(0.3461)\n",
      "7487 Training Loss: tensor(0.3488)\n",
      "7488 Training Loss: tensor(0.3554)\n",
      "7489 Training Loss: tensor(0.3480)\n",
      "7490 Training Loss: tensor(0.3470)\n",
      "7491 Training Loss: tensor(0.3511)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7492 Training Loss: tensor(0.3473)\n",
      "7493 Training Loss: tensor(0.3497)\n",
      "7494 Training Loss: tensor(0.3472)\n",
      "7495 Training Loss: tensor(0.3482)\n",
      "7496 Training Loss: tensor(0.3480)\n",
      "7497 Training Loss: tensor(0.3472)\n",
      "7498 Training Loss: tensor(0.3503)\n",
      "7499 Training Loss: tensor(0.3510)\n",
      "7500 Training Loss: tensor(0.3476)\n",
      "7501 Training Loss: tensor(0.3476)\n",
      "7502 Training Loss: tensor(0.3472)\n",
      "7503 Training Loss: tensor(0.3506)\n",
      "7504 Training Loss: tensor(0.3496)\n",
      "7505 Training Loss: tensor(0.3484)\n",
      "7506 Training Loss: tensor(0.3473)\n",
      "7507 Training Loss: tensor(0.3474)\n",
      "7508 Training Loss: tensor(0.3525)\n",
      "7509 Training Loss: tensor(0.3474)\n",
      "7510 Training Loss: tensor(0.3530)\n",
      "7511 Training Loss: tensor(0.3463)\n",
      "7512 Training Loss: tensor(0.3469)\n",
      "7513 Training Loss: tensor(0.3464)\n",
      "7514 Training Loss: tensor(0.3470)\n",
      "7515 Training Loss: tensor(0.3523)\n",
      "7516 Training Loss: tensor(0.3479)\n",
      "7517 Training Loss: tensor(0.3576)\n",
      "7518 Training Loss: tensor(0.3496)\n",
      "7519 Training Loss: tensor(0.3489)\n",
      "7520 Training Loss: tensor(0.3479)\n",
      "7521 Training Loss: tensor(0.3477)\n",
      "7522 Training Loss: tensor(0.3468)\n",
      "7523 Training Loss: tensor(0.3476)\n",
      "7524 Training Loss: tensor(0.3465)\n",
      "7525 Training Loss: tensor(0.3481)\n",
      "7526 Training Loss: tensor(0.3466)\n",
      "7527 Training Loss: tensor(0.3469)\n",
      "7528 Training Loss: tensor(0.3551)\n",
      "7529 Training Loss: tensor(0.3475)\n",
      "7530 Training Loss: tensor(0.3523)\n",
      "7531 Training Loss: tensor(0.3476)\n",
      "7532 Training Loss: tensor(0.3459)\n",
      "7533 Training Loss: tensor(0.3492)\n",
      "7534 Training Loss: tensor(0.3502)\n",
      "7535 Training Loss: tensor(0.3476)\n",
      "7536 Training Loss: tensor(0.3495)\n",
      "7537 Training Loss: tensor(0.3479)\n",
      "7538 Training Loss: tensor(0.3461)\n",
      "7539 Training Loss: tensor(0.3475)\n",
      "7540 Training Loss: tensor(0.3486)\n",
      "7541 Training Loss: tensor(0.3488)\n",
      "7542 Training Loss: tensor(0.3482)\n",
      "7543 Training Loss: tensor(0.3476)\n",
      "7544 Training Loss: tensor(0.3497)\n",
      "7545 Training Loss: tensor(0.3462)\n",
      "7546 Training Loss: tensor(0.3483)\n",
      "7547 Training Loss: tensor(0.3484)\n",
      "7548 Training Loss: tensor(0.3487)\n",
      "7549 Training Loss: tensor(0.3481)\n",
      "7550 Training Loss: tensor(0.3472)\n",
      "7551 Training Loss: tensor(0.3482)\n",
      "7552 Training Loss: tensor(0.3473)\n",
      "7553 Training Loss: tensor(0.3473)\n",
      "7554 Training Loss: tensor(0.3497)\n",
      "7555 Training Loss: tensor(0.3471)\n",
      "7556 Training Loss: tensor(0.3460)\n",
      "7557 Training Loss: tensor(0.3471)\n",
      "7558 Training Loss: tensor(0.3513)\n",
      "7559 Training Loss: tensor(0.3490)\n",
      "7560 Training Loss: tensor(0.3511)\n",
      "7561 Training Loss: tensor(0.3546)\n",
      "7562 Training Loss: tensor(0.3468)\n",
      "7563 Training Loss: tensor(0.3459)\n",
      "7564 Training Loss: tensor(0.3479)\n",
      "7565 Training Loss: tensor(0.3470)\n",
      "7566 Training Loss: tensor(0.3529)\n",
      "7567 Training Loss: tensor(0.3507)\n",
      "7568 Training Loss: tensor(0.3478)\n",
      "7569 Training Loss: tensor(0.3490)\n",
      "7570 Training Loss: tensor(0.3470)\n",
      "7571 Training Loss: tensor(0.3483)\n",
      "7572 Training Loss: tensor(0.3519)\n",
      "7573 Training Loss: tensor(0.3482)\n",
      "7574 Training Loss: tensor(0.3476)\n",
      "7575 Training Loss: tensor(0.3474)\n",
      "7576 Training Loss: tensor(0.3498)\n",
      "7577 Training Loss: tensor(0.3477)\n",
      "7578 Training Loss: tensor(0.3471)\n",
      "7579 Training Loss: tensor(0.3470)\n",
      "7580 Training Loss: tensor(0.3480)\n",
      "7581 Training Loss: tensor(0.3489)\n",
      "7582 Training Loss: tensor(0.3485)\n",
      "7583 Training Loss: tensor(0.3470)\n",
      "7584 Training Loss: tensor(0.3464)\n",
      "7585 Training Loss: tensor(0.3467)\n",
      "7586 Training Loss: tensor(0.3469)\n",
      "7587 Training Loss: tensor(0.3502)\n",
      "7588 Training Loss: tensor(0.3510)\n",
      "7589 Training Loss: tensor(0.3493)\n",
      "7590 Training Loss: tensor(0.3475)\n",
      "7591 Training Loss: tensor(0.3464)\n",
      "7592 Training Loss: tensor(0.3460)\n",
      "7593 Training Loss: tensor(0.3483)\n",
      "7594 Training Loss: tensor(0.3467)\n",
      "7595 Training Loss: tensor(0.3520)\n",
      "7596 Training Loss: tensor(0.3480)\n",
      "7597 Training Loss: tensor(0.3481)\n",
      "7598 Training Loss: tensor(0.3468)\n",
      "7599 Training Loss: tensor(0.3459)\n",
      "7600 Training Loss: tensor(0.3476)\n",
      "7601 Training Loss: tensor(0.3462)\n",
      "7602 Training Loss: tensor(0.3456)\n",
      "7603 Training Loss: tensor(0.3478)\n",
      "7604 Training Loss: tensor(0.3461)\n",
      "7605 Training Loss: tensor(0.3475)\n",
      "7606 Training Loss: tensor(0.3483)\n",
      "7607 Training Loss: tensor(0.3563)\n",
      "7608 Training Loss: tensor(0.3520)\n",
      "7609 Training Loss: tensor(0.3487)\n",
      "7610 Training Loss: tensor(0.3483)\n",
      "7611 Training Loss: tensor(0.3486)\n",
      "7612 Training Loss: tensor(0.3459)\n",
      "7613 Training Loss: tensor(0.3528)\n",
      "7614 Training Loss: tensor(0.3466)\n",
      "7615 Training Loss: tensor(0.3465)\n",
      "7616 Training Loss: tensor(0.3482)\n",
      "7617 Training Loss: tensor(0.3472)\n",
      "7618 Training Loss: tensor(0.3471)\n",
      "7619 Training Loss: tensor(0.3482)\n",
      "7620 Training Loss: tensor(0.3461)\n",
      "7621 Training Loss: tensor(0.3463)\n",
      "7622 Training Loss: tensor(0.3465)\n",
      "7623 Training Loss: tensor(0.3468)\n",
      "7624 Training Loss: tensor(0.3497)\n",
      "7625 Training Loss: tensor(0.3510)\n",
      "7626 Training Loss: tensor(0.3475)\n",
      "7627 Training Loss: tensor(0.3496)\n",
      "7628 Training Loss: tensor(0.3458)\n",
      "7629 Training Loss: tensor(0.3489)\n",
      "7630 Training Loss: tensor(0.3470)\n",
      "7631 Training Loss: tensor(0.3464)\n",
      "7632 Training Loss: tensor(0.3564)\n",
      "7633 Training Loss: tensor(0.3462)\n",
      "7634 Training Loss: tensor(0.3475)\n",
      "7635 Training Loss: tensor(0.3463)\n",
      "7636 Training Loss: tensor(0.3467)\n",
      "7637 Training Loss: tensor(0.3516)\n",
      "7638 Training Loss: tensor(0.3537)\n",
      "7639 Training Loss: tensor(0.3472)\n",
      "7640 Training Loss: tensor(0.3526)\n",
      "7641 Training Loss: tensor(0.3475)\n",
      "7642 Training Loss: tensor(0.3469)\n",
      "7643 Training Loss: tensor(0.3467)\n",
      "7644 Training Loss: tensor(0.3482)\n",
      "7645 Training Loss: tensor(0.3460)\n",
      "7646 Training Loss: tensor(0.3477)\n",
      "7647 Training Loss: tensor(0.3504)\n",
      "7648 Training Loss: tensor(0.3472)\n",
      "7649 Training Loss: tensor(0.3458)\n",
      "7650 Training Loss: tensor(0.3506)\n",
      "7651 Training Loss: tensor(0.3471)\n",
      "7652 Training Loss: tensor(0.3483)\n",
      "7653 Training Loss: tensor(0.3510)\n",
      "7654 Training Loss: tensor(0.3458)\n",
      "7655 Training Loss: tensor(0.3480)\n",
      "7656 Training Loss: tensor(0.3494)\n",
      "7657 Training Loss: tensor(0.3486)\n",
      "7658 Training Loss: tensor(0.3484)\n",
      "7659 Training Loss: tensor(0.3491)\n",
      "7660 Training Loss: tensor(0.3530)\n",
      "7661 Training Loss: tensor(0.3504)\n",
      "7662 Training Loss: tensor(0.3482)\n",
      "7663 Training Loss: tensor(0.3483)\n",
      "7664 Training Loss: tensor(0.3468)\n",
      "7665 Training Loss: tensor(0.3482)\n",
      "7666 Training Loss: tensor(0.3480)\n",
      "7667 Training Loss: tensor(0.3466)\n",
      "7668 Training Loss: tensor(0.3461)\n",
      "7669 Training Loss: tensor(0.3464)\n",
      "7670 Training Loss: tensor(0.3479)\n",
      "7671 Training Loss: tensor(0.3460)\n",
      "7672 Training Loss: tensor(0.3455)\n",
      "7673 Training Loss: tensor(0.3475)\n",
      "7674 Training Loss: tensor(0.3485)\n",
      "7675 Training Loss: tensor(0.3460)\n",
      "7676 Training Loss: tensor(0.3559)\n",
      "7677 Training Loss: tensor(0.3465)\n",
      "7678 Training Loss: tensor(0.3467)\n",
      "7679 Training Loss: tensor(0.3466)\n",
      "7680 Training Loss: tensor(0.3459)\n",
      "7681 Training Loss: tensor(0.3468)\n",
      "7682 Training Loss: tensor(0.3493)\n",
      "7683 Training Loss: tensor(0.3458)\n",
      "7684 Training Loss: tensor(0.3470)\n",
      "7685 Training Loss: tensor(0.3471)\n",
      "7686 Training Loss: tensor(0.3467)\n",
      "7687 Training Loss: tensor(0.3465)\n",
      "7688 Training Loss: tensor(0.3481)\n",
      "7689 Training Loss: tensor(0.3544)\n",
      "7690 Training Loss: tensor(0.3460)\n",
      "7691 Training Loss: tensor(0.3468)\n",
      "7692 Training Loss: tensor(0.3496)\n",
      "7693 Training Loss: tensor(0.3451)\n",
      "7694 Training Loss: tensor(0.3547)\n",
      "7695 Training Loss: tensor(0.3537)\n",
      "7696 Training Loss: tensor(0.3482)\n",
      "7697 Training Loss: tensor(0.3518)\n",
      "7698 Training Loss: tensor(0.3481)\n",
      "7699 Training Loss: tensor(0.3510)\n",
      "7700 Training Loss: tensor(0.3484)\n",
      "7701 Training Loss: tensor(0.3505)\n",
      "7702 Training Loss: tensor(0.3468)\n",
      "7703 Training Loss: tensor(0.3493)\n",
      "7704 Training Loss: tensor(0.3491)\n",
      "7705 Training Loss: tensor(0.3478)\n",
      "7706 Training Loss: tensor(0.3471)\n",
      "7707 Training Loss: tensor(0.3484)\n",
      "7708 Training Loss: tensor(0.3468)\n",
      "7709 Training Loss: tensor(0.3469)\n",
      "7710 Training Loss: tensor(0.3562)\n",
      "7711 Training Loss: tensor(0.3485)\n",
      "7712 Training Loss: tensor(0.3467)\n",
      "7713 Training Loss: tensor(0.3490)\n",
      "7714 Training Loss: tensor(0.3527)\n",
      "7715 Training Loss: tensor(0.3481)\n",
      "7716 Training Loss: tensor(0.3477)\n",
      "7717 Training Loss: tensor(0.3469)\n",
      "7718 Training Loss: tensor(0.3508)\n",
      "7719 Training Loss: tensor(0.3506)\n",
      "7720 Training Loss: tensor(0.3461)\n",
      "7721 Training Loss: tensor(0.3467)\n",
      "7722 Training Loss: tensor(0.3474)\n",
      "7723 Training Loss: tensor(0.3467)\n",
      "7724 Training Loss: tensor(0.3479)\n",
      "7725 Training Loss: tensor(0.3477)\n",
      "7726 Training Loss: tensor(0.3518)\n",
      "7727 Training Loss: tensor(0.3463)\n",
      "7728 Training Loss: tensor(0.3473)\n",
      "7729 Training Loss: tensor(0.3499)\n",
      "7730 Training Loss: tensor(0.3561)\n",
      "7731 Training Loss: tensor(0.3465)\n",
      "7732 Training Loss: tensor(0.3474)\n",
      "7733 Training Loss: tensor(0.3458)\n",
      "7734 Training Loss: tensor(0.3475)\n",
      "7735 Training Loss: tensor(0.3493)\n",
      "7736 Training Loss: tensor(0.3501)\n",
      "7737 Training Loss: tensor(0.3473)\n",
      "7738 Training Loss: tensor(0.3481)\n",
      "7739 Training Loss: tensor(0.3463)\n",
      "7740 Training Loss: tensor(0.3462)\n",
      "7741 Training Loss: tensor(0.3522)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7742 Training Loss: tensor(0.3462)\n",
      "7743 Training Loss: tensor(0.3461)\n",
      "7744 Training Loss: tensor(0.3467)\n",
      "7745 Training Loss: tensor(0.3486)\n",
      "7746 Training Loss: tensor(0.3485)\n",
      "7747 Training Loss: tensor(0.3485)\n",
      "7748 Training Loss: tensor(0.3466)\n",
      "7749 Training Loss: tensor(0.3473)\n",
      "7750 Training Loss: tensor(0.3464)\n",
      "7751 Training Loss: tensor(0.3491)\n",
      "7752 Training Loss: tensor(0.3495)\n",
      "7753 Training Loss: tensor(0.3511)\n",
      "7754 Training Loss: tensor(0.3457)\n",
      "7755 Training Loss: tensor(0.3469)\n",
      "7756 Training Loss: tensor(0.3520)\n",
      "7757 Training Loss: tensor(0.3511)\n",
      "7758 Training Loss: tensor(0.3458)\n",
      "7759 Training Loss: tensor(0.3465)\n",
      "7760 Training Loss: tensor(0.3468)\n",
      "7761 Training Loss: tensor(0.3461)\n",
      "7762 Training Loss: tensor(0.3470)\n",
      "7763 Training Loss: tensor(0.3477)\n",
      "7764 Training Loss: tensor(0.3461)\n",
      "7765 Training Loss: tensor(0.3475)\n",
      "7766 Training Loss: tensor(0.3484)\n",
      "7767 Training Loss: tensor(0.3473)\n",
      "7768 Training Loss: tensor(0.3464)\n",
      "7769 Training Loss: tensor(0.3455)\n",
      "7770 Training Loss: tensor(0.3556)\n",
      "7771 Training Loss: tensor(0.3469)\n",
      "7772 Training Loss: tensor(0.3451)\n",
      "7773 Training Loss: tensor(0.3469)\n",
      "7774 Training Loss: tensor(0.3575)\n",
      "7775 Training Loss: tensor(0.3462)\n",
      "7776 Training Loss: tensor(0.3553)\n",
      "7777 Training Loss: tensor(0.3489)\n",
      "7778 Training Loss: tensor(0.3490)\n",
      "7779 Training Loss: tensor(0.3506)\n",
      "7780 Training Loss: tensor(0.3476)\n",
      "7781 Training Loss: tensor(0.3475)\n",
      "7782 Training Loss: tensor(0.3477)\n",
      "7783 Training Loss: tensor(0.3491)\n",
      "7784 Training Loss: tensor(0.3481)\n",
      "7785 Training Loss: tensor(0.3476)\n",
      "7786 Training Loss: tensor(0.3501)\n",
      "7787 Training Loss: tensor(0.3513)\n",
      "7788 Training Loss: tensor(0.3462)\n",
      "7789 Training Loss: tensor(0.3474)\n",
      "7790 Training Loss: tensor(0.3488)\n",
      "7791 Training Loss: tensor(0.3473)\n",
      "7792 Training Loss: tensor(0.3468)\n",
      "7793 Training Loss: tensor(0.3473)\n",
      "7794 Training Loss: tensor(0.3508)\n",
      "7795 Training Loss: tensor(0.3477)\n",
      "7796 Training Loss: tensor(0.3520)\n",
      "7797 Training Loss: tensor(0.3469)\n",
      "7798 Training Loss: tensor(0.3472)\n",
      "7799 Training Loss: tensor(0.3472)\n",
      "7800 Training Loss: tensor(0.3498)\n",
      "7801 Training Loss: tensor(0.3459)\n",
      "7802 Training Loss: tensor(0.3501)\n",
      "7803 Training Loss: tensor(0.3466)\n",
      "7804 Training Loss: tensor(0.3489)\n",
      "7805 Training Loss: tensor(0.3481)\n",
      "7806 Training Loss: tensor(0.3469)\n",
      "7807 Training Loss: tensor(0.3471)\n",
      "7808 Training Loss: tensor(0.3462)\n",
      "7809 Training Loss: tensor(0.3490)\n",
      "7810 Training Loss: tensor(0.3460)\n",
      "7811 Training Loss: tensor(0.3461)\n",
      "7812 Training Loss: tensor(0.3465)\n",
      "7813 Training Loss: tensor(0.3474)\n",
      "7814 Training Loss: tensor(0.3501)\n",
      "7815 Training Loss: tensor(0.3487)\n",
      "7816 Training Loss: tensor(0.3463)\n",
      "7817 Training Loss: tensor(0.3489)\n",
      "7818 Training Loss: tensor(0.3459)\n",
      "7819 Training Loss: tensor(0.3465)\n",
      "7820 Training Loss: tensor(0.3497)\n",
      "7821 Training Loss: tensor(0.3459)\n",
      "7822 Training Loss: tensor(0.3511)\n",
      "7823 Training Loss: tensor(0.3528)\n",
      "7824 Training Loss: tensor(0.3469)\n",
      "7825 Training Loss: tensor(0.3475)\n",
      "7826 Training Loss: tensor(0.3471)\n",
      "7827 Training Loss: tensor(0.3472)\n",
      "7828 Training Loss: tensor(0.3458)\n",
      "7829 Training Loss: tensor(0.3481)\n",
      "7830 Training Loss: tensor(0.3463)\n",
      "7831 Training Loss: tensor(0.3489)\n",
      "7832 Training Loss: tensor(0.3478)\n",
      "7833 Training Loss: tensor(0.3466)\n",
      "7834 Training Loss: tensor(0.3458)\n",
      "7835 Training Loss: tensor(0.3496)\n",
      "7836 Training Loss: tensor(0.3459)\n",
      "7837 Training Loss: tensor(0.3484)\n",
      "7838 Training Loss: tensor(0.3599)\n",
      "7839 Training Loss: tensor(0.3462)\n",
      "7840 Training Loss: tensor(0.3519)\n",
      "7841 Training Loss: tensor(0.3513)\n",
      "7842 Training Loss: tensor(0.3468)\n",
      "7843 Training Loss: tensor(0.3469)\n",
      "7844 Training Loss: tensor(0.3494)\n",
      "7845 Training Loss: tensor(0.3476)\n",
      "7846 Training Loss: tensor(0.3466)\n",
      "7847 Training Loss: tensor(0.3489)\n",
      "7848 Training Loss: tensor(0.3499)\n",
      "7849 Training Loss: tensor(0.3476)\n",
      "7850 Training Loss: tensor(0.3479)\n",
      "7851 Training Loss: tensor(0.3472)\n",
      "7852 Training Loss: tensor(0.3454)\n",
      "7853 Training Loss: tensor(0.3464)\n",
      "7854 Training Loss: tensor(0.3522)\n",
      "7855 Training Loss: tensor(0.3476)\n",
      "7856 Training Loss: tensor(0.3473)\n",
      "7857 Training Loss: tensor(0.3470)\n",
      "7858 Training Loss: tensor(0.3466)\n",
      "7859 Training Loss: tensor(0.3477)\n",
      "7860 Training Loss: tensor(0.3463)\n",
      "7861 Training Loss: tensor(0.3500)\n",
      "7862 Training Loss: tensor(0.3468)\n",
      "7863 Training Loss: tensor(0.3536)\n",
      "7864 Training Loss: tensor(0.3463)\n",
      "7865 Training Loss: tensor(0.3460)\n",
      "7866 Training Loss: tensor(0.3536)\n",
      "7867 Training Loss: tensor(0.3464)\n",
      "7868 Training Loss: tensor(0.3479)\n",
      "7869 Training Loss: tensor(0.3555)\n",
      "7870 Training Loss: tensor(0.3491)\n",
      "7871 Training Loss: tensor(0.3483)\n",
      "7872 Training Loss: tensor(0.3474)\n",
      "7873 Training Loss: tensor(0.3465)\n",
      "7874 Training Loss: tensor(0.3468)\n",
      "7875 Training Loss: tensor(0.3475)\n",
      "7876 Training Loss: tensor(0.3504)\n",
      "7877 Training Loss: tensor(0.3496)\n",
      "7878 Training Loss: tensor(0.3498)\n",
      "7879 Training Loss: tensor(0.3478)\n",
      "7880 Training Loss: tensor(0.3480)\n",
      "7881 Training Loss: tensor(0.3520)\n",
      "7882 Training Loss: tensor(0.3465)\n",
      "7883 Training Loss: tensor(0.3481)\n",
      "7884 Training Loss: tensor(0.3472)\n",
      "7885 Training Loss: tensor(0.3475)\n",
      "7886 Training Loss: tensor(0.3550)\n",
      "7887 Training Loss: tensor(0.3466)\n",
      "7888 Training Loss: tensor(0.3470)\n",
      "7889 Training Loss: tensor(0.3551)\n",
      "7890 Training Loss: tensor(0.3469)\n",
      "7891 Training Loss: tensor(0.3521)\n",
      "7892 Training Loss: tensor(0.3476)\n",
      "7893 Training Loss: tensor(0.3485)\n",
      "7894 Training Loss: tensor(0.3491)\n",
      "7895 Training Loss: tensor(0.3463)\n",
      "7896 Training Loss: tensor(0.3498)\n",
      "7897 Training Loss: tensor(0.3508)\n",
      "7898 Training Loss: tensor(0.3475)\n",
      "7899 Training Loss: tensor(0.3477)\n",
      "7900 Training Loss: tensor(0.3487)\n",
      "7901 Training Loss: tensor(0.3481)\n",
      "7902 Training Loss: tensor(0.3473)\n",
      "7903 Training Loss: tensor(0.3461)\n",
      "7904 Training Loss: tensor(0.3478)\n",
      "7905 Training Loss: tensor(0.3473)\n",
      "7906 Training Loss: tensor(0.3470)\n",
      "7907 Training Loss: tensor(0.3553)\n",
      "7908 Training Loss: tensor(0.3533)\n",
      "7909 Training Loss: tensor(0.3460)\n",
      "7910 Training Loss: tensor(0.3547)\n",
      "7911 Training Loss: tensor(0.3470)\n",
      "7912 Training Loss: tensor(0.3500)\n",
      "7913 Training Loss: tensor(0.3511)\n",
      "7914 Training Loss: tensor(0.3488)\n",
      "7915 Training Loss: tensor(0.3484)\n",
      "7916 Training Loss: tensor(0.3470)\n",
      "7917 Training Loss: tensor(0.3477)\n",
      "7918 Training Loss: tensor(0.3473)\n",
      "7919 Training Loss: tensor(0.3510)\n",
      "7920 Training Loss: tensor(0.3470)\n",
      "7921 Training Loss: tensor(0.3482)\n",
      "7922 Training Loss: tensor(0.3469)\n",
      "7923 Training Loss: tensor(0.3503)\n",
      "7924 Training Loss: tensor(0.3487)\n",
      "7925 Training Loss: tensor(0.3468)\n",
      "7926 Training Loss: tensor(0.3533)\n",
      "7927 Training Loss: tensor(0.3516)\n",
      "7928 Training Loss: tensor(0.3468)\n",
      "7929 Training Loss: tensor(0.3475)\n",
      "7930 Training Loss: tensor(0.3462)\n",
      "7931 Training Loss: tensor(0.3508)\n",
      "7932 Training Loss: tensor(0.3472)\n",
      "7933 Training Loss: tensor(0.3508)\n",
      "7934 Training Loss: tensor(0.3460)\n",
      "7935 Training Loss: tensor(0.3471)\n",
      "7936 Training Loss: tensor(0.3506)\n",
      "7937 Training Loss: tensor(0.3469)\n",
      "7938 Training Loss: tensor(0.3517)\n",
      "7939 Training Loss: tensor(0.3489)\n",
      "7940 Training Loss: tensor(0.3469)\n",
      "7941 Training Loss: tensor(0.3471)\n",
      "7942 Training Loss: tensor(0.3470)\n",
      "7943 Training Loss: tensor(0.3481)\n",
      "7944 Training Loss: tensor(0.3474)\n",
      "7945 Training Loss: tensor(0.3491)\n",
      "7946 Training Loss: tensor(0.3473)\n",
      "7947 Training Loss: tensor(0.3473)\n",
      "7948 Training Loss: tensor(0.3470)\n",
      "7949 Training Loss: tensor(0.3458)\n",
      "7950 Training Loss: tensor(0.3477)\n",
      "7951 Training Loss: tensor(0.3461)\n",
      "7952 Training Loss: tensor(0.3467)\n",
      "7953 Training Loss: tensor(0.3457)\n",
      "7954 Training Loss: tensor(0.3475)\n",
      "7955 Training Loss: tensor(0.3631)\n",
      "7956 Training Loss: tensor(0.3517)\n",
      "7957 Training Loss: tensor(0.3460)\n",
      "7958 Training Loss: tensor(0.3470)\n",
      "7959 Training Loss: tensor(0.3466)\n",
      "7960 Training Loss: tensor(0.3469)\n",
      "7961 Training Loss: tensor(0.3478)\n",
      "7962 Training Loss: tensor(0.3468)\n",
      "7963 Training Loss: tensor(0.3503)\n",
      "7964 Training Loss: tensor(0.3486)\n",
      "7965 Training Loss: tensor(0.3466)\n",
      "7966 Training Loss: tensor(0.3515)\n",
      "7967 Training Loss: tensor(0.3486)\n",
      "7968 Training Loss: tensor(0.3474)\n",
      "7969 Training Loss: tensor(0.3496)\n",
      "7970 Training Loss: tensor(0.3485)\n",
      "7971 Training Loss: tensor(0.3514)\n",
      "7972 Training Loss: tensor(0.3473)\n",
      "7973 Training Loss: tensor(0.3472)\n",
      "7974 Training Loss: tensor(0.3527)\n",
      "7975 Training Loss: tensor(0.3480)\n",
      "7976 Training Loss: tensor(0.3473)\n",
      "7977 Training Loss: tensor(0.3470)\n",
      "7978 Training Loss: tensor(0.3466)\n",
      "7979 Training Loss: tensor(0.3476)\n",
      "7980 Training Loss: tensor(0.3510)\n",
      "7981 Training Loss: tensor(0.3468)\n",
      "7982 Training Loss: tensor(0.3527)\n",
      "7983 Training Loss: tensor(0.3483)\n",
      "7984 Training Loss: tensor(0.3507)\n",
      "7985 Training Loss: tensor(0.3494)\n",
      "7986 Training Loss: tensor(0.3496)\n",
      "7987 Training Loss: tensor(0.3475)\n",
      "7988 Training Loss: tensor(0.3484)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7989 Training Loss: tensor(0.3464)\n",
      "7990 Training Loss: tensor(0.3470)\n",
      "7991 Training Loss: tensor(0.3480)\n",
      "7992 Training Loss: tensor(0.3470)\n",
      "7993 Training Loss: tensor(0.3467)\n",
      "7994 Training Loss: tensor(0.3501)\n",
      "7995 Training Loss: tensor(0.3479)\n",
      "7996 Training Loss: tensor(0.3545)\n",
      "7997 Training Loss: tensor(0.3478)\n",
      "7998 Training Loss: tensor(0.3468)\n",
      "7999 Training Loss: tensor(0.3505)\n",
      "8000 Training Loss: tensor(0.3483)\n",
      "8001 Training Loss: tensor(0.3461)\n",
      "8002 Training Loss: tensor(0.3470)\n",
      "8003 Training Loss: tensor(0.3484)\n",
      "8004 Training Loss: tensor(0.3476)\n",
      "8005 Training Loss: tensor(0.3475)\n",
      "8006 Training Loss: tensor(0.3482)\n",
      "8007 Training Loss: tensor(0.3498)\n",
      "8008 Training Loss: tensor(0.3534)\n",
      "8009 Training Loss: tensor(0.3495)\n",
      "8010 Training Loss: tensor(0.3511)\n",
      "8011 Training Loss: tensor(0.3498)\n",
      "8012 Training Loss: tensor(0.3472)\n",
      "8013 Training Loss: tensor(0.3533)\n",
      "8014 Training Loss: tensor(0.3511)\n",
      "8015 Training Loss: tensor(0.3478)\n",
      "8016 Training Loss: tensor(0.3476)\n",
      "8017 Training Loss: tensor(0.3482)\n",
      "8018 Training Loss: tensor(0.3487)\n",
      "8019 Training Loss: tensor(0.3482)\n",
      "8020 Training Loss: tensor(0.3476)\n",
      "8021 Training Loss: tensor(0.3467)\n",
      "8022 Training Loss: tensor(0.3495)\n",
      "8023 Training Loss: tensor(0.3469)\n",
      "8024 Training Loss: tensor(0.3475)\n",
      "8025 Training Loss: tensor(0.3482)\n",
      "8026 Training Loss: tensor(0.3532)\n",
      "8027 Training Loss: tensor(0.3506)\n",
      "8028 Training Loss: tensor(0.3469)\n",
      "8029 Training Loss: tensor(0.3522)\n",
      "8030 Training Loss: tensor(0.3478)\n",
      "8031 Training Loss: tensor(0.3505)\n",
      "8032 Training Loss: tensor(0.3483)\n",
      "8033 Training Loss: tensor(0.3498)\n",
      "8034 Training Loss: tensor(0.3469)\n",
      "8035 Training Loss: tensor(0.3476)\n",
      "8036 Training Loss: tensor(0.3473)\n",
      "8037 Training Loss: tensor(0.3525)\n",
      "8038 Training Loss: tensor(0.3475)\n",
      "8039 Training Loss: tensor(0.3470)\n",
      "8040 Training Loss: tensor(0.3468)\n",
      "8041 Training Loss: tensor(0.3469)\n",
      "8042 Training Loss: tensor(0.3476)\n",
      "8043 Training Loss: tensor(0.3480)\n",
      "8044 Training Loss: tensor(0.3487)\n",
      "8045 Training Loss: tensor(0.3477)\n",
      "8046 Training Loss: tensor(0.3490)\n",
      "8047 Training Loss: tensor(0.3467)\n",
      "8048 Training Loss: tensor(0.3468)\n",
      "8049 Training Loss: tensor(0.3496)\n",
      "8050 Training Loss: tensor(0.3492)\n",
      "8051 Training Loss: tensor(0.3472)\n",
      "8052 Training Loss: tensor(0.3460)\n",
      "8053 Training Loss: tensor(0.3467)\n",
      "8054 Training Loss: tensor(0.3542)\n",
      "8055 Training Loss: tensor(0.3461)\n",
      "8056 Training Loss: tensor(0.3463)\n",
      "8057 Training Loss: tensor(0.3473)\n",
      "8058 Training Loss: tensor(0.3476)\n",
      "8059 Training Loss: tensor(0.3461)\n",
      "8060 Training Loss: tensor(0.3471)\n",
      "8061 Training Loss: tensor(0.3478)\n",
      "8062 Training Loss: tensor(0.3467)\n",
      "8063 Training Loss: tensor(0.3461)\n",
      "8064 Training Loss: tensor(0.3501)\n",
      "8065 Training Loss: tensor(0.3467)\n",
      "8066 Training Loss: tensor(0.3473)\n",
      "8067 Training Loss: tensor(0.3468)\n",
      "8068 Training Loss: tensor(0.3478)\n",
      "8069 Training Loss: tensor(0.3465)\n",
      "8070 Training Loss: tensor(0.3511)\n",
      "8071 Training Loss: tensor(0.3460)\n",
      "8072 Training Loss: tensor(0.3462)\n",
      "8073 Training Loss: tensor(0.3474)\n",
      "8074 Training Loss: tensor(0.3488)\n",
      "8075 Training Loss: tensor(0.3460)\n",
      "8076 Training Loss: tensor(0.3491)\n",
      "8077 Training Loss: tensor(0.3479)\n",
      "8078 Training Loss: tensor(0.3463)\n",
      "8079 Training Loss: tensor(0.3578)\n",
      "8080 Training Loss: tensor(0.3466)\n",
      "8081 Training Loss: tensor(0.3479)\n",
      "8082 Training Loss: tensor(0.3476)\n",
      "8083 Training Loss: tensor(0.3472)\n",
      "8084 Training Loss: tensor(0.3474)\n",
      "8085 Training Loss: tensor(0.3494)\n",
      "8086 Training Loss: tensor(0.3470)\n",
      "8087 Training Loss: tensor(0.3457)\n",
      "8088 Training Loss: tensor(0.3480)\n",
      "8089 Training Loss: tensor(0.3471)\n",
      "8090 Training Loss: tensor(0.3519)\n",
      "8091 Training Loss: tensor(0.3467)\n",
      "8092 Training Loss: tensor(0.3494)\n",
      "8093 Training Loss: tensor(0.3526)\n",
      "8094 Training Loss: tensor(0.3467)\n",
      "8095 Training Loss: tensor(0.3542)\n",
      "8096 Training Loss: tensor(0.3469)\n",
      "8097 Training Loss: tensor(0.3482)\n",
      "8098 Training Loss: tensor(0.3501)\n",
      "8099 Training Loss: tensor(0.3469)\n",
      "8100 Training Loss: tensor(0.3482)\n",
      "8101 Training Loss: tensor(0.3463)\n",
      "8102 Training Loss: tensor(0.3488)\n",
      "8103 Training Loss: tensor(0.3474)\n",
      "8104 Training Loss: tensor(0.3481)\n",
      "8105 Training Loss: tensor(0.3474)\n",
      "8106 Training Loss: tensor(0.3474)\n",
      "8107 Training Loss: tensor(0.3477)\n",
      "8108 Training Loss: tensor(0.3471)\n",
      "8109 Training Loss: tensor(0.3476)\n",
      "8110 Training Loss: tensor(0.3497)\n",
      "8111 Training Loss: tensor(0.3482)\n",
      "8112 Training Loss: tensor(0.3466)\n",
      "8113 Training Loss: tensor(0.3482)\n",
      "8114 Training Loss: tensor(0.3468)\n",
      "8115 Training Loss: tensor(0.3477)\n",
      "8116 Training Loss: tensor(0.3479)\n",
      "8117 Training Loss: tensor(0.3505)\n",
      "8118 Training Loss: tensor(0.3583)\n",
      "8119 Training Loss: tensor(0.3492)\n",
      "8120 Training Loss: tensor(0.3497)\n",
      "8121 Training Loss: tensor(0.3474)\n",
      "8122 Training Loss: tensor(0.3463)\n",
      "8123 Training Loss: tensor(0.3477)\n",
      "8124 Training Loss: tensor(0.3463)\n",
      "8125 Training Loss: tensor(0.3530)\n",
      "8126 Training Loss: tensor(0.3489)\n",
      "8127 Training Loss: tensor(0.3508)\n",
      "8128 Training Loss: tensor(0.3454)\n",
      "8129 Training Loss: tensor(0.3469)\n",
      "8130 Training Loss: tensor(0.3477)\n",
      "8131 Training Loss: tensor(0.3490)\n",
      "8132 Training Loss: tensor(0.3487)\n",
      "8133 Training Loss: tensor(0.3463)\n",
      "8134 Training Loss: tensor(0.3468)\n",
      "8135 Training Loss: tensor(0.3472)\n",
      "8136 Training Loss: tensor(0.3488)\n",
      "8137 Training Loss: tensor(0.3485)\n",
      "8138 Training Loss: tensor(0.3531)\n",
      "8139 Training Loss: tensor(0.3489)\n",
      "8140 Training Loss: tensor(0.3486)\n",
      "8141 Training Loss: tensor(0.3504)\n",
      "8142 Training Loss: tensor(0.3468)\n",
      "8143 Training Loss: tensor(0.3473)\n",
      "8144 Training Loss: tensor(0.3462)\n",
      "8145 Training Loss: tensor(0.3500)\n",
      "8146 Training Loss: tensor(0.3475)\n",
      "8147 Training Loss: tensor(0.3468)\n",
      "8148 Training Loss: tensor(0.3499)\n",
      "8149 Training Loss: tensor(0.3476)\n",
      "8150 Training Loss: tensor(0.3462)\n",
      "8151 Training Loss: tensor(0.3470)\n",
      "8152 Training Loss: tensor(0.3510)\n",
      "8153 Training Loss: tensor(0.3464)\n",
      "8154 Training Loss: tensor(0.3469)\n",
      "8155 Training Loss: tensor(0.3539)\n",
      "8156 Training Loss: tensor(0.3462)\n",
      "8157 Training Loss: tensor(0.3493)\n",
      "8158 Training Loss: tensor(0.3520)\n",
      "8159 Training Loss: tensor(0.3476)\n",
      "8160 Training Loss: tensor(0.3496)\n",
      "8161 Training Loss: tensor(0.3476)\n",
      "8162 Training Loss: tensor(0.3473)\n",
      "8163 Training Loss: tensor(0.3468)\n",
      "8164 Training Loss: tensor(0.3473)\n",
      "8165 Training Loss: tensor(0.3481)\n",
      "8166 Training Loss: tensor(0.3479)\n",
      "8167 Training Loss: tensor(0.3523)\n",
      "8168 Training Loss: tensor(0.3473)\n",
      "8169 Training Loss: tensor(0.3472)\n",
      "8170 Training Loss: tensor(0.3455)\n",
      "8171 Training Loss: tensor(0.3472)\n",
      "8172 Training Loss: tensor(0.3456)\n",
      "8173 Training Loss: tensor(0.3532)\n",
      "8174 Training Loss: tensor(0.3476)\n",
      "8175 Training Loss: tensor(0.3513)\n",
      "8176 Training Loss: tensor(0.3476)\n",
      "8177 Training Loss: tensor(0.3494)\n",
      "8178 Training Loss: tensor(0.3464)\n",
      "8179 Training Loss: tensor(0.3457)\n",
      "8180 Training Loss: tensor(0.3464)\n",
      "8181 Training Loss: tensor(0.3459)\n",
      "8182 Training Loss: tensor(0.3468)\n",
      "8183 Training Loss: tensor(0.3459)\n",
      "8184 Training Loss: tensor(0.3474)\n",
      "8185 Training Loss: tensor(0.3469)\n",
      "8186 Training Loss: tensor(0.3478)\n",
      "8187 Training Loss: tensor(0.3468)\n",
      "8188 Training Loss: tensor(0.3600)\n",
      "8189 Training Loss: tensor(0.3469)\n",
      "8190 Training Loss: tensor(0.3467)\n",
      "8191 Training Loss: tensor(0.3482)\n",
      "8192 Training Loss: tensor(0.3470)\n",
      "8193 Training Loss: tensor(0.3479)\n",
      "8194 Training Loss: tensor(0.3480)\n",
      "8195 Training Loss: tensor(0.3480)\n",
      "8196 Training Loss: tensor(0.3472)\n",
      "8197 Training Loss: tensor(0.3454)\n",
      "8198 Training Loss: tensor(0.3552)\n",
      "8199 Training Loss: tensor(0.3464)\n",
      "8200 Training Loss: tensor(0.3472)\n",
      "8201 Training Loss: tensor(0.3453)\n",
      "8202 Training Loss: tensor(0.3471)\n",
      "8203 Training Loss: tensor(0.3469)\n",
      "8204 Training Loss: tensor(0.3464)\n",
      "8205 Training Loss: tensor(0.3488)\n",
      "8206 Training Loss: tensor(0.3463)\n",
      "8207 Training Loss: tensor(0.3461)\n",
      "8208 Training Loss: tensor(0.3480)\n",
      "8209 Training Loss: tensor(0.3498)\n",
      "8210 Training Loss: tensor(0.3478)\n",
      "8211 Training Loss: tensor(0.3471)\n",
      "8212 Training Loss: tensor(0.3467)\n",
      "8213 Training Loss: tensor(0.3558)\n",
      "8214 Training Loss: tensor(0.3497)\n",
      "8215 Training Loss: tensor(0.3459)\n",
      "8216 Training Loss: tensor(0.3462)\n",
      "8217 Training Loss: tensor(0.3538)\n",
      "8218 Training Loss: tensor(0.3505)\n",
      "8219 Training Loss: tensor(0.3499)\n",
      "8220 Training Loss: tensor(0.3467)\n",
      "8221 Training Loss: tensor(0.3478)\n",
      "8222 Training Loss: tensor(0.3473)\n",
      "8223 Training Loss: tensor(0.3505)\n",
      "8224 Training Loss: tensor(0.3482)\n",
      "8225 Training Loss: tensor(0.3482)\n",
      "8226 Training Loss: tensor(0.3475)\n",
      "8227 Training Loss: tensor(0.3481)\n",
      "8228 Training Loss: tensor(0.3506)\n",
      "8229 Training Loss: tensor(0.3519)\n",
      "8230 Training Loss: tensor(0.3498)\n",
      "8231 Training Loss: tensor(0.3468)\n",
      "8232 Training Loss: tensor(0.3485)\n",
      "8233 Training Loss: tensor(0.3474)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8234 Training Loss: tensor(0.3492)\n",
      "8235 Training Loss: tensor(0.3479)\n",
      "8236 Training Loss: tensor(0.3503)\n",
      "8237 Training Loss: tensor(0.3529)\n",
      "8238 Training Loss: tensor(0.3466)\n",
      "8239 Training Loss: tensor(0.3489)\n",
      "8240 Training Loss: tensor(0.3464)\n",
      "8241 Training Loss: tensor(0.3472)\n",
      "8242 Training Loss: tensor(0.3528)\n",
      "8243 Training Loss: tensor(0.3492)\n",
      "8244 Training Loss: tensor(0.3466)\n",
      "8245 Training Loss: tensor(0.3511)\n",
      "8246 Training Loss: tensor(0.3467)\n",
      "8247 Training Loss: tensor(0.3480)\n",
      "8248 Training Loss: tensor(0.3502)\n",
      "8249 Training Loss: tensor(0.3464)\n",
      "8250 Training Loss: tensor(0.3467)\n",
      "8251 Training Loss: tensor(0.3469)\n",
      "8252 Training Loss: tensor(0.3498)\n",
      "8253 Training Loss: tensor(0.3464)\n",
      "8254 Training Loss: tensor(0.3477)\n",
      "8255 Training Loss: tensor(0.3483)\n",
      "8256 Training Loss: tensor(0.3468)\n",
      "8257 Training Loss: tensor(0.3465)\n",
      "8258 Training Loss: tensor(0.3479)\n",
      "8259 Training Loss: tensor(0.3513)\n",
      "8260 Training Loss: tensor(0.3496)\n",
      "8261 Training Loss: tensor(0.3491)\n",
      "8262 Training Loss: tensor(0.3454)\n",
      "8263 Training Loss: tensor(0.3461)\n",
      "8264 Training Loss: tensor(0.3475)\n",
      "8265 Training Loss: tensor(0.3459)\n",
      "8266 Training Loss: tensor(0.3514)\n",
      "8267 Training Loss: tensor(0.3470)\n",
      "8268 Training Loss: tensor(0.3543)\n",
      "8269 Training Loss: tensor(0.3486)\n",
      "8270 Training Loss: tensor(0.3491)\n",
      "8271 Training Loss: tensor(0.3525)\n",
      "8272 Training Loss: tensor(0.3477)\n",
      "8273 Training Loss: tensor(0.3489)\n",
      "8274 Training Loss: tensor(0.3490)\n",
      "8275 Training Loss: tensor(0.3483)\n",
      "8276 Training Loss: tensor(0.3491)\n",
      "8277 Training Loss: tensor(0.3489)\n",
      "8278 Training Loss: tensor(0.3484)\n",
      "8279 Training Loss: tensor(0.3471)\n",
      "8280 Training Loss: tensor(0.3482)\n",
      "8281 Training Loss: tensor(0.3475)\n",
      "8282 Training Loss: tensor(0.3482)\n",
      "8283 Training Loss: tensor(0.3475)\n",
      "8284 Training Loss: tensor(0.3472)\n",
      "8285 Training Loss: tensor(0.3492)\n",
      "8286 Training Loss: tensor(0.3461)\n",
      "8287 Training Loss: tensor(0.3473)\n",
      "8288 Training Loss: tensor(0.3467)\n",
      "8289 Training Loss: tensor(0.3467)\n",
      "8290 Training Loss: tensor(0.3466)\n",
      "8291 Training Loss: tensor(0.3493)\n",
      "8292 Training Loss: tensor(0.3473)\n",
      "8293 Training Loss: tensor(0.3485)\n",
      "8294 Training Loss: tensor(0.3472)\n",
      "8295 Training Loss: tensor(0.3495)\n",
      "8296 Training Loss: tensor(0.3457)\n",
      "8297 Training Loss: tensor(0.3464)\n",
      "8298 Training Loss: tensor(0.3456)\n",
      "8299 Training Loss: tensor(0.3481)\n",
      "8300 Training Loss: tensor(0.3458)\n",
      "8301 Training Loss: tensor(0.3468)\n",
      "8302 Training Loss: tensor(0.3465)\n",
      "8303 Training Loss: tensor(0.3598)\n",
      "8304 Training Loss: tensor(0.3523)\n",
      "8305 Training Loss: tensor(0.3464)\n",
      "8306 Training Loss: tensor(0.3496)\n",
      "8307 Training Loss: tensor(0.3466)\n",
      "8308 Training Loss: tensor(0.3462)\n",
      "8309 Training Loss: tensor(0.3473)\n",
      "8310 Training Loss: tensor(0.3468)\n",
      "8311 Training Loss: tensor(0.3478)\n",
      "8312 Training Loss: tensor(0.3507)\n",
      "8313 Training Loss: tensor(0.3461)\n",
      "8314 Training Loss: tensor(0.3466)\n",
      "8315 Training Loss: tensor(0.3467)\n",
      "8316 Training Loss: tensor(0.3459)\n",
      "8317 Training Loss: tensor(0.3559)\n",
      "8318 Training Loss: tensor(0.3471)\n",
      "8319 Training Loss: tensor(0.3464)\n",
      "8320 Training Loss: tensor(0.3576)\n",
      "8321 Training Loss: tensor(0.3461)\n",
      "8322 Training Loss: tensor(0.3468)\n",
      "8323 Training Loss: tensor(0.3469)\n",
      "8324 Training Loss: tensor(0.3551)\n",
      "8325 Training Loss: tensor(0.3475)\n",
      "8326 Training Loss: tensor(0.3492)\n",
      "8327 Training Loss: tensor(0.3476)\n",
      "8328 Training Loss: tensor(0.3484)\n",
      "8329 Training Loss: tensor(0.3480)\n",
      "8330 Training Loss: tensor(0.3472)\n",
      "8331 Training Loss: tensor(0.3527)\n",
      "8332 Training Loss: tensor(0.3469)\n",
      "8333 Training Loss: tensor(0.3474)\n",
      "8334 Training Loss: tensor(0.3529)\n",
      "8335 Training Loss: tensor(0.3527)\n",
      "8336 Training Loss: tensor(0.3489)\n",
      "8337 Training Loss: tensor(0.3473)\n",
      "8338 Training Loss: tensor(0.3514)\n",
      "8339 Training Loss: tensor(0.3475)\n",
      "8340 Training Loss: tensor(0.3501)\n",
      "8341 Training Loss: tensor(0.3483)\n",
      "8342 Training Loss: tensor(0.3476)\n",
      "8343 Training Loss: tensor(0.3522)\n",
      "8344 Training Loss: tensor(0.3478)\n",
      "8345 Training Loss: tensor(0.3458)\n",
      "8346 Training Loss: tensor(0.3489)\n",
      "8347 Training Loss: tensor(0.3551)\n",
      "8348 Training Loss: tensor(0.3469)\n",
      "8349 Training Loss: tensor(0.3461)\n",
      "8350 Training Loss: tensor(0.3462)\n",
      "8351 Training Loss: tensor(0.3467)\n",
      "8352 Training Loss: tensor(0.3498)\n",
      "8353 Training Loss: tensor(0.3463)\n",
      "8354 Training Loss: tensor(0.3491)\n",
      "8355 Training Loss: tensor(0.3512)\n",
      "8356 Training Loss: tensor(0.3470)\n",
      "8357 Training Loss: tensor(0.3532)\n",
      "8358 Training Loss: tensor(0.3500)\n",
      "8359 Training Loss: tensor(0.3477)\n",
      "8360 Training Loss: tensor(0.3479)\n",
      "8361 Training Loss: tensor(0.3474)\n",
      "8362 Training Loss: tensor(0.3470)\n",
      "8363 Training Loss: tensor(0.3472)\n",
      "8364 Training Loss: tensor(0.3470)\n",
      "8365 Training Loss: tensor(0.3471)\n",
      "8366 Training Loss: tensor(0.3488)\n",
      "8367 Training Loss: tensor(0.3464)\n",
      "8368 Training Loss: tensor(0.3505)\n",
      "8369 Training Loss: tensor(0.3541)\n",
      "8370 Training Loss: tensor(0.3503)\n",
      "8371 Training Loss: tensor(0.3472)\n",
      "8372 Training Loss: tensor(0.3472)\n",
      "8373 Training Loss: tensor(0.3477)\n",
      "8374 Training Loss: tensor(0.3469)\n",
      "8375 Training Loss: tensor(0.3460)\n",
      "8376 Training Loss: tensor(0.3471)\n",
      "8377 Training Loss: tensor(0.3469)\n",
      "8378 Training Loss: tensor(0.3485)\n",
      "8379 Training Loss: tensor(0.3476)\n",
      "8380 Training Loss: tensor(0.3467)\n",
      "8381 Training Loss: tensor(0.3502)\n",
      "8382 Training Loss: tensor(0.3463)\n",
      "8383 Training Loss: tensor(0.3457)\n",
      "8384 Training Loss: tensor(0.3468)\n",
      "8385 Training Loss: tensor(0.3462)\n",
      "8386 Training Loss: tensor(0.3463)\n",
      "8387 Training Loss: tensor(0.3452)\n",
      "8388 Training Loss: tensor(0.3489)\n",
      "8389 Training Loss: tensor(0.3461)\n",
      "8390 Training Loss: tensor(0.3476)\n",
      "8391 Training Loss: tensor(0.3461)\n",
      "8392 Training Loss: tensor(0.3486)\n",
      "8393 Training Loss: tensor(0.3494)\n",
      "8394 Training Loss: tensor(0.3496)\n",
      "8395 Training Loss: tensor(0.3450)\n",
      "8396 Training Loss: tensor(0.3486)\n",
      "8397 Training Loss: tensor(0.3537)\n",
      "8398 Training Loss: tensor(0.3461)\n",
      "8399 Training Loss: tensor(0.3452)\n",
      "8400 Training Loss: tensor(0.3493)\n",
      "8401 Training Loss: tensor(0.3498)\n",
      "8402 Training Loss: tensor(0.3529)\n",
      "8403 Training Loss: tensor(0.3472)\n",
      "8404 Training Loss: tensor(0.3495)\n",
      "8405 Training Loss: tensor(0.3464)\n",
      "8406 Training Loss: tensor(0.3495)\n",
      "8407 Training Loss: tensor(0.3476)\n",
      "8408 Training Loss: tensor(0.3470)\n",
      "8409 Training Loss: tensor(0.3480)\n",
      "8410 Training Loss: tensor(0.3480)\n",
      "8411 Training Loss: tensor(0.3477)\n",
      "8412 Training Loss: tensor(0.3467)\n",
      "8413 Training Loss: tensor(0.3464)\n",
      "8414 Training Loss: tensor(0.3524)\n",
      "8415 Training Loss: tensor(0.3462)\n",
      "8416 Training Loss: tensor(0.3475)\n",
      "8417 Training Loss: tensor(0.3486)\n",
      "8418 Training Loss: tensor(0.3491)\n",
      "8419 Training Loss: tensor(0.3465)\n",
      "8420 Training Loss: tensor(0.3519)\n",
      "8421 Training Loss: tensor(0.3506)\n",
      "8422 Training Loss: tensor(0.3514)\n",
      "8423 Training Loss: tensor(0.3462)\n",
      "8424 Training Loss: tensor(0.3509)\n",
      "8425 Training Loss: tensor(0.3502)\n",
      "8426 Training Loss: tensor(0.3468)\n",
      "8427 Training Loss: tensor(0.3468)\n",
      "8428 Training Loss: tensor(0.3478)\n",
      "8429 Training Loss: tensor(0.3529)\n",
      "8430 Training Loss: tensor(0.3468)\n",
      "8431 Training Loss: tensor(0.3489)\n",
      "8432 Training Loss: tensor(0.3504)\n",
      "8433 Training Loss: tensor(0.3482)\n",
      "8434 Training Loss: tensor(0.3464)\n",
      "8435 Training Loss: tensor(0.3482)\n",
      "8436 Training Loss: tensor(0.3468)\n",
      "8437 Training Loss: tensor(0.3477)\n",
      "8438 Training Loss: tensor(0.3494)\n",
      "8439 Training Loss: tensor(0.3469)\n",
      "8440 Training Loss: tensor(0.3492)\n",
      "8441 Training Loss: tensor(0.3498)\n",
      "8442 Training Loss: tensor(0.3523)\n",
      "8443 Training Loss: tensor(0.3480)\n",
      "8444 Training Loss: tensor(0.3533)\n",
      "8445 Training Loss: tensor(0.3466)\n",
      "8446 Training Loss: tensor(0.3469)\n",
      "8447 Training Loss: tensor(0.3478)\n",
      "8448 Training Loss: tensor(0.3462)\n",
      "8449 Training Loss: tensor(0.3468)\n",
      "8450 Training Loss: tensor(0.3469)\n",
      "8451 Training Loss: tensor(0.3461)\n",
      "8452 Training Loss: tensor(0.3501)\n",
      "8453 Training Loss: tensor(0.3488)\n",
      "8454 Training Loss: tensor(0.3472)\n",
      "8455 Training Loss: tensor(0.3466)\n",
      "8456 Training Loss: tensor(0.3474)\n",
      "8457 Training Loss: tensor(0.3465)\n",
      "8458 Training Loss: tensor(0.3569)\n",
      "8459 Training Loss: tensor(0.3470)\n",
      "8460 Training Loss: tensor(0.3471)\n",
      "8461 Training Loss: tensor(0.3468)\n",
      "8462 Training Loss: tensor(0.3513)\n",
      "8463 Training Loss: tensor(0.3470)\n",
      "8464 Training Loss: tensor(0.3491)\n",
      "8465 Training Loss: tensor(0.3481)\n",
      "8466 Training Loss: tensor(0.3474)\n",
      "8467 Training Loss: tensor(0.3516)\n",
      "8468 Training Loss: tensor(0.3461)\n",
      "8469 Training Loss: tensor(0.3500)\n",
      "8470 Training Loss: tensor(0.3464)\n",
      "8471 Training Loss: tensor(0.3465)\n",
      "8472 Training Loss: tensor(0.3459)\n",
      "8473 Training Loss: tensor(0.3511)\n",
      "8474 Training Loss: tensor(0.3499)\n",
      "8475 Training Loss: tensor(0.3489)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8476 Training Loss: tensor(0.3471)\n",
      "8477 Training Loss: tensor(0.3475)\n",
      "8478 Training Loss: tensor(0.3460)\n",
      "8479 Training Loss: tensor(0.3467)\n",
      "8480 Training Loss: tensor(0.3504)\n",
      "8481 Training Loss: tensor(0.3454)\n",
      "8482 Training Loss: tensor(0.3526)\n",
      "8483 Training Loss: tensor(0.3533)\n",
      "8484 Training Loss: tensor(0.3468)\n",
      "8485 Training Loss: tensor(0.3471)\n",
      "8486 Training Loss: tensor(0.3462)\n",
      "8487 Training Loss: tensor(0.3471)\n",
      "8488 Training Loss: tensor(0.3467)\n",
      "8489 Training Loss: tensor(0.3491)\n",
      "8490 Training Loss: tensor(0.3466)\n",
      "8491 Training Loss: tensor(0.3549)\n",
      "8492 Training Loss: tensor(0.3475)\n",
      "8493 Training Loss: tensor(0.3515)\n",
      "8494 Training Loss: tensor(0.3497)\n",
      "8495 Training Loss: tensor(0.3475)\n",
      "8496 Training Loss: tensor(0.3466)\n",
      "8497 Training Loss: tensor(0.3514)\n",
      "8498 Training Loss: tensor(0.3483)\n",
      "8499 Training Loss: tensor(0.3504)\n",
      "8500 Training Loss: tensor(0.3467)\n",
      "8501 Training Loss: tensor(0.3484)\n",
      "8502 Training Loss: tensor(0.3533)\n",
      "8503 Training Loss: tensor(0.3472)\n",
      "8504 Training Loss: tensor(0.3495)\n",
      "8505 Training Loss: tensor(0.3487)\n",
      "8506 Training Loss: tensor(0.3489)\n",
      "8507 Training Loss: tensor(0.3483)\n",
      "8508 Training Loss: tensor(0.3481)\n",
      "8509 Training Loss: tensor(0.3489)\n",
      "8510 Training Loss: tensor(0.3483)\n",
      "8511 Training Loss: tensor(0.3488)\n",
      "8512 Training Loss: tensor(0.3473)\n",
      "8513 Training Loss: tensor(0.3466)\n",
      "8514 Training Loss: tensor(0.3500)\n",
      "8515 Training Loss: tensor(0.3479)\n",
      "8516 Training Loss: tensor(0.3466)\n",
      "8517 Training Loss: tensor(0.3499)\n",
      "8518 Training Loss: tensor(0.3514)\n",
      "8519 Training Loss: tensor(0.3470)\n",
      "8520 Training Loss: tensor(0.3472)\n",
      "8521 Training Loss: tensor(0.3470)\n",
      "8522 Training Loss: tensor(0.3536)\n",
      "8523 Training Loss: tensor(0.3507)\n",
      "8524 Training Loss: tensor(0.3461)\n",
      "8525 Training Loss: tensor(0.3468)\n",
      "8526 Training Loss: tensor(0.3525)\n",
      "8527 Training Loss: tensor(0.3468)\n",
      "8528 Training Loss: tensor(0.3528)\n",
      "8529 Training Loss: tensor(0.3477)\n",
      "8530 Training Loss: tensor(0.3479)\n",
      "8531 Training Loss: tensor(0.3478)\n",
      "8532 Training Loss: tensor(0.3485)\n",
      "8533 Training Loss: tensor(0.3490)\n",
      "8534 Training Loss: tensor(0.3469)\n",
      "8535 Training Loss: tensor(0.3472)\n",
      "8536 Training Loss: tensor(0.3466)\n",
      "8537 Training Loss: tensor(0.3478)\n",
      "8538 Training Loss: tensor(0.3473)\n",
      "8539 Training Loss: tensor(0.3499)\n",
      "8540 Training Loss: tensor(0.3509)\n",
      "8541 Training Loss: tensor(0.3483)\n",
      "8542 Training Loss: tensor(0.3469)\n",
      "8543 Training Loss: tensor(0.3519)\n",
      "8544 Training Loss: tensor(0.3476)\n",
      "8545 Training Loss: tensor(0.3466)\n",
      "8546 Training Loss: tensor(0.3497)\n",
      "8547 Training Loss: tensor(0.3490)\n",
      "8548 Training Loss: tensor(0.3495)\n",
      "8549 Training Loss: tensor(0.3492)\n",
      "8550 Training Loss: tensor(0.3496)\n",
      "8551 Training Loss: tensor(0.3462)\n",
      "8552 Training Loss: tensor(0.3481)\n",
      "8553 Training Loss: tensor(0.3498)\n",
      "8554 Training Loss: tensor(0.3486)\n",
      "8555 Training Loss: tensor(0.3461)\n",
      "8556 Training Loss: tensor(0.3463)\n",
      "8557 Training Loss: tensor(0.3514)\n",
      "8558 Training Loss: tensor(0.3504)\n",
      "8559 Training Loss: tensor(0.3499)\n",
      "8560 Training Loss: tensor(0.3486)\n",
      "8561 Training Loss: tensor(0.3485)\n",
      "8562 Training Loss: tensor(0.3462)\n",
      "8563 Training Loss: tensor(0.3491)\n",
      "8564 Training Loss: tensor(0.3464)\n",
      "8565 Training Loss: tensor(0.3483)\n",
      "8566 Training Loss: tensor(0.3470)\n",
      "8567 Training Loss: tensor(0.3495)\n",
      "8568 Training Loss: tensor(0.3465)\n",
      "8569 Training Loss: tensor(0.3487)\n",
      "8570 Training Loss: tensor(0.3471)\n",
      "8571 Training Loss: tensor(0.3468)\n",
      "8572 Training Loss: tensor(0.3465)\n",
      "8573 Training Loss: tensor(0.3513)\n",
      "8574 Training Loss: tensor(0.3462)\n",
      "8575 Training Loss: tensor(0.3481)\n",
      "8576 Training Loss: tensor(0.3492)\n",
      "8577 Training Loss: tensor(0.3467)\n",
      "8578 Training Loss: tensor(0.3487)\n",
      "8579 Training Loss: tensor(0.3535)\n",
      "8580 Training Loss: tensor(0.3470)\n",
      "8581 Training Loss: tensor(0.3479)\n",
      "8582 Training Loss: tensor(0.3488)\n",
      "8583 Training Loss: tensor(0.3465)\n",
      "8584 Training Loss: tensor(0.3458)\n",
      "8585 Training Loss: tensor(0.3501)\n",
      "8586 Training Loss: tensor(0.3554)\n",
      "8587 Training Loss: tensor(0.3502)\n",
      "8588 Training Loss: tensor(0.3508)\n",
      "8589 Training Loss: tensor(0.3493)\n",
      "8590 Training Loss: tensor(0.3470)\n",
      "8591 Training Loss: tensor(0.3496)\n",
      "8592 Training Loss: tensor(0.3476)\n",
      "8593 Training Loss: tensor(0.3476)\n",
      "8594 Training Loss: tensor(0.3471)\n",
      "8595 Training Loss: tensor(0.3500)\n",
      "8596 Training Loss: tensor(0.3458)\n",
      "8597 Training Loss: tensor(0.3478)\n",
      "8598 Training Loss: tensor(0.3473)\n",
      "8599 Training Loss: tensor(0.3497)\n",
      "8600 Training Loss: tensor(0.3512)\n",
      "8601 Training Loss: tensor(0.3527)\n",
      "8602 Training Loss: tensor(0.3474)\n",
      "8603 Training Loss: tensor(0.3478)\n",
      "8604 Training Loss: tensor(0.3486)\n",
      "8605 Training Loss: tensor(0.3479)\n",
      "8606 Training Loss: tensor(0.3468)\n",
      "8607 Training Loss: tensor(0.3478)\n",
      "8608 Training Loss: tensor(0.3535)\n",
      "8609 Training Loss: tensor(0.3470)\n",
      "8610 Training Loss: tensor(0.3520)\n",
      "8611 Training Loss: tensor(0.3470)\n",
      "8612 Training Loss: tensor(0.3494)\n",
      "8613 Training Loss: tensor(0.3471)\n",
      "8614 Training Loss: tensor(0.3458)\n",
      "8615 Training Loss: tensor(0.3474)\n",
      "8616 Training Loss: tensor(0.3493)\n",
      "8617 Training Loss: tensor(0.3467)\n",
      "8618 Training Loss: tensor(0.3531)\n",
      "8619 Training Loss: tensor(0.3459)\n",
      "8620 Training Loss: tensor(0.3463)\n",
      "8621 Training Loss: tensor(0.3472)\n",
      "8622 Training Loss: tensor(0.3469)\n",
      "8623 Training Loss: tensor(0.3462)\n",
      "8624 Training Loss: tensor(0.3465)\n",
      "8625 Training Loss: tensor(0.3581)\n",
      "8626 Training Loss: tensor(0.3468)\n",
      "8627 Training Loss: tensor(0.3463)\n",
      "8628 Training Loss: tensor(0.3479)\n",
      "8629 Training Loss: tensor(0.3494)\n",
      "8630 Training Loss: tensor(0.3549)\n",
      "8631 Training Loss: tensor(0.3512)\n",
      "8632 Training Loss: tensor(0.3468)\n",
      "8633 Training Loss: tensor(0.3464)\n",
      "8634 Training Loss: tensor(0.3484)\n",
      "8635 Training Loss: tensor(0.3504)\n",
      "8636 Training Loss: tensor(0.3465)\n",
      "8637 Training Loss: tensor(0.3470)\n",
      "8638 Training Loss: tensor(0.3526)\n",
      "8639 Training Loss: tensor(0.3461)\n",
      "8640 Training Loss: tensor(0.3516)\n",
      "8641 Training Loss: tensor(0.3476)\n",
      "8642 Training Loss: tensor(0.3478)\n",
      "8643 Training Loss: tensor(0.3488)\n",
      "8644 Training Loss: tensor(0.3487)\n",
      "8645 Training Loss: tensor(0.3479)\n",
      "8646 Training Loss: tensor(0.3469)\n",
      "8647 Training Loss: tensor(0.3473)\n",
      "8648 Training Loss: tensor(0.3474)\n",
      "8649 Training Loss: tensor(0.3496)\n",
      "8650 Training Loss: tensor(0.3458)\n",
      "8651 Training Loss: tensor(0.3485)\n",
      "8652 Training Loss: tensor(0.3460)\n",
      "8653 Training Loss: tensor(0.3478)\n",
      "8654 Training Loss: tensor(0.3459)\n",
      "8655 Training Loss: tensor(0.3525)\n",
      "8656 Training Loss: tensor(0.3488)\n",
      "8657 Training Loss: tensor(0.3455)\n",
      "8658 Training Loss: tensor(0.3467)\n",
      "8659 Training Loss: tensor(0.3466)\n",
      "8660 Training Loss: tensor(0.3486)\n",
      "8661 Training Loss: tensor(0.3468)\n",
      "8662 Training Loss: tensor(0.3471)\n",
      "8663 Training Loss: tensor(0.3470)\n",
      "8664 Training Loss: tensor(0.3466)\n",
      "8665 Training Loss: tensor(0.3512)\n",
      "8666 Training Loss: tensor(0.3558)\n",
      "8667 Training Loss: tensor(0.3451)\n",
      "8668 Training Loss: tensor(0.3465)\n",
      "8669 Training Loss: tensor(0.3479)\n",
      "8670 Training Loss: tensor(0.3458)\n",
      "8671 Training Loss: tensor(0.3462)\n",
      "8672 Training Loss: tensor(0.3461)\n",
      "8673 Training Loss: tensor(0.3469)\n",
      "8674 Training Loss: tensor(0.3501)\n",
      "8675 Training Loss: tensor(0.3455)\n",
      "8676 Training Loss: tensor(0.3498)\n",
      "8677 Training Loss: tensor(0.3477)\n",
      "8678 Training Loss: tensor(0.3465)\n",
      "8679 Training Loss: tensor(0.3468)\n",
      "8680 Training Loss: tensor(0.3471)\n",
      "8681 Training Loss: tensor(0.3470)\n",
      "8682 Training Loss: tensor(0.3477)\n",
      "8683 Training Loss: tensor(0.3485)\n",
      "8684 Training Loss: tensor(0.3470)\n",
      "8685 Training Loss: tensor(0.3467)\n",
      "8686 Training Loss: tensor(0.3481)\n",
      "8687 Training Loss: tensor(0.3494)\n",
      "8688 Training Loss: tensor(0.3492)\n",
      "8689 Training Loss: tensor(0.3464)\n",
      "8690 Training Loss: tensor(0.3453)\n",
      "8691 Training Loss: tensor(0.3452)\n",
      "8692 Training Loss: tensor(0.3456)\n",
      "8693 Training Loss: tensor(0.3461)\n",
      "8694 Training Loss: tensor(0.3505)\n",
      "8695 Training Loss: tensor(0.3474)\n",
      "8696 Training Loss: tensor(0.3515)\n",
      "8697 Training Loss: tensor(0.3479)\n",
      "8698 Training Loss: tensor(0.3453)\n",
      "8699 Training Loss: tensor(0.3485)\n",
      "8700 Training Loss: tensor(0.3449)\n",
      "8701 Training Loss: tensor(0.3465)\n",
      "8702 Training Loss: tensor(0.3468)\n",
      "8703 Training Loss: tensor(0.3456)\n",
      "8704 Training Loss: tensor(0.3457)\n",
      "8705 Training Loss: tensor(0.3505)\n",
      "8706 Training Loss: tensor(0.3460)\n",
      "8707 Training Loss: tensor(0.3457)\n",
      "8708 Training Loss: tensor(0.3565)\n",
      "8709 Training Loss: tensor(0.3448)\n",
      "8710 Training Loss: tensor(0.3508)\n",
      "8711 Training Loss: tensor(0.3499)\n",
      "8712 Training Loss: tensor(0.3503)\n",
      "8713 Training Loss: tensor(0.3462)\n",
      "8714 Training Loss: tensor(0.3505)\n",
      "8715 Training Loss: tensor(0.3454)\n",
      "8716 Training Loss: tensor(0.3534)\n",
      "8717 Training Loss: tensor(0.3499)\n",
      "8718 Training Loss: tensor(0.3458)\n",
      "8719 Training Loss: tensor(0.3507)\n",
      "8720 Training Loss: tensor(0.3458)\n",
      "8721 Training Loss: tensor(0.3490)\n",
      "8722 Training Loss: tensor(0.3471)\n",
      "8723 Training Loss: tensor(0.3493)\n",
      "8724 Training Loss: tensor(0.3500)\n",
      "8725 Training Loss: tensor(0.3472)\n",
      "8726 Training Loss: tensor(0.3489)\n",
      "8727 Training Loss: tensor(0.3489)\n",
      "8728 Training Loss: tensor(0.3475)\n",
      "8729 Training Loss: tensor(0.3484)\n",
      "8730 Training Loss: tensor(0.3473)\n",
      "8731 Training Loss: tensor(0.3507)\n",
      "8732 Training Loss: tensor(0.3473)\n",
      "8733 Training Loss: tensor(0.3460)\n",
      "8734 Training Loss: tensor(0.3462)\n",
      "8735 Training Loss: tensor(0.3473)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8736 Training Loss: tensor(0.3470)\n",
      "8737 Training Loss: tensor(0.3506)\n",
      "8738 Training Loss: tensor(0.3502)\n",
      "8739 Training Loss: tensor(0.3520)\n",
      "8740 Training Loss: tensor(0.3486)\n",
      "8741 Training Loss: tensor(0.3454)\n",
      "8742 Training Loss: tensor(0.3501)\n",
      "8743 Training Loss: tensor(0.3532)\n",
      "8744 Training Loss: tensor(0.3476)\n",
      "8745 Training Loss: tensor(0.3564)\n",
      "8746 Training Loss: tensor(0.3465)\n",
      "8747 Training Loss: tensor(0.3457)\n",
      "8748 Training Loss: tensor(0.3457)\n",
      "8749 Training Loss: tensor(0.3464)\n",
      "8750 Training Loss: tensor(0.3467)\n",
      "8751 Training Loss: tensor(0.3504)\n",
      "8752 Training Loss: tensor(0.3489)\n",
      "8753 Training Loss: tensor(0.3481)\n",
      "8754 Training Loss: tensor(0.3487)\n",
      "8755 Training Loss: tensor(0.3465)\n",
      "8756 Training Loss: tensor(0.3471)\n",
      "8757 Training Loss: tensor(0.3482)\n",
      "8758 Training Loss: tensor(0.3481)\n",
      "8759 Training Loss: tensor(0.3468)\n",
      "8760 Training Loss: tensor(0.3466)\n",
      "8761 Training Loss: tensor(0.3459)\n",
      "8762 Training Loss: tensor(0.3466)\n",
      "8763 Training Loss: tensor(0.3476)\n",
      "8764 Training Loss: tensor(0.3467)\n",
      "8765 Training Loss: tensor(0.3513)\n",
      "8766 Training Loss: tensor(0.3475)\n",
      "8767 Training Loss: tensor(0.3486)\n",
      "8768 Training Loss: tensor(0.3525)\n",
      "8769 Training Loss: tensor(0.3468)\n",
      "8770 Training Loss: tensor(0.3460)\n",
      "8771 Training Loss: tensor(0.3468)\n",
      "8772 Training Loss: tensor(0.3519)\n",
      "8773 Training Loss: tensor(0.3465)\n",
      "8774 Training Loss: tensor(0.3468)\n",
      "8775 Training Loss: tensor(0.3456)\n",
      "8776 Training Loss: tensor(0.3455)\n",
      "8777 Training Loss: tensor(0.3479)\n",
      "8778 Training Loss: tensor(0.3465)\n",
      "8779 Training Loss: tensor(0.3559)\n",
      "8780 Training Loss: tensor(0.3470)\n",
      "8781 Training Loss: tensor(0.3507)\n",
      "8782 Training Loss: tensor(0.3487)\n",
      "8783 Training Loss: tensor(0.3483)\n",
      "8784 Training Loss: tensor(0.3472)\n",
      "8785 Training Loss: tensor(0.3499)\n",
      "8786 Training Loss: tensor(0.3473)\n",
      "8787 Training Loss: tensor(0.3474)\n",
      "8788 Training Loss: tensor(0.3465)\n",
      "8789 Training Loss: tensor(0.3522)\n",
      "8790 Training Loss: tensor(0.3461)\n",
      "8791 Training Loss: tensor(0.3463)\n",
      "8792 Training Loss: tensor(0.3540)\n",
      "8793 Training Loss: tensor(0.3504)\n",
      "8794 Training Loss: tensor(0.3478)\n",
      "8795 Training Loss: tensor(0.3468)\n",
      "8796 Training Loss: tensor(0.3481)\n",
      "8797 Training Loss: tensor(0.3464)\n",
      "8798 Training Loss: tensor(0.3475)\n",
      "8799 Training Loss: tensor(0.3476)\n",
      "8800 Training Loss: tensor(0.3456)\n",
      "8801 Training Loss: tensor(0.3462)\n",
      "8802 Training Loss: tensor(0.3547)\n",
      "8803 Training Loss: tensor(0.3483)\n",
      "8804 Training Loss: tensor(0.3491)\n",
      "8805 Training Loss: tensor(0.3469)\n",
      "8806 Training Loss: tensor(0.3479)\n",
      "8807 Training Loss: tensor(0.3503)\n",
      "8808 Training Loss: tensor(0.3463)\n",
      "8809 Training Loss: tensor(0.3500)\n",
      "8810 Training Loss: tensor(0.3503)\n",
      "8811 Training Loss: tensor(0.3468)\n",
      "8812 Training Loss: tensor(0.3464)\n",
      "8813 Training Loss: tensor(0.3489)\n",
      "8814 Training Loss: tensor(0.3490)\n",
      "8815 Training Loss: tensor(0.3487)\n",
      "8816 Training Loss: tensor(0.3464)\n",
      "8817 Training Loss: tensor(0.3524)\n",
      "8818 Training Loss: tensor(0.3473)\n",
      "8819 Training Loss: tensor(0.3483)\n",
      "8820 Training Loss: tensor(0.3470)\n",
      "8821 Training Loss: tensor(0.3460)\n",
      "8822 Training Loss: tensor(0.3487)\n",
      "8823 Training Loss: tensor(0.3465)\n",
      "8824 Training Loss: tensor(0.3472)\n",
      "8825 Training Loss: tensor(0.3505)\n",
      "8826 Training Loss: tensor(0.3462)\n",
      "8827 Training Loss: tensor(0.3498)\n",
      "8828 Training Loss: tensor(0.3454)\n",
      "8829 Training Loss: tensor(0.3459)\n",
      "8830 Training Loss: tensor(0.3483)\n",
      "8831 Training Loss: tensor(0.3465)\n",
      "8832 Training Loss: tensor(0.3478)\n",
      "8833 Training Loss: tensor(0.3465)\n",
      "8834 Training Loss: tensor(0.3469)\n",
      "8835 Training Loss: tensor(0.3483)\n",
      "8836 Training Loss: tensor(0.3463)\n",
      "8837 Training Loss: tensor(0.3506)\n",
      "8838 Training Loss: tensor(0.3463)\n",
      "8839 Training Loss: tensor(0.3574)\n",
      "8840 Training Loss: tensor(0.3471)\n",
      "8841 Training Loss: tensor(0.3504)\n",
      "8842 Training Loss: tensor(0.3462)\n",
      "8843 Training Loss: tensor(0.3468)\n",
      "8844 Training Loss: tensor(0.3460)\n",
      "8845 Training Loss: tensor(0.3473)\n",
      "8846 Training Loss: tensor(0.3469)\n",
      "8847 Training Loss: tensor(0.3458)\n",
      "8848 Training Loss: tensor(0.3477)\n",
      "8849 Training Loss: tensor(0.3471)\n",
      "8850 Training Loss: tensor(0.3469)\n",
      "8851 Training Loss: tensor(0.3469)\n",
      "8852 Training Loss: tensor(0.3465)\n",
      "8853 Training Loss: tensor(0.3470)\n",
      "8854 Training Loss: tensor(0.3458)\n",
      "8855 Training Loss: tensor(0.3496)\n",
      "8856 Training Loss: tensor(0.3464)\n",
      "8857 Training Loss: tensor(0.3466)\n",
      "8858 Training Loss: tensor(0.3471)\n",
      "8859 Training Loss: tensor(0.3478)\n",
      "8860 Training Loss: tensor(0.3463)\n",
      "8861 Training Loss: tensor(0.3466)\n",
      "8862 Training Loss: tensor(0.3516)\n",
      "8863 Training Loss: tensor(0.3455)\n",
      "8864 Training Loss: tensor(0.3484)\n",
      "8865 Training Loss: tensor(0.3455)\n",
      "8866 Training Loss: tensor(0.3459)\n",
      "8867 Training Loss: tensor(0.3464)\n",
      "8868 Training Loss: tensor(0.3483)\n",
      "8869 Training Loss: tensor(0.3456)\n",
      "8870 Training Loss: tensor(0.3453)\n",
      "8871 Training Loss: tensor(0.3514)\n",
      "8872 Training Loss: tensor(0.3461)\n",
      "8873 Training Loss: tensor(0.3502)\n",
      "8874 Training Loss: tensor(0.3474)\n",
      "8875 Training Loss: tensor(0.3571)\n",
      "8876 Training Loss: tensor(0.3547)\n",
      "8877 Training Loss: tensor(0.3495)\n",
      "8878 Training Loss: tensor(0.3497)\n",
      "8879 Training Loss: tensor(0.3484)\n",
      "8880 Training Loss: tensor(0.3471)\n",
      "8881 Training Loss: tensor(0.3488)\n",
      "8882 Training Loss: tensor(0.3489)\n",
      "8883 Training Loss: tensor(0.3519)\n",
      "8884 Training Loss: tensor(0.3480)\n",
      "8885 Training Loss: tensor(0.3469)\n",
      "8886 Training Loss: tensor(0.3496)\n",
      "8887 Training Loss: tensor(0.3489)\n",
      "8888 Training Loss: tensor(0.3481)\n",
      "8889 Training Loss: tensor(0.3480)\n",
      "8890 Training Loss: tensor(0.3470)\n",
      "8891 Training Loss: tensor(0.3474)\n",
      "8892 Training Loss: tensor(0.3476)\n",
      "8893 Training Loss: tensor(0.3467)\n",
      "8894 Training Loss: tensor(0.3456)\n",
      "8895 Training Loss: tensor(0.3496)\n",
      "8896 Training Loss: tensor(0.3492)\n",
      "8897 Training Loss: tensor(0.3467)\n",
      "8898 Training Loss: tensor(0.3459)\n",
      "8899 Training Loss: tensor(0.3478)\n",
      "8900 Training Loss: tensor(0.3467)\n",
      "8901 Training Loss: tensor(0.3473)\n",
      "8902 Training Loss: tensor(0.3554)\n",
      "8903 Training Loss: tensor(0.3465)\n",
      "8904 Training Loss: tensor(0.3471)\n",
      "8905 Training Loss: tensor(0.3463)\n",
      "8906 Training Loss: tensor(0.3536)\n",
      "8907 Training Loss: tensor(0.3561)\n",
      "8908 Training Loss: tensor(0.3479)\n",
      "8909 Training Loss: tensor(0.3469)\n",
      "8910 Training Loss: tensor(0.3466)\n",
      "8911 Training Loss: tensor(0.3478)\n",
      "8912 Training Loss: tensor(0.3477)\n",
      "8913 Training Loss: tensor(0.3482)\n",
      "8914 Training Loss: tensor(0.3468)\n",
      "8915 Training Loss: tensor(0.3497)\n",
      "8916 Training Loss: tensor(0.3543)\n",
      "8917 Training Loss: tensor(0.3472)\n",
      "8918 Training Loss: tensor(0.3463)\n",
      "8919 Training Loss: tensor(0.3476)\n",
      "8920 Training Loss: tensor(0.3476)\n",
      "8921 Training Loss: tensor(0.3495)\n",
      "8922 Training Loss: tensor(0.3493)\n",
      "8923 Training Loss: tensor(0.3497)\n",
      "8924 Training Loss: tensor(0.3512)\n",
      "8925 Training Loss: tensor(0.3462)\n",
      "8926 Training Loss: tensor(0.3463)\n",
      "8927 Training Loss: tensor(0.3485)\n",
      "8928 Training Loss: tensor(0.3461)\n",
      "8929 Training Loss: tensor(0.3481)\n",
      "8930 Training Loss: tensor(0.3469)\n",
      "8931 Training Loss: tensor(0.3480)\n",
      "8932 Training Loss: tensor(0.3462)\n",
      "8933 Training Loss: tensor(0.3453)\n",
      "8934 Training Loss: tensor(0.3460)\n",
      "8935 Training Loss: tensor(0.3449)\n",
      "8936 Training Loss: tensor(0.3465)\n",
      "8937 Training Loss: tensor(0.3504)\n",
      "8938 Training Loss: tensor(0.3462)\n",
      "8939 Training Loss: tensor(0.3483)\n",
      "8940 Training Loss: tensor(0.3460)\n",
      "8941 Training Loss: tensor(0.3659)\n",
      "8942 Training Loss: tensor(0.3507)\n",
      "8943 Training Loss: tensor(0.3478)\n",
      "8944 Training Loss: tensor(0.3466)\n",
      "8945 Training Loss: tensor(0.3473)\n",
      "8946 Training Loss: tensor(0.3468)\n",
      "8947 Training Loss: tensor(0.3478)\n",
      "8948 Training Loss: tensor(0.3485)\n",
      "8949 Training Loss: tensor(0.3476)\n",
      "8950 Training Loss: tensor(0.3476)\n",
      "8951 Training Loss: tensor(0.3471)\n",
      "8952 Training Loss: tensor(0.3477)\n",
      "8953 Training Loss: tensor(0.3475)\n",
      "8954 Training Loss: tensor(0.3499)\n",
      "8955 Training Loss: tensor(0.3498)\n",
      "8956 Training Loss: tensor(0.3480)\n",
      "8957 Training Loss: tensor(0.3469)\n",
      "8958 Training Loss: tensor(0.3457)\n",
      "8959 Training Loss: tensor(0.3520)\n",
      "8960 Training Loss: tensor(0.3495)\n",
      "8961 Training Loss: tensor(0.3514)\n",
      "8962 Training Loss: tensor(0.3459)\n",
      "8963 Training Loss: tensor(0.3487)\n",
      "8964 Training Loss: tensor(0.3566)\n",
      "8965 Training Loss: tensor(0.3476)\n",
      "8966 Training Loss: tensor(0.3473)\n",
      "8967 Training Loss: tensor(0.3466)\n",
      "8968 Training Loss: tensor(0.3480)\n",
      "8969 Training Loss: tensor(0.3505)\n",
      "8970 Training Loss: tensor(0.3474)\n",
      "8971 Training Loss: tensor(0.3462)\n",
      "8972 Training Loss: tensor(0.3494)\n",
      "8973 Training Loss: tensor(0.3471)\n",
      "8974 Training Loss: tensor(0.3474)\n",
      "8975 Training Loss: tensor(0.3462)\n",
      "8976 Training Loss: tensor(0.3498)\n",
      "8977 Training Loss: tensor(0.3496)\n",
      "8978 Training Loss: tensor(0.3467)\n",
      "8979 Training Loss: tensor(0.3471)\n",
      "8980 Training Loss: tensor(0.3464)\n",
      "8981 Training Loss: tensor(0.3469)\n",
      "8982 Training Loss: tensor(0.3468)\n",
      "8983 Training Loss: tensor(0.3462)\n",
      "8984 Training Loss: tensor(0.3458)\n",
      "8985 Training Loss: tensor(0.3472)\n",
      "8986 Training Loss: tensor(0.3483)\n",
      "8987 Training Loss: tensor(0.3462)\n",
      "8988 Training Loss: tensor(0.3466)\n",
      "8989 Training Loss: tensor(0.3470)\n",
      "8990 Training Loss: tensor(0.3456)\n",
      "8991 Training Loss: tensor(0.3450)\n",
      "8992 Training Loss: tensor(0.3547)\n",
      "8993 Training Loss: tensor(0.3469)\n",
      "8994 Training Loss: tensor(0.3495)\n",
      "8995 Training Loss: tensor(0.3455)\n",
      "8996 Training Loss: tensor(0.3454)\n",
      "8997 Training Loss: tensor(0.3476)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8998 Training Loss: tensor(0.3491)\n",
      "8999 Training Loss: tensor(0.3471)\n",
      "9000 Training Loss: tensor(0.3512)\n",
      "9001 Training Loss: tensor(0.3465)\n",
      "9002 Training Loss: tensor(0.3483)\n",
      "9003 Training Loss: tensor(0.3459)\n",
      "9004 Training Loss: tensor(0.3466)\n",
      "9005 Training Loss: tensor(0.3498)\n",
      "9006 Training Loss: tensor(0.3523)\n",
      "9007 Training Loss: tensor(0.3472)\n",
      "9008 Training Loss: tensor(0.3500)\n",
      "9009 Training Loss: tensor(0.3471)\n",
      "9010 Training Loss: tensor(0.3463)\n",
      "9011 Training Loss: tensor(0.3542)\n",
      "9012 Training Loss: tensor(0.3457)\n",
      "9013 Training Loss: tensor(0.3471)\n",
      "9014 Training Loss: tensor(0.3455)\n",
      "9015 Training Loss: tensor(0.3463)\n",
      "9016 Training Loss: tensor(0.3468)\n",
      "9017 Training Loss: tensor(0.3495)\n",
      "9018 Training Loss: tensor(0.3458)\n",
      "9019 Training Loss: tensor(0.3510)\n",
      "9020 Training Loss: tensor(0.3466)\n",
      "9021 Training Loss: tensor(0.3480)\n",
      "9022 Training Loss: tensor(0.3496)\n",
      "9023 Training Loss: tensor(0.3477)\n",
      "9024 Training Loss: tensor(0.3517)\n",
      "9025 Training Loss: tensor(0.3473)\n",
      "9026 Training Loss: tensor(0.3527)\n",
      "9027 Training Loss: tensor(0.3452)\n",
      "9028 Training Loss: tensor(0.3522)\n",
      "9029 Training Loss: tensor(0.3458)\n",
      "9030 Training Loss: tensor(0.3464)\n",
      "9031 Training Loss: tensor(0.3473)\n",
      "9032 Training Loss: tensor(0.3504)\n",
      "9033 Training Loss: tensor(0.3478)\n",
      "9034 Training Loss: tensor(0.3483)\n",
      "9035 Training Loss: tensor(0.3466)\n",
      "9036 Training Loss: tensor(0.3478)\n",
      "9037 Training Loss: tensor(0.3508)\n",
      "9038 Training Loss: tensor(0.3485)\n",
      "9039 Training Loss: tensor(0.3463)\n",
      "9040 Training Loss: tensor(0.3473)\n",
      "9041 Training Loss: tensor(0.3463)\n",
      "9042 Training Loss: tensor(0.3462)\n",
      "9043 Training Loss: tensor(0.3467)\n",
      "9044 Training Loss: tensor(0.3460)\n",
      "9045 Training Loss: tensor(0.3473)\n",
      "9046 Training Loss: tensor(0.3480)\n",
      "9047 Training Loss: tensor(0.3476)\n",
      "9048 Training Loss: tensor(0.3460)\n",
      "9049 Training Loss: tensor(0.3467)\n",
      "9050 Training Loss: tensor(0.3464)\n",
      "9051 Training Loss: tensor(0.3472)\n",
      "9052 Training Loss: tensor(0.3456)\n",
      "9053 Training Loss: tensor(0.3529)\n",
      "9054 Training Loss: tensor(0.3462)\n",
      "9055 Training Loss: tensor(0.3452)\n",
      "9056 Training Loss: tensor(0.3504)\n",
      "9057 Training Loss: tensor(0.3457)\n",
      "9058 Training Loss: tensor(0.3475)\n",
      "9059 Training Loss: tensor(0.3464)\n",
      "9060 Training Loss: tensor(0.3453)\n",
      "9061 Training Loss: tensor(0.3469)\n",
      "9062 Training Loss: tensor(0.3477)\n",
      "9063 Training Loss: tensor(0.3521)\n",
      "9064 Training Loss: tensor(0.3479)\n",
      "9065 Training Loss: tensor(0.3456)\n",
      "9066 Training Loss: tensor(0.3455)\n",
      "9067 Training Loss: tensor(0.3468)\n",
      "9068 Training Loss: tensor(0.3462)\n",
      "9069 Training Loss: tensor(0.3479)\n",
      "9070 Training Loss: tensor(0.3487)\n",
      "9071 Training Loss: tensor(0.3474)\n",
      "9072 Training Loss: tensor(0.3483)\n",
      "9073 Training Loss: tensor(0.3495)\n",
      "9074 Training Loss: tensor(0.3497)\n",
      "9075 Training Loss: tensor(0.3500)\n",
      "9076 Training Loss: tensor(0.3486)\n",
      "9077 Training Loss: tensor(0.3464)\n",
      "9078 Training Loss: tensor(0.3456)\n",
      "9079 Training Loss: tensor(0.3455)\n",
      "9080 Training Loss: tensor(0.3461)\n",
      "9081 Training Loss: tensor(0.3463)\n",
      "9082 Training Loss: tensor(0.3537)\n",
      "9083 Training Loss: tensor(0.3468)\n",
      "9084 Training Loss: tensor(0.3474)\n",
      "9085 Training Loss: tensor(0.3464)\n",
      "9086 Training Loss: tensor(0.3475)\n",
      "9087 Training Loss: tensor(0.3451)\n",
      "9088 Training Loss: tensor(0.3465)\n",
      "9089 Training Loss: tensor(0.3465)\n",
      "9090 Training Loss: tensor(0.3480)\n",
      "9091 Training Loss: tensor(0.3457)\n",
      "9092 Training Loss: tensor(0.3536)\n",
      "9093 Training Loss: tensor(0.3542)\n",
      "9094 Training Loss: tensor(0.3467)\n",
      "9095 Training Loss: tensor(0.3503)\n",
      "9096 Training Loss: tensor(0.3472)\n",
      "9097 Training Loss: tensor(0.3467)\n",
      "9098 Training Loss: tensor(0.3480)\n",
      "9099 Training Loss: tensor(0.3503)\n",
      "9100 Training Loss: tensor(0.3473)\n",
      "9101 Training Loss: tensor(0.3505)\n",
      "9102 Training Loss: tensor(0.3460)\n",
      "9103 Training Loss: tensor(0.3462)\n",
      "9104 Training Loss: tensor(0.3474)\n",
      "9105 Training Loss: tensor(0.3530)\n",
      "9106 Training Loss: tensor(0.3459)\n",
      "9107 Training Loss: tensor(0.3470)\n",
      "9108 Training Loss: tensor(0.3488)\n",
      "9109 Training Loss: tensor(0.3463)\n",
      "9110 Training Loss: tensor(0.3455)\n",
      "9111 Training Loss: tensor(0.3525)\n",
      "9112 Training Loss: tensor(0.3496)\n",
      "9113 Training Loss: tensor(0.3518)\n",
      "9114 Training Loss: tensor(0.3521)\n",
      "9115 Training Loss: tensor(0.3503)\n",
      "9116 Training Loss: tensor(0.3476)\n",
      "9117 Training Loss: tensor(0.3497)\n",
      "9118 Training Loss: tensor(0.3494)\n",
      "9119 Training Loss: tensor(0.3478)\n",
      "9120 Training Loss: tensor(0.3484)\n",
      "9121 Training Loss: tensor(0.3480)\n",
      "9122 Training Loss: tensor(0.3473)\n",
      "9123 Training Loss: tensor(0.3472)\n",
      "9124 Training Loss: tensor(0.3489)\n",
      "9125 Training Loss: tensor(0.3482)\n",
      "9126 Training Loss: tensor(0.3462)\n",
      "9127 Training Loss: tensor(0.3465)\n",
      "9128 Training Loss: tensor(0.3516)\n",
      "9129 Training Loss: tensor(0.3514)\n",
      "9130 Training Loss: tensor(0.3489)\n",
      "9131 Training Loss: tensor(0.3477)\n",
      "9132 Training Loss: tensor(0.3465)\n",
      "9133 Training Loss: tensor(0.3509)\n",
      "9134 Training Loss: tensor(0.3484)\n",
      "9135 Training Loss: tensor(0.3466)\n",
      "9136 Training Loss: tensor(0.3499)\n",
      "9137 Training Loss: tensor(0.3459)\n",
      "9138 Training Loss: tensor(0.3480)\n",
      "9139 Training Loss: tensor(0.3468)\n",
      "9140 Training Loss: tensor(0.3470)\n",
      "9141 Training Loss: tensor(0.3511)\n",
      "9142 Training Loss: tensor(0.3466)\n",
      "9143 Training Loss: tensor(0.3483)\n",
      "9144 Training Loss: tensor(0.3466)\n",
      "9145 Training Loss: tensor(0.3464)\n",
      "9146 Training Loss: tensor(0.3480)\n",
      "9147 Training Loss: tensor(0.3477)\n",
      "9148 Training Loss: tensor(0.3458)\n",
      "9149 Training Loss: tensor(0.3497)\n",
      "9150 Training Loss: tensor(0.3464)\n",
      "9151 Training Loss: tensor(0.3484)\n",
      "9152 Training Loss: tensor(0.3450)\n",
      "9153 Training Loss: tensor(0.3500)\n",
      "9154 Training Loss: tensor(0.3457)\n",
      "9155 Training Loss: tensor(0.3456)\n",
      "9156 Training Loss: tensor(0.3456)\n",
      "9157 Training Loss: tensor(0.3449)\n",
      "9158 Training Loss: tensor(0.3463)\n",
      "9159 Training Loss: tensor(0.3453)\n",
      "9160 Training Loss: tensor(0.3474)\n",
      "9161 Training Loss: tensor(0.3529)\n",
      "9162 Training Loss: tensor(0.3466)\n",
      "9163 Training Loss: tensor(0.3463)\n",
      "9164 Training Loss: tensor(0.3471)\n",
      "9165 Training Loss: tensor(0.3472)\n",
      "9166 Training Loss: tensor(0.3466)\n",
      "9167 Training Loss: tensor(0.3494)\n",
      "9168 Training Loss: tensor(0.3464)\n",
      "9169 Training Loss: tensor(0.3457)\n",
      "9170 Training Loss: tensor(0.3456)\n",
      "9171 Training Loss: tensor(0.3512)\n",
      "9172 Training Loss: tensor(0.3449)\n",
      "9173 Training Loss: tensor(0.3464)\n",
      "9174 Training Loss: tensor(0.3457)\n",
      "9175 Training Loss: tensor(0.3526)\n",
      "9176 Training Loss: tensor(0.3461)\n",
      "9177 Training Loss: tensor(0.3482)\n",
      "9178 Training Loss: tensor(0.3448)\n",
      "9179 Training Loss: tensor(0.3454)\n",
      "9180 Training Loss: tensor(0.3451)\n",
      "9181 Training Loss: tensor(0.3483)\n",
      "9182 Training Loss: tensor(0.3494)\n",
      "9183 Training Loss: tensor(0.3463)\n",
      "9184 Training Loss: tensor(0.3454)\n",
      "9185 Training Loss: tensor(0.3456)\n",
      "9186 Training Loss: tensor(0.3449)\n",
      "9187 Training Loss: tensor(0.3447)\n",
      "9188 Training Loss: tensor(0.3468)\n",
      "9189 Training Loss: tensor(0.3491)\n",
      "9190 Training Loss: tensor(0.3458)\n",
      "9191 Training Loss: tensor(0.3459)\n",
      "9192 Training Loss: tensor(0.3457)\n",
      "9193 Training Loss: tensor(0.3459)\n",
      "9194 Training Loss: tensor(0.3489)\n",
      "9195 Training Loss: tensor(0.3485)\n",
      "9196 Training Loss: tensor(0.3447)\n",
      "9197 Training Loss: tensor(0.3459)\n",
      "9198 Training Loss: tensor(0.3450)\n",
      "9199 Training Loss: tensor(0.3446)\n",
      "9200 Training Loss: tensor(0.3460)\n",
      "9201 Training Loss: tensor(0.3454)\n",
      "9202 Training Loss: tensor(0.3447)\n",
      "9203 Training Loss: tensor(0.3468)\n",
      "9204 Training Loss: tensor(0.3452)\n",
      "9205 Training Loss: tensor(0.3589)\n",
      "9206 Training Loss: tensor(0.3455)\n",
      "9207 Training Loss: tensor(0.3485)\n",
      "9208 Training Loss: tensor(0.3456)\n",
      "9209 Training Loss: tensor(0.3453)\n",
      "9210 Training Loss: tensor(0.3464)\n",
      "9211 Training Loss: tensor(0.3443)\n",
      "9212 Training Loss: tensor(0.3479)\n",
      "9213 Training Loss: tensor(0.3449)\n",
      "9214 Training Loss: tensor(0.3456)\n",
      "9215 Training Loss: tensor(0.3500)\n",
      "9216 Training Loss: tensor(0.3522)\n",
      "9217 Training Loss: tensor(0.3554)\n",
      "9218 Training Loss: tensor(0.3451)\n",
      "9219 Training Loss: tensor(0.3528)\n",
      "9220 Training Loss: tensor(0.3482)\n",
      "9221 Training Loss: tensor(0.3458)\n",
      "9222 Training Loss: tensor(0.3470)\n",
      "9223 Training Loss: tensor(0.3463)\n",
      "9224 Training Loss: tensor(0.3473)\n",
      "9225 Training Loss: tensor(0.3458)\n",
      "9226 Training Loss: tensor(0.3458)\n",
      "9227 Training Loss: tensor(0.3470)\n",
      "9228 Training Loss: tensor(0.3478)\n",
      "9229 Training Loss: tensor(0.3467)\n",
      "9230 Training Loss: tensor(0.3480)\n",
      "9231 Training Loss: tensor(0.3460)\n",
      "9232 Training Loss: tensor(0.3452)\n",
      "9233 Training Loss: tensor(0.3452)\n",
      "9234 Training Loss: tensor(0.3490)\n",
      "9235 Training Loss: tensor(0.3505)\n",
      "9236 Training Loss: tensor(0.3499)\n",
      "9237 Training Loss: tensor(0.3456)\n",
      "9238 Training Loss: tensor(0.3466)\n",
      "9239 Training Loss: tensor(0.3481)\n",
      "9240 Training Loss: tensor(0.3455)\n",
      "9241 Training Loss: tensor(0.3634)\n",
      "9242 Training Loss: tensor(0.3511)\n",
      "9243 Training Loss: tensor(0.3474)\n",
      "9244 Training Loss: tensor(0.3459)\n",
      "9245 Training Loss: tensor(0.3490)\n",
      "9246 Training Loss: tensor(0.3467)\n",
      "9247 Training Loss: tensor(0.3494)\n",
      "9248 Training Loss: tensor(0.3461)\n",
      "9249 Training Loss: tensor(0.3494)\n",
      "9250 Training Loss: tensor(0.3480)\n",
      "9251 Training Loss: tensor(0.3478)\n",
      "9252 Training Loss: tensor(0.3529)\n",
      "9253 Training Loss: tensor(0.3475)\n",
      "9254 Training Loss: tensor(0.3467)\n",
      "9255 Training Loss: tensor(0.3491)\n",
      "9256 Training Loss: tensor(0.3467)\n",
      "9257 Training Loss: tensor(0.3527)\n",
      "9258 Training Loss: tensor(0.3463)\n",
      "9259 Training Loss: tensor(0.3467)\n",
      "9260 Training Loss: tensor(0.3460)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9261 Training Loss: tensor(0.3462)\n",
      "9262 Training Loss: tensor(0.3474)\n",
      "9263 Training Loss: tensor(0.3469)\n",
      "9264 Training Loss: tensor(0.3466)\n",
      "9265 Training Loss: tensor(0.3461)\n",
      "9266 Training Loss: tensor(0.3469)\n",
      "9267 Training Loss: tensor(0.3483)\n",
      "9268 Training Loss: tensor(0.3458)\n",
      "9269 Training Loss: tensor(0.3461)\n",
      "9270 Training Loss: tensor(0.3455)\n",
      "9271 Training Loss: tensor(0.3460)\n",
      "9272 Training Loss: tensor(0.3464)\n",
      "9273 Training Loss: tensor(0.3466)\n",
      "9274 Training Loss: tensor(0.3456)\n",
      "9275 Training Loss: tensor(0.3465)\n",
      "9276 Training Loss: tensor(0.3501)\n",
      "9277 Training Loss: tensor(0.3456)\n",
      "9278 Training Loss: tensor(0.3451)\n",
      "9279 Training Loss: tensor(0.3500)\n",
      "9280 Training Loss: tensor(0.3486)\n",
      "9281 Training Loss: tensor(0.3508)\n",
      "9282 Training Loss: tensor(0.3460)\n",
      "9283 Training Loss: tensor(0.3459)\n",
      "9284 Training Loss: tensor(0.3461)\n",
      "9285 Training Loss: tensor(0.3465)\n",
      "9286 Training Loss: tensor(0.3499)\n",
      "9287 Training Loss: tensor(0.3471)\n",
      "9288 Training Loss: tensor(0.3448)\n",
      "9289 Training Loss: tensor(0.3452)\n",
      "9290 Training Loss: tensor(0.3453)\n",
      "9291 Training Loss: tensor(0.3458)\n",
      "9292 Training Loss: tensor(0.3465)\n",
      "9293 Training Loss: tensor(0.3452)\n",
      "9294 Training Loss: tensor(0.3446)\n",
      "9295 Training Loss: tensor(0.3450)\n",
      "9296 Training Loss: tensor(0.3592)\n",
      "9297 Training Loss: tensor(0.3472)\n",
      "9298 Training Loss: tensor(0.3450)\n",
      "9299 Training Loss: tensor(0.3497)\n",
      "9300 Training Loss: tensor(0.3458)\n",
      "9301 Training Loss: tensor(0.3455)\n",
      "9302 Training Loss: tensor(0.3507)\n",
      "9303 Training Loss: tensor(0.3455)\n",
      "9304 Training Loss: tensor(0.3479)\n",
      "9305 Training Loss: tensor(0.3466)\n",
      "9306 Training Loss: tensor(0.3486)\n",
      "9307 Training Loss: tensor(0.3455)\n",
      "9308 Training Loss: tensor(0.3453)\n",
      "9309 Training Loss: tensor(0.3456)\n",
      "9310 Training Loss: tensor(0.3509)\n",
      "9311 Training Loss: tensor(0.3482)\n",
      "9312 Training Loss: tensor(0.3447)\n",
      "9313 Training Loss: tensor(0.3487)\n",
      "9314 Training Loss: tensor(0.3455)\n",
      "9315 Training Loss: tensor(0.3559)\n",
      "9316 Training Loss: tensor(0.3457)\n",
      "9317 Training Loss: tensor(0.3498)\n",
      "9318 Training Loss: tensor(0.3463)\n",
      "9319 Training Loss: tensor(0.3490)\n",
      "9320 Training Loss: tensor(0.3459)\n",
      "9321 Training Loss: tensor(0.3467)\n",
      "9322 Training Loss: tensor(0.3464)\n",
      "9323 Training Loss: tensor(0.3479)\n",
      "9324 Training Loss: tensor(0.3494)\n",
      "9325 Training Loss: tensor(0.3501)\n",
      "9326 Training Loss: tensor(0.3469)\n",
      "9327 Training Loss: tensor(0.3469)\n",
      "9328 Training Loss: tensor(0.3501)\n",
      "9329 Training Loss: tensor(0.3452)\n",
      "9330 Training Loss: tensor(0.3481)\n",
      "9331 Training Loss: tensor(0.3518)\n",
      "9332 Training Loss: tensor(0.3456)\n",
      "9333 Training Loss: tensor(0.3468)\n",
      "9334 Training Loss: tensor(0.3516)\n",
      "9335 Training Loss: tensor(0.3481)\n",
      "9336 Training Loss: tensor(0.3472)\n",
      "9337 Training Loss: tensor(0.3470)\n",
      "9338 Training Loss: tensor(0.3472)\n",
      "9339 Training Loss: tensor(0.3478)\n",
      "9340 Training Loss: tensor(0.3465)\n",
      "9341 Training Loss: tensor(0.3478)\n",
      "9342 Training Loss: tensor(0.3459)\n",
      "9343 Training Loss: tensor(0.3459)\n",
      "9344 Training Loss: tensor(0.3479)\n",
      "9345 Training Loss: tensor(0.3456)\n",
      "9346 Training Loss: tensor(0.3545)\n",
      "9347 Training Loss: tensor(0.3463)\n",
      "9348 Training Loss: tensor(0.3466)\n",
      "9349 Training Loss: tensor(0.3453)\n",
      "9350 Training Loss: tensor(0.3466)\n",
      "9351 Training Loss: tensor(0.3514)\n",
      "9352 Training Loss: tensor(0.3488)\n",
      "9353 Training Loss: tensor(0.3476)\n",
      "9354 Training Loss: tensor(0.3464)\n",
      "9355 Training Loss: tensor(0.3471)\n",
      "9356 Training Loss: tensor(0.3496)\n",
      "9357 Training Loss: tensor(0.3474)\n",
      "9358 Training Loss: tensor(0.3477)\n",
      "9359 Training Loss: tensor(0.3465)\n",
      "9360 Training Loss: tensor(0.3464)\n",
      "9361 Training Loss: tensor(0.3470)\n",
      "9362 Training Loss: tensor(0.3461)\n",
      "9363 Training Loss: tensor(0.3464)\n",
      "9364 Training Loss: tensor(0.3455)\n",
      "9365 Training Loss: tensor(0.3452)\n",
      "9366 Training Loss: tensor(0.3480)\n",
      "9367 Training Loss: tensor(0.3449)\n",
      "9368 Training Loss: tensor(0.3492)\n",
      "9369 Training Loss: tensor(0.3491)\n",
      "9370 Training Loss: tensor(0.3462)\n",
      "9371 Training Loss: tensor(0.3457)\n",
      "9372 Training Loss: tensor(0.3469)\n",
      "9373 Training Loss: tensor(0.3482)\n",
      "9374 Training Loss: tensor(0.3482)\n",
      "9375 Training Loss: tensor(0.3454)\n",
      "9376 Training Loss: tensor(0.3455)\n",
      "9377 Training Loss: tensor(0.3469)\n",
      "9378 Training Loss: tensor(0.3465)\n",
      "9379 Training Loss: tensor(0.3473)\n",
      "9380 Training Loss: tensor(0.3449)\n",
      "9381 Training Loss: tensor(0.3496)\n",
      "9382 Training Loss: tensor(0.3445)\n",
      "9383 Training Loss: tensor(0.3510)\n",
      "9384 Training Loss: tensor(0.3458)\n",
      "9385 Training Loss: tensor(0.3524)\n",
      "9386 Training Loss: tensor(0.3462)\n",
      "9387 Training Loss: tensor(0.3458)\n",
      "9388 Training Loss: tensor(0.3521)\n",
      "9389 Training Loss: tensor(0.3479)\n",
      "9390 Training Loss: tensor(0.3478)\n",
      "9391 Training Loss: tensor(0.3537)\n",
      "9392 Training Loss: tensor(0.3522)\n",
      "9393 Training Loss: tensor(0.3458)\n",
      "9394 Training Loss: tensor(0.3471)\n",
      "9395 Training Loss: tensor(0.3481)\n",
      "9396 Training Loss: tensor(0.3466)\n",
      "9397 Training Loss: tensor(0.3472)\n",
      "9398 Training Loss: tensor(0.3513)\n",
      "9399 Training Loss: tensor(0.3481)\n",
      "9400 Training Loss: tensor(0.3485)\n",
      "9401 Training Loss: tensor(0.3464)\n",
      "9402 Training Loss: tensor(0.3484)\n",
      "9403 Training Loss: tensor(0.3471)\n",
      "9404 Training Loss: tensor(0.3463)\n",
      "9405 Training Loss: tensor(0.3464)\n",
      "9406 Training Loss: tensor(0.3485)\n",
      "9407 Training Loss: tensor(0.3530)\n",
      "9408 Training Loss: tensor(0.3469)\n",
      "9409 Training Loss: tensor(0.3539)\n",
      "9410 Training Loss: tensor(0.3474)\n",
      "9411 Training Loss: tensor(0.3493)\n",
      "9412 Training Loss: tensor(0.3454)\n",
      "9413 Training Loss: tensor(0.3551)\n",
      "9414 Training Loss: tensor(0.3460)\n",
      "9415 Training Loss: tensor(0.3502)\n",
      "9416 Training Loss: tensor(0.3507)\n",
      "9417 Training Loss: tensor(0.3485)\n",
      "9418 Training Loss: tensor(0.3504)\n",
      "9419 Training Loss: tensor(0.3462)\n",
      "9420 Training Loss: tensor(0.3471)\n",
      "9421 Training Loss: tensor(0.3495)\n",
      "9422 Training Loss: tensor(0.3488)\n",
      "9423 Training Loss: tensor(0.3476)\n",
      "9424 Training Loss: tensor(0.3481)\n",
      "9425 Training Loss: tensor(0.3474)\n",
      "9426 Training Loss: tensor(0.3469)\n",
      "9427 Training Loss: tensor(0.3465)\n",
      "9428 Training Loss: tensor(0.3470)\n",
      "9429 Training Loss: tensor(0.3529)\n",
      "9430 Training Loss: tensor(0.3456)\n",
      "9431 Training Loss: tensor(0.3489)\n",
      "9432 Training Loss: tensor(0.3519)\n",
      "9433 Training Loss: tensor(0.3540)\n",
      "9434 Training Loss: tensor(0.3462)\n",
      "9435 Training Loss: tensor(0.3489)\n",
      "9436 Training Loss: tensor(0.3473)\n",
      "9437 Training Loss: tensor(0.3505)\n",
      "9438 Training Loss: tensor(0.3463)\n",
      "9439 Training Loss: tensor(0.3467)\n",
      "9440 Training Loss: tensor(0.3480)\n",
      "9441 Training Loss: tensor(0.3484)\n",
      "9442 Training Loss: tensor(0.3461)\n",
      "9443 Training Loss: tensor(0.3485)\n",
      "9444 Training Loss: tensor(0.3497)\n",
      "9445 Training Loss: tensor(0.3467)\n",
      "9446 Training Loss: tensor(0.3506)\n",
      "9447 Training Loss: tensor(0.3492)\n",
      "9448 Training Loss: tensor(0.3465)\n",
      "9449 Training Loss: tensor(0.3502)\n",
      "9450 Training Loss: tensor(0.3570)\n",
      "9451 Training Loss: tensor(0.3465)\n",
      "9452 Training Loss: tensor(0.3462)\n",
      "9453 Training Loss: tensor(0.3478)\n",
      "9454 Training Loss: tensor(0.3500)\n",
      "9455 Training Loss: tensor(0.3478)\n",
      "9456 Training Loss: tensor(0.3471)\n",
      "9457 Training Loss: tensor(0.3471)\n",
      "9458 Training Loss: tensor(0.3468)\n",
      "9459 Training Loss: tensor(0.3466)\n",
      "9460 Training Loss: tensor(0.3480)\n",
      "9461 Training Loss: tensor(0.3474)\n",
      "9462 Training Loss: tensor(0.3510)\n",
      "9463 Training Loss: tensor(0.3537)\n",
      "9464 Training Loss: tensor(0.3468)\n",
      "9465 Training Loss: tensor(0.3463)\n",
      "9466 Training Loss: tensor(0.3494)\n",
      "9467 Training Loss: tensor(0.3488)\n",
      "9468 Training Loss: tensor(0.3470)\n",
      "9469 Training Loss: tensor(0.3469)\n",
      "9470 Training Loss: tensor(0.3459)\n",
      "9471 Training Loss: tensor(0.3476)\n",
      "9472 Training Loss: tensor(0.3460)\n",
      "9473 Training Loss: tensor(0.3481)\n",
      "9474 Training Loss: tensor(0.3474)\n",
      "9475 Training Loss: tensor(0.3503)\n",
      "9476 Training Loss: tensor(0.3466)\n",
      "9477 Training Loss: tensor(0.3469)\n",
      "9478 Training Loss: tensor(0.3464)\n",
      "9479 Training Loss: tensor(0.3498)\n",
      "9480 Training Loss: tensor(0.3535)\n",
      "9481 Training Loss: tensor(0.3509)\n",
      "9482 Training Loss: tensor(0.3469)\n",
      "9483 Training Loss: tensor(0.3470)\n",
      "9484 Training Loss: tensor(0.3465)\n",
      "9485 Training Loss: tensor(0.3462)\n",
      "9486 Training Loss: tensor(0.3464)\n",
      "9487 Training Loss: tensor(0.3470)\n",
      "9488 Training Loss: tensor(0.3463)\n",
      "9489 Training Loss: tensor(0.3459)\n",
      "9490 Training Loss: tensor(0.3482)\n",
      "9491 Training Loss: tensor(0.3494)\n",
      "9492 Training Loss: tensor(0.3462)\n",
      "9493 Training Loss: tensor(0.3467)\n",
      "9494 Training Loss: tensor(0.3481)\n",
      "9495 Training Loss: tensor(0.3468)\n",
      "9496 Training Loss: tensor(0.3481)\n",
      "9497 Training Loss: tensor(0.3459)\n",
      "9498 Training Loss: tensor(0.3484)\n",
      "9499 Training Loss: tensor(0.3514)\n",
      "9500 Training Loss: tensor(0.3471)\n",
      "9501 Training Loss: tensor(0.3486)\n",
      "9502 Training Loss: tensor(0.3468)\n",
      "9503 Training Loss: tensor(0.3472)\n",
      "9504 Training Loss: tensor(0.3488)\n",
      "9505 Training Loss: tensor(0.3502)\n",
      "9506 Training Loss: tensor(0.3472)\n",
      "9507 Training Loss: tensor(0.3468)\n",
      "9508 Training Loss: tensor(0.3457)\n",
      "9509 Training Loss: tensor(0.3465)\n",
      "9510 Training Loss: tensor(0.3464)\n",
      "9511 Training Loss: tensor(0.3453)\n",
      "9512 Training Loss: tensor(0.3455)\n",
      "9513 Training Loss: tensor(0.3456)\n",
      "9514 Training Loss: tensor(0.3472)\n",
      "9515 Training Loss: tensor(0.3476)\n",
      "9516 Training Loss: tensor(0.3452)\n",
      "9517 Training Loss: tensor(0.3532)\n",
      "9518 Training Loss: tensor(0.3503)\n",
      "9519 Training Loss: tensor(0.3471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9520 Training Loss: tensor(0.3451)\n",
      "9521 Training Loss: tensor(0.3455)\n",
      "9522 Training Loss: tensor(0.3463)\n",
      "9523 Training Loss: tensor(0.3561)\n",
      "9524 Training Loss: tensor(0.3454)\n",
      "9525 Training Loss: tensor(0.3448)\n",
      "9526 Training Loss: tensor(0.3462)\n",
      "9527 Training Loss: tensor(0.3461)\n",
      "9528 Training Loss: tensor(0.3464)\n",
      "9529 Training Loss: tensor(0.3522)\n",
      "9530 Training Loss: tensor(0.3455)\n",
      "9531 Training Loss: tensor(0.3463)\n",
      "9532 Training Loss: tensor(0.3482)\n",
      "9533 Training Loss: tensor(0.3477)\n",
      "9534 Training Loss: tensor(0.3491)\n",
      "9535 Training Loss: tensor(0.3461)\n",
      "9536 Training Loss: tensor(0.3473)\n",
      "9537 Training Loss: tensor(0.3459)\n",
      "9538 Training Loss: tensor(0.3480)\n",
      "9539 Training Loss: tensor(0.3476)\n",
      "9540 Training Loss: tensor(0.3464)\n",
      "9541 Training Loss: tensor(0.3469)\n",
      "9542 Training Loss: tensor(0.3462)\n",
      "9543 Training Loss: tensor(0.3468)\n",
      "9544 Training Loss: tensor(0.3525)\n",
      "9545 Training Loss: tensor(0.3491)\n",
      "9546 Training Loss: tensor(0.3510)\n",
      "9547 Training Loss: tensor(0.3467)\n",
      "9548 Training Loss: tensor(0.3461)\n",
      "9549 Training Loss: tensor(0.3470)\n",
      "9550 Training Loss: tensor(0.3503)\n",
      "9551 Training Loss: tensor(0.3451)\n",
      "9552 Training Loss: tensor(0.3457)\n",
      "9553 Training Loss: tensor(0.3492)\n",
      "9554 Training Loss: tensor(0.3451)\n",
      "9555 Training Loss: tensor(0.3461)\n",
      "9556 Training Loss: tensor(0.3458)\n",
      "9557 Training Loss: tensor(0.3461)\n",
      "9558 Training Loss: tensor(0.3453)\n",
      "9559 Training Loss: tensor(0.3462)\n",
      "9560 Training Loss: tensor(0.3516)\n",
      "9561 Training Loss: tensor(0.3451)\n",
      "9562 Training Loss: tensor(0.3462)\n",
      "9563 Training Loss: tensor(0.3497)\n",
      "9564 Training Loss: tensor(0.3453)\n",
      "9565 Training Loss: tensor(0.3454)\n",
      "9566 Training Loss: tensor(0.3466)\n",
      "9567 Training Loss: tensor(0.3456)\n",
      "9568 Training Loss: tensor(0.3470)\n",
      "9569 Training Loss: tensor(0.3471)\n",
      "9570 Training Loss: tensor(0.3473)\n",
      "9571 Training Loss: tensor(0.3633)\n",
      "9572 Training Loss: tensor(0.3447)\n",
      "9573 Training Loss: tensor(0.3463)\n",
      "9574 Training Loss: tensor(0.3460)\n",
      "9575 Training Loss: tensor(0.3498)\n",
      "9576 Training Loss: tensor(0.3458)\n",
      "9577 Training Loss: tensor(0.3460)\n",
      "9578 Training Loss: tensor(0.3463)\n",
      "9579 Training Loss: tensor(0.3477)\n",
      "9580 Training Loss: tensor(0.3476)\n",
      "9581 Training Loss: tensor(0.3465)\n",
      "9582 Training Loss: tensor(0.3467)\n",
      "9583 Training Loss: tensor(0.3476)\n",
      "9584 Training Loss: tensor(0.3480)\n",
      "9585 Training Loss: tensor(0.3472)\n",
      "9586 Training Loss: tensor(0.3515)\n",
      "9587 Training Loss: tensor(0.3473)\n",
      "9588 Training Loss: tensor(0.3467)\n",
      "9589 Training Loss: tensor(0.3464)\n",
      "9590 Training Loss: tensor(0.3467)\n",
      "9591 Training Loss: tensor(0.3456)\n",
      "9592 Training Loss: tensor(0.3454)\n",
      "9593 Training Loss: tensor(0.3470)\n",
      "9594 Training Loss: tensor(0.3473)\n",
      "9595 Training Loss: tensor(0.3464)\n",
      "9596 Training Loss: tensor(0.3479)\n",
      "9597 Training Loss: tensor(0.3539)\n",
      "9598 Training Loss: tensor(0.3499)\n",
      "9599 Training Loss: tensor(0.3464)\n",
      "9600 Training Loss: tensor(0.3480)\n",
      "9601 Training Loss: tensor(0.3485)\n",
      "9602 Training Loss: tensor(0.3469)\n",
      "9603 Training Loss: tensor(0.3496)\n",
      "9604 Training Loss: tensor(0.3494)\n",
      "9605 Training Loss: tensor(0.3488)\n",
      "9606 Training Loss: tensor(0.3457)\n",
      "9607 Training Loss: tensor(0.3498)\n",
      "9608 Training Loss: tensor(0.3543)\n",
      "9609 Training Loss: tensor(0.3471)\n",
      "9610 Training Loss: tensor(0.3467)\n",
      "9611 Training Loss: tensor(0.3467)\n",
      "9612 Training Loss: tensor(0.3482)\n",
      "9613 Training Loss: tensor(0.3463)\n",
      "9614 Training Loss: tensor(0.3473)\n",
      "9615 Training Loss: tensor(0.3482)\n",
      "9616 Training Loss: tensor(0.3459)\n",
      "9617 Training Loss: tensor(0.3495)\n",
      "9618 Training Loss: tensor(0.3473)\n",
      "9619 Training Loss: tensor(0.3468)\n",
      "9620 Training Loss: tensor(0.3487)\n",
      "9621 Training Loss: tensor(0.3461)\n",
      "9622 Training Loss: tensor(0.3479)\n",
      "9623 Training Loss: tensor(0.3457)\n",
      "9624 Training Loss: tensor(0.3448)\n",
      "9625 Training Loss: tensor(0.3470)\n",
      "9626 Training Loss: tensor(0.3478)\n",
      "9627 Training Loss: tensor(0.3474)\n",
      "9628 Training Loss: tensor(0.3477)\n",
      "9629 Training Loss: tensor(0.3576)\n",
      "9630 Training Loss: tensor(0.3463)\n",
      "9631 Training Loss: tensor(0.3464)\n",
      "9632 Training Loss: tensor(0.3459)\n",
      "9633 Training Loss: tensor(0.3479)\n",
      "9634 Training Loss: tensor(0.3461)\n",
      "9635 Training Loss: tensor(0.3470)\n",
      "9636 Training Loss: tensor(0.3454)\n",
      "9637 Training Loss: tensor(0.3458)\n",
      "9638 Training Loss: tensor(0.3497)\n",
      "9639 Training Loss: tensor(0.3465)\n",
      "9640 Training Loss: tensor(0.3455)\n",
      "9641 Training Loss: tensor(0.3554)\n",
      "9642 Training Loss: tensor(0.3461)\n",
      "9643 Training Loss: tensor(0.3454)\n",
      "9644 Training Loss: tensor(0.3458)\n",
      "9645 Training Loss: tensor(0.3483)\n",
      "9646 Training Loss: tensor(0.3481)\n",
      "9647 Training Loss: tensor(0.3468)\n",
      "9648 Training Loss: tensor(0.3452)\n",
      "9649 Training Loss: tensor(0.3461)\n",
      "9650 Training Loss: tensor(0.3458)\n",
      "9651 Training Loss: tensor(0.3490)\n",
      "9652 Training Loss: tensor(0.3467)\n",
      "9653 Training Loss: tensor(0.3453)\n",
      "9654 Training Loss: tensor(0.3462)\n",
      "9655 Training Loss: tensor(0.3487)\n",
      "9656 Training Loss: tensor(0.3468)\n",
      "9657 Training Loss: tensor(0.3447)\n",
      "9658 Training Loss: tensor(0.3462)\n",
      "9659 Training Loss: tensor(0.3504)\n",
      "9660 Training Loss: tensor(0.3460)\n",
      "9661 Training Loss: tensor(0.3510)\n",
      "9662 Training Loss: tensor(0.3472)\n",
      "9663 Training Loss: tensor(0.3456)\n",
      "9664 Training Loss: tensor(0.3458)\n",
      "9665 Training Loss: tensor(0.3471)\n",
      "9666 Training Loss: tensor(0.3457)\n",
      "9667 Training Loss: tensor(0.3458)\n",
      "9668 Training Loss: tensor(0.3448)\n",
      "9669 Training Loss: tensor(0.3452)\n",
      "9670 Training Loss: tensor(0.3484)\n",
      "9671 Training Loss: tensor(0.3487)\n",
      "9672 Training Loss: tensor(0.3520)\n",
      "9673 Training Loss: tensor(0.3465)\n",
      "9674 Training Loss: tensor(0.3447)\n",
      "9675 Training Loss: tensor(0.3506)\n",
      "9676 Training Loss: tensor(0.3448)\n",
      "9677 Training Loss: tensor(0.3485)\n",
      "9678 Training Loss: tensor(0.3457)\n",
      "9679 Training Loss: tensor(0.3451)\n",
      "9680 Training Loss: tensor(0.3471)\n",
      "9681 Training Loss: tensor(0.3464)\n",
      "9682 Training Loss: tensor(0.3455)\n",
      "9683 Training Loss: tensor(0.3492)\n",
      "9684 Training Loss: tensor(0.3454)\n",
      "9685 Training Loss: tensor(0.3468)\n",
      "9686 Training Loss: tensor(0.3496)\n",
      "9687 Training Loss: tensor(0.3457)\n",
      "9688 Training Loss: tensor(0.3448)\n",
      "9689 Training Loss: tensor(0.3479)\n",
      "9690 Training Loss: tensor(0.3470)\n",
      "9691 Training Loss: tensor(0.3498)\n",
      "9692 Training Loss: tensor(0.3476)\n",
      "9693 Training Loss: tensor(0.3456)\n",
      "9694 Training Loss: tensor(0.3461)\n",
      "9695 Training Loss: tensor(0.3461)\n",
      "9696 Training Loss: tensor(0.3507)\n",
      "9697 Training Loss: tensor(0.3496)\n",
      "9698 Training Loss: tensor(0.3460)\n",
      "9699 Training Loss: tensor(0.3483)\n",
      "9700 Training Loss: tensor(0.3465)\n",
      "9701 Training Loss: tensor(0.3487)\n",
      "9702 Training Loss: tensor(0.3449)\n",
      "9703 Training Loss: tensor(0.3466)\n",
      "9704 Training Loss: tensor(0.3456)\n",
      "9705 Training Loss: tensor(0.3454)\n",
      "9706 Training Loss: tensor(0.3465)\n",
      "9707 Training Loss: tensor(0.3464)\n",
      "9708 Training Loss: tensor(0.3483)\n",
      "9709 Training Loss: tensor(0.3574)\n",
      "9710 Training Loss: tensor(0.3475)\n",
      "9711 Training Loss: tensor(0.3456)\n",
      "9712 Training Loss: tensor(0.3468)\n",
      "9713 Training Loss: tensor(0.3455)\n",
      "9714 Training Loss: tensor(0.3477)\n",
      "9715 Training Loss: tensor(0.3458)\n",
      "9716 Training Loss: tensor(0.3469)\n",
      "9717 Training Loss: tensor(0.3488)\n",
      "9718 Training Loss: tensor(0.3515)\n",
      "9719 Training Loss: tensor(0.3514)\n",
      "9720 Training Loss: tensor(0.3496)\n",
      "9721 Training Loss: tensor(0.3466)\n",
      "9722 Training Loss: tensor(0.3465)\n",
      "9723 Training Loss: tensor(0.3474)\n",
      "9724 Training Loss: tensor(0.3454)\n",
      "9725 Training Loss: tensor(0.3462)\n",
      "9726 Training Loss: tensor(0.3463)\n",
      "9727 Training Loss: tensor(0.3547)\n",
      "9728 Training Loss: tensor(0.3471)\n",
      "9729 Training Loss: tensor(0.3479)\n",
      "9730 Training Loss: tensor(0.3459)\n",
      "9731 Training Loss: tensor(0.3470)\n",
      "9732 Training Loss: tensor(0.3476)\n",
      "9733 Training Loss: tensor(0.3458)\n",
      "9734 Training Loss: tensor(0.3466)\n",
      "9735 Training Loss: tensor(0.3463)\n",
      "9736 Training Loss: tensor(0.3491)\n",
      "9737 Training Loss: tensor(0.3477)\n",
      "9738 Training Loss: tensor(0.3457)\n",
      "9739 Training Loss: tensor(0.3458)\n",
      "9740 Training Loss: tensor(0.3464)\n",
      "9741 Training Loss: tensor(0.3458)\n",
      "9742 Training Loss: tensor(0.3453)\n",
      "9743 Training Loss: tensor(0.3461)\n",
      "9744 Training Loss: tensor(0.3469)\n",
      "9745 Training Loss: tensor(0.3469)\n",
      "9746 Training Loss: tensor(0.3451)\n",
      "9747 Training Loss: tensor(0.3525)\n",
      "9748 Training Loss: tensor(0.3456)\n",
      "9749 Training Loss: tensor(0.3476)\n",
      "9750 Training Loss: tensor(0.3455)\n",
      "9751 Training Loss: tensor(0.3471)\n",
      "9752 Training Loss: tensor(0.3455)\n",
      "9753 Training Loss: tensor(0.3449)\n",
      "9754 Training Loss: tensor(0.3478)\n",
      "9755 Training Loss: tensor(0.3456)\n",
      "9756 Training Loss: tensor(0.3453)\n",
      "9757 Training Loss: tensor(0.3445)\n",
      "9758 Training Loss: tensor(0.3463)\n",
      "9759 Training Loss: tensor(0.3443)\n",
      "9760 Training Loss: tensor(0.3468)\n",
      "9761 Training Loss: tensor(0.3460)\n",
      "9762 Training Loss: tensor(0.3442)\n",
      "9763 Training Loss: tensor(0.3575)\n",
      "9764 Training Loss: tensor(0.3511)\n",
      "9765 Training Loss: tensor(0.3463)\n",
      "9766 Training Loss: tensor(0.3495)\n",
      "9767 Training Loss: tensor(0.3462)\n",
      "9768 Training Loss: tensor(0.3461)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9769 Training Loss: tensor(0.3459)\n",
      "9770 Training Loss: tensor(0.3465)\n",
      "9771 Training Loss: tensor(0.3458)\n",
      "9772 Training Loss: tensor(0.3469)\n",
      "9773 Training Loss: tensor(0.3454)\n",
      "9774 Training Loss: tensor(0.3445)\n",
      "9775 Training Loss: tensor(0.3484)\n",
      "9776 Training Loss: tensor(0.3515)\n",
      "9777 Training Loss: tensor(0.3456)\n",
      "9778 Training Loss: tensor(0.3524)\n",
      "9779 Training Loss: tensor(0.3460)\n",
      "9780 Training Loss: tensor(0.3477)\n",
      "9781 Training Loss: tensor(0.3457)\n",
      "9782 Training Loss: tensor(0.3470)\n",
      "9783 Training Loss: tensor(0.3467)\n",
      "9784 Training Loss: tensor(0.3486)\n",
      "9785 Training Loss: tensor(0.3502)\n",
      "9786 Training Loss: tensor(0.3523)\n",
      "9787 Training Loss: tensor(0.3461)\n",
      "9788 Training Loss: tensor(0.3455)\n",
      "9789 Training Loss: tensor(0.3485)\n",
      "9790 Training Loss: tensor(0.3473)\n",
      "9791 Training Loss: tensor(0.3464)\n",
      "9792 Training Loss: tensor(0.3491)\n",
      "9793 Training Loss: tensor(0.3460)\n",
      "9794 Training Loss: tensor(0.3507)\n",
      "9795 Training Loss: tensor(0.3457)\n",
      "9796 Training Loss: tensor(0.3461)\n",
      "9797 Training Loss: tensor(0.3468)\n",
      "9798 Training Loss: tensor(0.3452)\n",
      "9799 Training Loss: tensor(0.3518)\n",
      "9800 Training Loss: tensor(0.3458)\n",
      "9801 Training Loss: tensor(0.3456)\n",
      "9802 Training Loss: tensor(0.3469)\n",
      "9803 Training Loss: tensor(0.3462)\n",
      "9804 Training Loss: tensor(0.3460)\n",
      "9805 Training Loss: tensor(0.3507)\n",
      "9806 Training Loss: tensor(0.3527)\n",
      "9807 Training Loss: tensor(0.3453)\n",
      "9808 Training Loss: tensor(0.3450)\n",
      "9809 Training Loss: tensor(0.3503)\n",
      "9810 Training Loss: tensor(0.3458)\n",
      "9811 Training Loss: tensor(0.3472)\n",
      "9812 Training Loss: tensor(0.3464)\n",
      "9813 Training Loss: tensor(0.3484)\n",
      "9814 Training Loss: tensor(0.3458)\n",
      "9815 Training Loss: tensor(0.3465)\n",
      "9816 Training Loss: tensor(0.3462)\n",
      "9817 Training Loss: tensor(0.3525)\n",
      "9818 Training Loss: tensor(0.3494)\n",
      "9819 Training Loss: tensor(0.3485)\n",
      "9820 Training Loss: tensor(0.3461)\n",
      "9821 Training Loss: tensor(0.3470)\n",
      "9822 Training Loss: tensor(0.3511)\n",
      "9823 Training Loss: tensor(0.3483)\n",
      "9824 Training Loss: tensor(0.3492)\n",
      "9825 Training Loss: tensor(0.3486)\n",
      "9826 Training Loss: tensor(0.3502)\n",
      "9827 Training Loss: tensor(0.3462)\n",
      "9828 Training Loss: tensor(0.3474)\n",
      "9829 Training Loss: tensor(0.3484)\n",
      "9830 Training Loss: tensor(0.3510)\n",
      "9831 Training Loss: tensor(0.3465)\n",
      "9832 Training Loss: tensor(0.3489)\n",
      "9833 Training Loss: tensor(0.3464)\n",
      "9834 Training Loss: tensor(0.3477)\n",
      "9835 Training Loss: tensor(0.3508)\n",
      "9836 Training Loss: tensor(0.3470)\n",
      "9837 Training Loss: tensor(0.3465)\n",
      "9838 Training Loss: tensor(0.3467)\n",
      "9839 Training Loss: tensor(0.3463)\n",
      "9840 Training Loss: tensor(0.3473)\n",
      "9841 Training Loss: tensor(0.3469)\n",
      "9842 Training Loss: tensor(0.3458)\n",
      "9843 Training Loss: tensor(0.3462)\n",
      "9844 Training Loss: tensor(0.3475)\n",
      "9845 Training Loss: tensor(0.3478)\n",
      "9846 Training Loss: tensor(0.3467)\n",
      "9847 Training Loss: tensor(0.3459)\n",
      "9848 Training Loss: tensor(0.3462)\n",
      "9849 Training Loss: tensor(0.3470)\n",
      "9850 Training Loss: tensor(0.3476)\n",
      "9851 Training Loss: tensor(0.3500)\n",
      "9852 Training Loss: tensor(0.3499)\n",
      "9853 Training Loss: tensor(0.3487)\n",
      "9854 Training Loss: tensor(0.3459)\n",
      "9855 Training Loss: tensor(0.3474)\n",
      "9856 Training Loss: tensor(0.3482)\n",
      "9857 Training Loss: tensor(0.3512)\n",
      "9858 Training Loss: tensor(0.3461)\n",
      "9859 Training Loss: tensor(0.3469)\n",
      "9860 Training Loss: tensor(0.3516)\n",
      "9861 Training Loss: tensor(0.3475)\n",
      "9862 Training Loss: tensor(0.3464)\n",
      "9863 Training Loss: tensor(0.3469)\n",
      "9864 Training Loss: tensor(0.3488)\n",
      "9865 Training Loss: tensor(0.3481)\n",
      "9866 Training Loss: tensor(0.3468)\n",
      "9867 Training Loss: tensor(0.3459)\n",
      "9868 Training Loss: tensor(0.3464)\n",
      "9869 Training Loss: tensor(0.3540)\n",
      "9870 Training Loss: tensor(0.3459)\n",
      "9871 Training Loss: tensor(0.3476)\n",
      "9872 Training Loss: tensor(0.3461)\n",
      "9873 Training Loss: tensor(0.3465)\n",
      "9874 Training Loss: tensor(0.3480)\n",
      "9875 Training Loss: tensor(0.3470)\n",
      "9876 Training Loss: tensor(0.3470)\n",
      "9877 Training Loss: tensor(0.3526)\n",
      "9878 Training Loss: tensor(0.3501)\n",
      "9879 Training Loss: tensor(0.3465)\n",
      "9880 Training Loss: tensor(0.3485)\n",
      "9881 Training Loss: tensor(0.3476)\n",
      "9882 Training Loss: tensor(0.3461)\n",
      "9883 Training Loss: tensor(0.3464)\n",
      "9884 Training Loss: tensor(0.3467)\n",
      "9885 Training Loss: tensor(0.3456)\n",
      "9886 Training Loss: tensor(0.3498)\n",
      "9887 Training Loss: tensor(0.3462)\n",
      "9888 Training Loss: tensor(0.3458)\n",
      "9889 Training Loss: tensor(0.3468)\n",
      "9890 Training Loss: tensor(0.3462)\n",
      "9891 Training Loss: tensor(0.3479)\n",
      "9892 Training Loss: tensor(0.3457)\n",
      "9893 Training Loss: tensor(0.3499)\n",
      "9894 Training Loss: tensor(0.3465)\n",
      "9895 Training Loss: tensor(0.3460)\n",
      "9896 Training Loss: tensor(0.3456)\n",
      "9897 Training Loss: tensor(0.3463)\n",
      "9898 Training Loss: tensor(0.3567)\n",
      "9899 Training Loss: tensor(0.3470)\n",
      "9900 Training Loss: tensor(0.3452)\n",
      "9901 Training Loss: tensor(0.3454)\n",
      "9902 Training Loss: tensor(0.3460)\n",
      "9903 Training Loss: tensor(0.3459)\n",
      "9904 Training Loss: tensor(0.3465)\n",
      "9905 Training Loss: tensor(0.3471)\n",
      "9906 Training Loss: tensor(0.3501)\n",
      "9907 Training Loss: tensor(0.3509)\n",
      "9908 Training Loss: tensor(0.3473)\n",
      "9909 Training Loss: tensor(0.3465)\n",
      "9910 Training Loss: tensor(0.3469)\n",
      "9911 Training Loss: tensor(0.3464)\n",
      "9912 Training Loss: tensor(0.3467)\n",
      "9913 Training Loss: tensor(0.3494)\n",
      "9914 Training Loss: tensor(0.3475)\n",
      "9915 Training Loss: tensor(0.3520)\n",
      "9916 Training Loss: tensor(0.3466)\n",
      "9917 Training Loss: tensor(0.3464)\n",
      "9918 Training Loss: tensor(0.3472)\n",
      "9919 Training Loss: tensor(0.3471)\n",
      "9920 Training Loss: tensor(0.3457)\n",
      "9921 Training Loss: tensor(0.3466)\n",
      "9922 Training Loss: tensor(0.3482)\n",
      "9923 Training Loss: tensor(0.3465)\n",
      "9924 Training Loss: tensor(0.3498)\n",
      "9925 Training Loss: tensor(0.3486)\n",
      "9926 Training Loss: tensor(0.3463)\n",
      "9927 Training Loss: tensor(0.3468)\n",
      "9928 Training Loss: tensor(0.3458)\n",
      "9929 Training Loss: tensor(0.3465)\n",
      "9930 Training Loss: tensor(0.3493)\n",
      "9931 Training Loss: tensor(0.3457)\n",
      "9932 Training Loss: tensor(0.3496)\n",
      "9933 Training Loss: tensor(0.3477)\n",
      "9934 Training Loss: tensor(0.3466)\n",
      "9935 Training Loss: tensor(0.3457)\n",
      "9936 Training Loss: tensor(0.3503)\n",
      "9937 Training Loss: tensor(0.3542)\n",
      "9938 Training Loss: tensor(0.3455)\n",
      "9939 Training Loss: tensor(0.3509)\n",
      "9940 Training Loss: tensor(0.3460)\n",
      "9941 Training Loss: tensor(0.3511)\n",
      "9942 Training Loss: tensor(0.3471)\n",
      "9943 Training Loss: tensor(0.3451)\n",
      "9944 Training Loss: tensor(0.3536)\n",
      "9945 Training Loss: tensor(0.3473)\n",
      "9946 Training Loss: tensor(0.3495)\n",
      "9947 Training Loss: tensor(0.3467)\n",
      "9948 Training Loss: tensor(0.3482)\n",
      "9949 Training Loss: tensor(0.3497)\n",
      "9950 Training Loss: tensor(0.3459)\n",
      "9951 Training Loss: tensor(0.3468)\n",
      "9952 Training Loss: tensor(0.3470)\n",
      "9953 Training Loss: tensor(0.3474)\n",
      "9954 Training Loss: tensor(0.3494)\n",
      "9955 Training Loss: tensor(0.3457)\n",
      "9956 Training Loss: tensor(0.3461)\n",
      "9957 Training Loss: tensor(0.3466)\n",
      "9958 Training Loss: tensor(0.3486)\n",
      "9959 Training Loss: tensor(0.3522)\n",
      "9960 Training Loss: tensor(0.3455)\n",
      "9961 Training Loss: tensor(0.3460)\n",
      "9962 Training Loss: tensor(0.3474)\n",
      "9963 Training Loss: tensor(0.3464)\n",
      "9964 Training Loss: tensor(0.3461)\n",
      "9965 Training Loss: tensor(0.3478)\n",
      "9966 Training Loss: tensor(0.3456)\n",
      "9967 Training Loss: tensor(0.3459)\n",
      "9968 Training Loss: tensor(0.3455)\n",
      "9969 Training Loss: tensor(0.3529)\n",
      "9970 Training Loss: tensor(0.3459)\n",
      "9971 Training Loss: tensor(0.3454)\n",
      "9972 Training Loss: tensor(0.3452)\n",
      "9973 Training Loss: tensor(0.3483)\n",
      "9974 Training Loss: tensor(0.3470)\n",
      "9975 Training Loss: tensor(0.3508)\n",
      "9976 Training Loss: tensor(0.3503)\n",
      "9977 Training Loss: tensor(0.3456)\n",
      "9978 Training Loss: tensor(0.3459)\n",
      "9979 Training Loss: tensor(0.3472)\n",
      "9980 Training Loss: tensor(0.3461)\n",
      "9981 Training Loss: tensor(0.3454)\n",
      "9982 Training Loss: tensor(0.3481)\n",
      "9983 Training Loss: tensor(0.3463)\n",
      "9984 Training Loss: tensor(0.3500)\n",
      "9985 Training Loss: tensor(0.3518)\n",
      "9986 Training Loss: tensor(0.3455)\n",
      "9987 Training Loss: tensor(0.3462)\n",
      "9988 Training Loss: tensor(0.3473)\n",
      "9989 Training Loss: tensor(0.3490)\n",
      "9990 Training Loss: tensor(0.3527)\n",
      "9991 Training Loss: tensor(0.3468)\n",
      "9992 Training Loss: tensor(0.3455)\n",
      "9993 Training Loss: tensor(0.3482)\n",
      "9994 Training Loss: tensor(0.3547)\n",
      "9995 Training Loss: tensor(0.3455)\n",
      "9996 Training Loss: tensor(0.3471)\n",
      "9997 Training Loss: tensor(0.3466)\n",
      "9998 Training Loss: tensor(0.3449)\n",
      "9999 Training Loss: tensor(0.3499)\n",
      "10000 Training Loss: tensor(0.3483)\n",
      "10001 Training Loss: tensor(0.3465)\n",
      "10002 Training Loss: tensor(0.3532)\n",
      "10003 Training Loss: tensor(0.3465)\n",
      "10004 Training Loss: tensor(0.3463)\n",
      "10005 Training Loss: tensor(0.3476)\n",
      "10006 Training Loss: tensor(0.3482)\n",
      "10007 Training Loss: tensor(0.3458)\n",
      "10008 Training Loss: tensor(0.3461)\n",
      "10009 Training Loss: tensor(0.3496)\n",
      "10010 Training Loss: tensor(0.3502)\n",
      "10011 Training Loss: tensor(0.3479)\n",
      "10012 Training Loss: tensor(0.3460)\n",
      "10013 Training Loss: tensor(0.3546)\n",
      "10014 Training Loss: tensor(0.3464)\n",
      "10015 Training Loss: tensor(0.3461)\n",
      "10016 Training Loss: tensor(0.3468)\n",
      "10017 Training Loss: tensor(0.3456)\n",
      "10018 Training Loss: tensor(0.3471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10019 Training Loss: tensor(0.3455)\n",
      "10020 Training Loss: tensor(0.3455)\n",
      "10021 Training Loss: tensor(0.3511)\n",
      "10022 Training Loss: tensor(0.3501)\n",
      "10023 Training Loss: tensor(0.3469)\n",
      "10024 Training Loss: tensor(0.3574)\n",
      "10025 Training Loss: tensor(0.3454)\n",
      "10026 Training Loss: tensor(0.3461)\n",
      "10027 Training Loss: tensor(0.3455)\n",
      "10028 Training Loss: tensor(0.3474)\n",
      "10029 Training Loss: tensor(0.3468)\n",
      "10030 Training Loss: tensor(0.3492)\n",
      "10031 Training Loss: tensor(0.3464)\n",
      "10032 Training Loss: tensor(0.3470)\n",
      "10033 Training Loss: tensor(0.3520)\n",
      "10034 Training Loss: tensor(0.3471)\n",
      "10035 Training Loss: tensor(0.3543)\n",
      "10036 Training Loss: tensor(0.3463)\n",
      "10037 Training Loss: tensor(0.3459)\n",
      "10038 Training Loss: tensor(0.3476)\n",
      "10039 Training Loss: tensor(0.3516)\n",
      "10040 Training Loss: tensor(0.3472)\n",
      "10041 Training Loss: tensor(0.3498)\n",
      "10042 Training Loss: tensor(0.3496)\n",
      "10043 Training Loss: tensor(0.3490)\n",
      "10044 Training Loss: tensor(0.3472)\n",
      "10045 Training Loss: tensor(0.3504)\n",
      "10046 Training Loss: tensor(0.3464)\n",
      "10047 Training Loss: tensor(0.3467)\n",
      "10048 Training Loss: tensor(0.3505)\n",
      "10049 Training Loss: tensor(0.3501)\n",
      "10050 Training Loss: tensor(0.3502)\n",
      "10051 Training Loss: tensor(0.3466)\n",
      "10052 Training Loss: tensor(0.3464)\n",
      "10053 Training Loss: tensor(0.3472)\n",
      "10054 Training Loss: tensor(0.3465)\n",
      "10055 Training Loss: tensor(0.3477)\n",
      "10056 Training Loss: tensor(0.3475)\n",
      "10057 Training Loss: tensor(0.3474)\n",
      "10058 Training Loss: tensor(0.3465)\n",
      "10059 Training Loss: tensor(0.3469)\n",
      "10060 Training Loss: tensor(0.3472)\n",
      "10061 Training Loss: tensor(0.3476)\n",
      "10062 Training Loss: tensor(0.3463)\n",
      "10063 Training Loss: tensor(0.3469)\n",
      "10064 Training Loss: tensor(0.3463)\n",
      "10065 Training Loss: tensor(0.3458)\n",
      "10066 Training Loss: tensor(0.3455)\n",
      "10067 Training Loss: tensor(0.3451)\n",
      "10068 Training Loss: tensor(0.3483)\n",
      "10069 Training Loss: tensor(0.3497)\n",
      "10070 Training Loss: tensor(0.3536)\n",
      "10071 Training Loss: tensor(0.3460)\n",
      "10072 Training Loss: tensor(0.3492)\n",
      "10073 Training Loss: tensor(0.3551)\n",
      "10074 Training Loss: tensor(0.3469)\n",
      "10075 Training Loss: tensor(0.3517)\n",
      "10076 Training Loss: tensor(0.3455)\n",
      "10077 Training Loss: tensor(0.3520)\n",
      "10078 Training Loss: tensor(0.3476)\n",
      "10079 Training Loss: tensor(0.3463)\n",
      "10080 Training Loss: tensor(0.3473)\n",
      "10081 Training Loss: tensor(0.3482)\n",
      "10082 Training Loss: tensor(0.3475)\n",
      "10083 Training Loss: tensor(0.3477)\n",
      "10084 Training Loss: tensor(0.3502)\n",
      "10085 Training Loss: tensor(0.3465)\n",
      "10086 Training Loss: tensor(0.3472)\n",
      "10087 Training Loss: tensor(0.3481)\n",
      "10088 Training Loss: tensor(0.3507)\n",
      "10089 Training Loss: tensor(0.3488)\n",
      "10090 Training Loss: tensor(0.3463)\n",
      "10091 Training Loss: tensor(0.3485)\n",
      "10092 Training Loss: tensor(0.3490)\n",
      "10093 Training Loss: tensor(0.3499)\n",
      "10094 Training Loss: tensor(0.3489)\n",
      "10095 Training Loss: tensor(0.3462)\n",
      "10096 Training Loss: tensor(0.3457)\n",
      "10097 Training Loss: tensor(0.3459)\n",
      "10098 Training Loss: tensor(0.3460)\n",
      "10099 Training Loss: tensor(0.3466)\n",
      "10100 Training Loss: tensor(0.3458)\n",
      "10101 Training Loss: tensor(0.3455)\n",
      "10102 Training Loss: tensor(0.3454)\n",
      "10103 Training Loss: tensor(0.3458)\n",
      "10104 Training Loss: tensor(0.3465)\n",
      "10105 Training Loss: tensor(0.3456)\n",
      "10106 Training Loss: tensor(0.3458)\n",
      "10107 Training Loss: tensor(0.3452)\n",
      "10108 Training Loss: tensor(0.3455)\n",
      "10109 Training Loss: tensor(0.3487)\n",
      "10110 Training Loss: tensor(0.3469)\n",
      "10111 Training Loss: tensor(0.3472)\n",
      "10112 Training Loss: tensor(0.3504)\n",
      "10113 Training Loss: tensor(0.3470)\n",
      "10114 Training Loss: tensor(0.3532)\n",
      "10115 Training Loss: tensor(0.3530)\n",
      "10116 Training Loss: tensor(0.3442)\n",
      "10117 Training Loss: tensor(0.3536)\n",
      "10118 Training Loss: tensor(0.3455)\n",
      "10119 Training Loss: tensor(0.3458)\n",
      "10120 Training Loss: tensor(0.3462)\n",
      "10121 Training Loss: tensor(0.3468)\n",
      "10122 Training Loss: tensor(0.3471)\n",
      "10123 Training Loss: tensor(0.3455)\n",
      "10124 Training Loss: tensor(0.3463)\n",
      "10125 Training Loss: tensor(0.3475)\n",
      "10126 Training Loss: tensor(0.3463)\n",
      "10127 Training Loss: tensor(0.3470)\n",
      "10128 Training Loss: tensor(0.3514)\n",
      "10129 Training Loss: tensor(0.3458)\n",
      "10130 Training Loss: tensor(0.3449)\n",
      "10131 Training Loss: tensor(0.3458)\n",
      "10132 Training Loss: tensor(0.3487)\n",
      "10133 Training Loss: tensor(0.3508)\n",
      "10134 Training Loss: tensor(0.3470)\n",
      "10135 Training Loss: tensor(0.3458)\n",
      "10136 Training Loss: tensor(0.3469)\n",
      "10137 Training Loss: tensor(0.3461)\n",
      "10138 Training Loss: tensor(0.3452)\n",
      "10139 Training Loss: tensor(0.3463)\n",
      "10140 Training Loss: tensor(0.3465)\n",
      "10141 Training Loss: tensor(0.3542)\n",
      "10142 Training Loss: tensor(0.3443)\n",
      "10143 Training Loss: tensor(0.3455)\n",
      "10144 Training Loss: tensor(0.3457)\n",
      "10145 Training Loss: tensor(0.3461)\n",
      "10146 Training Loss: tensor(0.3460)\n",
      "10147 Training Loss: tensor(0.3469)\n",
      "10148 Training Loss: tensor(0.3453)\n",
      "10149 Training Loss: tensor(0.3461)\n",
      "10150 Training Loss: tensor(0.3455)\n",
      "10151 Training Loss: tensor(0.3491)\n",
      "10152 Training Loss: tensor(0.3443)\n",
      "10153 Training Loss: tensor(0.3475)\n",
      "10154 Training Loss: tensor(0.3464)\n",
      "10155 Training Loss: tensor(0.3453)\n",
      "10156 Training Loss: tensor(0.3463)\n",
      "10157 Training Loss: tensor(0.3466)\n",
      "10158 Training Loss: tensor(0.3465)\n",
      "10159 Training Loss: tensor(0.3479)\n",
      "10160 Training Loss: tensor(0.3485)\n",
      "10161 Training Loss: tensor(0.3461)\n",
      "10162 Training Loss: tensor(0.3465)\n",
      "10163 Training Loss: tensor(0.3469)\n",
      "10164 Training Loss: tensor(0.3448)\n",
      "10165 Training Loss: tensor(0.3457)\n",
      "10166 Training Loss: tensor(0.3498)\n",
      "10167 Training Loss: tensor(0.3492)\n",
      "10168 Training Loss: tensor(0.3457)\n",
      "10169 Training Loss: tensor(0.3454)\n",
      "10170 Training Loss: tensor(0.3462)\n",
      "10171 Training Loss: tensor(0.3457)\n",
      "10172 Training Loss: tensor(0.3480)\n",
      "10173 Training Loss: tensor(0.3484)\n",
      "10174 Training Loss: tensor(0.3447)\n",
      "10175 Training Loss: tensor(0.3453)\n",
      "10176 Training Loss: tensor(0.3471)\n",
      "10177 Training Loss: tensor(0.3452)\n",
      "10178 Training Loss: tensor(0.3452)\n",
      "10179 Training Loss: tensor(0.3465)\n",
      "10180 Training Loss: tensor(0.3499)\n",
      "10181 Training Loss: tensor(0.3443)\n",
      "10182 Training Loss: tensor(0.3517)\n",
      "10183 Training Loss: tensor(0.3457)\n",
      "10184 Training Loss: tensor(0.3462)\n",
      "10185 Training Loss: tensor(0.3468)\n",
      "10186 Training Loss: tensor(0.3441)\n",
      "10187 Training Loss: tensor(0.3455)\n",
      "10188 Training Loss: tensor(0.3451)\n",
      "10189 Training Loss: tensor(0.3453)\n",
      "10190 Training Loss: tensor(0.3443)\n",
      "10191 Training Loss: tensor(0.3467)\n",
      "10192 Training Loss: tensor(0.3443)\n",
      "10193 Training Loss: tensor(0.3453)\n",
      "10194 Training Loss: tensor(0.3596)\n",
      "10195 Training Loss: tensor(0.3559)\n",
      "10196 Training Loss: tensor(0.3489)\n",
      "10197 Training Loss: tensor(0.3476)\n",
      "10198 Training Loss: tensor(0.3455)\n",
      "10199 Training Loss: tensor(0.3491)\n",
      "10200 Training Loss: tensor(0.3472)\n",
      "10201 Training Loss: tensor(0.3497)\n",
      "10202 Training Loss: tensor(0.3470)\n",
      "10203 Training Loss: tensor(0.3499)\n",
      "10204 Training Loss: tensor(0.3473)\n",
      "10205 Training Loss: tensor(0.3515)\n",
      "10206 Training Loss: tensor(0.3468)\n",
      "10207 Training Loss: tensor(0.3485)\n",
      "10208 Training Loss: tensor(0.3463)\n",
      "10209 Training Loss: tensor(0.3465)\n",
      "10210 Training Loss: tensor(0.3488)\n",
      "10211 Training Loss: tensor(0.3459)\n",
      "10212 Training Loss: tensor(0.3466)\n",
      "10213 Training Loss: tensor(0.3477)\n",
      "10214 Training Loss: tensor(0.3463)\n",
      "10215 Training Loss: tensor(0.3457)\n",
      "10216 Training Loss: tensor(0.3458)\n",
      "10217 Training Loss: tensor(0.3459)\n",
      "10218 Training Loss: tensor(0.3535)\n",
      "10219 Training Loss: tensor(0.3463)\n",
      "10220 Training Loss: tensor(0.3450)\n",
      "10221 Training Loss: tensor(0.3486)\n",
      "10222 Training Loss: tensor(0.3455)\n",
      "10223 Training Loss: tensor(0.3498)\n",
      "10224 Training Loss: tensor(0.3464)\n",
      "10225 Training Loss: tensor(0.3454)\n",
      "10226 Training Loss: tensor(0.3460)\n",
      "10227 Training Loss: tensor(0.3469)\n",
      "10228 Training Loss: tensor(0.3545)\n",
      "10229 Training Loss: tensor(0.3459)\n",
      "10230 Training Loss: tensor(0.3490)\n",
      "10231 Training Loss: tensor(0.3453)\n",
      "10232 Training Loss: tensor(0.3474)\n",
      "10233 Training Loss: tensor(0.3447)\n",
      "10234 Training Loss: tensor(0.3453)\n",
      "10235 Training Loss: tensor(0.3476)\n",
      "10236 Training Loss: tensor(0.3495)\n",
      "10237 Training Loss: tensor(0.3464)\n",
      "10238 Training Loss: tensor(0.3462)\n",
      "10239 Training Loss: tensor(0.3507)\n",
      "10240 Training Loss: tensor(0.3498)\n",
      "10241 Training Loss: tensor(0.3487)\n",
      "10242 Training Loss: tensor(0.3458)\n",
      "10243 Training Loss: tensor(0.3472)\n",
      "10244 Training Loss: tensor(0.3558)\n",
      "10245 Training Loss: tensor(0.3466)\n",
      "10246 Training Loss: tensor(0.3478)\n",
      "10247 Training Loss: tensor(0.3455)\n",
      "10248 Training Loss: tensor(0.3483)\n",
      "10249 Training Loss: tensor(0.3497)\n",
      "10250 Training Loss: tensor(0.3484)\n",
      "10251 Training Loss: tensor(0.3456)\n",
      "10252 Training Loss: tensor(0.3528)\n",
      "10253 Training Loss: tensor(0.3484)\n",
      "10254 Training Loss: tensor(0.3506)\n",
      "10255 Training Loss: tensor(0.3495)\n",
      "10256 Training Loss: tensor(0.3471)\n",
      "10257 Training Loss: tensor(0.3482)\n",
      "10258 Training Loss: tensor(0.3470)\n",
      "10259 Training Loss: tensor(0.3460)\n",
      "10260 Training Loss: tensor(0.3456)\n",
      "10261 Training Loss: tensor(0.3492)\n",
      "10262 Training Loss: tensor(0.3469)\n",
      "10263 Training Loss: tensor(0.3513)\n",
      "10264 Training Loss: tensor(0.3458)\n",
      "10265 Training Loss: tensor(0.3461)\n",
      "10266 Training Loss: tensor(0.3512)\n",
      "10267 Training Loss: tensor(0.3468)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10268 Training Loss: tensor(0.3474)\n",
      "10269 Training Loss: tensor(0.3469)\n",
      "10270 Training Loss: tensor(0.3468)\n",
      "10271 Training Loss: tensor(0.3470)\n",
      "10272 Training Loss: tensor(0.3528)\n",
      "10273 Training Loss: tensor(0.3482)\n",
      "10274 Training Loss: tensor(0.3468)\n",
      "10275 Training Loss: tensor(0.3451)\n",
      "10276 Training Loss: tensor(0.3505)\n",
      "10277 Training Loss: tensor(0.3457)\n",
      "10278 Training Loss: tensor(0.3466)\n",
      "10279 Training Loss: tensor(0.3466)\n",
      "10280 Training Loss: tensor(0.3464)\n",
      "10281 Training Loss: tensor(0.3453)\n",
      "10282 Training Loss: tensor(0.3465)\n",
      "10283 Training Loss: tensor(0.3485)\n",
      "10284 Training Loss: tensor(0.3459)\n",
      "10285 Training Loss: tensor(0.3485)\n",
      "10286 Training Loss: tensor(0.3510)\n",
      "10287 Training Loss: tensor(0.3460)\n",
      "10288 Training Loss: tensor(0.3449)\n",
      "10289 Training Loss: tensor(0.3451)\n",
      "10290 Training Loss: tensor(0.3505)\n",
      "10291 Training Loss: tensor(0.3479)\n",
      "10292 Training Loss: tensor(0.3472)\n",
      "10293 Training Loss: tensor(0.3471)\n",
      "10294 Training Loss: tensor(0.3467)\n",
      "10295 Training Loss: tensor(0.3465)\n",
      "10296 Training Loss: tensor(0.3478)\n",
      "10297 Training Loss: tensor(0.3448)\n",
      "10298 Training Loss: tensor(0.3493)\n",
      "10299 Training Loss: tensor(0.3458)\n",
      "10300 Training Loss: tensor(0.3469)\n",
      "10301 Training Loss: tensor(0.3460)\n",
      "10302 Training Loss: tensor(0.3467)\n",
      "10303 Training Loss: tensor(0.3482)\n",
      "10304 Training Loss: tensor(0.3456)\n",
      "10305 Training Loss: tensor(0.3480)\n",
      "10306 Training Loss: tensor(0.3507)\n",
      "10307 Training Loss: tensor(0.3466)\n",
      "10308 Training Loss: tensor(0.3501)\n",
      "10309 Training Loss: tensor(0.3505)\n",
      "10310 Training Loss: tensor(0.3500)\n",
      "10311 Training Loss: tensor(0.3465)\n",
      "10312 Training Loss: tensor(0.3465)\n",
      "10313 Training Loss: tensor(0.3481)\n",
      "10314 Training Loss: tensor(0.3474)\n",
      "10315 Training Loss: tensor(0.3465)\n",
      "10316 Training Loss: tensor(0.3465)\n",
      "10317 Training Loss: tensor(0.3475)\n",
      "10318 Training Loss: tensor(0.3465)\n",
      "10319 Training Loss: tensor(0.3465)\n",
      "10320 Training Loss: tensor(0.3452)\n",
      "10321 Training Loss: tensor(0.3478)\n",
      "10322 Training Loss: tensor(0.3591)\n",
      "10323 Training Loss: tensor(0.3467)\n",
      "10324 Training Loss: tensor(0.3465)\n",
      "10325 Training Loss: tensor(0.3462)\n",
      "10326 Training Loss: tensor(0.3454)\n",
      "10327 Training Loss: tensor(0.3454)\n",
      "10328 Training Loss: tensor(0.3461)\n",
      "10329 Training Loss: tensor(0.3498)\n",
      "10330 Training Loss: tensor(0.3460)\n",
      "10331 Training Loss: tensor(0.3496)\n",
      "10332 Training Loss: tensor(0.3571)\n",
      "10333 Training Loss: tensor(0.3455)\n",
      "10334 Training Loss: tensor(0.3479)\n",
      "10335 Training Loss: tensor(0.3508)\n",
      "10336 Training Loss: tensor(0.3471)\n",
      "10337 Training Loss: tensor(0.3480)\n",
      "10338 Training Loss: tensor(0.3500)\n",
      "10339 Training Loss: tensor(0.3489)\n",
      "10340 Training Loss: tensor(0.3479)\n",
      "10341 Training Loss: tensor(0.3518)\n",
      "10342 Training Loss: tensor(0.3482)\n",
      "10343 Training Loss: tensor(0.3486)\n",
      "10344 Training Loss: tensor(0.3471)\n",
      "10345 Training Loss: tensor(0.3461)\n",
      "10346 Training Loss: tensor(0.3479)\n",
      "10347 Training Loss: tensor(0.3469)\n",
      "10348 Training Loss: tensor(0.3472)\n",
      "10349 Training Loss: tensor(0.3463)\n",
      "10350 Training Loss: tensor(0.3517)\n",
      "10351 Training Loss: tensor(0.3467)\n",
      "10352 Training Loss: tensor(0.3475)\n",
      "10353 Training Loss: tensor(0.3460)\n",
      "10354 Training Loss: tensor(0.3454)\n",
      "10355 Training Loss: tensor(0.3502)\n",
      "10356 Training Loss: tensor(0.3465)\n",
      "10357 Training Loss: tensor(0.3494)\n",
      "10358 Training Loss: tensor(0.3517)\n",
      "10359 Training Loss: tensor(0.3472)\n",
      "10360 Training Loss: tensor(0.3460)\n",
      "10361 Training Loss: tensor(0.3461)\n",
      "10362 Training Loss: tensor(0.3499)\n",
      "10363 Training Loss: tensor(0.3458)\n",
      "10364 Training Loss: tensor(0.3485)\n",
      "10365 Training Loss: tensor(0.3456)\n",
      "10366 Training Loss: tensor(0.3460)\n",
      "10367 Training Loss: tensor(0.3514)\n",
      "10368 Training Loss: tensor(0.3501)\n",
      "10369 Training Loss: tensor(0.3465)\n",
      "10370 Training Loss: tensor(0.3463)\n",
      "10371 Training Loss: tensor(0.3453)\n",
      "10372 Training Loss: tensor(0.3469)\n",
      "10373 Training Loss: tensor(0.3474)\n",
      "10374 Training Loss: tensor(0.3479)\n",
      "10375 Training Loss: tensor(0.3453)\n",
      "10376 Training Loss: tensor(0.3484)\n",
      "10377 Training Loss: tensor(0.3473)\n",
      "10378 Training Loss: tensor(0.3468)\n",
      "10379 Training Loss: tensor(0.3462)\n",
      "10380 Training Loss: tensor(0.3472)\n",
      "10381 Training Loss: tensor(0.3463)\n",
      "10382 Training Loss: tensor(0.3457)\n",
      "10383 Training Loss: tensor(0.3480)\n",
      "10384 Training Loss: tensor(0.3500)\n",
      "10385 Training Loss: tensor(0.3454)\n",
      "10386 Training Loss: tensor(0.3483)\n",
      "10387 Training Loss: tensor(0.3451)\n",
      "10388 Training Loss: tensor(0.3456)\n",
      "10389 Training Loss: tensor(0.3465)\n",
      "10390 Training Loss: tensor(0.3468)\n",
      "10391 Training Loss: tensor(0.3463)\n",
      "10392 Training Loss: tensor(0.3470)\n",
      "10393 Training Loss: tensor(0.3510)\n",
      "10394 Training Loss: tensor(0.3449)\n",
      "10395 Training Loss: tensor(0.3467)\n",
      "10396 Training Loss: tensor(0.3474)\n",
      "10397 Training Loss: tensor(0.3505)\n",
      "10398 Training Loss: tensor(0.3469)\n",
      "10399 Training Loss: tensor(0.3517)\n",
      "10400 Training Loss: tensor(0.3512)\n",
      "10401 Training Loss: tensor(0.3476)\n",
      "10402 Training Loss: tensor(0.3460)\n",
      "10403 Training Loss: tensor(0.3470)\n",
      "10404 Training Loss: tensor(0.3472)\n",
      "10405 Training Loss: tensor(0.3476)\n",
      "10406 Training Loss: tensor(0.3451)\n",
      "10407 Training Loss: tensor(0.3502)\n",
      "10408 Training Loss: tensor(0.3461)\n",
      "10409 Training Loss: tensor(0.3457)\n",
      "10410 Training Loss: tensor(0.3544)\n",
      "10411 Training Loss: tensor(0.3463)\n",
      "10412 Training Loss: tensor(0.3463)\n",
      "10413 Training Loss: tensor(0.3485)\n",
      "10414 Training Loss: tensor(0.3477)\n",
      "10415 Training Loss: tensor(0.3488)\n",
      "10416 Training Loss: tensor(0.3489)\n",
      "10417 Training Loss: tensor(0.3462)\n",
      "10418 Training Loss: tensor(0.3480)\n",
      "10419 Training Loss: tensor(0.3494)\n",
      "10420 Training Loss: tensor(0.3460)\n",
      "10421 Training Loss: tensor(0.3473)\n",
      "10422 Training Loss: tensor(0.3459)\n",
      "10423 Training Loss: tensor(0.3468)\n",
      "10424 Training Loss: tensor(0.3476)\n",
      "10425 Training Loss: tensor(0.3519)\n",
      "10426 Training Loss: tensor(0.3456)\n",
      "10427 Training Loss: tensor(0.3508)\n",
      "10428 Training Loss: tensor(0.3496)\n",
      "10429 Training Loss: tensor(0.3462)\n",
      "10430 Training Loss: tensor(0.3461)\n",
      "10431 Training Loss: tensor(0.3469)\n",
      "10432 Training Loss: tensor(0.3489)\n",
      "10433 Training Loss: tensor(0.3458)\n",
      "10434 Training Loss: tensor(0.3505)\n",
      "10435 Training Loss: tensor(0.3463)\n",
      "10436 Training Loss: tensor(0.3456)\n",
      "10437 Training Loss: tensor(0.3461)\n",
      "10438 Training Loss: tensor(0.3465)\n",
      "10439 Training Loss: tensor(0.3457)\n",
      "10440 Training Loss: tensor(0.3470)\n",
      "10441 Training Loss: tensor(0.3476)\n",
      "10442 Training Loss: tensor(0.3448)\n",
      "10443 Training Loss: tensor(0.3451)\n",
      "10444 Training Loss: tensor(0.3462)\n",
      "10445 Training Loss: tensor(0.3446)\n",
      "10446 Training Loss: tensor(0.3497)\n",
      "10447 Training Loss: tensor(0.3464)\n",
      "10448 Training Loss: tensor(0.3522)\n",
      "10449 Training Loss: tensor(0.3451)\n",
      "10450 Training Loss: tensor(0.3450)\n",
      "10451 Training Loss: tensor(0.3455)\n",
      "10452 Training Loss: tensor(0.3461)\n",
      "10453 Training Loss: tensor(0.3461)\n",
      "10454 Training Loss: tensor(0.3454)\n",
      "10455 Training Loss: tensor(0.3448)\n",
      "10456 Training Loss: tensor(0.3449)\n",
      "10457 Training Loss: tensor(0.3447)\n",
      "10458 Training Loss: tensor(0.3449)\n",
      "10459 Training Loss: tensor(0.3461)\n",
      "10460 Training Loss: tensor(0.3493)\n",
      "10461 Training Loss: tensor(0.3451)\n",
      "10462 Training Loss: tensor(0.3556)\n",
      "10463 Training Loss: tensor(0.3500)\n",
      "10464 Training Loss: tensor(0.3491)\n",
      "10465 Training Loss: tensor(0.3487)\n",
      "10466 Training Loss: tensor(0.3449)\n",
      "10467 Training Loss: tensor(0.3464)\n",
      "10468 Training Loss: tensor(0.3453)\n",
      "10469 Training Loss: tensor(0.3483)\n",
      "10470 Training Loss: tensor(0.3525)\n",
      "10471 Training Loss: tensor(0.3507)\n",
      "10472 Training Loss: tensor(0.3477)\n",
      "10473 Training Loss: tensor(0.3501)\n",
      "10474 Training Loss: tensor(0.3470)\n",
      "10475 Training Loss: tensor(0.3473)\n",
      "10476 Training Loss: tensor(0.3488)\n",
      "10477 Training Loss: tensor(0.3513)\n",
      "10478 Training Loss: tensor(0.3465)\n",
      "10479 Training Loss: tensor(0.3460)\n",
      "10480 Training Loss: tensor(0.3479)\n",
      "10481 Training Loss: tensor(0.3469)\n",
      "10482 Training Loss: tensor(0.3478)\n",
      "10483 Training Loss: tensor(0.3474)\n",
      "10484 Training Loss: tensor(0.3468)\n",
      "10485 Training Loss: tensor(0.3467)\n",
      "10486 Training Loss: tensor(0.3475)\n",
      "10487 Training Loss: tensor(0.3459)\n",
      "10488 Training Loss: tensor(0.3462)\n",
      "10489 Training Loss: tensor(0.3458)\n",
      "10490 Training Loss: tensor(0.3485)\n",
      "10491 Training Loss: tensor(0.3460)\n",
      "10492 Training Loss: tensor(0.3449)\n",
      "10493 Training Loss: tensor(0.3460)\n",
      "10494 Training Loss: tensor(0.3445)\n",
      "10495 Training Loss: tensor(0.3506)\n",
      "10496 Training Loss: tensor(0.3502)\n",
      "10497 Training Loss: tensor(0.3455)\n",
      "10498 Training Loss: tensor(0.3447)\n",
      "10499 Training Loss: tensor(0.3446)\n",
      "10500 Training Loss: tensor(0.3458)\n",
      "10501 Training Loss: tensor(0.3473)\n",
      "10502 Training Loss: tensor(0.3453)\n",
      "10503 Training Loss: tensor(0.3453)\n",
      "10504 Training Loss: tensor(0.3445)\n",
      "10505 Training Loss: tensor(0.3449)\n",
      "10506 Training Loss: tensor(0.3494)\n",
      "10507 Training Loss: tensor(0.3498)\n",
      "10508 Training Loss: tensor(0.3453)\n",
      "10509 Training Loss: tensor(0.3458)\n",
      "10510 Training Loss: tensor(0.3512)\n",
      "10511 Training Loss: tensor(0.3446)\n",
      "10512 Training Loss: tensor(0.3459)\n",
      "10513 Training Loss: tensor(0.3451)\n",
      "10514 Training Loss: tensor(0.3473)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10515 Training Loss: tensor(0.3465)\n",
      "10516 Training Loss: tensor(0.3458)\n",
      "10517 Training Loss: tensor(0.3451)\n",
      "10518 Training Loss: tensor(0.3449)\n",
      "10519 Training Loss: tensor(0.3525)\n",
      "10520 Training Loss: tensor(0.3450)\n",
      "10521 Training Loss: tensor(0.3521)\n",
      "10522 Training Loss: tensor(0.3456)\n",
      "10523 Training Loss: tensor(0.3462)\n",
      "10524 Training Loss: tensor(0.3469)\n",
      "10525 Training Loss: tensor(0.3478)\n",
      "10526 Training Loss: tensor(0.3470)\n",
      "10527 Training Loss: tensor(0.3454)\n",
      "10528 Training Loss: tensor(0.3494)\n",
      "10529 Training Loss: tensor(0.3459)\n",
      "10530 Training Loss: tensor(0.3463)\n",
      "10531 Training Loss: tensor(0.3528)\n",
      "10532 Training Loss: tensor(0.3450)\n",
      "10533 Training Loss: tensor(0.3466)\n",
      "10534 Training Loss: tensor(0.3449)\n",
      "10535 Training Loss: tensor(0.3463)\n",
      "10536 Training Loss: tensor(0.3453)\n",
      "10537 Training Loss: tensor(0.3460)\n",
      "10538 Training Loss: tensor(0.3450)\n",
      "10539 Training Loss: tensor(0.3454)\n",
      "10540 Training Loss: tensor(0.3448)\n",
      "10541 Training Loss: tensor(0.3449)\n",
      "10542 Training Loss: tensor(0.3445)\n",
      "10543 Training Loss: tensor(0.3454)\n",
      "10544 Training Loss: tensor(0.3453)\n",
      "10545 Training Loss: tensor(0.3582)\n",
      "10546 Training Loss: tensor(0.3454)\n",
      "10547 Training Loss: tensor(0.3464)\n",
      "10548 Training Loss: tensor(0.3459)\n",
      "10549 Training Loss: tensor(0.3469)\n",
      "10550 Training Loss: tensor(0.3454)\n",
      "10551 Training Loss: tensor(0.3453)\n",
      "10552 Training Loss: tensor(0.3465)\n",
      "10553 Training Loss: tensor(0.3452)\n",
      "10554 Training Loss: tensor(0.3464)\n",
      "10555 Training Loss: tensor(0.3462)\n",
      "10556 Training Loss: tensor(0.3463)\n",
      "10557 Training Loss: tensor(0.3559)\n",
      "10558 Training Loss: tensor(0.3467)\n",
      "10559 Training Loss: tensor(0.3556)\n",
      "10560 Training Loss: tensor(0.3459)\n",
      "10561 Training Loss: tensor(0.3451)\n",
      "10562 Training Loss: tensor(0.3462)\n",
      "10563 Training Loss: tensor(0.3460)\n",
      "10564 Training Loss: tensor(0.3528)\n",
      "10565 Training Loss: tensor(0.3494)\n",
      "10566 Training Loss: tensor(0.3473)\n",
      "10567 Training Loss: tensor(0.3467)\n",
      "10568 Training Loss: tensor(0.3450)\n",
      "10569 Training Loss: tensor(0.3474)\n",
      "10570 Training Loss: tensor(0.3456)\n",
      "10571 Training Loss: tensor(0.3516)\n",
      "10572 Training Loss: tensor(0.3461)\n",
      "10573 Training Loss: tensor(0.3465)\n",
      "10574 Training Loss: tensor(0.3456)\n",
      "10575 Training Loss: tensor(0.3468)\n",
      "10576 Training Loss: tensor(0.3463)\n",
      "10577 Training Loss: tensor(0.3448)\n",
      "10578 Training Loss: tensor(0.3467)\n",
      "10579 Training Loss: tensor(0.3462)\n",
      "10580 Training Loss: tensor(0.3525)\n",
      "10581 Training Loss: tensor(0.3453)\n",
      "10582 Training Loss: tensor(0.3461)\n",
      "10583 Training Loss: tensor(0.3457)\n",
      "10584 Training Loss: tensor(0.3460)\n",
      "10585 Training Loss: tensor(0.3453)\n",
      "10586 Training Loss: tensor(0.3456)\n",
      "10587 Training Loss: tensor(0.3457)\n",
      "10588 Training Loss: tensor(0.3463)\n",
      "10589 Training Loss: tensor(0.3466)\n",
      "10590 Training Loss: tensor(0.3497)\n",
      "10591 Training Loss: tensor(0.3456)\n",
      "10592 Training Loss: tensor(0.3454)\n",
      "10593 Training Loss: tensor(0.3477)\n",
      "10594 Training Loss: tensor(0.3468)\n",
      "10595 Training Loss: tensor(0.3473)\n",
      "10596 Training Loss: tensor(0.3455)\n",
      "10597 Training Loss: tensor(0.3453)\n",
      "10598 Training Loss: tensor(0.3461)\n",
      "10599 Training Loss: tensor(0.3458)\n",
      "10600 Training Loss: tensor(0.3490)\n",
      "10601 Training Loss: tensor(0.3521)\n",
      "10602 Training Loss: tensor(0.3466)\n",
      "10603 Training Loss: tensor(0.3630)\n",
      "10604 Training Loss: tensor(0.3520)\n",
      "10605 Training Loss: tensor(0.3497)\n",
      "10606 Training Loss: tensor(0.3476)\n",
      "10607 Training Loss: tensor(0.3462)\n",
      "10608 Training Loss: tensor(0.3464)\n",
      "10609 Training Loss: tensor(0.3466)\n",
      "10610 Training Loss: tensor(0.3472)\n",
      "10611 Training Loss: tensor(0.3485)\n",
      "10612 Training Loss: tensor(0.3497)\n",
      "10613 Training Loss: tensor(0.3464)\n",
      "10614 Training Loss: tensor(0.3465)\n",
      "10615 Training Loss: tensor(0.3467)\n",
      "10616 Training Loss: tensor(0.3459)\n",
      "10617 Training Loss: tensor(0.3465)\n",
      "10618 Training Loss: tensor(0.3453)\n",
      "10619 Training Loss: tensor(0.3481)\n",
      "10620 Training Loss: tensor(0.3456)\n",
      "10621 Training Loss: tensor(0.3466)\n",
      "10622 Training Loss: tensor(0.3458)\n",
      "10623 Training Loss: tensor(0.3449)\n",
      "10624 Training Loss: tensor(0.3455)\n",
      "10625 Training Loss: tensor(0.3496)\n",
      "10626 Training Loss: tensor(0.3527)\n",
      "10627 Training Loss: tensor(0.3527)\n",
      "10628 Training Loss: tensor(0.3460)\n",
      "10629 Training Loss: tensor(0.3473)\n",
      "10630 Training Loss: tensor(0.3454)\n",
      "10631 Training Loss: tensor(0.3503)\n",
      "10632 Training Loss: tensor(0.3462)\n",
      "10633 Training Loss: tensor(0.3464)\n",
      "10634 Training Loss: tensor(0.3514)\n",
      "10635 Training Loss: tensor(0.3455)\n",
      "10636 Training Loss: tensor(0.3476)\n",
      "10637 Training Loss: tensor(0.3462)\n",
      "10638 Training Loss: tensor(0.3459)\n",
      "10639 Training Loss: tensor(0.3479)\n",
      "10640 Training Loss: tensor(0.3476)\n",
      "10641 Training Loss: tensor(0.3465)\n",
      "10642 Training Loss: tensor(0.3468)\n",
      "10643 Training Loss: tensor(0.3462)\n",
      "10644 Training Loss: tensor(0.3463)\n",
      "10645 Training Loss: tensor(0.3450)\n",
      "10646 Training Loss: tensor(0.3475)\n",
      "10647 Training Loss: tensor(0.3474)\n",
      "10648 Training Loss: tensor(0.3480)\n",
      "10649 Training Loss: tensor(0.3489)\n",
      "10650 Training Loss: tensor(0.3454)\n",
      "10651 Training Loss: tensor(0.3462)\n",
      "10652 Training Loss: tensor(0.3448)\n",
      "10653 Training Loss: tensor(0.3459)\n",
      "10654 Training Loss: tensor(0.3506)\n",
      "10655 Training Loss: tensor(0.3466)\n",
      "10656 Training Loss: tensor(0.3506)\n",
      "10657 Training Loss: tensor(0.3471)\n",
      "10658 Training Loss: tensor(0.3513)\n",
      "10659 Training Loss: tensor(0.3511)\n",
      "10660 Training Loss: tensor(0.3449)\n",
      "10661 Training Loss: tensor(0.3459)\n",
      "10662 Training Loss: tensor(0.3460)\n",
      "10663 Training Loss: tensor(0.3455)\n",
      "10664 Training Loss: tensor(0.3487)\n",
      "10665 Training Loss: tensor(0.3478)\n",
      "10666 Training Loss: tensor(0.3483)\n",
      "10667 Training Loss: tensor(0.3468)\n",
      "10668 Training Loss: tensor(0.3474)\n",
      "10669 Training Loss: tensor(0.3462)\n",
      "10670 Training Loss: tensor(0.3498)\n",
      "10671 Training Loss: tensor(0.3454)\n",
      "10672 Training Loss: tensor(0.3457)\n",
      "10673 Training Loss: tensor(0.3459)\n",
      "10674 Training Loss: tensor(0.3465)\n",
      "10675 Training Loss: tensor(0.3474)\n",
      "10676 Training Loss: tensor(0.3514)\n",
      "10677 Training Loss: tensor(0.3529)\n",
      "10678 Training Loss: tensor(0.3451)\n",
      "10679 Training Loss: tensor(0.3485)\n",
      "10680 Training Loss: tensor(0.3484)\n",
      "10681 Training Loss: tensor(0.3459)\n",
      "10682 Training Loss: tensor(0.3482)\n",
      "10683 Training Loss: tensor(0.3464)\n",
      "10684 Training Loss: tensor(0.3541)\n",
      "10685 Training Loss: tensor(0.3463)\n",
      "10686 Training Loss: tensor(0.3465)\n",
      "10687 Training Loss: tensor(0.3490)\n",
      "10688 Training Loss: tensor(0.3458)\n",
      "10689 Training Loss: tensor(0.3504)\n",
      "10690 Training Loss: tensor(0.3496)\n",
      "10691 Training Loss: tensor(0.3463)\n",
      "10692 Training Loss: tensor(0.3468)\n",
      "10693 Training Loss: tensor(0.3492)\n",
      "10694 Training Loss: tensor(0.3455)\n",
      "10695 Training Loss: tensor(0.3461)\n",
      "10696 Training Loss: tensor(0.3479)\n",
      "10697 Training Loss: tensor(0.3451)\n",
      "10698 Training Loss: tensor(0.3468)\n",
      "10699 Training Loss: tensor(0.3510)\n",
      "10700 Training Loss: tensor(0.3489)\n",
      "10701 Training Loss: tensor(0.3500)\n",
      "10702 Training Loss: tensor(0.3456)\n",
      "10703 Training Loss: tensor(0.3453)\n",
      "10704 Training Loss: tensor(0.3462)\n",
      "10705 Training Loss: tensor(0.3471)\n",
      "10706 Training Loss: tensor(0.3463)\n",
      "10707 Training Loss: tensor(0.3474)\n",
      "10708 Training Loss: tensor(0.3479)\n",
      "10709 Training Loss: tensor(0.3457)\n",
      "10710 Training Loss: tensor(0.3522)\n",
      "10711 Training Loss: tensor(0.3460)\n",
      "10712 Training Loss: tensor(0.3447)\n",
      "10713 Training Loss: tensor(0.3453)\n",
      "10714 Training Loss: tensor(0.3489)\n",
      "10715 Training Loss: tensor(0.3454)\n",
      "10716 Training Loss: tensor(0.3459)\n",
      "10717 Training Loss: tensor(0.3451)\n",
      "10718 Training Loss: tensor(0.3458)\n",
      "10719 Training Loss: tensor(0.3448)\n",
      "10720 Training Loss: tensor(0.3462)\n",
      "10721 Training Loss: tensor(0.3445)\n",
      "10722 Training Loss: tensor(0.3520)\n",
      "10723 Training Loss: tensor(0.3455)\n",
      "10724 Training Loss: tensor(0.3450)\n",
      "10725 Training Loss: tensor(0.3451)\n",
      "10726 Training Loss: tensor(0.3469)\n",
      "10727 Training Loss: tensor(0.3498)\n",
      "10728 Training Loss: tensor(0.3553)\n",
      "10729 Training Loss: tensor(0.3518)\n",
      "10730 Training Loss: tensor(0.3478)\n",
      "10731 Training Loss: tensor(0.3509)\n",
      "10732 Training Loss: tensor(0.3457)\n",
      "10733 Training Loss: tensor(0.3467)\n",
      "10734 Training Loss: tensor(0.3461)\n",
      "10735 Training Loss: tensor(0.3466)\n",
      "10736 Training Loss: tensor(0.3469)\n",
      "10737 Training Loss: tensor(0.3464)\n",
      "10738 Training Loss: tensor(0.3456)\n",
      "10739 Training Loss: tensor(0.3469)\n",
      "10740 Training Loss: tensor(0.3480)\n",
      "10741 Training Loss: tensor(0.3465)\n",
      "10742 Training Loss: tensor(0.3486)\n",
      "10743 Training Loss: tensor(0.3485)\n",
      "10744 Training Loss: tensor(0.3461)\n",
      "10745 Training Loss: tensor(0.3481)\n",
      "10746 Training Loss: tensor(0.3458)\n",
      "10747 Training Loss: tensor(0.3461)\n",
      "10748 Training Loss: tensor(0.3451)\n",
      "10749 Training Loss: tensor(0.3455)\n",
      "10750 Training Loss: tensor(0.3469)\n",
      "10751 Training Loss: tensor(0.3467)\n",
      "10752 Training Loss: tensor(0.3487)\n",
      "10753 Training Loss: tensor(0.3452)\n",
      "10754 Training Loss: tensor(0.3466)\n",
      "10755 Training Loss: tensor(0.3449)\n",
      "10756 Training Loss: tensor(0.3456)\n",
      "10757 Training Loss: tensor(0.3457)\n",
      "10758 Training Loss: tensor(0.3446)\n",
      "10759 Training Loss: tensor(0.3472)\n",
      "10760 Training Loss: tensor(0.3455)\n",
      "10761 Training Loss: tensor(0.3449)\n",
      "10762 Training Loss: tensor(0.3457)\n",
      "10763 Training Loss: tensor(0.3491)\n",
      "10764 Training Loss: tensor(0.3453)\n",
      "10765 Training Loss: tensor(0.3546)\n",
      "10766 Training Loss: tensor(0.3463)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10767 Training Loss: tensor(0.3458)\n",
      "10768 Training Loss: tensor(0.3457)\n",
      "10769 Training Loss: tensor(0.3507)\n",
      "10770 Training Loss: tensor(0.3454)\n",
      "10771 Training Loss: tensor(0.3468)\n",
      "10772 Training Loss: tensor(0.3495)\n",
      "10773 Training Loss: tensor(0.3507)\n",
      "10774 Training Loss: tensor(0.3459)\n",
      "10775 Training Loss: tensor(0.3465)\n",
      "10776 Training Loss: tensor(0.3464)\n",
      "10777 Training Loss: tensor(0.3464)\n",
      "10778 Training Loss: tensor(0.3489)\n",
      "10779 Training Loss: tensor(0.3453)\n",
      "10780 Training Loss: tensor(0.3458)\n",
      "10781 Training Loss: tensor(0.3455)\n",
      "10782 Training Loss: tensor(0.3460)\n",
      "10783 Training Loss: tensor(0.3461)\n",
      "10784 Training Loss: tensor(0.3462)\n",
      "10785 Training Loss: tensor(0.3457)\n",
      "10786 Training Loss: tensor(0.3454)\n",
      "10787 Training Loss: tensor(0.3446)\n",
      "10788 Training Loss: tensor(0.3447)\n",
      "10789 Training Loss: tensor(0.3500)\n",
      "10790 Training Loss: tensor(0.3470)\n",
      "10791 Training Loss: tensor(0.3456)\n",
      "10792 Training Loss: tensor(0.3463)\n",
      "10793 Training Loss: tensor(0.3445)\n",
      "10794 Training Loss: tensor(0.3449)\n",
      "10795 Training Loss: tensor(0.3455)\n",
      "10796 Training Loss: tensor(0.3452)\n",
      "10797 Training Loss: tensor(0.3483)\n",
      "10798 Training Loss: tensor(0.3466)\n",
      "10799 Training Loss: tensor(0.3461)\n",
      "10800 Training Loss: tensor(0.3443)\n",
      "10801 Training Loss: tensor(0.3454)\n",
      "10802 Training Loss: tensor(0.3445)\n",
      "10803 Training Loss: tensor(0.3441)\n",
      "10804 Training Loss: tensor(0.3483)\n",
      "10805 Training Loss: tensor(0.3484)\n",
      "10806 Training Loss: tensor(0.3452)\n",
      "10807 Training Loss: tensor(0.3452)\n",
      "10808 Training Loss: tensor(0.3456)\n",
      "10809 Training Loss: tensor(0.3510)\n",
      "10810 Training Loss: tensor(0.3446)\n",
      "10811 Training Loss: tensor(0.3443)\n",
      "10812 Training Loss: tensor(0.3468)\n",
      "10813 Training Loss: tensor(0.3489)\n",
      "10814 Training Loss: tensor(0.3455)\n",
      "10815 Training Loss: tensor(0.3443)\n",
      "10816 Training Loss: tensor(0.3452)\n",
      "10817 Training Loss: tensor(0.3462)\n",
      "10818 Training Loss: tensor(0.3441)\n",
      "10819 Training Loss: tensor(0.3462)\n",
      "10820 Training Loss: tensor(0.3443)\n",
      "10821 Training Loss: tensor(0.3456)\n",
      "10822 Training Loss: tensor(0.3482)\n",
      "10823 Training Loss: tensor(0.3492)\n",
      "10824 Training Loss: tensor(0.3447)\n",
      "10825 Training Loss: tensor(0.3476)\n",
      "10826 Training Loss: tensor(0.3504)\n",
      "10827 Training Loss: tensor(0.3465)\n",
      "10828 Training Loss: tensor(0.3450)\n",
      "10829 Training Loss: tensor(0.3485)\n",
      "10830 Training Loss: tensor(0.3472)\n",
      "10831 Training Loss: tensor(0.3459)\n",
      "10832 Training Loss: tensor(0.3455)\n",
      "10833 Training Loss: tensor(0.3506)\n",
      "10834 Training Loss: tensor(0.3450)\n",
      "10835 Training Loss: tensor(0.3478)\n",
      "10836 Training Loss: tensor(0.3452)\n",
      "10837 Training Loss: tensor(0.3450)\n",
      "10838 Training Loss: tensor(0.3481)\n",
      "10839 Training Loss: tensor(0.3478)\n",
      "10840 Training Loss: tensor(0.3485)\n",
      "10841 Training Loss: tensor(0.3461)\n",
      "10842 Training Loss: tensor(0.3475)\n",
      "10843 Training Loss: tensor(0.3475)\n",
      "10844 Training Loss: tensor(0.3474)\n",
      "10845 Training Loss: tensor(0.3459)\n",
      "10846 Training Loss: tensor(0.3480)\n",
      "10847 Training Loss: tensor(0.3475)\n",
      "10848 Training Loss: tensor(0.3459)\n",
      "10849 Training Loss: tensor(0.3458)\n",
      "10850 Training Loss: tensor(0.3451)\n",
      "10851 Training Loss: tensor(0.3462)\n",
      "10852 Training Loss: tensor(0.3478)\n",
      "10853 Training Loss: tensor(0.3470)\n",
      "10854 Training Loss: tensor(0.3446)\n",
      "10855 Training Loss: tensor(0.3452)\n",
      "10856 Training Loss: tensor(0.3495)\n",
      "10857 Training Loss: tensor(0.3452)\n",
      "10858 Training Loss: tensor(0.3460)\n",
      "10859 Training Loss: tensor(0.3459)\n",
      "10860 Training Loss: tensor(0.3471)\n",
      "10861 Training Loss: tensor(0.3455)\n",
      "10862 Training Loss: tensor(0.3452)\n",
      "10863 Training Loss: tensor(0.3473)\n",
      "10864 Training Loss: tensor(0.3509)\n",
      "10865 Training Loss: tensor(0.3495)\n",
      "10866 Training Loss: tensor(0.3465)\n",
      "10867 Training Loss: tensor(0.3447)\n",
      "10868 Training Loss: tensor(0.3483)\n",
      "10869 Training Loss: tensor(0.3470)\n",
      "10870 Training Loss: tensor(0.3473)\n",
      "10871 Training Loss: tensor(0.3482)\n",
      "10872 Training Loss: tensor(0.3457)\n",
      "10873 Training Loss: tensor(0.3448)\n",
      "10874 Training Loss: tensor(0.3514)\n",
      "10875 Training Loss: tensor(0.3483)\n",
      "10876 Training Loss: tensor(0.3449)\n",
      "10877 Training Loss: tensor(0.3455)\n",
      "10878 Training Loss: tensor(0.3453)\n",
      "10879 Training Loss: tensor(0.3457)\n",
      "10880 Training Loss: tensor(0.3513)\n",
      "10881 Training Loss: tensor(0.3457)\n",
      "10882 Training Loss: tensor(0.3475)\n",
      "10883 Training Loss: tensor(0.3443)\n",
      "10884 Training Loss: tensor(0.3448)\n",
      "10885 Training Loss: tensor(0.3454)\n",
      "10886 Training Loss: tensor(0.3441)\n",
      "10887 Training Loss: tensor(0.3468)\n",
      "10888 Training Loss: tensor(0.3450)\n",
      "10889 Training Loss: tensor(0.3499)\n",
      "10890 Training Loss: tensor(0.3444)\n",
      "10891 Training Loss: tensor(0.3445)\n",
      "10892 Training Loss: tensor(0.3471)\n",
      "10893 Training Loss: tensor(0.3457)\n",
      "10894 Training Loss: tensor(0.3459)\n",
      "10895 Training Loss: tensor(0.3467)\n",
      "10896 Training Loss: tensor(0.3482)\n",
      "10897 Training Loss: tensor(0.3485)\n",
      "10898 Training Loss: tensor(0.3469)\n",
      "10899 Training Loss: tensor(0.3444)\n",
      "10900 Training Loss: tensor(0.3452)\n",
      "10901 Training Loss: tensor(0.3459)\n",
      "10902 Training Loss: tensor(0.3456)\n",
      "10903 Training Loss: tensor(0.3442)\n",
      "10904 Training Loss: tensor(0.3465)\n",
      "10905 Training Loss: tensor(0.3449)\n",
      "10906 Training Loss: tensor(0.3464)\n",
      "10907 Training Loss: tensor(0.3439)\n",
      "10908 Training Loss: tensor(0.3448)\n",
      "10909 Training Loss: tensor(0.3481)\n",
      "10910 Training Loss: tensor(0.3491)\n",
      "10911 Training Loss: tensor(0.3455)\n",
      "10912 Training Loss: tensor(0.3475)\n",
      "10913 Training Loss: tensor(0.3441)\n",
      "10914 Training Loss: tensor(0.3444)\n",
      "10915 Training Loss: tensor(0.3459)\n",
      "10916 Training Loss: tensor(0.3523)\n",
      "10917 Training Loss: tensor(0.3498)\n",
      "10918 Training Loss: tensor(0.3440)\n",
      "10919 Training Loss: tensor(0.3558)\n",
      "10920 Training Loss: tensor(0.3449)\n",
      "10921 Training Loss: tensor(0.3454)\n",
      "10922 Training Loss: tensor(0.3477)\n",
      "10923 Training Loss: tensor(0.3458)\n",
      "10924 Training Loss: tensor(0.3485)\n",
      "10925 Training Loss: tensor(0.3458)\n",
      "10926 Training Loss: tensor(0.3468)\n",
      "10927 Training Loss: tensor(0.3449)\n",
      "10928 Training Loss: tensor(0.3458)\n",
      "10929 Training Loss: tensor(0.3456)\n",
      "10930 Training Loss: tensor(0.3473)\n",
      "10931 Training Loss: tensor(0.3466)\n",
      "10932 Training Loss: tensor(0.3463)\n",
      "10933 Training Loss: tensor(0.3469)\n",
      "10934 Training Loss: tensor(0.3442)\n",
      "10935 Training Loss: tensor(0.3465)\n",
      "10936 Training Loss: tensor(0.3485)\n",
      "10937 Training Loss: tensor(0.3456)\n",
      "10938 Training Loss: tensor(0.3513)\n",
      "10939 Training Loss: tensor(0.3457)\n",
      "10940 Training Loss: tensor(0.3451)\n",
      "10941 Training Loss: tensor(0.3487)\n",
      "10942 Training Loss: tensor(0.3456)\n",
      "10943 Training Loss: tensor(0.3458)\n",
      "10944 Training Loss: tensor(0.3472)\n",
      "10945 Training Loss: tensor(0.3481)\n",
      "10946 Training Loss: tensor(0.3475)\n",
      "10947 Training Loss: tensor(0.3455)\n",
      "10948 Training Loss: tensor(0.3467)\n",
      "10949 Training Loss: tensor(0.3450)\n",
      "10950 Training Loss: tensor(0.3496)\n",
      "10951 Training Loss: tensor(0.3441)\n",
      "10952 Training Loss: tensor(0.3453)\n",
      "10953 Training Loss: tensor(0.3462)\n",
      "10954 Training Loss: tensor(0.3508)\n",
      "10955 Training Loss: tensor(0.3465)\n",
      "10956 Training Loss: tensor(0.3465)\n",
      "10957 Training Loss: tensor(0.3473)\n",
      "10958 Training Loss: tensor(0.3459)\n",
      "10959 Training Loss: tensor(0.3470)\n",
      "10960 Training Loss: tensor(0.3461)\n",
      "10961 Training Loss: tensor(0.3461)\n",
      "10962 Training Loss: tensor(0.3473)\n",
      "10963 Training Loss: tensor(0.3491)\n",
      "10964 Training Loss: tensor(0.3500)\n",
      "10965 Training Loss: tensor(0.3467)\n",
      "10966 Training Loss: tensor(0.3448)\n",
      "10967 Training Loss: tensor(0.3463)\n",
      "10968 Training Loss: tensor(0.3508)\n",
      "10969 Training Loss: tensor(0.3457)\n",
      "10970 Training Loss: tensor(0.3456)\n",
      "10971 Training Loss: tensor(0.3462)\n",
      "10972 Training Loss: tensor(0.3454)\n",
      "10973 Training Loss: tensor(0.3468)\n",
      "10974 Training Loss: tensor(0.3464)\n",
      "10975 Training Loss: tensor(0.3460)\n",
      "10976 Training Loss: tensor(0.3467)\n",
      "10977 Training Loss: tensor(0.3459)\n",
      "10978 Training Loss: tensor(0.3450)\n",
      "10979 Training Loss: tensor(0.3484)\n",
      "10980 Training Loss: tensor(0.3502)\n",
      "10981 Training Loss: tensor(0.3466)\n",
      "10982 Training Loss: tensor(0.3463)\n",
      "10983 Training Loss: tensor(0.3506)\n",
      "10984 Training Loss: tensor(0.3458)\n",
      "10985 Training Loss: tensor(0.3455)\n",
      "10986 Training Loss: tensor(0.3440)\n",
      "10987 Training Loss: tensor(0.3448)\n",
      "10988 Training Loss: tensor(0.3481)\n",
      "10989 Training Loss: tensor(0.3452)\n",
      "10990 Training Loss: tensor(0.3442)\n",
      "10991 Training Loss: tensor(0.3452)\n",
      "10992 Training Loss: tensor(0.3447)\n",
      "10993 Training Loss: tensor(0.3489)\n",
      "10994 Training Loss: tensor(0.3449)\n",
      "10995 Training Loss: tensor(0.3449)\n",
      "10996 Training Loss: tensor(0.3466)\n",
      "10997 Training Loss: tensor(0.3469)\n",
      "10998 Training Loss: tensor(0.3449)\n",
      "10999 Training Loss: tensor(0.3456)\n",
      "11000 Training Loss: tensor(0.3449)\n",
      "11001 Training Loss: tensor(0.3450)\n",
      "11002 Training Loss: tensor(0.3449)\n",
      "11003 Training Loss: tensor(0.3458)\n",
      "11004 Training Loss: tensor(0.3481)\n",
      "11005 Training Loss: tensor(0.3454)\n",
      "11006 Training Loss: tensor(0.3480)\n",
      "11007 Training Loss: tensor(0.3453)\n",
      "11008 Training Loss: tensor(0.3462)\n",
      "11009 Training Loss: tensor(0.3450)\n",
      "11010 Training Loss: tensor(0.3462)\n",
      "11011 Training Loss: tensor(0.3454)\n",
      "11012 Training Loss: tensor(0.3452)\n",
      "11013 Training Loss: tensor(0.3456)\n",
      "11014 Training Loss: tensor(0.3480)\n",
      "11015 Training Loss: tensor(0.3439)\n",
      "11016 Training Loss: tensor(0.3484)\n",
      "11017 Training Loss: tensor(0.3464)\n",
      "11018 Training Loss: tensor(0.3540)\n",
      "11019 Training Loss: tensor(0.3503)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11020 Training Loss: tensor(0.3447)\n",
      "11021 Training Loss: tensor(0.3457)\n",
      "11022 Training Loss: tensor(0.3468)\n",
      "11023 Training Loss: tensor(0.3444)\n",
      "11024 Training Loss: tensor(0.3462)\n",
      "11025 Training Loss: tensor(0.3464)\n",
      "11026 Training Loss: tensor(0.3467)\n",
      "11027 Training Loss: tensor(0.3455)\n",
      "11028 Training Loss: tensor(0.3458)\n",
      "11029 Training Loss: tensor(0.3457)\n",
      "11030 Training Loss: tensor(0.3457)\n",
      "11031 Training Loss: tensor(0.3445)\n",
      "11032 Training Loss: tensor(0.3502)\n",
      "11033 Training Loss: tensor(0.3446)\n",
      "11034 Training Loss: tensor(0.3459)\n",
      "11035 Training Loss: tensor(0.3465)\n",
      "11036 Training Loss: tensor(0.3447)\n",
      "11037 Training Loss: tensor(0.3443)\n",
      "11038 Training Loss: tensor(0.3469)\n",
      "11039 Training Loss: tensor(0.3465)\n",
      "11040 Training Loss: tensor(0.3487)\n",
      "11041 Training Loss: tensor(0.3455)\n",
      "11042 Training Loss: tensor(0.3478)\n",
      "11043 Training Loss: tensor(0.3466)\n",
      "11044 Training Loss: tensor(0.3495)\n",
      "11045 Training Loss: tensor(0.3444)\n",
      "11046 Training Loss: tensor(0.3443)\n",
      "11047 Training Loss: tensor(0.3441)\n",
      "11048 Training Loss: tensor(0.3451)\n",
      "11049 Training Loss: tensor(0.3447)\n",
      "11050 Training Loss: tensor(0.3456)\n",
      "11051 Training Loss: tensor(0.3531)\n",
      "11052 Training Loss: tensor(0.3458)\n",
      "11053 Training Loss: tensor(0.3462)\n",
      "11054 Training Loss: tensor(0.3497)\n",
      "11055 Training Loss: tensor(0.3437)\n",
      "11056 Training Loss: tensor(0.3476)\n",
      "11057 Training Loss: tensor(0.3453)\n",
      "11058 Training Loss: tensor(0.3453)\n",
      "11059 Training Loss: tensor(0.3453)\n",
      "11060 Training Loss: tensor(0.3454)\n",
      "11061 Training Loss: tensor(0.3516)\n",
      "11062 Training Loss: tensor(0.3476)\n",
      "11063 Training Loss: tensor(0.3475)\n",
      "11064 Training Loss: tensor(0.3457)\n",
      "11065 Training Loss: tensor(0.3484)\n",
      "11066 Training Loss: tensor(0.3456)\n",
      "11067 Training Loss: tensor(0.3456)\n",
      "11068 Training Loss: tensor(0.3480)\n",
      "11069 Training Loss: tensor(0.3459)\n",
      "11070 Training Loss: tensor(0.3446)\n",
      "11071 Training Loss: tensor(0.3465)\n",
      "11072 Training Loss: tensor(0.3524)\n",
      "11073 Training Loss: tensor(0.3468)\n",
      "11074 Training Loss: tensor(0.3470)\n",
      "11075 Training Loss: tensor(0.3453)\n",
      "11076 Training Loss: tensor(0.3452)\n",
      "11077 Training Loss: tensor(0.3467)\n",
      "11078 Training Loss: tensor(0.3471)\n",
      "11079 Training Loss: tensor(0.3440)\n",
      "11080 Training Loss: tensor(0.3442)\n",
      "11081 Training Loss: tensor(0.3469)\n",
      "11082 Training Loss: tensor(0.3452)\n",
      "11083 Training Loss: tensor(0.3452)\n",
      "11084 Training Loss: tensor(0.3451)\n",
      "11085 Training Loss: tensor(0.3493)\n",
      "11086 Training Loss: tensor(0.3448)\n",
      "11087 Training Loss: tensor(0.3467)\n",
      "11088 Training Loss: tensor(0.3445)\n",
      "11089 Training Loss: tensor(0.3460)\n",
      "11090 Training Loss: tensor(0.3452)\n",
      "11091 Training Loss: tensor(0.3449)\n",
      "11092 Training Loss: tensor(0.3446)\n",
      "11093 Training Loss: tensor(0.3474)\n",
      "11094 Training Loss: tensor(0.3452)\n",
      "11095 Training Loss: tensor(0.3459)\n",
      "11096 Training Loss: tensor(0.3496)\n",
      "11097 Training Loss: tensor(0.3444)\n",
      "11098 Training Loss: tensor(0.3460)\n",
      "11099 Training Loss: tensor(0.3440)\n",
      "11100 Training Loss: tensor(0.3449)\n",
      "11101 Training Loss: tensor(0.3444)\n",
      "11102 Training Loss: tensor(0.3471)\n",
      "11103 Training Loss: tensor(0.3494)\n",
      "11104 Training Loss: tensor(0.3448)\n",
      "11105 Training Loss: tensor(0.3518)\n",
      "11106 Training Loss: tensor(0.3445)\n",
      "11107 Training Loss: tensor(0.3451)\n",
      "11108 Training Loss: tensor(0.3497)\n",
      "11109 Training Loss: tensor(0.3516)\n",
      "11110 Training Loss: tensor(0.3458)\n",
      "11111 Training Loss: tensor(0.3479)\n",
      "11112 Training Loss: tensor(0.3485)\n",
      "11113 Training Loss: tensor(0.3476)\n",
      "11114 Training Loss: tensor(0.3455)\n",
      "11115 Training Loss: tensor(0.3454)\n",
      "11116 Training Loss: tensor(0.3462)\n",
      "11117 Training Loss: tensor(0.3448)\n",
      "11118 Training Loss: tensor(0.3456)\n",
      "11119 Training Loss: tensor(0.3446)\n",
      "11120 Training Loss: tensor(0.3459)\n",
      "11121 Training Loss: tensor(0.3563)\n",
      "11122 Training Loss: tensor(0.3472)\n",
      "11123 Training Loss: tensor(0.3457)\n",
      "11124 Training Loss: tensor(0.3507)\n",
      "11125 Training Loss: tensor(0.3447)\n",
      "11126 Training Loss: tensor(0.3447)\n",
      "11127 Training Loss: tensor(0.3526)\n",
      "11128 Training Loss: tensor(0.3457)\n",
      "11129 Training Loss: tensor(0.3450)\n",
      "11130 Training Loss: tensor(0.3479)\n",
      "11131 Training Loss: tensor(0.3485)\n",
      "11132 Training Loss: tensor(0.3466)\n",
      "11133 Training Loss: tensor(0.3451)\n",
      "11134 Training Loss: tensor(0.3443)\n",
      "11135 Training Loss: tensor(0.3443)\n",
      "11136 Training Loss: tensor(0.3451)\n",
      "11137 Training Loss: tensor(0.3457)\n",
      "11138 Training Loss: tensor(0.3480)\n",
      "11139 Training Loss: tensor(0.3487)\n",
      "11140 Training Loss: tensor(0.3458)\n",
      "11141 Training Loss: tensor(0.3454)\n",
      "11142 Training Loss: tensor(0.3529)\n",
      "11143 Training Loss: tensor(0.3460)\n",
      "11144 Training Loss: tensor(0.3448)\n",
      "11145 Training Loss: tensor(0.3463)\n",
      "11146 Training Loss: tensor(0.3449)\n",
      "11147 Training Loss: tensor(0.3454)\n",
      "11148 Training Loss: tensor(0.3457)\n",
      "11149 Training Loss: tensor(0.3474)\n",
      "11150 Training Loss: tensor(0.3484)\n",
      "11151 Training Loss: tensor(0.3464)\n",
      "11152 Training Loss: tensor(0.3474)\n",
      "11153 Training Loss: tensor(0.3449)\n",
      "11154 Training Loss: tensor(0.3473)\n",
      "11155 Training Loss: tensor(0.3474)\n",
      "11156 Training Loss: tensor(0.3458)\n",
      "11157 Training Loss: tensor(0.3482)\n",
      "11158 Training Loss: tensor(0.3442)\n",
      "11159 Training Loss: tensor(0.3449)\n",
      "11160 Training Loss: tensor(0.3463)\n",
      "11161 Training Loss: tensor(0.3448)\n",
      "11162 Training Loss: tensor(0.3450)\n",
      "11163 Training Loss: tensor(0.3463)\n",
      "11164 Training Loss: tensor(0.3445)\n",
      "11165 Training Loss: tensor(0.3440)\n",
      "11166 Training Loss: tensor(0.3456)\n",
      "11167 Training Loss: tensor(0.3490)\n",
      "11168 Training Loss: tensor(0.3450)\n",
      "11169 Training Loss: tensor(0.3447)\n",
      "11170 Training Loss: tensor(0.3445)\n",
      "11171 Training Loss: tensor(0.3496)\n",
      "11172 Training Loss: tensor(0.3454)\n",
      "11173 Training Loss: tensor(0.3481)\n",
      "11174 Training Loss: tensor(0.3462)\n",
      "11175 Training Loss: tensor(0.3454)\n",
      "11176 Training Loss: tensor(0.3480)\n",
      "11177 Training Loss: tensor(0.3476)\n",
      "11178 Training Loss: tensor(0.3517)\n",
      "11179 Training Loss: tensor(0.3468)\n",
      "11180 Training Loss: tensor(0.3450)\n",
      "11181 Training Loss: tensor(0.3458)\n",
      "11182 Training Loss: tensor(0.3454)\n",
      "11183 Training Loss: tensor(0.3457)\n",
      "11184 Training Loss: tensor(0.3451)\n",
      "11185 Training Loss: tensor(0.3454)\n",
      "11186 Training Loss: tensor(0.3458)\n",
      "11187 Training Loss: tensor(0.3452)\n",
      "11188 Training Loss: tensor(0.3463)\n",
      "11189 Training Loss: tensor(0.3450)\n",
      "11190 Training Loss: tensor(0.3531)\n",
      "11191 Training Loss: tensor(0.3448)\n",
      "11192 Training Loss: tensor(0.3448)\n",
      "11193 Training Loss: tensor(0.3482)\n",
      "11194 Training Loss: tensor(0.3448)\n",
      "11195 Training Loss: tensor(0.3457)\n",
      "11196 Training Loss: tensor(0.3456)\n",
      "11197 Training Loss: tensor(0.3464)\n",
      "11198 Training Loss: tensor(0.3462)\n",
      "11199 Training Loss: tensor(0.3460)\n",
      "11200 Training Loss: tensor(0.3444)\n",
      "11201 Training Loss: tensor(0.3453)\n",
      "11202 Training Loss: tensor(0.3494)\n",
      "11203 Training Loss: tensor(0.3457)\n",
      "11204 Training Loss: tensor(0.3442)\n",
      "11205 Training Loss: tensor(0.3506)\n",
      "11206 Training Loss: tensor(0.3460)\n",
      "11207 Training Loss: tensor(0.3454)\n",
      "11208 Training Loss: tensor(0.3469)\n",
      "11209 Training Loss: tensor(0.3476)\n",
      "11210 Training Loss: tensor(0.3470)\n",
      "11211 Training Loss: tensor(0.3460)\n",
      "11212 Training Loss: tensor(0.3531)\n",
      "11213 Training Loss: tensor(0.3456)\n",
      "11214 Training Loss: tensor(0.3457)\n",
      "11215 Training Loss: tensor(0.3443)\n",
      "11216 Training Loss: tensor(0.3454)\n",
      "11217 Training Loss: tensor(0.3555)\n",
      "11218 Training Loss: tensor(0.3460)\n",
      "11219 Training Loss: tensor(0.3483)\n",
      "11220 Training Loss: tensor(0.3462)\n",
      "11221 Training Loss: tensor(0.3462)\n",
      "11222 Training Loss: tensor(0.3477)\n",
      "11223 Training Loss: tensor(0.3472)\n",
      "11224 Training Loss: tensor(0.3449)\n",
      "11225 Training Loss: tensor(0.3468)\n",
      "11226 Training Loss: tensor(0.3467)\n",
      "11227 Training Loss: tensor(0.3493)\n",
      "11228 Training Loss: tensor(0.3447)\n",
      "11229 Training Loss: tensor(0.3470)\n",
      "11230 Training Loss: tensor(0.3452)\n",
      "11231 Training Loss: tensor(0.3506)\n",
      "11232 Training Loss: tensor(0.3450)\n",
      "11233 Training Loss: tensor(0.3474)\n",
      "11234 Training Loss: tensor(0.3486)\n",
      "11235 Training Loss: tensor(0.3562)\n",
      "11236 Training Loss: tensor(0.3476)\n",
      "11237 Training Loss: tensor(0.3454)\n",
      "11238 Training Loss: tensor(0.3456)\n",
      "11239 Training Loss: tensor(0.3482)\n",
      "11240 Training Loss: tensor(0.3481)\n",
      "11241 Training Loss: tensor(0.3454)\n",
      "11242 Training Loss: tensor(0.3502)\n",
      "11243 Training Loss: tensor(0.3466)\n",
      "11244 Training Loss: tensor(0.3483)\n",
      "11245 Training Loss: tensor(0.3471)\n",
      "11246 Training Loss: tensor(0.3469)\n",
      "11247 Training Loss: tensor(0.3470)\n",
      "11248 Training Loss: tensor(0.3467)\n",
      "11249 Training Loss: tensor(0.3455)\n",
      "11250 Training Loss: tensor(0.3503)\n",
      "11251 Training Loss: tensor(0.3459)\n",
      "11252 Training Loss: tensor(0.3471)\n",
      "11253 Training Loss: tensor(0.3509)\n",
      "11254 Training Loss: tensor(0.3476)\n",
      "11255 Training Loss: tensor(0.3455)\n",
      "11256 Training Loss: tensor(0.3491)\n",
      "11257 Training Loss: tensor(0.3460)\n",
      "11258 Training Loss: tensor(0.3446)\n",
      "11259 Training Loss: tensor(0.3472)\n",
      "11260 Training Loss: tensor(0.3447)\n",
      "11261 Training Loss: tensor(0.3454)\n",
      "11262 Training Loss: tensor(0.3457)\n",
      "11263 Training Loss: tensor(0.3453)\n",
      "11264 Training Loss: tensor(0.3452)\n",
      "11265 Training Loss: tensor(0.3484)\n",
      "11266 Training Loss: tensor(0.3453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11267 Training Loss: tensor(0.3449)\n",
      "11268 Training Loss: tensor(0.3530)\n",
      "11269 Training Loss: tensor(0.3466)\n",
      "11270 Training Loss: tensor(0.3450)\n",
      "11271 Training Loss: tensor(0.3449)\n",
      "11272 Training Loss: tensor(0.3514)\n",
      "11273 Training Loss: tensor(0.3461)\n",
      "11274 Training Loss: tensor(0.3505)\n",
      "11275 Training Loss: tensor(0.3492)\n",
      "11276 Training Loss: tensor(0.3465)\n",
      "11277 Training Loss: tensor(0.3494)\n",
      "11278 Training Loss: tensor(0.3468)\n",
      "11279 Training Loss: tensor(0.3487)\n",
      "11280 Training Loss: tensor(0.3461)\n",
      "11281 Training Loss: tensor(0.3514)\n",
      "11282 Training Loss: tensor(0.3473)\n",
      "11283 Training Loss: tensor(0.3484)\n",
      "11284 Training Loss: tensor(0.3474)\n",
      "11285 Training Loss: tensor(0.3470)\n",
      "11286 Training Loss: tensor(0.3524)\n",
      "11287 Training Loss: tensor(0.3471)\n",
      "11288 Training Loss: tensor(0.3465)\n",
      "11289 Training Loss: tensor(0.3471)\n",
      "11290 Training Loss: tensor(0.3456)\n",
      "11291 Training Loss: tensor(0.3459)\n",
      "11292 Training Loss: tensor(0.3486)\n",
      "11293 Training Loss: tensor(0.3459)\n",
      "11294 Training Loss: tensor(0.3453)\n",
      "11295 Training Loss: tensor(0.3484)\n",
      "11296 Training Loss: tensor(0.3452)\n",
      "11297 Training Loss: tensor(0.3463)\n",
      "11298 Training Loss: tensor(0.3463)\n",
      "11299 Training Loss: tensor(0.3465)\n",
      "11300 Training Loss: tensor(0.3461)\n",
      "11301 Training Loss: tensor(0.3499)\n",
      "11302 Training Loss: tensor(0.3460)\n",
      "11303 Training Loss: tensor(0.3470)\n",
      "11304 Training Loss: tensor(0.3476)\n",
      "11305 Training Loss: tensor(0.3457)\n",
      "11306 Training Loss: tensor(0.3451)\n",
      "11307 Training Loss: tensor(0.3451)\n",
      "11308 Training Loss: tensor(0.3457)\n",
      "11309 Training Loss: tensor(0.3449)\n",
      "11310 Training Loss: tensor(0.3537)\n",
      "11311 Training Loss: tensor(0.3454)\n",
      "11312 Training Loss: tensor(0.3446)\n",
      "11313 Training Loss: tensor(0.3444)\n",
      "11314 Training Loss: tensor(0.3450)\n",
      "11315 Training Loss: tensor(0.3456)\n",
      "11316 Training Loss: tensor(0.3458)\n",
      "11317 Training Loss: tensor(0.3452)\n",
      "11318 Training Loss: tensor(0.3457)\n",
      "11319 Training Loss: tensor(0.3454)\n",
      "11320 Training Loss: tensor(0.3443)\n",
      "11321 Training Loss: tensor(0.3479)\n",
      "11322 Training Loss: tensor(0.3471)\n",
      "11323 Training Loss: tensor(0.3502)\n",
      "11324 Training Loss: tensor(0.3455)\n",
      "11325 Training Loss: tensor(0.3456)\n",
      "11326 Training Loss: tensor(0.3492)\n",
      "11327 Training Loss: tensor(0.3460)\n",
      "11328 Training Loss: tensor(0.3454)\n",
      "11329 Training Loss: tensor(0.3438)\n",
      "11330 Training Loss: tensor(0.3450)\n",
      "11331 Training Loss: tensor(0.3445)\n",
      "11332 Training Loss: tensor(0.3447)\n",
      "11333 Training Loss: tensor(0.3453)\n",
      "11334 Training Loss: tensor(0.3470)\n",
      "11335 Training Loss: tensor(0.3459)\n",
      "11336 Training Loss: tensor(0.3461)\n",
      "11337 Training Loss: tensor(0.3459)\n",
      "11338 Training Loss: tensor(0.3491)\n",
      "11339 Training Loss: tensor(0.3506)\n",
      "11340 Training Loss: tensor(0.3457)\n",
      "11341 Training Loss: tensor(0.3458)\n",
      "11342 Training Loss: tensor(0.3462)\n",
      "11343 Training Loss: tensor(0.3449)\n",
      "11344 Training Loss: tensor(0.3455)\n",
      "11345 Training Loss: tensor(0.3471)\n",
      "11346 Training Loss: tensor(0.3467)\n",
      "11347 Training Loss: tensor(0.3470)\n",
      "11348 Training Loss: tensor(0.3457)\n",
      "11349 Training Loss: tensor(0.3502)\n",
      "11350 Training Loss: tensor(0.3499)\n",
      "11351 Training Loss: tensor(0.3449)\n",
      "11352 Training Loss: tensor(0.3457)\n",
      "11353 Training Loss: tensor(0.3467)\n",
      "11354 Training Loss: tensor(0.3448)\n",
      "11355 Training Loss: tensor(0.3507)\n",
      "11356 Training Loss: tensor(0.3497)\n",
      "11357 Training Loss: tensor(0.3450)\n",
      "11358 Training Loss: tensor(0.3451)\n",
      "11359 Training Loss: tensor(0.3459)\n",
      "11360 Training Loss: tensor(0.3443)\n",
      "11361 Training Loss: tensor(0.3451)\n",
      "11362 Training Loss: tensor(0.3452)\n",
      "11363 Training Loss: tensor(0.3460)\n",
      "11364 Training Loss: tensor(0.3450)\n",
      "11365 Training Loss: tensor(0.3466)\n",
      "11366 Training Loss: tensor(0.3458)\n",
      "11367 Training Loss: tensor(0.3460)\n",
      "11368 Training Loss: tensor(0.3523)\n",
      "11369 Training Loss: tensor(0.3462)\n",
      "11370 Training Loss: tensor(0.3478)\n",
      "11371 Training Loss: tensor(0.3440)\n",
      "11372 Training Loss: tensor(0.3453)\n",
      "11373 Training Loss: tensor(0.3504)\n",
      "11374 Training Loss: tensor(0.3497)\n",
      "11375 Training Loss: tensor(0.3453)\n",
      "11376 Training Loss: tensor(0.3464)\n",
      "11377 Training Loss: tensor(0.3448)\n",
      "11378 Training Loss: tensor(0.3439)\n",
      "11379 Training Loss: tensor(0.3447)\n",
      "11380 Training Loss: tensor(0.3457)\n",
      "11381 Training Loss: tensor(0.3505)\n",
      "11382 Training Loss: tensor(0.3461)\n",
      "11383 Training Loss: tensor(0.3456)\n",
      "11384 Training Loss: tensor(0.3458)\n",
      "11385 Training Loss: tensor(0.3455)\n",
      "11386 Training Loss: tensor(0.3530)\n",
      "11387 Training Loss: tensor(0.3448)\n",
      "11388 Training Loss: tensor(0.3457)\n",
      "11389 Training Loss: tensor(0.3458)\n",
      "11390 Training Loss: tensor(0.3452)\n",
      "11391 Training Loss: tensor(0.3498)\n",
      "11392 Training Loss: tensor(0.3471)\n",
      "11393 Training Loss: tensor(0.3482)\n",
      "11394 Training Loss: tensor(0.3455)\n",
      "11395 Training Loss: tensor(0.3483)\n",
      "11396 Training Loss: tensor(0.3448)\n",
      "11397 Training Loss: tensor(0.3452)\n",
      "11398 Training Loss: tensor(0.3479)\n",
      "11399 Training Loss: tensor(0.3461)\n",
      "11400 Training Loss: tensor(0.3454)\n",
      "11401 Training Loss: tensor(0.3453)\n",
      "11402 Training Loss: tensor(0.3475)\n",
      "11403 Training Loss: tensor(0.3459)\n",
      "11404 Training Loss: tensor(0.3450)\n",
      "11405 Training Loss: tensor(0.3449)\n",
      "11406 Training Loss: tensor(0.3447)\n",
      "11407 Training Loss: tensor(0.3444)\n",
      "11408 Training Loss: tensor(0.3449)\n",
      "11409 Training Loss: tensor(0.3443)\n",
      "11410 Training Loss: tensor(0.3465)\n",
      "11411 Training Loss: tensor(0.3442)\n",
      "11412 Training Loss: tensor(0.3459)\n",
      "11413 Training Loss: tensor(0.3460)\n",
      "11414 Training Loss: tensor(0.3485)\n",
      "11415 Training Loss: tensor(0.3468)\n",
      "11416 Training Loss: tensor(0.3449)\n",
      "11417 Training Loss: tensor(0.3517)\n",
      "11418 Training Loss: tensor(0.3460)\n",
      "11419 Training Loss: tensor(0.3458)\n",
      "11420 Training Loss: tensor(0.3467)\n",
      "11421 Training Loss: tensor(0.3464)\n",
      "11422 Training Loss: tensor(0.3441)\n",
      "11423 Training Loss: tensor(0.3544)\n",
      "11424 Training Loss: tensor(0.3446)\n",
      "11425 Training Loss: tensor(0.3474)\n",
      "11426 Training Loss: tensor(0.3465)\n",
      "11427 Training Loss: tensor(0.3467)\n",
      "11428 Training Loss: tensor(0.3469)\n",
      "11429 Training Loss: tensor(0.3469)\n",
      "11430 Training Loss: tensor(0.3462)\n",
      "11431 Training Loss: tensor(0.3469)\n",
      "11432 Training Loss: tensor(0.3463)\n",
      "11433 Training Loss: tensor(0.3455)\n",
      "11434 Training Loss: tensor(0.3449)\n",
      "11435 Training Loss: tensor(0.3472)\n",
      "11436 Training Loss: tensor(0.3456)\n",
      "11437 Training Loss: tensor(0.3492)\n",
      "11438 Training Loss: tensor(0.3452)\n",
      "11439 Training Loss: tensor(0.3454)\n",
      "11440 Training Loss: tensor(0.3460)\n",
      "11441 Training Loss: tensor(0.3451)\n",
      "11442 Training Loss: tensor(0.3456)\n",
      "11443 Training Loss: tensor(0.3448)\n",
      "11444 Training Loss: tensor(0.3465)\n",
      "11445 Training Loss: tensor(0.3510)\n",
      "11446 Training Loss: tensor(0.3462)\n",
      "11447 Training Loss: tensor(0.3449)\n",
      "11448 Training Loss: tensor(0.3583)\n",
      "11449 Training Loss: tensor(0.3465)\n",
      "11450 Training Loss: tensor(0.3518)\n",
      "11451 Training Loss: tensor(0.3441)\n",
      "11452 Training Loss: tensor(0.3498)\n",
      "11453 Training Loss: tensor(0.3467)\n",
      "11454 Training Loss: tensor(0.3472)\n",
      "11455 Training Loss: tensor(0.3457)\n",
      "11456 Training Loss: tensor(0.3458)\n",
      "11457 Training Loss: tensor(0.3452)\n",
      "11458 Training Loss: tensor(0.3480)\n",
      "11459 Training Loss: tensor(0.3460)\n",
      "11460 Training Loss: tensor(0.3468)\n",
      "11461 Training Loss: tensor(0.3467)\n",
      "11462 Training Loss: tensor(0.3453)\n",
      "11463 Training Loss: tensor(0.3507)\n",
      "11464 Training Loss: tensor(0.3457)\n",
      "11465 Training Loss: tensor(0.3476)\n",
      "11466 Training Loss: tensor(0.3451)\n",
      "11467 Training Loss: tensor(0.3453)\n",
      "11468 Training Loss: tensor(0.3441)\n",
      "11469 Training Loss: tensor(0.3457)\n",
      "11470 Training Loss: tensor(0.3457)\n",
      "11471 Training Loss: tensor(0.3454)\n",
      "11472 Training Loss: tensor(0.3465)\n",
      "11473 Training Loss: tensor(0.3451)\n",
      "11474 Training Loss: tensor(0.3475)\n",
      "11475 Training Loss: tensor(0.3452)\n",
      "11476 Training Loss: tensor(0.3449)\n",
      "11477 Training Loss: tensor(0.3448)\n",
      "11478 Training Loss: tensor(0.3448)\n",
      "11479 Training Loss: tensor(0.3442)\n",
      "11480 Training Loss: tensor(0.3447)\n",
      "11481 Training Loss: tensor(0.3474)\n",
      "11482 Training Loss: tensor(0.3461)\n",
      "11483 Training Loss: tensor(0.3678)\n",
      "11484 Training Loss: tensor(0.3459)\n",
      "11485 Training Loss: tensor(0.3560)\n",
      "11486 Training Loss: tensor(0.3446)\n",
      "11487 Training Loss: tensor(0.3464)\n",
      "11488 Training Loss: tensor(0.3463)\n",
      "11489 Training Loss: tensor(0.3512)\n",
      "11490 Training Loss: tensor(0.3462)\n",
      "11491 Training Loss: tensor(0.3472)\n",
      "11492 Training Loss: tensor(0.3474)\n",
      "11493 Training Loss: tensor(0.3471)\n",
      "11494 Training Loss: tensor(0.3464)\n",
      "11495 Training Loss: tensor(0.3487)\n",
      "11496 Training Loss: tensor(0.3472)\n",
      "11497 Training Loss: tensor(0.3457)\n",
      "11498 Training Loss: tensor(0.3479)\n",
      "11499 Training Loss: tensor(0.3465)\n",
      "11500 Training Loss: tensor(0.3479)\n",
      "11501 Training Loss: tensor(0.3465)\n",
      "11502 Training Loss: tensor(0.3457)\n",
      "11503 Training Loss: tensor(0.3459)\n",
      "11504 Training Loss: tensor(0.3449)\n",
      "11505 Training Loss: tensor(0.3456)\n",
      "11506 Training Loss: tensor(0.3465)\n",
      "11507 Training Loss: tensor(0.3494)\n",
      "11508 Training Loss: tensor(0.3458)\n",
      "11509 Training Loss: tensor(0.3456)\n",
      "11510 Training Loss: tensor(0.3501)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11511 Training Loss: tensor(0.3440)\n",
      "11512 Training Loss: tensor(0.3475)\n",
      "11513 Training Loss: tensor(0.3445)\n",
      "11514 Training Loss: tensor(0.3450)\n",
      "11515 Training Loss: tensor(0.3450)\n",
      "11516 Training Loss: tensor(0.3442)\n",
      "11517 Training Loss: tensor(0.3511)\n",
      "11518 Training Loss: tensor(0.3656)\n",
      "11519 Training Loss: tensor(0.3453)\n",
      "11520 Training Loss: tensor(0.3497)\n",
      "11521 Training Loss: tensor(0.3451)\n",
      "11522 Training Loss: tensor(0.3451)\n",
      "11523 Training Loss: tensor(0.3459)\n",
      "11524 Training Loss: tensor(0.3481)\n",
      "11525 Training Loss: tensor(0.3473)\n",
      "11526 Training Loss: tensor(0.3462)\n",
      "11527 Training Loss: tensor(0.3462)\n",
      "11528 Training Loss: tensor(0.3454)\n",
      "11529 Training Loss: tensor(0.3467)\n",
      "11530 Training Loss: tensor(0.3477)\n",
      "11531 Training Loss: tensor(0.3475)\n",
      "11532 Training Loss: tensor(0.3465)\n",
      "11533 Training Loss: tensor(0.3469)\n",
      "11534 Training Loss: tensor(0.3466)\n",
      "11535 Training Loss: tensor(0.3471)\n",
      "11536 Training Loss: tensor(0.3464)\n",
      "11537 Training Loss: tensor(0.3466)\n",
      "11538 Training Loss: tensor(0.3456)\n",
      "11539 Training Loss: tensor(0.3470)\n",
      "11540 Training Loss: tensor(0.3462)\n",
      "11541 Training Loss: tensor(0.3466)\n",
      "11542 Training Loss: tensor(0.3459)\n",
      "11543 Training Loss: tensor(0.3464)\n",
      "11544 Training Loss: tensor(0.3479)\n",
      "11545 Training Loss: tensor(0.3447)\n",
      "11546 Training Loss: tensor(0.3439)\n",
      "11547 Training Loss: tensor(0.3444)\n",
      "11548 Training Loss: tensor(0.3481)\n",
      "11549 Training Loss: tensor(0.3509)\n",
      "11550 Training Loss: tensor(0.3524)\n",
      "11551 Training Loss: tensor(0.3500)\n",
      "11552 Training Loss: tensor(0.3453)\n",
      "11553 Training Loss: tensor(0.3526)\n",
      "11554 Training Loss: tensor(0.3465)\n",
      "11555 Training Loss: tensor(0.3455)\n",
      "11556 Training Loss: tensor(0.3468)\n",
      "11557 Training Loss: tensor(0.3457)\n",
      "11558 Training Loss: tensor(0.3454)\n",
      "11559 Training Loss: tensor(0.3458)\n",
      "11560 Training Loss: tensor(0.3508)\n",
      "11561 Training Loss: tensor(0.3476)\n",
      "11562 Training Loss: tensor(0.3450)\n",
      "11563 Training Loss: tensor(0.3447)\n",
      "11564 Training Loss: tensor(0.3456)\n",
      "11565 Training Loss: tensor(0.3468)\n",
      "11566 Training Loss: tensor(0.3450)\n",
      "11567 Training Loss: tensor(0.3458)\n",
      "11568 Training Loss: tensor(0.3467)\n",
      "11569 Training Loss: tensor(0.3510)\n",
      "11570 Training Loss: tensor(0.3476)\n",
      "11571 Training Loss: tensor(0.3453)\n",
      "11572 Training Loss: tensor(0.3451)\n",
      "11573 Training Loss: tensor(0.3487)\n",
      "11574 Training Loss: tensor(0.3487)\n",
      "11575 Training Loss: tensor(0.3475)\n",
      "11576 Training Loss: tensor(0.3448)\n",
      "11577 Training Loss: tensor(0.3448)\n",
      "11578 Training Loss: tensor(0.3450)\n",
      "11579 Training Loss: tensor(0.3459)\n",
      "11580 Training Loss: tensor(0.3455)\n",
      "11581 Training Loss: tensor(0.3447)\n",
      "11582 Training Loss: tensor(0.3457)\n",
      "11583 Training Loss: tensor(0.3451)\n",
      "11584 Training Loss: tensor(0.3466)\n",
      "11585 Training Loss: tensor(0.3449)\n",
      "11586 Training Loss: tensor(0.3453)\n",
      "11587 Training Loss: tensor(0.3500)\n",
      "11588 Training Loss: tensor(0.3445)\n",
      "11589 Training Loss: tensor(0.3445)\n",
      "11590 Training Loss: tensor(0.3445)\n",
      "11591 Training Loss: tensor(0.3522)\n",
      "11592 Training Loss: tensor(0.3443)\n",
      "11593 Training Loss: tensor(0.3447)\n",
      "11594 Training Loss: tensor(0.3450)\n",
      "11595 Training Loss: tensor(0.3447)\n",
      "11596 Training Loss: tensor(0.3515)\n",
      "11597 Training Loss: tensor(0.3439)\n",
      "11598 Training Loss: tensor(0.3451)\n",
      "11599 Training Loss: tensor(0.3443)\n",
      "11600 Training Loss: tensor(0.3454)\n",
      "11601 Training Loss: tensor(0.3452)\n",
      "11602 Training Loss: tensor(0.3452)\n",
      "11603 Training Loss: tensor(0.3446)\n",
      "11604 Training Loss: tensor(0.3450)\n",
      "11605 Training Loss: tensor(0.3508)\n",
      "11606 Training Loss: tensor(0.3470)\n",
      "11607 Training Loss: tensor(0.3446)\n",
      "11608 Training Loss: tensor(0.3442)\n",
      "11609 Training Loss: tensor(0.3443)\n",
      "11610 Training Loss: tensor(0.3461)\n",
      "11611 Training Loss: tensor(0.3501)\n",
      "11612 Training Loss: tensor(0.3433)\n",
      "11613 Training Loss: tensor(0.3466)\n",
      "11614 Training Loss: tensor(0.3442)\n",
      "11615 Training Loss: tensor(0.3452)\n",
      "11616 Training Loss: tensor(0.3437)\n",
      "11617 Training Loss: tensor(0.3450)\n",
      "11618 Training Loss: tensor(0.3442)\n",
      "11619 Training Loss: tensor(0.3461)\n",
      "11620 Training Loss: tensor(0.3516)\n",
      "11621 Training Loss: tensor(0.3450)\n",
      "11622 Training Loss: tensor(0.3445)\n",
      "11623 Training Loss: tensor(0.3442)\n",
      "11624 Training Loss: tensor(0.3471)\n",
      "11625 Training Loss: tensor(0.3457)\n",
      "11626 Training Loss: tensor(0.3436)\n",
      "11627 Training Loss: tensor(0.3437)\n",
      "11628 Training Loss: tensor(0.3444)\n",
      "11629 Training Loss: tensor(0.3445)\n",
      "11630 Training Loss: tensor(0.3440)\n",
      "11631 Training Loss: tensor(0.3443)\n",
      "11632 Training Loss: tensor(0.3459)\n",
      "11633 Training Loss: tensor(0.3465)\n",
      "11634 Training Loss: tensor(0.3451)\n",
      "11635 Training Loss: tensor(0.3532)\n",
      "11636 Training Loss: tensor(0.3443)\n",
      "11637 Training Loss: tensor(0.3435)\n",
      "11638 Training Loss: tensor(0.3499)\n",
      "11639 Training Loss: tensor(0.3437)\n",
      "11640 Training Loss: tensor(0.3439)\n",
      "11641 Training Loss: tensor(0.3446)\n",
      "11642 Training Loss: tensor(0.3444)\n",
      "11643 Training Loss: tensor(0.3486)\n",
      "11644 Training Loss: tensor(0.3449)\n",
      "11645 Training Loss: tensor(0.3445)\n",
      "11646 Training Loss: tensor(0.3451)\n",
      "11647 Training Loss: tensor(0.3461)\n",
      "11648 Training Loss: tensor(0.3442)\n",
      "11649 Training Loss: tensor(0.3555)\n",
      "11650 Training Loss: tensor(0.3440)\n",
      "11651 Training Loss: tensor(0.3441)\n",
      "11652 Training Loss: tensor(0.3432)\n",
      "11653 Training Loss: tensor(0.3467)\n",
      "11654 Training Loss: tensor(0.3452)\n",
      "11655 Training Loss: tensor(0.3539)\n",
      "11656 Training Loss: tensor(0.3477)\n",
      "11657 Training Loss: tensor(0.3454)\n",
      "11658 Training Loss: tensor(0.3462)\n",
      "11659 Training Loss: tensor(0.3489)\n",
      "11660 Training Loss: tensor(0.3450)\n",
      "11661 Training Loss: tensor(0.3469)\n",
      "11662 Training Loss: tensor(0.3442)\n",
      "11663 Training Loss: tensor(0.3458)\n",
      "11664 Training Loss: tensor(0.3446)\n",
      "11665 Training Loss: tensor(0.3468)\n",
      "11666 Training Loss: tensor(0.3450)\n",
      "11667 Training Loss: tensor(0.3461)\n",
      "11668 Training Loss: tensor(0.3451)\n",
      "11669 Training Loss: tensor(0.3442)\n",
      "11670 Training Loss: tensor(0.3450)\n",
      "11671 Training Loss: tensor(0.3453)\n",
      "11672 Training Loss: tensor(0.3517)\n",
      "11673 Training Loss: tensor(0.3498)\n",
      "11674 Training Loss: tensor(0.3455)\n",
      "11675 Training Loss: tensor(0.3496)\n",
      "11676 Training Loss: tensor(0.3454)\n",
      "11677 Training Loss: tensor(0.3456)\n",
      "11678 Training Loss: tensor(0.3455)\n",
      "11679 Training Loss: tensor(0.3442)\n",
      "11680 Training Loss: tensor(0.3447)\n",
      "11681 Training Loss: tensor(0.3565)\n",
      "11682 Training Loss: tensor(0.3467)\n",
      "11683 Training Loss: tensor(0.3506)\n",
      "11684 Training Loss: tensor(0.3466)\n",
      "11685 Training Loss: tensor(0.3489)\n",
      "11686 Training Loss: tensor(0.3450)\n",
      "11687 Training Loss: tensor(0.3445)\n",
      "11688 Training Loss: tensor(0.3452)\n",
      "11689 Training Loss: tensor(0.3492)\n",
      "11690 Training Loss: tensor(0.3457)\n",
      "11691 Training Loss: tensor(0.3526)\n",
      "11692 Training Loss: tensor(0.3457)\n",
      "11693 Training Loss: tensor(0.3468)\n",
      "11694 Training Loss: tensor(0.3461)\n",
      "11695 Training Loss: tensor(0.3453)\n",
      "11696 Training Loss: tensor(0.3537)\n",
      "11697 Training Loss: tensor(0.3457)\n",
      "11698 Training Loss: tensor(0.3453)\n",
      "11699 Training Loss: tensor(0.3459)\n",
      "11700 Training Loss: tensor(0.3451)\n",
      "11701 Training Loss: tensor(0.3481)\n",
      "11702 Training Loss: tensor(0.3479)\n",
      "11703 Training Loss: tensor(0.3449)\n",
      "11704 Training Loss: tensor(0.3466)\n",
      "11705 Training Loss: tensor(0.3471)\n",
      "11706 Training Loss: tensor(0.3448)\n",
      "11707 Training Loss: tensor(0.3448)\n",
      "11708 Training Loss: tensor(0.3460)\n",
      "11709 Training Loss: tensor(0.3450)\n",
      "11710 Training Loss: tensor(0.3484)\n",
      "11711 Training Loss: tensor(0.3447)\n",
      "11712 Training Loss: tensor(0.3435)\n",
      "11713 Training Loss: tensor(0.3452)\n",
      "11714 Training Loss: tensor(0.3446)\n",
      "11715 Training Loss: tensor(0.3449)\n",
      "11716 Training Loss: tensor(0.3459)\n",
      "11717 Training Loss: tensor(0.3507)\n",
      "11718 Training Loss: tensor(0.3461)\n",
      "11719 Training Loss: tensor(0.3455)\n",
      "11720 Training Loss: tensor(0.3485)\n",
      "11721 Training Loss: tensor(0.3440)\n",
      "11722 Training Loss: tensor(0.3472)\n",
      "11723 Training Loss: tensor(0.3448)\n",
      "11724 Training Loss: tensor(0.3459)\n",
      "11725 Training Loss: tensor(0.3446)\n",
      "11726 Training Loss: tensor(0.3523)\n",
      "11727 Training Loss: tensor(0.3505)\n",
      "11728 Training Loss: tensor(0.3442)\n",
      "11729 Training Loss: tensor(0.3495)\n",
      "11730 Training Loss: tensor(0.3442)\n",
      "11731 Training Loss: tensor(0.3451)\n",
      "11732 Training Loss: tensor(0.3456)\n",
      "11733 Training Loss: tensor(0.3470)\n",
      "11734 Training Loss: tensor(0.3474)\n",
      "11735 Training Loss: tensor(0.3457)\n",
      "11736 Training Loss: tensor(0.3451)\n",
      "11737 Training Loss: tensor(0.3452)\n",
      "11738 Training Loss: tensor(0.3448)\n",
      "11739 Training Loss: tensor(0.3458)\n",
      "11740 Training Loss: tensor(0.3520)\n",
      "11741 Training Loss: tensor(0.3456)\n",
      "11742 Training Loss: tensor(0.3505)\n",
      "11743 Training Loss: tensor(0.3450)\n",
      "11744 Training Loss: tensor(0.3458)\n",
      "11745 Training Loss: tensor(0.3443)\n",
      "11746 Training Loss: tensor(0.3447)\n",
      "11747 Training Loss: tensor(0.3456)\n",
      "11748 Training Loss: tensor(0.3445)\n",
      "11749 Training Loss: tensor(0.3457)\n",
      "11750 Training Loss: tensor(0.3463)\n",
      "11751 Training Loss: tensor(0.3479)\n",
      "11752 Training Loss: tensor(0.3445)\n",
      "11753 Training Loss: tensor(0.3443)\n",
      "11754 Training Loss: tensor(0.3463)\n",
      "11755 Training Loss: tensor(0.3450)\n",
      "11756 Training Loss: tensor(0.3469)\n",
      "11757 Training Loss: tensor(0.3451)\n",
      "11758 Training Loss: tensor(0.3446)\n",
      "11759 Training Loss: tensor(0.3442)\n",
      "11760 Training Loss: tensor(0.3455)\n",
      "11761 Training Loss: tensor(0.3445)\n",
      "11762 Training Loss: tensor(0.3442)\n",
      "11763 Training Loss: tensor(0.3458)\n",
      "11764 Training Loss: tensor(0.3464)\n",
      "11765 Training Loss: tensor(0.3512)\n",
      "11766 Training Loss: tensor(0.3516)\n",
      "11767 Training Loss: tensor(0.3450)\n",
      "11768 Training Loss: tensor(0.3471)\n",
      "11769 Training Loss: tensor(0.3454)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11770 Training Loss: tensor(0.3452)\n",
      "11771 Training Loss: tensor(0.3477)\n",
      "11772 Training Loss: tensor(0.3470)\n",
      "11773 Training Loss: tensor(0.3448)\n",
      "11774 Training Loss: tensor(0.3457)\n",
      "11775 Training Loss: tensor(0.3452)\n",
      "11776 Training Loss: tensor(0.3457)\n",
      "11777 Training Loss: tensor(0.3458)\n",
      "11778 Training Loss: tensor(0.3464)\n",
      "11779 Training Loss: tensor(0.3461)\n",
      "11780 Training Loss: tensor(0.3464)\n",
      "11781 Training Loss: tensor(0.3464)\n",
      "11782 Training Loss: tensor(0.3458)\n",
      "11783 Training Loss: tensor(0.3533)\n",
      "11784 Training Loss: tensor(0.3443)\n",
      "11785 Training Loss: tensor(0.3444)\n",
      "11786 Training Loss: tensor(0.3439)\n",
      "11787 Training Loss: tensor(0.3462)\n",
      "11788 Training Loss: tensor(0.3446)\n",
      "11789 Training Loss: tensor(0.3447)\n",
      "11790 Training Loss: tensor(0.3444)\n",
      "11791 Training Loss: tensor(0.3482)\n",
      "11792 Training Loss: tensor(0.3472)\n",
      "11793 Training Loss: tensor(0.3474)\n",
      "11794 Training Loss: tensor(0.3476)\n",
      "11795 Training Loss: tensor(0.3455)\n",
      "11796 Training Loss: tensor(0.3454)\n",
      "11797 Training Loss: tensor(0.3448)\n",
      "11798 Training Loss: tensor(0.3472)\n",
      "11799 Training Loss: tensor(0.3438)\n",
      "11800 Training Loss: tensor(0.3451)\n",
      "11801 Training Loss: tensor(0.3475)\n",
      "11802 Training Loss: tensor(0.3442)\n",
      "11803 Training Loss: tensor(0.3443)\n",
      "11804 Training Loss: tensor(0.3469)\n",
      "11805 Training Loss: tensor(0.3446)\n",
      "11806 Training Loss: tensor(0.3473)\n",
      "11807 Training Loss: tensor(0.3513)\n",
      "11808 Training Loss: tensor(0.3528)\n",
      "11809 Training Loss: tensor(0.3450)\n",
      "11810 Training Loss: tensor(0.3451)\n",
      "11811 Training Loss: tensor(0.3516)\n",
      "11812 Training Loss: tensor(0.3449)\n",
      "11813 Training Loss: tensor(0.3451)\n",
      "11814 Training Loss: tensor(0.3442)\n",
      "11815 Training Loss: tensor(0.3436)\n",
      "11816 Training Loss: tensor(0.3519)\n",
      "11817 Training Loss: tensor(0.3459)\n",
      "11818 Training Loss: tensor(0.3456)\n",
      "11819 Training Loss: tensor(0.3475)\n",
      "11820 Training Loss: tensor(0.3467)\n",
      "11821 Training Loss: tensor(0.3450)\n",
      "11822 Training Loss: tensor(0.3551)\n",
      "11823 Training Loss: tensor(0.3516)\n",
      "11824 Training Loss: tensor(0.3451)\n",
      "11825 Training Loss: tensor(0.3500)\n",
      "11826 Training Loss: tensor(0.3484)\n",
      "11827 Training Loss: tensor(0.3461)\n",
      "11828 Training Loss: tensor(0.3478)\n",
      "11829 Training Loss: tensor(0.3470)\n",
      "11830 Training Loss: tensor(0.3454)\n",
      "11831 Training Loss: tensor(0.3460)\n",
      "11832 Training Loss: tensor(0.3472)\n",
      "11833 Training Loss: tensor(0.3453)\n",
      "11834 Training Loss: tensor(0.3478)\n",
      "11835 Training Loss: tensor(0.3455)\n",
      "11836 Training Loss: tensor(0.3455)\n",
      "11837 Training Loss: tensor(0.3452)\n",
      "11838 Training Loss: tensor(0.3452)\n",
      "11839 Training Loss: tensor(0.3442)\n",
      "11840 Training Loss: tensor(0.3461)\n",
      "11841 Training Loss: tensor(0.3469)\n",
      "11842 Training Loss: tensor(0.3465)\n",
      "11843 Training Loss: tensor(0.3458)\n",
      "11844 Training Loss: tensor(0.3453)\n",
      "11845 Training Loss: tensor(0.3478)\n",
      "11846 Training Loss: tensor(0.3453)\n",
      "11847 Training Loss: tensor(0.3457)\n",
      "11848 Training Loss: tensor(0.3448)\n",
      "11849 Training Loss: tensor(0.3486)\n",
      "11850 Training Loss: tensor(0.3483)\n",
      "11851 Training Loss: tensor(0.3451)\n",
      "11852 Training Loss: tensor(0.3446)\n",
      "11853 Training Loss: tensor(0.3464)\n",
      "11854 Training Loss: tensor(0.3447)\n",
      "11855 Training Loss: tensor(0.3467)\n",
      "11856 Training Loss: tensor(0.3477)\n",
      "11857 Training Loss: tensor(0.3448)\n",
      "11858 Training Loss: tensor(0.3473)\n",
      "11859 Training Loss: tensor(0.3447)\n",
      "11860 Training Loss: tensor(0.3465)\n",
      "11861 Training Loss: tensor(0.3453)\n",
      "11862 Training Loss: tensor(0.3472)\n",
      "11863 Training Loss: tensor(0.3473)\n",
      "11864 Training Loss: tensor(0.3446)\n",
      "11865 Training Loss: tensor(0.3448)\n",
      "11866 Training Loss: tensor(0.3447)\n",
      "11867 Training Loss: tensor(0.3438)\n",
      "11868 Training Loss: tensor(0.3455)\n",
      "11869 Training Loss: tensor(0.3449)\n",
      "11870 Training Loss: tensor(0.3450)\n",
      "11871 Training Loss: tensor(0.3473)\n",
      "11872 Training Loss: tensor(0.3454)\n",
      "11873 Training Loss: tensor(0.3452)\n",
      "11874 Training Loss: tensor(0.3458)\n",
      "11875 Training Loss: tensor(0.3472)\n",
      "11876 Training Loss: tensor(0.3490)\n",
      "11877 Training Loss: tensor(0.3436)\n",
      "11878 Training Loss: tensor(0.3442)\n",
      "11879 Training Loss: tensor(0.3463)\n",
      "11880 Training Loss: tensor(0.3444)\n",
      "11881 Training Loss: tensor(0.3442)\n",
      "11882 Training Loss: tensor(0.3453)\n",
      "11883 Training Loss: tensor(0.3453)\n",
      "11884 Training Loss: tensor(0.3448)\n",
      "11885 Training Loss: tensor(0.3505)\n",
      "11886 Training Loss: tensor(0.3526)\n",
      "11887 Training Loss: tensor(0.3475)\n",
      "11888 Training Loss: tensor(0.3449)\n",
      "11889 Training Loss: tensor(0.3462)\n",
      "11890 Training Loss: tensor(0.3445)\n",
      "11891 Training Loss: tensor(0.3471)\n",
      "11892 Training Loss: tensor(0.3455)\n",
      "11893 Training Loss: tensor(0.3455)\n",
      "11894 Training Loss: tensor(0.3446)\n",
      "11895 Training Loss: tensor(0.3448)\n",
      "11896 Training Loss: tensor(0.3443)\n",
      "11897 Training Loss: tensor(0.3456)\n",
      "11898 Training Loss: tensor(0.3442)\n",
      "11899 Training Loss: tensor(0.3451)\n",
      "11900 Training Loss: tensor(0.3478)\n",
      "11901 Training Loss: tensor(0.3453)\n",
      "11902 Training Loss: tensor(0.3452)\n",
      "11903 Training Loss: tensor(0.3463)\n",
      "11904 Training Loss: tensor(0.3467)\n",
      "11905 Training Loss: tensor(0.3471)\n",
      "11906 Training Loss: tensor(0.3442)\n",
      "11907 Training Loss: tensor(0.3453)\n",
      "11908 Training Loss: tensor(0.3441)\n",
      "11909 Training Loss: tensor(0.3489)\n",
      "11910 Training Loss: tensor(0.3475)\n",
      "11911 Training Loss: tensor(0.3499)\n",
      "11912 Training Loss: tensor(0.3440)\n",
      "11913 Training Loss: tensor(0.3512)\n",
      "11914 Training Loss: tensor(0.3510)\n",
      "11915 Training Loss: tensor(0.3451)\n",
      "11916 Training Loss: tensor(0.3510)\n",
      "11917 Training Loss: tensor(0.3449)\n",
      "11918 Training Loss: tensor(0.3444)\n",
      "11919 Training Loss: tensor(0.3441)\n",
      "11920 Training Loss: tensor(0.3462)\n",
      "11921 Training Loss: tensor(0.3476)\n",
      "11922 Training Loss: tensor(0.3457)\n",
      "11923 Training Loss: tensor(0.3460)\n",
      "11924 Training Loss: tensor(0.3482)\n",
      "11925 Training Loss: tensor(0.3454)\n",
      "11926 Training Loss: tensor(0.3480)\n",
      "11927 Training Loss: tensor(0.3486)\n",
      "11928 Training Loss: tensor(0.3459)\n",
      "11929 Training Loss: tensor(0.3445)\n",
      "11930 Training Loss: tensor(0.3495)\n",
      "11931 Training Loss: tensor(0.3527)\n",
      "11932 Training Loss: tensor(0.3454)\n",
      "11933 Training Loss: tensor(0.3475)\n",
      "11934 Training Loss: tensor(0.3458)\n",
      "11935 Training Loss: tensor(0.3468)\n",
      "11936 Training Loss: tensor(0.3488)\n",
      "11937 Training Loss: tensor(0.3445)\n",
      "11938 Training Loss: tensor(0.3477)\n",
      "11939 Training Loss: tensor(0.3470)\n",
      "11940 Training Loss: tensor(0.3452)\n",
      "11941 Training Loss: tensor(0.3454)\n",
      "11942 Training Loss: tensor(0.3447)\n",
      "11943 Training Loss: tensor(0.3446)\n",
      "11944 Training Loss: tensor(0.3455)\n",
      "11945 Training Loss: tensor(0.3459)\n",
      "11946 Training Loss: tensor(0.3449)\n",
      "11947 Training Loss: tensor(0.3454)\n",
      "11948 Training Loss: tensor(0.3454)\n",
      "11949 Training Loss: tensor(0.3444)\n",
      "11950 Training Loss: tensor(0.3454)\n",
      "11951 Training Loss: tensor(0.3431)\n",
      "11952 Training Loss: tensor(0.3450)\n",
      "11953 Training Loss: tensor(0.3446)\n",
      "11954 Training Loss: tensor(0.3467)\n",
      "11955 Training Loss: tensor(0.3487)\n",
      "11956 Training Loss: tensor(0.3468)\n",
      "11957 Training Loss: tensor(0.3488)\n",
      "11958 Training Loss: tensor(0.3450)\n",
      "11959 Training Loss: tensor(0.3451)\n",
      "11960 Training Loss: tensor(0.3454)\n",
      "11961 Training Loss: tensor(0.3457)\n",
      "11962 Training Loss: tensor(0.3452)\n",
      "11963 Training Loss: tensor(0.3444)\n",
      "11964 Training Loss: tensor(0.3437)\n",
      "11965 Training Loss: tensor(0.3537)\n",
      "11966 Training Loss: tensor(0.3490)\n",
      "11967 Training Loss: tensor(0.3447)\n",
      "11968 Training Loss: tensor(0.3452)\n",
      "11969 Training Loss: tensor(0.3444)\n",
      "11970 Training Loss: tensor(0.3480)\n",
      "11971 Training Loss: tensor(0.3451)\n",
      "11972 Training Loss: tensor(0.3462)\n",
      "11973 Training Loss: tensor(0.3457)\n",
      "11974 Training Loss: tensor(0.3482)\n",
      "11975 Training Loss: tensor(0.3465)\n",
      "11976 Training Loss: tensor(0.3451)\n",
      "11977 Training Loss: tensor(0.3459)\n",
      "11978 Training Loss: tensor(0.3475)\n",
      "11979 Training Loss: tensor(0.3447)\n",
      "11980 Training Loss: tensor(0.3465)\n",
      "11981 Training Loss: tensor(0.3454)\n",
      "11982 Training Loss: tensor(0.3464)\n",
      "11983 Training Loss: tensor(0.3461)\n",
      "11984 Training Loss: tensor(0.3455)\n",
      "11985 Training Loss: tensor(0.3444)\n",
      "11986 Training Loss: tensor(0.3460)\n",
      "11987 Training Loss: tensor(0.3450)\n",
      "11988 Training Loss: tensor(0.3445)\n",
      "11989 Training Loss: tensor(0.3446)\n",
      "11990 Training Loss: tensor(0.3482)\n",
      "11991 Training Loss: tensor(0.3459)\n",
      "11992 Training Loss: tensor(0.3435)\n",
      "11993 Training Loss: tensor(0.3451)\n",
      "11994 Training Loss: tensor(0.3484)\n",
      "11995 Training Loss: tensor(0.3440)\n",
      "11996 Training Loss: tensor(0.3440)\n",
      "11997 Training Loss: tensor(0.3443)\n",
      "11998 Training Loss: tensor(0.3443)\n",
      "11999 Training Loss: tensor(0.3432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 Training Loss: tensor(0.3442)\n",
      "12001 Training Loss: tensor(0.3434)\n",
      "12002 Training Loss: tensor(0.3441)\n",
      "12003 Training Loss: tensor(0.3521)\n",
      "12004 Training Loss: tensor(0.3436)\n",
      "12005 Training Loss: tensor(0.3451)\n",
      "12006 Training Loss: tensor(0.3429)\n",
      "12007 Training Loss: tensor(0.3447)\n",
      "12008 Training Loss: tensor(0.3501)\n",
      "12009 Training Loss: tensor(0.3439)\n",
      "12010 Training Loss: tensor(0.3468)\n",
      "12011 Training Loss: tensor(0.3440)\n",
      "12012 Training Loss: tensor(0.3504)\n",
      "12013 Training Loss: tensor(0.3461)\n",
      "12014 Training Loss: tensor(0.3504)\n",
      "12015 Training Loss: tensor(0.3443)\n",
      "12016 Training Loss: tensor(0.3454)\n",
      "12017 Training Loss: tensor(0.3469)\n",
      "12018 Training Loss: tensor(0.3450)\n",
      "12019 Training Loss: tensor(0.3450)\n",
      "12020 Training Loss: tensor(0.3446)\n",
      "12021 Training Loss: tensor(0.3448)\n",
      "12022 Training Loss: tensor(0.3462)\n",
      "12023 Training Loss: tensor(0.3491)\n",
      "12024 Training Loss: tensor(0.3450)\n",
      "12025 Training Loss: tensor(0.3448)\n",
      "12026 Training Loss: tensor(0.3452)\n",
      "12027 Training Loss: tensor(0.3458)\n",
      "12028 Training Loss: tensor(0.3454)\n",
      "12029 Training Loss: tensor(0.3441)\n",
      "12030 Training Loss: tensor(0.3447)\n",
      "12031 Training Loss: tensor(0.3446)\n",
      "12032 Training Loss: tensor(0.3452)\n",
      "12033 Training Loss: tensor(0.3449)\n",
      "12034 Training Loss: tensor(0.3476)\n",
      "12035 Training Loss: tensor(0.3470)\n",
      "12036 Training Loss: tensor(0.3466)\n",
      "12037 Training Loss: tensor(0.3431)\n",
      "12038 Training Loss: tensor(0.3444)\n",
      "12039 Training Loss: tensor(0.3437)\n",
      "12040 Training Loss: tensor(0.3437)\n",
      "12041 Training Loss: tensor(0.3437)\n",
      "12042 Training Loss: tensor(0.3444)\n",
      "12043 Training Loss: tensor(0.3458)\n",
      "12044 Training Loss: tensor(0.3436)\n",
      "12045 Training Loss: tensor(0.3440)\n",
      "12046 Training Loss: tensor(0.3431)\n",
      "12047 Training Loss: tensor(0.3435)\n",
      "12048 Training Loss: tensor(0.3453)\n",
      "12049 Training Loss: tensor(0.3579)\n",
      "12050 Training Loss: tensor(0.3443)\n",
      "12051 Training Loss: tensor(0.3582)\n",
      "12052 Training Loss: tensor(0.3438)\n",
      "12053 Training Loss: tensor(0.3449)\n",
      "12054 Training Loss: tensor(0.3463)\n",
      "12055 Training Loss: tensor(0.3448)\n",
      "12056 Training Loss: tensor(0.3469)\n",
      "12057 Training Loss: tensor(0.3466)\n",
      "12058 Training Loss: tensor(0.3587)\n",
      "12059 Training Loss: tensor(0.3478)\n",
      "12060 Training Loss: tensor(0.3467)\n",
      "12061 Training Loss: tensor(0.3462)\n",
      "12062 Training Loss: tensor(0.3445)\n",
      "12063 Training Loss: tensor(0.3461)\n",
      "12064 Training Loss: tensor(0.3477)\n",
      "12065 Training Loss: tensor(0.3458)\n",
      "12066 Training Loss: tensor(0.3444)\n",
      "12067 Training Loss: tensor(0.3467)\n",
      "12068 Training Loss: tensor(0.3464)\n",
      "12069 Training Loss: tensor(0.3464)\n",
      "12070 Training Loss: tensor(0.3473)\n",
      "12071 Training Loss: tensor(0.3457)\n",
      "12072 Training Loss: tensor(0.3449)\n",
      "12073 Training Loss: tensor(0.3444)\n",
      "12074 Training Loss: tensor(0.3466)\n",
      "12075 Training Loss: tensor(0.3451)\n",
      "12076 Training Loss: tensor(0.3456)\n",
      "12077 Training Loss: tensor(0.3449)\n",
      "12078 Training Loss: tensor(0.3442)\n",
      "12079 Training Loss: tensor(0.3524)\n",
      "12080 Training Loss: tensor(0.3445)\n",
      "12081 Training Loss: tensor(0.3472)\n",
      "12082 Training Loss: tensor(0.3448)\n",
      "12083 Training Loss: tensor(0.3446)\n",
      "12084 Training Loss: tensor(0.3454)\n",
      "12085 Training Loss: tensor(0.3451)\n",
      "12086 Training Loss: tensor(0.3449)\n",
      "12087 Training Loss: tensor(0.3445)\n",
      "12088 Training Loss: tensor(0.3456)\n",
      "12089 Training Loss: tensor(0.3472)\n",
      "12090 Training Loss: tensor(0.3469)\n",
      "12091 Training Loss: tensor(0.3441)\n",
      "12092 Training Loss: tensor(0.3437)\n",
      "12093 Training Loss: tensor(0.3539)\n",
      "12094 Training Loss: tensor(0.3499)\n",
      "12095 Training Loss: tensor(0.3459)\n",
      "12096 Training Loss: tensor(0.3456)\n",
      "12097 Training Loss: tensor(0.3455)\n",
      "12098 Training Loss: tensor(0.3462)\n",
      "12099 Training Loss: tensor(0.3439)\n",
      "12100 Training Loss: tensor(0.3480)\n",
      "12101 Training Loss: tensor(0.3454)\n",
      "12102 Training Loss: tensor(0.3447)\n",
      "12103 Training Loss: tensor(0.3444)\n",
      "12104 Training Loss: tensor(0.3446)\n",
      "12105 Training Loss: tensor(0.3486)\n",
      "12106 Training Loss: tensor(0.3443)\n",
      "12107 Training Loss: tensor(0.3446)\n",
      "12108 Training Loss: tensor(0.3442)\n",
      "12109 Training Loss: tensor(0.3444)\n",
      "12110 Training Loss: tensor(0.3441)\n",
      "12111 Training Loss: tensor(0.3444)\n",
      "12112 Training Loss: tensor(0.3431)\n",
      "12113 Training Loss: tensor(0.3457)\n",
      "12114 Training Loss: tensor(0.3523)\n",
      "12115 Training Loss: tensor(0.3444)\n",
      "12116 Training Loss: tensor(0.3436)\n",
      "12117 Training Loss: tensor(0.3461)\n",
      "12118 Training Loss: tensor(0.3457)\n",
      "12119 Training Loss: tensor(0.3445)\n",
      "12120 Training Loss: tensor(0.3477)\n",
      "12121 Training Loss: tensor(0.3447)\n",
      "12122 Training Loss: tensor(0.3435)\n",
      "12123 Training Loss: tensor(0.3452)\n",
      "12124 Training Loss: tensor(0.3478)\n",
      "12125 Training Loss: tensor(0.3456)\n",
      "12126 Training Loss: tensor(0.3468)\n",
      "12127 Training Loss: tensor(0.3452)\n",
      "12128 Training Loss: tensor(0.3452)\n",
      "12129 Training Loss: tensor(0.3493)\n",
      "12130 Training Loss: tensor(0.3440)\n",
      "12131 Training Loss: tensor(0.3499)\n",
      "12132 Training Loss: tensor(0.3437)\n",
      "12133 Training Loss: tensor(0.3463)\n",
      "12134 Training Loss: tensor(0.3578)\n",
      "12135 Training Loss: tensor(0.3469)\n",
      "12136 Training Loss: tensor(0.3442)\n",
      "12137 Training Loss: tensor(0.3444)\n",
      "12138 Training Loss: tensor(0.3455)\n",
      "12139 Training Loss: tensor(0.3453)\n",
      "12140 Training Loss: tensor(0.3488)\n",
      "12141 Training Loss: tensor(0.3464)\n",
      "12142 Training Loss: tensor(0.3501)\n",
      "12143 Training Loss: tensor(0.3449)\n",
      "12144 Training Loss: tensor(0.3452)\n",
      "12145 Training Loss: tensor(0.3512)\n",
      "12146 Training Loss: tensor(0.3502)\n",
      "12147 Training Loss: tensor(0.3453)\n",
      "12148 Training Loss: tensor(0.3468)\n",
      "12149 Training Loss: tensor(0.3459)\n",
      "12150 Training Loss: tensor(0.3461)\n",
      "12151 Training Loss: tensor(0.3453)\n",
      "12152 Training Loss: tensor(0.3454)\n",
      "12153 Training Loss: tensor(0.3453)\n",
      "12154 Training Loss: tensor(0.3453)\n",
      "12155 Training Loss: tensor(0.3454)\n",
      "12156 Training Loss: tensor(0.3464)\n",
      "12157 Training Loss: tensor(0.3451)\n",
      "12158 Training Loss: tensor(0.3451)\n",
      "12159 Training Loss: tensor(0.3447)\n",
      "12160 Training Loss: tensor(0.3470)\n",
      "12161 Training Loss: tensor(0.3475)\n",
      "12162 Training Loss: tensor(0.3479)\n",
      "12163 Training Loss: tensor(0.3438)\n",
      "12164 Training Loss: tensor(0.3444)\n",
      "12165 Training Loss: tensor(0.3494)\n",
      "12166 Training Loss: tensor(0.3491)\n",
      "12167 Training Loss: tensor(0.3453)\n",
      "12168 Training Loss: tensor(0.3465)\n",
      "12169 Training Loss: tensor(0.3453)\n",
      "12170 Training Loss: tensor(0.3450)\n",
      "12171 Training Loss: tensor(0.3463)\n",
      "12172 Training Loss: tensor(0.3485)\n",
      "12173 Training Loss: tensor(0.3438)\n",
      "12174 Training Loss: tensor(0.3449)\n",
      "12175 Training Loss: tensor(0.3443)\n",
      "12176 Training Loss: tensor(0.3449)\n",
      "12177 Training Loss: tensor(0.3457)\n",
      "12178 Training Loss: tensor(0.3440)\n",
      "12179 Training Loss: tensor(0.3452)\n",
      "12180 Training Loss: tensor(0.3450)\n",
      "12181 Training Loss: tensor(0.3444)\n",
      "12182 Training Loss: tensor(0.3454)\n",
      "12183 Training Loss: tensor(0.3447)\n",
      "12184 Training Loss: tensor(0.3473)\n",
      "12185 Training Loss: tensor(0.3441)\n",
      "12186 Training Loss: tensor(0.3476)\n",
      "12187 Training Loss: tensor(0.3440)\n",
      "12188 Training Loss: tensor(0.3450)\n",
      "12189 Training Loss: tensor(0.3540)\n",
      "12190 Training Loss: tensor(0.3434)\n",
      "12191 Training Loss: tensor(0.3438)\n",
      "12192 Training Loss: tensor(0.3440)\n",
      "12193 Training Loss: tensor(0.3437)\n",
      "12194 Training Loss: tensor(0.3442)\n",
      "12195 Training Loss: tensor(0.3440)\n",
      "12196 Training Loss: tensor(0.3503)\n",
      "12197 Training Loss: tensor(0.3474)\n",
      "12198 Training Loss: tensor(0.3467)\n",
      "12199 Training Loss: tensor(0.3447)\n",
      "12200 Training Loss: tensor(0.3477)\n",
      "12201 Training Loss: tensor(0.3439)\n",
      "12202 Training Loss: tensor(0.3466)\n",
      "12203 Training Loss: tensor(0.3461)\n",
      "12204 Training Loss: tensor(0.3442)\n",
      "12205 Training Loss: tensor(0.3500)\n",
      "12206 Training Loss: tensor(0.3442)\n",
      "12207 Training Loss: tensor(0.3451)\n",
      "12208 Training Loss: tensor(0.3450)\n",
      "12209 Training Loss: tensor(0.3463)\n",
      "12210 Training Loss: tensor(0.3440)\n",
      "12211 Training Loss: tensor(0.3439)\n",
      "12212 Training Loss: tensor(0.3447)\n",
      "12213 Training Loss: tensor(0.3481)\n",
      "12214 Training Loss: tensor(0.3463)\n",
      "12215 Training Loss: tensor(0.3449)\n",
      "12216 Training Loss: tensor(0.3444)\n",
      "12217 Training Loss: tensor(0.3438)\n",
      "12218 Training Loss: tensor(0.3525)\n",
      "12219 Training Loss: tensor(0.3472)\n",
      "12220 Training Loss: tensor(0.3456)\n",
      "12221 Training Loss: tensor(0.3438)\n",
      "12222 Training Loss: tensor(0.3441)\n",
      "12223 Training Loss: tensor(0.3448)\n",
      "12224 Training Loss: tensor(0.3442)\n",
      "12225 Training Loss: tensor(0.3450)\n",
      "12226 Training Loss: tensor(0.3456)\n",
      "12227 Training Loss: tensor(0.3459)\n",
      "12228 Training Loss: tensor(0.3441)\n",
      "12229 Training Loss: tensor(0.3439)\n",
      "12230 Training Loss: tensor(0.3445)\n",
      "12231 Training Loss: tensor(0.3450)\n",
      "12232 Training Loss: tensor(0.3456)\n",
      "12233 Training Loss: tensor(0.3455)\n",
      "12234 Training Loss: tensor(0.3438)\n",
      "12235 Training Loss: tensor(0.3435)\n",
      "12236 Training Loss: tensor(0.3462)\n",
      "12237 Training Loss: tensor(0.3438)\n",
      "12238 Training Loss: tensor(0.3466)\n",
      "12239 Training Loss: tensor(0.3434)\n",
      "12240 Training Loss: tensor(0.3436)\n",
      "12241 Training Loss: tensor(0.3518)\n",
      "12242 Training Loss: tensor(0.3432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12243 Training Loss: tensor(0.3436)\n",
      "12244 Training Loss: tensor(0.3459)\n",
      "12245 Training Loss: tensor(0.3479)\n",
      "12246 Training Loss: tensor(0.3467)\n",
      "12247 Training Loss: tensor(0.3437)\n",
      "12248 Training Loss: tensor(0.3427)\n",
      "12249 Training Loss: tensor(0.3553)\n",
      "12250 Training Loss: tensor(0.3433)\n",
      "12251 Training Loss: tensor(0.3452)\n",
      "12252 Training Loss: tensor(0.3434)\n",
      "12253 Training Loss: tensor(0.3451)\n",
      "12254 Training Loss: tensor(0.3439)\n",
      "12255 Training Loss: tensor(0.3444)\n",
      "12256 Training Loss: tensor(0.3463)\n",
      "12257 Training Loss: tensor(0.3481)\n",
      "12258 Training Loss: tensor(0.3439)\n",
      "12259 Training Loss: tensor(0.3489)\n",
      "12260 Training Loss: tensor(0.3434)\n",
      "12261 Training Loss: tensor(0.3452)\n",
      "12262 Training Loss: tensor(0.3481)\n",
      "12263 Training Loss: tensor(0.3430)\n",
      "12264 Training Loss: tensor(0.3475)\n",
      "12265 Training Loss: tensor(0.3444)\n",
      "12266 Training Loss: tensor(0.3442)\n",
      "12267 Training Loss: tensor(0.3432)\n",
      "12268 Training Loss: tensor(0.3453)\n",
      "12269 Training Loss: tensor(0.3437)\n",
      "12270 Training Loss: tensor(0.3455)\n",
      "12271 Training Loss: tensor(0.3437)\n",
      "12272 Training Loss: tensor(0.3587)\n",
      "12273 Training Loss: tensor(0.3475)\n",
      "12274 Training Loss: tensor(0.3471)\n",
      "12275 Training Loss: tensor(0.3442)\n",
      "12276 Training Loss: tensor(0.3455)\n",
      "12277 Training Loss: tensor(0.3510)\n",
      "12278 Training Loss: tensor(0.3459)\n",
      "12279 Training Loss: tensor(0.3461)\n",
      "12280 Training Loss: tensor(0.3442)\n",
      "12281 Training Loss: tensor(0.3447)\n",
      "12282 Training Loss: tensor(0.3439)\n",
      "12283 Training Loss: tensor(0.3476)\n",
      "12284 Training Loss: tensor(0.3446)\n",
      "12285 Training Loss: tensor(0.3446)\n",
      "12286 Training Loss: tensor(0.3455)\n",
      "12287 Training Loss: tensor(0.3476)\n",
      "12288 Training Loss: tensor(0.3443)\n",
      "12289 Training Loss: tensor(0.3441)\n",
      "12290 Training Loss: tensor(0.3523)\n",
      "12291 Training Loss: tensor(0.3452)\n",
      "12292 Training Loss: tensor(0.3438)\n",
      "12293 Training Loss: tensor(0.3437)\n",
      "12294 Training Loss: tensor(0.3465)\n",
      "12295 Training Loss: tensor(0.3436)\n",
      "12296 Training Loss: tensor(0.3474)\n",
      "12297 Training Loss: tensor(0.3440)\n",
      "12298 Training Loss: tensor(0.3437)\n",
      "12299 Training Loss: tensor(0.3436)\n",
      "12300 Training Loss: tensor(0.3506)\n",
      "12301 Training Loss: tensor(0.3437)\n",
      "12302 Training Loss: tensor(0.3449)\n",
      "12303 Training Loss: tensor(0.3441)\n",
      "12304 Training Loss: tensor(0.3447)\n",
      "12305 Training Loss: tensor(0.3483)\n",
      "12306 Training Loss: tensor(0.3485)\n",
      "12307 Training Loss: tensor(0.3437)\n",
      "12308 Training Loss: tensor(0.3468)\n",
      "12309 Training Loss: tensor(0.3454)\n",
      "12310 Training Loss: tensor(0.3449)\n",
      "12311 Training Loss: tensor(0.3430)\n",
      "12312 Training Loss: tensor(0.3459)\n",
      "12313 Training Loss: tensor(0.3434)\n",
      "12314 Training Loss: tensor(0.3452)\n",
      "12315 Training Loss: tensor(0.3432)\n",
      "12316 Training Loss: tensor(0.3450)\n",
      "12317 Training Loss: tensor(0.3492)\n",
      "12318 Training Loss: tensor(0.3432)\n",
      "12319 Training Loss: tensor(0.3446)\n",
      "12320 Training Loss: tensor(0.3488)\n",
      "12321 Training Loss: tensor(0.3569)\n",
      "12322 Training Loss: tensor(0.3436)\n",
      "12323 Training Loss: tensor(0.3447)\n",
      "12324 Training Loss: tensor(0.3467)\n",
      "12325 Training Loss: tensor(0.3453)\n",
      "12326 Training Loss: tensor(0.3453)\n",
      "12327 Training Loss: tensor(0.3483)\n",
      "12328 Training Loss: tensor(0.3450)\n",
      "12329 Training Loss: tensor(0.3439)\n",
      "12330 Training Loss: tensor(0.3439)\n",
      "12331 Training Loss: tensor(0.3469)\n",
      "12332 Training Loss: tensor(0.3443)\n",
      "12333 Training Loss: tensor(0.3443)\n",
      "12334 Training Loss: tensor(0.3437)\n",
      "12335 Training Loss: tensor(0.3438)\n",
      "12336 Training Loss: tensor(0.3446)\n",
      "12337 Training Loss: tensor(0.3491)\n",
      "12338 Training Loss: tensor(0.3501)\n",
      "12339 Training Loss: tensor(0.3480)\n",
      "12340 Training Loss: tensor(0.3436)\n",
      "12341 Training Loss: tensor(0.3463)\n",
      "12342 Training Loss: tensor(0.3444)\n",
      "12343 Training Loss: tensor(0.3445)\n",
      "12344 Training Loss: tensor(0.3525)\n",
      "12345 Training Loss: tensor(0.3442)\n",
      "12346 Training Loss: tensor(0.3524)\n",
      "12347 Training Loss: tensor(0.3461)\n",
      "12348 Training Loss: tensor(0.3439)\n",
      "12349 Training Loss: tensor(0.3444)\n",
      "12350 Training Loss: tensor(0.3442)\n",
      "12351 Training Loss: tensor(0.3451)\n",
      "12352 Training Loss: tensor(0.3451)\n",
      "12353 Training Loss: tensor(0.3467)\n",
      "12354 Training Loss: tensor(0.3442)\n",
      "12355 Training Loss: tensor(0.3442)\n",
      "12356 Training Loss: tensor(0.3439)\n",
      "12357 Training Loss: tensor(0.3458)\n",
      "12358 Training Loss: tensor(0.3447)\n",
      "12359 Training Loss: tensor(0.3447)\n",
      "12360 Training Loss: tensor(0.3440)\n",
      "12361 Training Loss: tensor(0.3459)\n",
      "12362 Training Loss: tensor(0.3451)\n",
      "12363 Training Loss: tensor(0.3476)\n",
      "12364 Training Loss: tensor(0.3492)\n",
      "12365 Training Loss: tensor(0.3467)\n",
      "12366 Training Loss: tensor(0.3455)\n",
      "12367 Training Loss: tensor(0.3435)\n",
      "12368 Training Loss: tensor(0.3436)\n",
      "12369 Training Loss: tensor(0.3448)\n",
      "12370 Training Loss: tensor(0.3440)\n",
      "12371 Training Loss: tensor(0.3451)\n",
      "12372 Training Loss: tensor(0.3445)\n",
      "12373 Training Loss: tensor(0.3431)\n",
      "12374 Training Loss: tensor(0.3438)\n",
      "12375 Training Loss: tensor(0.3465)\n",
      "12376 Training Loss: tensor(0.3451)\n",
      "12377 Training Loss: tensor(0.3455)\n",
      "12378 Training Loss: tensor(0.3493)\n",
      "12379 Training Loss: tensor(0.3454)\n",
      "12380 Training Loss: tensor(0.3478)\n",
      "12381 Training Loss: tensor(0.3522)\n",
      "12382 Training Loss: tensor(0.3441)\n",
      "12383 Training Loss: tensor(0.3443)\n",
      "12384 Training Loss: tensor(0.3447)\n",
      "12385 Training Loss: tensor(0.3466)\n",
      "12386 Training Loss: tensor(0.3450)\n",
      "12387 Training Loss: tensor(0.3457)\n",
      "12388 Training Loss: tensor(0.3449)\n",
      "12389 Training Loss: tensor(0.3477)\n",
      "12390 Training Loss: tensor(0.3459)\n",
      "12391 Training Loss: tensor(0.3440)\n",
      "12392 Training Loss: tensor(0.3440)\n",
      "12393 Training Loss: tensor(0.3465)\n",
      "12394 Training Loss: tensor(0.3446)\n",
      "12395 Training Loss: tensor(0.3440)\n",
      "12396 Training Loss: tensor(0.3440)\n",
      "12397 Training Loss: tensor(0.3432)\n",
      "12398 Training Loss: tensor(0.3455)\n",
      "12399 Training Loss: tensor(0.3438)\n",
      "12400 Training Loss: tensor(0.3432)\n",
      "12401 Training Loss: tensor(0.3437)\n",
      "12402 Training Loss: tensor(0.3497)\n",
      "12403 Training Loss: tensor(0.3429)\n",
      "12404 Training Loss: tensor(0.3435)\n",
      "12405 Training Loss: tensor(0.3433)\n",
      "12406 Training Loss: tensor(0.3467)\n",
      "12407 Training Loss: tensor(0.3439)\n",
      "12408 Training Loss: tensor(0.3431)\n",
      "12409 Training Loss: tensor(0.3437)\n",
      "12410 Training Loss: tensor(0.3503)\n",
      "12411 Training Loss: tensor(0.3443)\n",
      "12412 Training Loss: tensor(0.3436)\n",
      "12413 Training Loss: tensor(0.3483)\n",
      "12414 Training Loss: tensor(0.3465)\n",
      "12415 Training Loss: tensor(0.3484)\n",
      "12416 Training Loss: tensor(0.3439)\n",
      "12417 Training Loss: tensor(0.3440)\n",
      "12418 Training Loss: tensor(0.3477)\n",
      "12419 Training Loss: tensor(0.3445)\n",
      "12420 Training Loss: tensor(0.3448)\n",
      "12421 Training Loss: tensor(0.3463)\n",
      "12422 Training Loss: tensor(0.3461)\n",
      "12423 Training Loss: tensor(0.3444)\n",
      "12424 Training Loss: tensor(0.3442)\n",
      "12425 Training Loss: tensor(0.3459)\n",
      "12426 Training Loss: tensor(0.3450)\n",
      "12427 Training Loss: tensor(0.3450)\n",
      "12428 Training Loss: tensor(0.3459)\n",
      "12429 Training Loss: tensor(0.3440)\n",
      "12430 Training Loss: tensor(0.3438)\n",
      "12431 Training Loss: tensor(0.3437)\n",
      "12432 Training Loss: tensor(0.3466)\n",
      "12433 Training Loss: tensor(0.3450)\n",
      "12434 Training Loss: tensor(0.3427)\n",
      "12435 Training Loss: tensor(0.3472)\n",
      "12436 Training Loss: tensor(0.3465)\n",
      "12437 Training Loss: tensor(0.3434)\n",
      "12438 Training Loss: tensor(0.3443)\n",
      "12439 Training Loss: tensor(0.3515)\n",
      "12440 Training Loss: tensor(0.3438)\n",
      "12441 Training Loss: tensor(0.3442)\n",
      "12442 Training Loss: tensor(0.3435)\n",
      "12443 Training Loss: tensor(0.3456)\n",
      "12444 Training Loss: tensor(0.3451)\n",
      "12445 Training Loss: tensor(0.3438)\n",
      "12446 Training Loss: tensor(0.3439)\n",
      "12447 Training Loss: tensor(0.3468)\n",
      "12448 Training Loss: tensor(0.3434)\n",
      "12449 Training Loss: tensor(0.3435)\n",
      "12450 Training Loss: tensor(0.3447)\n",
      "12451 Training Loss: tensor(0.3452)\n",
      "12452 Training Loss: tensor(0.3433)\n",
      "12453 Training Loss: tensor(0.3431)\n",
      "12454 Training Loss: tensor(0.3437)\n",
      "12455 Training Loss: tensor(0.3442)\n",
      "12456 Training Loss: tensor(0.3443)\n",
      "12457 Training Loss: tensor(0.3435)\n",
      "12458 Training Loss: tensor(0.3439)\n",
      "12459 Training Loss: tensor(0.3635)\n",
      "12460 Training Loss: tensor(0.3441)\n",
      "12461 Training Loss: tensor(0.3444)\n",
      "12462 Training Loss: tensor(0.3446)\n",
      "12463 Training Loss: tensor(0.3436)\n",
      "12464 Training Loss: tensor(0.3442)\n",
      "12465 Training Loss: tensor(0.3442)\n",
      "12466 Training Loss: tensor(0.3460)\n",
      "12467 Training Loss: tensor(0.3445)\n",
      "12468 Training Loss: tensor(0.3426)\n",
      "12469 Training Loss: tensor(0.3446)\n",
      "12470 Training Loss: tensor(0.3441)\n",
      "12471 Training Loss: tensor(0.3450)\n",
      "12472 Training Loss: tensor(0.3438)\n",
      "12473 Training Loss: tensor(0.3443)\n",
      "12474 Training Loss: tensor(0.3434)\n",
      "12475 Training Loss: tensor(0.3437)\n",
      "12476 Training Loss: tensor(0.3433)\n",
      "12477 Training Loss: tensor(0.3433)\n",
      "12478 Training Loss: tensor(0.3457)\n",
      "12479 Training Loss: tensor(0.3475)\n",
      "12480 Training Loss: tensor(0.3427)\n",
      "12481 Training Loss: tensor(0.3450)\n",
      "12482 Training Loss: tensor(0.3506)\n",
      "12483 Training Loss: tensor(0.3447)\n",
      "12484 Training Loss: tensor(0.3431)\n",
      "12485 Training Loss: tensor(0.3435)\n",
      "12486 Training Loss: tensor(0.3428)\n",
      "12487 Training Loss: tensor(0.3433)\n",
      "12488 Training Loss: tensor(0.3455)\n",
      "12489 Training Loss: tensor(0.3469)\n",
      "12490 Training Loss: tensor(0.3437)\n",
      "12491 Training Loss: tensor(0.3436)\n",
      "12492 Training Loss: tensor(0.3458)\n",
      "12493 Training Loss: tensor(0.3430)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12494 Training Loss: tensor(0.3446)\n",
      "12495 Training Loss: tensor(0.3462)\n",
      "12496 Training Loss: tensor(0.3431)\n",
      "12497 Training Loss: tensor(0.3471)\n",
      "12498 Training Loss: tensor(0.3478)\n",
      "12499 Training Loss: tensor(0.3465)\n",
      "12500 Training Loss: tensor(0.3463)\n",
      "12501 Training Loss: tensor(0.3450)\n",
      "12502 Training Loss: tensor(0.3457)\n",
      "12503 Training Loss: tensor(0.3437)\n",
      "12504 Training Loss: tensor(0.3469)\n",
      "12505 Training Loss: tensor(0.3472)\n",
      "12506 Training Loss: tensor(0.3435)\n",
      "12507 Training Loss: tensor(0.3438)\n",
      "12508 Training Loss: tensor(0.3434)\n",
      "12509 Training Loss: tensor(0.3450)\n",
      "12510 Training Loss: tensor(0.3438)\n",
      "12511 Training Loss: tensor(0.3456)\n",
      "12512 Training Loss: tensor(0.3440)\n",
      "12513 Training Loss: tensor(0.3562)\n",
      "12514 Training Loss: tensor(0.3511)\n",
      "12515 Training Loss: tensor(0.3470)\n",
      "12516 Training Loss: tensor(0.3440)\n",
      "12517 Training Loss: tensor(0.3494)\n",
      "12518 Training Loss: tensor(0.3450)\n",
      "12519 Training Loss: tensor(0.3451)\n",
      "12520 Training Loss: tensor(0.3458)\n",
      "12521 Training Loss: tensor(0.3440)\n",
      "12522 Training Loss: tensor(0.3505)\n",
      "12523 Training Loss: tensor(0.3437)\n",
      "12524 Training Loss: tensor(0.3438)\n",
      "12525 Training Loss: tensor(0.3443)\n",
      "12526 Training Loss: tensor(0.3449)\n",
      "12527 Training Loss: tensor(0.3466)\n",
      "12528 Training Loss: tensor(0.3445)\n",
      "12529 Training Loss: tensor(0.3478)\n",
      "12530 Training Loss: tensor(0.3444)\n",
      "12531 Training Loss: tensor(0.3442)\n",
      "12532 Training Loss: tensor(0.3444)\n",
      "12533 Training Loss: tensor(0.3499)\n",
      "12534 Training Loss: tensor(0.3445)\n",
      "12535 Training Loss: tensor(0.3428)\n",
      "12536 Training Loss: tensor(0.3459)\n",
      "12537 Training Loss: tensor(0.3462)\n",
      "12538 Training Loss: tensor(0.3435)\n",
      "12539 Training Loss: tensor(0.3450)\n",
      "12540 Training Loss: tensor(0.3434)\n",
      "12541 Training Loss: tensor(0.3462)\n",
      "12542 Training Loss: tensor(0.3443)\n",
      "12543 Training Loss: tensor(0.3448)\n",
      "12544 Training Loss: tensor(0.3446)\n",
      "12545 Training Loss: tensor(0.3442)\n",
      "12546 Training Loss: tensor(0.3434)\n",
      "12547 Training Loss: tensor(0.3445)\n",
      "12548 Training Loss: tensor(0.3452)\n",
      "12549 Training Loss: tensor(0.3459)\n",
      "12550 Training Loss: tensor(0.3441)\n",
      "12551 Training Loss: tensor(0.3437)\n",
      "12552 Training Loss: tensor(0.3465)\n",
      "12553 Training Loss: tensor(0.3454)\n",
      "12554 Training Loss: tensor(0.3441)\n",
      "12555 Training Loss: tensor(0.3432)\n",
      "12556 Training Loss: tensor(0.3433)\n",
      "12557 Training Loss: tensor(0.3425)\n",
      "12558 Training Loss: tensor(0.3428)\n",
      "12559 Training Loss: tensor(0.3466)\n",
      "12560 Training Loss: tensor(0.3433)\n",
      "12561 Training Loss: tensor(0.3433)\n",
      "12562 Training Loss: tensor(0.3514)\n",
      "12563 Training Loss: tensor(0.3510)\n",
      "12564 Training Loss: tensor(0.3430)\n",
      "12565 Training Loss: tensor(0.3451)\n",
      "12566 Training Loss: tensor(0.3447)\n",
      "12567 Training Loss: tensor(0.3450)\n",
      "12568 Training Loss: tensor(0.3462)\n",
      "12569 Training Loss: tensor(0.3466)\n",
      "12570 Training Loss: tensor(0.3439)\n",
      "12571 Training Loss: tensor(0.3457)\n",
      "12572 Training Loss: tensor(0.3442)\n",
      "12573 Training Loss: tensor(0.3465)\n",
      "12574 Training Loss: tensor(0.3431)\n",
      "12575 Training Loss: tensor(0.3432)\n",
      "12576 Training Loss: tensor(0.3458)\n",
      "12577 Training Loss: tensor(0.3437)\n",
      "12578 Training Loss: tensor(0.3438)\n",
      "12579 Training Loss: tensor(0.3513)\n",
      "12580 Training Loss: tensor(0.3432)\n",
      "12581 Training Loss: tensor(0.3444)\n",
      "12582 Training Loss: tensor(0.3449)\n",
      "12583 Training Loss: tensor(0.3478)\n",
      "12584 Training Loss: tensor(0.3443)\n",
      "12585 Training Loss: tensor(0.3498)\n",
      "12586 Training Loss: tensor(0.3440)\n",
      "12587 Training Loss: tensor(0.3516)\n",
      "12588 Training Loss: tensor(0.3443)\n",
      "12589 Training Loss: tensor(0.3451)\n",
      "12590 Training Loss: tensor(0.3516)\n",
      "12591 Training Loss: tensor(0.3465)\n",
      "12592 Training Loss: tensor(0.3472)\n",
      "12593 Training Loss: tensor(0.3456)\n",
      "12594 Training Loss: tensor(0.3453)\n",
      "12595 Training Loss: tensor(0.3456)\n",
      "12596 Training Loss: tensor(0.3451)\n",
      "12597 Training Loss: tensor(0.3449)\n",
      "12598 Training Loss: tensor(0.3446)\n",
      "12599 Training Loss: tensor(0.3438)\n",
      "12600 Training Loss: tensor(0.3459)\n",
      "12601 Training Loss: tensor(0.3463)\n",
      "12602 Training Loss: tensor(0.3449)\n",
      "12603 Training Loss: tensor(0.3449)\n",
      "12604 Training Loss: tensor(0.3438)\n",
      "12605 Training Loss: tensor(0.3440)\n",
      "12606 Training Loss: tensor(0.3445)\n",
      "12607 Training Loss: tensor(0.3441)\n",
      "12608 Training Loss: tensor(0.3445)\n",
      "12609 Training Loss: tensor(0.3469)\n",
      "12610 Training Loss: tensor(0.3432)\n",
      "12611 Training Loss: tensor(0.3508)\n",
      "12612 Training Loss: tensor(0.3426)\n",
      "12613 Training Loss: tensor(0.3443)\n",
      "12614 Training Loss: tensor(0.3433)\n",
      "12615 Training Loss: tensor(0.3464)\n",
      "12616 Training Loss: tensor(0.3443)\n",
      "12617 Training Loss: tensor(0.3430)\n",
      "12618 Training Loss: tensor(0.3452)\n",
      "12619 Training Loss: tensor(0.3508)\n",
      "12620 Training Loss: tensor(0.3446)\n",
      "12621 Training Loss: tensor(0.3436)\n",
      "12622 Training Loss: tensor(0.3458)\n",
      "12623 Training Loss: tensor(0.3459)\n",
      "12624 Training Loss: tensor(0.3446)\n",
      "12625 Training Loss: tensor(0.3468)\n",
      "12626 Training Loss: tensor(0.3470)\n",
      "12627 Training Loss: tensor(0.3473)\n",
      "12628 Training Loss: tensor(0.3455)\n",
      "12629 Training Loss: tensor(0.3449)\n",
      "12630 Training Loss: tensor(0.3441)\n",
      "12631 Training Loss: tensor(0.3437)\n",
      "12632 Training Loss: tensor(0.3440)\n",
      "12633 Training Loss: tensor(0.3449)\n",
      "12634 Training Loss: tensor(0.3486)\n",
      "12635 Training Loss: tensor(0.3451)\n",
      "12636 Training Loss: tensor(0.3455)\n",
      "12637 Training Loss: tensor(0.3468)\n",
      "12638 Training Loss: tensor(0.3454)\n",
      "12639 Training Loss: tensor(0.3438)\n",
      "12640 Training Loss: tensor(0.3443)\n",
      "12641 Training Loss: tensor(0.3433)\n",
      "12642 Training Loss: tensor(0.3430)\n",
      "12643 Training Loss: tensor(0.3449)\n",
      "12644 Training Loss: tensor(0.3438)\n",
      "12645 Training Loss: tensor(0.3483)\n",
      "12646 Training Loss: tensor(0.3488)\n",
      "12647 Training Loss: tensor(0.3466)\n",
      "12648 Training Loss: tensor(0.3491)\n",
      "12649 Training Loss: tensor(0.3429)\n",
      "12650 Training Loss: tensor(0.3431)\n",
      "12651 Training Loss: tensor(0.3456)\n",
      "12652 Training Loss: tensor(0.3458)\n",
      "12653 Training Loss: tensor(0.3445)\n",
      "12654 Training Loss: tensor(0.3430)\n",
      "12655 Training Loss: tensor(0.3495)\n",
      "12656 Training Loss: tensor(0.3450)\n",
      "12657 Training Loss: tensor(0.3454)\n",
      "12658 Training Loss: tensor(0.3440)\n",
      "12659 Training Loss: tensor(0.3424)\n",
      "12660 Training Loss: tensor(0.3439)\n",
      "12661 Training Loss: tensor(0.3430)\n",
      "12662 Training Loss: tensor(0.3440)\n",
      "12663 Training Loss: tensor(0.3454)\n",
      "12664 Training Loss: tensor(0.3420)\n",
      "12665 Training Loss: tensor(0.3452)\n",
      "12666 Training Loss: tensor(0.3450)\n",
      "12667 Training Loss: tensor(0.3475)\n",
      "12668 Training Loss: tensor(0.3434)\n",
      "12669 Training Loss: tensor(0.3451)\n",
      "12670 Training Loss: tensor(0.3594)\n",
      "12671 Training Loss: tensor(0.3435)\n",
      "12672 Training Loss: tensor(0.3449)\n",
      "12673 Training Loss: tensor(0.3434)\n",
      "12674 Training Loss: tensor(0.3459)\n",
      "12675 Training Loss: tensor(0.3433)\n",
      "12676 Training Loss: tensor(0.3457)\n",
      "12677 Training Loss: tensor(0.3431)\n",
      "12678 Training Loss: tensor(0.3470)\n",
      "12679 Training Loss: tensor(0.3485)\n",
      "12680 Training Loss: tensor(0.3445)\n",
      "12681 Training Loss: tensor(0.3453)\n",
      "12682 Training Loss: tensor(0.3439)\n",
      "12683 Training Loss: tensor(0.3437)\n",
      "12684 Training Loss: tensor(0.3450)\n",
      "12685 Training Loss: tensor(0.3432)\n",
      "12686 Training Loss: tensor(0.3459)\n",
      "12687 Training Loss: tensor(0.3436)\n",
      "12688 Training Loss: tensor(0.3459)\n",
      "12689 Training Loss: tensor(0.3437)\n",
      "12690 Training Loss: tensor(0.3431)\n",
      "12691 Training Loss: tensor(0.3437)\n",
      "12692 Training Loss: tensor(0.3495)\n",
      "12693 Training Loss: tensor(0.3427)\n",
      "12694 Training Loss: tensor(0.3431)\n",
      "12695 Training Loss: tensor(0.3429)\n",
      "12696 Training Loss: tensor(0.3496)\n",
      "12697 Training Loss: tensor(0.3435)\n",
      "12698 Training Loss: tensor(0.3460)\n",
      "12699 Training Loss: tensor(0.3497)\n",
      "12700 Training Loss: tensor(0.3432)\n",
      "12701 Training Loss: tensor(0.3453)\n",
      "12702 Training Loss: tensor(0.3451)\n",
      "12703 Training Loss: tensor(0.3446)\n",
      "12704 Training Loss: tensor(0.3464)\n",
      "12705 Training Loss: tensor(0.3451)\n",
      "12706 Training Loss: tensor(0.3444)\n",
      "12707 Training Loss: tensor(0.3443)\n",
      "12708 Training Loss: tensor(0.3439)\n",
      "12709 Training Loss: tensor(0.3448)\n",
      "12710 Training Loss: tensor(0.3465)\n",
      "12711 Training Loss: tensor(0.3450)\n",
      "12712 Training Loss: tensor(0.3434)\n",
      "12713 Training Loss: tensor(0.3441)\n",
      "12714 Training Loss: tensor(0.3531)\n",
      "12715 Training Loss: tensor(0.3438)\n",
      "12716 Training Loss: tensor(0.3472)\n",
      "12717 Training Loss: tensor(0.3443)\n",
      "12718 Training Loss: tensor(0.3430)\n",
      "12719 Training Loss: tensor(0.3444)\n",
      "12720 Training Loss: tensor(0.3438)\n",
      "12721 Training Loss: tensor(0.3427)\n",
      "12722 Training Loss: tensor(0.3442)\n",
      "12723 Training Loss: tensor(0.3449)\n",
      "12724 Training Loss: tensor(0.3484)\n",
      "12725 Training Loss: tensor(0.3517)\n",
      "12726 Training Loss: tensor(0.3446)\n",
      "12727 Training Loss: tensor(0.3430)\n",
      "12728 Training Loss: tensor(0.3434)\n",
      "12729 Training Loss: tensor(0.3535)\n",
      "12730 Training Loss: tensor(0.3430)\n",
      "12731 Training Loss: tensor(0.3444)\n",
      "12732 Training Loss: tensor(0.3454)\n",
      "12733 Training Loss: tensor(0.3460)\n",
      "12734 Training Loss: tensor(0.3431)\n",
      "12735 Training Loss: tensor(0.3474)\n",
      "12736 Training Loss: tensor(0.3434)\n",
      "12737 Training Loss: tensor(0.3445)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12738 Training Loss: tensor(0.3450)\n",
      "12739 Training Loss: tensor(0.3443)\n",
      "12740 Training Loss: tensor(0.3432)\n",
      "12741 Training Loss: tensor(0.3461)\n",
      "12742 Training Loss: tensor(0.3457)\n",
      "12743 Training Loss: tensor(0.3453)\n",
      "12744 Training Loss: tensor(0.3456)\n",
      "12745 Training Loss: tensor(0.3433)\n",
      "12746 Training Loss: tensor(0.3442)\n",
      "12747 Training Loss: tensor(0.3446)\n",
      "12748 Training Loss: tensor(0.3452)\n",
      "12749 Training Loss: tensor(0.3460)\n",
      "12750 Training Loss: tensor(0.3435)\n",
      "12751 Training Loss: tensor(0.3436)\n",
      "12752 Training Loss: tensor(0.3439)\n",
      "12753 Training Loss: tensor(0.3437)\n",
      "12754 Training Loss: tensor(0.3430)\n",
      "12755 Training Loss: tensor(0.3440)\n",
      "12756 Training Loss: tensor(0.3480)\n",
      "12757 Training Loss: tensor(0.3439)\n",
      "12758 Training Loss: tensor(0.3453)\n",
      "12759 Training Loss: tensor(0.3424)\n",
      "12760 Training Loss: tensor(0.3442)\n",
      "12761 Training Loss: tensor(0.3429)\n",
      "12762 Training Loss: tensor(0.3436)\n",
      "12763 Training Loss: tensor(0.3452)\n",
      "12764 Training Loss: tensor(0.3468)\n",
      "12765 Training Loss: tensor(0.3450)\n",
      "12766 Training Loss: tensor(0.3462)\n",
      "12767 Training Loss: tensor(0.3469)\n",
      "12768 Training Loss: tensor(0.3432)\n",
      "12769 Training Loss: tensor(0.3445)\n",
      "12770 Training Loss: tensor(0.3431)\n",
      "12771 Training Loss: tensor(0.3431)\n",
      "12772 Training Loss: tensor(0.3448)\n",
      "12773 Training Loss: tensor(0.3439)\n",
      "12774 Training Loss: tensor(0.3433)\n",
      "12775 Training Loss: tensor(0.3442)\n",
      "12776 Training Loss: tensor(0.3437)\n",
      "12777 Training Loss: tensor(0.3425)\n",
      "12778 Training Loss: tensor(0.3452)\n",
      "12779 Training Loss: tensor(0.3432)\n",
      "12780 Training Loss: tensor(0.3455)\n",
      "12781 Training Loss: tensor(0.3445)\n",
      "12782 Training Loss: tensor(0.3432)\n",
      "12783 Training Loss: tensor(0.3448)\n",
      "12784 Training Loss: tensor(0.3427)\n",
      "12785 Training Loss: tensor(0.3446)\n",
      "12786 Training Loss: tensor(0.3424)\n",
      "12787 Training Loss: tensor(0.3584)\n",
      "12788 Training Loss: tensor(0.3437)\n",
      "12789 Training Loss: tensor(0.3442)\n",
      "12790 Training Loss: tensor(0.3426)\n",
      "12791 Training Loss: tensor(0.3465)\n",
      "12792 Training Loss: tensor(0.3455)\n",
      "12793 Training Loss: tensor(0.3441)\n",
      "12794 Training Loss: tensor(0.3512)\n",
      "12795 Training Loss: tensor(0.3439)\n",
      "12796 Training Loss: tensor(0.3449)\n",
      "12797 Training Loss: tensor(0.3441)\n",
      "12798 Training Loss: tensor(0.3443)\n",
      "12799 Training Loss: tensor(0.3456)\n",
      "12800 Training Loss: tensor(0.3428)\n",
      "12801 Training Loss: tensor(0.3534)\n",
      "12802 Training Loss: tensor(0.3467)\n",
      "12803 Training Loss: tensor(0.3440)\n",
      "12804 Training Loss: tensor(0.3440)\n",
      "12805 Training Loss: tensor(0.3475)\n",
      "12806 Training Loss: tensor(0.3440)\n",
      "12807 Training Loss: tensor(0.3445)\n",
      "12808 Training Loss: tensor(0.3454)\n",
      "12809 Training Loss: tensor(0.3481)\n",
      "12810 Training Loss: tensor(0.3463)\n",
      "12811 Training Loss: tensor(0.3449)\n",
      "12812 Training Loss: tensor(0.3445)\n",
      "12813 Training Loss: tensor(0.3447)\n",
      "12814 Training Loss: tensor(0.3437)\n",
      "12815 Training Loss: tensor(0.3479)\n",
      "12816 Training Loss: tensor(0.3439)\n",
      "12817 Training Loss: tensor(0.3445)\n",
      "12818 Training Loss: tensor(0.3435)\n",
      "12819 Training Loss: tensor(0.3442)\n",
      "12820 Training Loss: tensor(0.3451)\n",
      "12821 Training Loss: tensor(0.3439)\n",
      "12822 Training Loss: tensor(0.3458)\n",
      "12823 Training Loss: tensor(0.3432)\n",
      "12824 Training Loss: tensor(0.3434)\n",
      "12825 Training Loss: tensor(0.3497)\n",
      "12826 Training Loss: tensor(0.3431)\n",
      "12827 Training Loss: tensor(0.3425)\n",
      "12828 Training Loss: tensor(0.3431)\n",
      "12829 Training Loss: tensor(0.3436)\n",
      "12830 Training Loss: tensor(0.3443)\n",
      "12831 Training Loss: tensor(0.3440)\n",
      "12832 Training Loss: tensor(0.3471)\n",
      "12833 Training Loss: tensor(0.3460)\n",
      "12834 Training Loss: tensor(0.3435)\n",
      "12835 Training Loss: tensor(0.3512)\n",
      "12836 Training Loss: tensor(0.3450)\n",
      "12837 Training Loss: tensor(0.3463)\n",
      "12838 Training Loss: tensor(0.3437)\n",
      "12839 Training Loss: tensor(0.3437)\n",
      "12840 Training Loss: tensor(0.3470)\n",
      "12841 Training Loss: tensor(0.3437)\n",
      "12842 Training Loss: tensor(0.3441)\n",
      "12843 Training Loss: tensor(0.3457)\n",
      "12844 Training Loss: tensor(0.3440)\n",
      "12845 Training Loss: tensor(0.3441)\n",
      "12846 Training Loss: tensor(0.3455)\n",
      "12847 Training Loss: tensor(0.3445)\n",
      "12848 Training Loss: tensor(0.3440)\n",
      "12849 Training Loss: tensor(0.3444)\n",
      "12850 Training Loss: tensor(0.3468)\n",
      "12851 Training Loss: tensor(0.3445)\n",
      "12852 Training Loss: tensor(0.3434)\n",
      "12853 Training Loss: tensor(0.3445)\n",
      "12854 Training Loss: tensor(0.3485)\n",
      "12855 Training Loss: tensor(0.3440)\n",
      "12856 Training Loss: tensor(0.3439)\n",
      "12857 Training Loss: tensor(0.3491)\n",
      "12858 Training Loss: tensor(0.3467)\n",
      "12859 Training Loss: tensor(0.3450)\n",
      "12860 Training Loss: tensor(0.3464)\n",
      "12861 Training Loss: tensor(0.3434)\n",
      "12862 Training Loss: tensor(0.3431)\n",
      "12863 Training Loss: tensor(0.3434)\n",
      "12864 Training Loss: tensor(0.3473)\n",
      "12865 Training Loss: tensor(0.3432)\n",
      "12866 Training Loss: tensor(0.3434)\n",
      "12867 Training Loss: tensor(0.3437)\n",
      "12868 Training Loss: tensor(0.3473)\n",
      "12869 Training Loss: tensor(0.3427)\n",
      "12870 Training Loss: tensor(0.3450)\n",
      "12871 Training Loss: tensor(0.3464)\n",
      "12872 Training Loss: tensor(0.3440)\n",
      "12873 Training Loss: tensor(0.3437)\n",
      "12874 Training Loss: tensor(0.3425)\n",
      "12875 Training Loss: tensor(0.3436)\n",
      "12876 Training Loss: tensor(0.3437)\n",
      "12877 Training Loss: tensor(0.3423)\n",
      "12878 Training Loss: tensor(0.3432)\n",
      "12879 Training Loss: tensor(0.3427)\n",
      "12880 Training Loss: tensor(0.3436)\n",
      "12881 Training Loss: tensor(0.3443)\n",
      "12882 Training Loss: tensor(0.3451)\n",
      "12883 Training Loss: tensor(0.3500)\n",
      "12884 Training Loss: tensor(0.3425)\n",
      "12885 Training Loss: tensor(0.3433)\n",
      "12886 Training Loss: tensor(0.3427)\n",
      "12887 Training Loss: tensor(0.3428)\n",
      "12888 Training Loss: tensor(0.3437)\n",
      "12889 Training Loss: tensor(0.3429)\n",
      "12890 Training Loss: tensor(0.3465)\n",
      "12891 Training Loss: tensor(0.3433)\n",
      "12892 Training Loss: tensor(0.3463)\n",
      "12893 Training Loss: tensor(0.3427)\n",
      "12894 Training Loss: tensor(0.3507)\n",
      "12895 Training Loss: tensor(0.3508)\n",
      "12896 Training Loss: tensor(0.3422)\n",
      "12897 Training Loss: tensor(0.3423)\n",
      "12898 Training Loss: tensor(0.3425)\n",
      "12899 Training Loss: tensor(0.3493)\n",
      "12900 Training Loss: tensor(0.3427)\n",
      "12901 Training Loss: tensor(0.3439)\n",
      "12902 Training Loss: tensor(0.3428)\n",
      "12903 Training Loss: tensor(0.3430)\n",
      "12904 Training Loss: tensor(0.3521)\n",
      "12905 Training Loss: tensor(0.3443)\n",
      "12906 Training Loss: tensor(0.3483)\n",
      "12907 Training Loss: tensor(0.3441)\n",
      "12908 Training Loss: tensor(0.3440)\n",
      "12909 Training Loss: tensor(0.3460)\n",
      "12910 Training Loss: tensor(0.3450)\n",
      "12911 Training Loss: tensor(0.3453)\n",
      "12912 Training Loss: tensor(0.3447)\n",
      "12913 Training Loss: tensor(0.3440)\n",
      "12914 Training Loss: tensor(0.3433)\n",
      "12915 Training Loss: tensor(0.3437)\n",
      "12916 Training Loss: tensor(0.3426)\n",
      "12917 Training Loss: tensor(0.3450)\n",
      "12918 Training Loss: tensor(0.3438)\n",
      "12919 Training Loss: tensor(0.3476)\n",
      "12920 Training Loss: tensor(0.3429)\n",
      "12921 Training Loss: tensor(0.3584)\n",
      "12922 Training Loss: tensor(0.3450)\n",
      "12923 Training Loss: tensor(0.3434)\n",
      "12924 Training Loss: tensor(0.3482)\n",
      "12925 Training Loss: tensor(0.3433)\n",
      "12926 Training Loss: tensor(0.3461)\n",
      "12927 Training Loss: tensor(0.3432)\n",
      "12928 Training Loss: tensor(0.3445)\n",
      "12929 Training Loss: tensor(0.3448)\n",
      "12930 Training Loss: tensor(0.3434)\n",
      "12931 Training Loss: tensor(0.3435)\n",
      "12932 Training Loss: tensor(0.3435)\n",
      "12933 Training Loss: tensor(0.3443)\n",
      "12934 Training Loss: tensor(0.3435)\n",
      "12935 Training Loss: tensor(0.3474)\n",
      "12936 Training Loss: tensor(0.3441)\n",
      "12937 Training Loss: tensor(0.3433)\n",
      "12938 Training Loss: tensor(0.3433)\n",
      "12939 Training Loss: tensor(0.3483)\n",
      "12940 Training Loss: tensor(0.3445)\n",
      "12941 Training Loss: tensor(0.3426)\n",
      "12942 Training Loss: tensor(0.3442)\n",
      "12943 Training Loss: tensor(0.3499)\n",
      "12944 Training Loss: tensor(0.3422)\n",
      "12945 Training Loss: tensor(0.3437)\n",
      "12946 Training Loss: tensor(0.3437)\n",
      "12947 Training Loss: tensor(0.3428)\n",
      "12948 Training Loss: tensor(0.3514)\n",
      "12949 Training Loss: tensor(0.3430)\n",
      "12950 Training Loss: tensor(0.3519)\n",
      "12951 Training Loss: tensor(0.3444)\n",
      "12952 Training Loss: tensor(0.3443)\n",
      "12953 Training Loss: tensor(0.3446)\n",
      "12954 Training Loss: tensor(0.3441)\n",
      "12955 Training Loss: tensor(0.3440)\n",
      "12956 Training Loss: tensor(0.3438)\n",
      "12957 Training Loss: tensor(0.3437)\n",
      "12958 Training Loss: tensor(0.3433)\n",
      "12959 Training Loss: tensor(0.3435)\n",
      "12960 Training Loss: tensor(0.3443)\n",
      "12961 Training Loss: tensor(0.3446)\n",
      "12962 Training Loss: tensor(0.3469)\n",
      "12963 Training Loss: tensor(0.3445)\n",
      "12964 Training Loss: tensor(0.3432)\n",
      "12965 Training Loss: tensor(0.3510)\n",
      "12966 Training Loss: tensor(0.3436)\n",
      "12967 Training Loss: tensor(0.3433)\n",
      "12968 Training Loss: tensor(0.3442)\n",
      "12969 Training Loss: tensor(0.3447)\n",
      "12970 Training Loss: tensor(0.3435)\n",
      "12971 Training Loss: tensor(0.3473)\n",
      "12972 Training Loss: tensor(0.3451)\n",
      "12973 Training Loss: tensor(0.3441)\n",
      "12974 Training Loss: tensor(0.3443)\n",
      "12975 Training Loss: tensor(0.3445)\n",
      "12976 Training Loss: tensor(0.3437)\n",
      "12977 Training Loss: tensor(0.3435)\n",
      "12978 Training Loss: tensor(0.3425)\n",
      "12979 Training Loss: tensor(0.3444)\n",
      "12980 Training Loss: tensor(0.3519)\n",
      "12981 Training Loss: tensor(0.3462)\n",
      "12982 Training Loss: tensor(0.3430)\n",
      "12983 Training Loss: tensor(0.3442)\n",
      "12984 Training Loss: tensor(0.3445)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12985 Training Loss: tensor(0.3432)\n",
      "12986 Training Loss: tensor(0.3483)\n",
      "12987 Training Loss: tensor(0.3430)\n",
      "12988 Training Loss: tensor(0.3430)\n",
      "12989 Training Loss: tensor(0.3431)\n",
      "12990 Training Loss: tensor(0.3432)\n",
      "12991 Training Loss: tensor(0.3456)\n",
      "12992 Training Loss: tensor(0.3452)\n",
      "12993 Training Loss: tensor(0.3453)\n",
      "12994 Training Loss: tensor(0.3485)\n",
      "12995 Training Loss: tensor(0.3455)\n",
      "12996 Training Loss: tensor(0.3434)\n",
      "12997 Training Loss: tensor(0.3428)\n",
      "12998 Training Loss: tensor(0.3430)\n",
      "12999 Training Loss: tensor(0.3443)\n",
      "13000 Training Loss: tensor(0.3447)\n",
      "13001 Training Loss: tensor(0.3475)\n",
      "13002 Training Loss: tensor(0.3440)\n",
      "13003 Training Loss: tensor(0.3425)\n",
      "13004 Training Loss: tensor(0.3435)\n",
      "13005 Training Loss: tensor(0.3440)\n",
      "13006 Training Loss: tensor(0.3441)\n",
      "13007 Training Loss: tensor(0.3450)\n",
      "13008 Training Loss: tensor(0.3442)\n",
      "13009 Training Loss: tensor(0.3465)\n",
      "13010 Training Loss: tensor(0.3441)\n",
      "13011 Training Loss: tensor(0.3432)\n",
      "13012 Training Loss: tensor(0.3481)\n",
      "13013 Training Loss: tensor(0.3435)\n",
      "13014 Training Loss: tensor(0.3467)\n",
      "13015 Training Loss: tensor(0.3431)\n",
      "13016 Training Loss: tensor(0.3433)\n",
      "13017 Training Loss: tensor(0.3441)\n",
      "13018 Training Loss: tensor(0.3440)\n",
      "13019 Training Loss: tensor(0.3483)\n",
      "13020 Training Loss: tensor(0.3457)\n",
      "13021 Training Loss: tensor(0.3452)\n",
      "13022 Training Loss: tensor(0.3460)\n",
      "13023 Training Loss: tensor(0.3436)\n",
      "13024 Training Loss: tensor(0.3437)\n",
      "13025 Training Loss: tensor(0.3441)\n",
      "13026 Training Loss: tensor(0.3444)\n",
      "13027 Training Loss: tensor(0.3435)\n",
      "13028 Training Loss: tensor(0.3434)\n",
      "13029 Training Loss: tensor(0.3440)\n",
      "13030 Training Loss: tensor(0.3432)\n",
      "13031 Training Loss: tensor(0.3439)\n",
      "13032 Training Loss: tensor(0.3429)\n",
      "13033 Training Loss: tensor(0.3438)\n",
      "13034 Training Loss: tensor(0.3424)\n",
      "13035 Training Loss: tensor(0.3428)\n",
      "13036 Training Loss: tensor(0.3432)\n",
      "13037 Training Loss: tensor(0.3445)\n",
      "13038 Training Loss: tensor(0.3426)\n",
      "13039 Training Loss: tensor(0.3442)\n",
      "13040 Training Loss: tensor(0.3552)\n",
      "13041 Training Loss: tensor(0.3425)\n",
      "13042 Training Loss: tensor(0.3418)\n",
      "13043 Training Loss: tensor(0.3441)\n",
      "13044 Training Loss: tensor(0.3434)\n",
      "13045 Training Loss: tensor(0.3443)\n",
      "13046 Training Loss: tensor(0.3438)\n",
      "13047 Training Loss: tensor(0.3483)\n",
      "13048 Training Loss: tensor(0.3436)\n",
      "13049 Training Loss: tensor(0.3425)\n",
      "13050 Training Loss: tensor(0.3434)\n",
      "13051 Training Loss: tensor(0.3545)\n",
      "13052 Training Loss: tensor(0.3434)\n",
      "13053 Training Loss: tensor(0.3430)\n",
      "13054 Training Loss: tensor(0.3469)\n",
      "13055 Training Loss: tensor(0.3455)\n",
      "13056 Training Loss: tensor(0.3429)\n",
      "13057 Training Loss: tensor(0.3484)\n",
      "13058 Training Loss: tensor(0.3461)\n",
      "13059 Training Loss: tensor(0.3441)\n",
      "13060 Training Loss: tensor(0.3439)\n",
      "13061 Training Loss: tensor(0.3438)\n",
      "13062 Training Loss: tensor(0.3442)\n",
      "13063 Training Loss: tensor(0.3439)\n",
      "13064 Training Loss: tensor(0.3431)\n",
      "13065 Training Loss: tensor(0.3491)\n",
      "13066 Training Loss: tensor(0.3450)\n",
      "13067 Training Loss: tensor(0.3438)\n",
      "13068 Training Loss: tensor(0.3481)\n",
      "13069 Training Loss: tensor(0.3438)\n",
      "13070 Training Loss: tensor(0.3440)\n",
      "13071 Training Loss: tensor(0.3444)\n",
      "13072 Training Loss: tensor(0.3434)\n",
      "13073 Training Loss: tensor(0.3435)\n",
      "13074 Training Loss: tensor(0.3434)\n",
      "13075 Training Loss: tensor(0.3434)\n",
      "13076 Training Loss: tensor(0.3428)\n",
      "13077 Training Loss: tensor(0.3429)\n",
      "13078 Training Loss: tensor(0.3436)\n",
      "13079 Training Loss: tensor(0.3427)\n",
      "13080 Training Loss: tensor(0.3434)\n",
      "13081 Training Loss: tensor(0.3430)\n",
      "13082 Training Loss: tensor(0.3424)\n",
      "13083 Training Loss: tensor(0.3430)\n",
      "13084 Training Loss: tensor(0.3420)\n",
      "13085 Training Loss: tensor(0.3415)\n",
      "13086 Training Loss: tensor(0.3475)\n",
      "13087 Training Loss: tensor(0.3526)\n",
      "13088 Training Loss: tensor(0.3441)\n",
      "13089 Training Loss: tensor(0.3417)\n",
      "13090 Training Loss: tensor(0.3432)\n",
      "13091 Training Loss: tensor(0.3568)\n",
      "13092 Training Loss: tensor(0.3423)\n",
      "13093 Training Loss: tensor(0.3425)\n",
      "13094 Training Loss: tensor(0.3492)\n",
      "13095 Training Loss: tensor(0.3445)\n",
      "13096 Training Loss: tensor(0.3443)\n",
      "13097 Training Loss: tensor(0.3459)\n",
      "13098 Training Loss: tensor(0.3439)\n",
      "13099 Training Loss: tensor(0.3484)\n",
      "13100 Training Loss: tensor(0.3442)\n",
      "13101 Training Loss: tensor(0.3432)\n",
      "13102 Training Loss: tensor(0.3448)\n",
      "13103 Training Loss: tensor(0.3439)\n",
      "13104 Training Loss: tensor(0.3449)\n",
      "13105 Training Loss: tensor(0.3436)\n",
      "13106 Training Loss: tensor(0.3433)\n",
      "13107 Training Loss: tensor(0.3438)\n",
      "13108 Training Loss: tensor(0.3449)\n",
      "13109 Training Loss: tensor(0.3430)\n",
      "13110 Training Loss: tensor(0.3444)\n",
      "13111 Training Loss: tensor(0.3427)\n",
      "13112 Training Loss: tensor(0.3426)\n",
      "13113 Training Loss: tensor(0.3426)\n",
      "13114 Training Loss: tensor(0.3505)\n",
      "13115 Training Loss: tensor(0.3487)\n",
      "13116 Training Loss: tensor(0.3424)\n",
      "13117 Training Loss: tensor(0.3438)\n",
      "13118 Training Loss: tensor(0.3427)\n",
      "13119 Training Loss: tensor(0.3438)\n",
      "13120 Training Loss: tensor(0.3496)\n",
      "13121 Training Loss: tensor(0.3418)\n",
      "13122 Training Loss: tensor(0.3431)\n",
      "13123 Training Loss: tensor(0.3441)\n",
      "13124 Training Loss: tensor(0.3436)\n",
      "13125 Training Loss: tensor(0.3456)\n",
      "13126 Training Loss: tensor(0.3448)\n",
      "13127 Training Loss: tensor(0.3429)\n",
      "13128 Training Loss: tensor(0.3431)\n",
      "13129 Training Loss: tensor(0.3437)\n",
      "13130 Training Loss: tensor(0.3422)\n",
      "13131 Training Loss: tensor(0.3434)\n",
      "13132 Training Loss: tensor(0.3432)\n",
      "13133 Training Loss: tensor(0.3451)\n",
      "13134 Training Loss: tensor(0.3418)\n",
      "13135 Training Loss: tensor(0.3434)\n",
      "13136 Training Loss: tensor(0.3480)\n",
      "13137 Training Loss: tensor(0.3462)\n",
      "13138 Training Loss: tensor(0.3471)\n",
      "13139 Training Loss: tensor(0.3472)\n",
      "13140 Training Loss: tensor(0.3420)\n",
      "13141 Training Loss: tensor(0.3449)\n",
      "13142 Training Loss: tensor(0.3447)\n",
      "13143 Training Loss: tensor(0.3442)\n",
      "13144 Training Loss: tensor(0.3446)\n",
      "13145 Training Loss: tensor(0.3426)\n",
      "13146 Training Loss: tensor(0.3435)\n",
      "13147 Training Loss: tensor(0.3447)\n",
      "13148 Training Loss: tensor(0.3434)\n",
      "13149 Training Loss: tensor(0.3446)\n",
      "13150 Training Loss: tensor(0.3434)\n",
      "13151 Training Loss: tensor(0.3444)\n",
      "13152 Training Loss: tensor(0.3434)\n",
      "13153 Training Loss: tensor(0.3506)\n",
      "13154 Training Loss: tensor(0.3437)\n",
      "13155 Training Loss: tensor(0.3430)\n",
      "13156 Training Loss: tensor(0.3475)\n",
      "13157 Training Loss: tensor(0.3427)\n",
      "13158 Training Loss: tensor(0.3437)\n",
      "13159 Training Loss: tensor(0.3444)\n",
      "13160 Training Loss: tensor(0.3447)\n",
      "13161 Training Loss: tensor(0.3496)\n",
      "13162 Training Loss: tensor(0.3446)\n",
      "13163 Training Loss: tensor(0.3430)\n",
      "13164 Training Loss: tensor(0.3456)\n",
      "13165 Training Loss: tensor(0.3434)\n",
      "13166 Training Loss: tensor(0.3429)\n",
      "13167 Training Loss: tensor(0.3424)\n",
      "13168 Training Loss: tensor(0.3422)\n",
      "13169 Training Loss: tensor(0.3422)\n",
      "13170 Training Loss: tensor(0.3494)\n",
      "13171 Training Loss: tensor(0.3426)\n",
      "13172 Training Loss: tensor(0.3427)\n",
      "13173 Training Loss: tensor(0.3459)\n",
      "13174 Training Loss: tensor(0.3435)\n",
      "13175 Training Loss: tensor(0.3420)\n",
      "13176 Training Loss: tensor(0.3435)\n",
      "13177 Training Loss: tensor(0.3494)\n",
      "13178 Training Loss: tensor(0.3423)\n",
      "13179 Training Loss: tensor(0.3457)\n",
      "13180 Training Loss: tensor(0.3434)\n",
      "13181 Training Loss: tensor(0.3424)\n",
      "13182 Training Loss: tensor(0.3428)\n",
      "13183 Training Loss: tensor(0.3426)\n",
      "13184 Training Loss: tensor(0.3430)\n",
      "13185 Training Loss: tensor(0.3425)\n",
      "13186 Training Loss: tensor(0.3460)\n",
      "13187 Training Loss: tensor(0.3452)\n",
      "13188 Training Loss: tensor(0.3451)\n",
      "13189 Training Loss: tensor(0.3425)\n",
      "13190 Training Loss: tensor(0.3423)\n",
      "13191 Training Loss: tensor(0.3430)\n",
      "13192 Training Loss: tensor(0.3436)\n",
      "13193 Training Loss: tensor(0.3452)\n",
      "13194 Training Loss: tensor(0.3425)\n",
      "13195 Training Loss: tensor(0.3422)\n",
      "13196 Training Loss: tensor(0.3427)\n",
      "13197 Training Loss: tensor(0.3461)\n",
      "13198 Training Loss: tensor(0.3443)\n",
      "13199 Training Loss: tensor(0.3426)\n",
      "13200 Training Loss: tensor(0.3496)\n",
      "13201 Training Loss: tensor(0.3442)\n",
      "13202 Training Loss: tensor(0.3431)\n",
      "13203 Training Loss: tensor(0.3418)\n",
      "13204 Training Loss: tensor(0.3445)\n",
      "13205 Training Loss: tensor(0.3429)\n",
      "13206 Training Loss: tensor(0.3424)\n",
      "13207 Training Loss: tensor(0.3424)\n",
      "13208 Training Loss: tensor(0.3429)\n",
      "13209 Training Loss: tensor(0.3446)\n",
      "13210 Training Loss: tensor(0.3426)\n",
      "13211 Training Loss: tensor(0.3425)\n",
      "13212 Training Loss: tensor(0.3433)\n",
      "13213 Training Loss: tensor(0.3442)\n",
      "13214 Training Loss: tensor(0.3421)\n",
      "13215 Training Loss: tensor(0.3440)\n",
      "13216 Training Loss: tensor(0.3428)\n",
      "13217 Training Loss: tensor(0.3417)\n",
      "13218 Training Loss: tensor(0.3421)\n",
      "13219 Training Loss: tensor(0.3421)\n",
      "13220 Training Loss: tensor(0.3429)\n",
      "13221 Training Loss: tensor(0.3489)\n",
      "13222 Training Loss: tensor(0.3410)\n",
      "13223 Training Loss: tensor(0.3413)\n",
      "13224 Training Loss: tensor(0.3413)\n",
      "13225 Training Loss: tensor(0.3471)\n",
      "13226 Training Loss: tensor(0.3424)\n",
      "13227 Training Loss: tensor(0.3419)\n",
      "13228 Training Loss: tensor(0.3430)\n",
      "13229 Training Loss: tensor(0.3420)\n",
      "13230 Training Loss: tensor(0.3448)\n",
      "13231 Training Loss: tensor(0.3429)\n",
      "13232 Training Loss: tensor(0.3502)\n",
      "13233 Training Loss: tensor(0.3437)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13234 Training Loss: tensor(0.3427)\n",
      "13235 Training Loss: tensor(0.3480)\n",
      "13236 Training Loss: tensor(0.3417)\n",
      "13237 Training Loss: tensor(0.3415)\n",
      "13238 Training Loss: tensor(0.3416)\n",
      "13239 Training Loss: tensor(0.3450)\n",
      "13240 Training Loss: tensor(0.3416)\n",
      "13241 Training Loss: tensor(0.3426)\n",
      "13242 Training Loss: tensor(0.3418)\n",
      "13243 Training Loss: tensor(0.3423)\n",
      "13244 Training Loss: tensor(0.3435)\n",
      "13245 Training Loss: tensor(0.3429)\n",
      "13246 Training Loss: tensor(0.3430)\n",
      "13247 Training Loss: tensor(0.3431)\n",
      "13248 Training Loss: tensor(0.3422)\n",
      "13249 Training Loss: tensor(0.3466)\n",
      "13250 Training Loss: tensor(0.3476)\n",
      "13251 Training Loss: tensor(0.3439)\n",
      "13252 Training Loss: tensor(0.3425)\n",
      "13253 Training Loss: tensor(0.3446)\n",
      "13254 Training Loss: tensor(0.3428)\n",
      "13255 Training Loss: tensor(0.3412)\n",
      "13256 Training Loss: tensor(0.3418)\n",
      "13257 Training Loss: tensor(0.3434)\n",
      "13258 Training Loss: tensor(0.3485)\n",
      "13259 Training Loss: tensor(0.3445)\n",
      "13260 Training Loss: tensor(0.3417)\n",
      "13261 Training Loss: tensor(0.3422)\n",
      "13262 Training Loss: tensor(0.3407)\n",
      "13263 Training Loss: tensor(0.3457)\n",
      "13264 Training Loss: tensor(0.3546)\n",
      "13265 Training Loss: tensor(0.3420)\n",
      "13266 Training Loss: tensor(0.3431)\n",
      "13267 Training Loss: tensor(0.3415)\n",
      "13268 Training Loss: tensor(0.3424)\n",
      "13269 Training Loss: tensor(0.3424)\n",
      "13270 Training Loss: tensor(0.3412)\n",
      "13271 Training Loss: tensor(0.3457)\n",
      "13272 Training Loss: tensor(0.3429)\n",
      "13273 Training Loss: tensor(0.3423)\n",
      "13274 Training Loss: tensor(0.3464)\n",
      "13275 Training Loss: tensor(0.3424)\n",
      "13276 Training Loss: tensor(0.3441)\n",
      "13277 Training Loss: tensor(0.3413)\n",
      "13278 Training Loss: tensor(0.3446)\n",
      "13279 Training Loss: tensor(0.3424)\n",
      "13280 Training Loss: tensor(0.3430)\n",
      "13281 Training Loss: tensor(0.3416)\n",
      "13282 Training Loss: tensor(0.3424)\n",
      "13283 Training Loss: tensor(0.3421)\n",
      "13284 Training Loss: tensor(0.3419)\n",
      "13285 Training Loss: tensor(0.3412)\n",
      "13286 Training Loss: tensor(0.3496)\n",
      "13287 Training Loss: tensor(0.3461)\n",
      "13288 Training Loss: tensor(0.3413)\n",
      "13289 Training Loss: tensor(0.3408)\n",
      "13290 Training Loss: tensor(0.3414)\n",
      "13291 Training Loss: tensor(0.3538)\n",
      "13292 Training Loss: tensor(0.3426)\n",
      "13293 Training Loss: tensor(0.3472)\n",
      "13294 Training Loss: tensor(0.3494)\n",
      "13295 Training Loss: tensor(0.3417)\n",
      "13296 Training Loss: tensor(0.3448)\n",
      "13297 Training Loss: tensor(0.3451)\n",
      "13298 Training Loss: tensor(0.3421)\n",
      "13299 Training Loss: tensor(0.3440)\n",
      "13300 Training Loss: tensor(0.3434)\n",
      "13301 Training Loss: tensor(0.3479)\n",
      "13302 Training Loss: tensor(0.3429)\n",
      "13303 Training Loss: tensor(0.3426)\n",
      "13304 Training Loss: tensor(0.3478)\n",
      "13305 Training Loss: tensor(0.3444)\n",
      "13306 Training Loss: tensor(0.3427)\n",
      "13307 Training Loss: tensor(0.3437)\n",
      "13308 Training Loss: tensor(0.3423)\n",
      "13309 Training Loss: tensor(0.3421)\n",
      "13310 Training Loss: tensor(0.3441)\n",
      "13311 Training Loss: tensor(0.3418)\n",
      "13312 Training Loss: tensor(0.3440)\n",
      "13313 Training Loss: tensor(0.3427)\n",
      "13314 Training Loss: tensor(0.3421)\n",
      "13315 Training Loss: tensor(0.3422)\n",
      "13316 Training Loss: tensor(0.3431)\n",
      "13317 Training Loss: tensor(0.3441)\n",
      "13318 Training Loss: tensor(0.3462)\n",
      "13319 Training Loss: tensor(0.3426)\n",
      "13320 Training Loss: tensor(0.3532)\n",
      "13321 Training Loss: tensor(0.3419)\n",
      "13322 Training Loss: tensor(0.3442)\n",
      "13323 Training Loss: tensor(0.3417)\n",
      "13324 Training Loss: tensor(0.3419)\n",
      "13325 Training Loss: tensor(0.3430)\n",
      "13326 Training Loss: tensor(0.3452)\n",
      "13327 Training Loss: tensor(0.3417)\n",
      "13328 Training Loss: tensor(0.3416)\n",
      "13329 Training Loss: tensor(0.3443)\n",
      "13330 Training Loss: tensor(0.3426)\n",
      "13331 Training Loss: tensor(0.3421)\n",
      "13332 Training Loss: tensor(0.3422)\n",
      "13333 Training Loss: tensor(0.3439)\n",
      "13334 Training Loss: tensor(0.3423)\n",
      "13335 Training Loss: tensor(0.3429)\n",
      "13336 Training Loss: tensor(0.3417)\n",
      "13337 Training Loss: tensor(0.3424)\n",
      "13338 Training Loss: tensor(0.3419)\n",
      "13339 Training Loss: tensor(0.3435)\n",
      "13340 Training Loss: tensor(0.3502)\n",
      "13341 Training Loss: tensor(0.3425)\n",
      "13342 Training Loss: tensor(0.3455)\n",
      "13343 Training Loss: tensor(0.3420)\n",
      "13344 Training Loss: tensor(0.3414)\n",
      "13345 Training Loss: tensor(0.3439)\n",
      "13346 Training Loss: tensor(0.3419)\n",
      "13347 Training Loss: tensor(0.3424)\n",
      "13348 Training Loss: tensor(0.3432)\n",
      "13349 Training Loss: tensor(0.3416)\n",
      "13350 Training Loss: tensor(0.3417)\n",
      "13351 Training Loss: tensor(0.3430)\n",
      "13352 Training Loss: tensor(0.3415)\n",
      "13353 Training Loss: tensor(0.3417)\n",
      "13354 Training Loss: tensor(0.3427)\n",
      "13355 Training Loss: tensor(0.3409)\n",
      "13356 Training Loss: tensor(0.3489)\n",
      "13357 Training Loss: tensor(0.3420)\n",
      "13358 Training Loss: tensor(0.3504)\n",
      "13359 Training Loss: tensor(0.3420)\n",
      "13360 Training Loss: tensor(0.3410)\n",
      "13361 Training Loss: tensor(0.3417)\n",
      "13362 Training Loss: tensor(0.3427)\n",
      "13363 Training Loss: tensor(0.3429)\n",
      "13364 Training Loss: tensor(0.3437)\n",
      "13365 Training Loss: tensor(0.3469)\n",
      "13366 Training Loss: tensor(0.3424)\n",
      "13367 Training Loss: tensor(0.3417)\n",
      "13368 Training Loss: tensor(0.3447)\n",
      "13369 Training Loss: tensor(0.3486)\n",
      "13370 Training Loss: tensor(0.3424)\n",
      "13371 Training Loss: tensor(0.3422)\n",
      "13372 Training Loss: tensor(0.3466)\n",
      "13373 Training Loss: tensor(0.3433)\n",
      "13374 Training Loss: tensor(0.3417)\n",
      "13375 Training Loss: tensor(0.3428)\n",
      "13376 Training Loss: tensor(0.3413)\n",
      "13377 Training Loss: tensor(0.3491)\n",
      "13378 Training Loss: tensor(0.3424)\n",
      "13379 Training Loss: tensor(0.3429)\n",
      "13380 Training Loss: tensor(0.3415)\n",
      "13381 Training Loss: tensor(0.3419)\n",
      "13382 Training Loss: tensor(0.3417)\n",
      "13383 Training Loss: tensor(0.3428)\n",
      "13384 Training Loss: tensor(0.3412)\n",
      "13385 Training Loss: tensor(0.3484)\n",
      "13386 Training Loss: tensor(0.3420)\n",
      "13387 Training Loss: tensor(0.3416)\n",
      "13388 Training Loss: tensor(0.3480)\n",
      "13389 Training Loss: tensor(0.3425)\n",
      "13390 Training Loss: tensor(0.3509)\n",
      "13391 Training Loss: tensor(0.3424)\n",
      "13392 Training Loss: tensor(0.3438)\n",
      "13393 Training Loss: tensor(0.3421)\n",
      "13394 Training Loss: tensor(0.3440)\n",
      "13395 Training Loss: tensor(0.3432)\n",
      "13396 Training Loss: tensor(0.3430)\n",
      "13397 Training Loss: tensor(0.3428)\n",
      "13398 Training Loss: tensor(0.3419)\n",
      "13399 Training Loss: tensor(0.3430)\n",
      "13400 Training Loss: tensor(0.3419)\n",
      "13401 Training Loss: tensor(0.3422)\n",
      "13402 Training Loss: tensor(0.3427)\n",
      "13403 Training Loss: tensor(0.3442)\n",
      "13404 Training Loss: tensor(0.3410)\n",
      "13405 Training Loss: tensor(0.3427)\n",
      "13406 Training Loss: tensor(0.3422)\n",
      "13407 Training Loss: tensor(0.3411)\n",
      "13408 Training Loss: tensor(0.3420)\n",
      "13409 Training Loss: tensor(0.3521)\n",
      "13410 Training Loss: tensor(0.3453)\n",
      "13411 Training Loss: tensor(0.3418)\n",
      "13412 Training Loss: tensor(0.3419)\n",
      "13413 Training Loss: tensor(0.3428)\n",
      "13414 Training Loss: tensor(0.3417)\n",
      "13415 Training Loss: tensor(0.3432)\n",
      "13416 Training Loss: tensor(0.3423)\n",
      "13417 Training Loss: tensor(0.3413)\n",
      "13418 Training Loss: tensor(0.3410)\n",
      "13419 Training Loss: tensor(0.3476)\n",
      "13420 Training Loss: tensor(0.3414)\n",
      "13421 Training Loss: tensor(0.3438)\n",
      "13422 Training Loss: tensor(0.3420)\n",
      "13423 Training Loss: tensor(0.3428)\n",
      "13424 Training Loss: tensor(0.3445)\n",
      "13425 Training Loss: tensor(0.3446)\n",
      "13426 Training Loss: tensor(0.3438)\n",
      "13427 Training Loss: tensor(0.3417)\n",
      "13428 Training Loss: tensor(0.3423)\n",
      "13429 Training Loss: tensor(0.3413)\n",
      "13430 Training Loss: tensor(0.3457)\n",
      "13431 Training Loss: tensor(0.3424)\n",
      "13432 Training Loss: tensor(0.3428)\n",
      "13433 Training Loss: tensor(0.3427)\n",
      "13434 Training Loss: tensor(0.3418)\n",
      "13435 Training Loss: tensor(0.3430)\n",
      "13436 Training Loss: tensor(0.3463)\n",
      "13437 Training Loss: tensor(0.3421)\n",
      "13438 Training Loss: tensor(0.3411)\n",
      "13439 Training Loss: tensor(0.3431)\n",
      "13440 Training Loss: tensor(0.3436)\n",
      "13441 Training Loss: tensor(0.3417)\n",
      "13442 Training Loss: tensor(0.3412)\n",
      "13443 Training Loss: tensor(0.3415)\n",
      "13444 Training Loss: tensor(0.3422)\n",
      "13445 Training Loss: tensor(0.3415)\n",
      "13446 Training Loss: tensor(0.3425)\n",
      "13447 Training Loss: tensor(0.3502)\n",
      "13448 Training Loss: tensor(0.3406)\n",
      "13449 Training Loss: tensor(0.3415)\n",
      "13450 Training Loss: tensor(0.3447)\n",
      "13451 Training Loss: tensor(0.3491)\n",
      "13452 Training Loss: tensor(0.3441)\n",
      "13453 Training Loss: tensor(0.3489)\n",
      "13454 Training Loss: tensor(0.3422)\n",
      "13455 Training Loss: tensor(0.3418)\n",
      "13456 Training Loss: tensor(0.3444)\n",
      "13457 Training Loss: tensor(0.3428)\n",
      "13458 Training Loss: tensor(0.3423)\n",
      "13459 Training Loss: tensor(0.3430)\n",
      "13460 Training Loss: tensor(0.3427)\n",
      "13461 Training Loss: tensor(0.3463)\n",
      "13462 Training Loss: tensor(0.3452)\n",
      "13463 Training Loss: tensor(0.3458)\n",
      "13464 Training Loss: tensor(0.3454)\n",
      "13465 Training Loss: tensor(0.3427)\n",
      "13466 Training Loss: tensor(0.3491)\n",
      "13467 Training Loss: tensor(0.3458)\n",
      "13468 Training Loss: tensor(0.3430)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13469 Training Loss: tensor(0.3435)\n",
      "13470 Training Loss: tensor(0.3467)\n",
      "13471 Training Loss: tensor(0.3437)\n",
      "13472 Training Loss: tensor(0.3454)\n",
      "13473 Training Loss: tensor(0.3435)\n",
      "13474 Training Loss: tensor(0.3450)\n",
      "13475 Training Loss: tensor(0.3463)\n",
      "13476 Training Loss: tensor(0.3423)\n",
      "13477 Training Loss: tensor(0.3428)\n",
      "13478 Training Loss: tensor(0.3437)\n",
      "13479 Training Loss: tensor(0.3435)\n",
      "13480 Training Loss: tensor(0.3428)\n",
      "13481 Training Loss: tensor(0.3416)\n",
      "13482 Training Loss: tensor(0.3416)\n",
      "13483 Training Loss: tensor(0.3428)\n",
      "13484 Training Loss: tensor(0.3417)\n",
      "13485 Training Loss: tensor(0.3418)\n",
      "13486 Training Loss: tensor(0.3420)\n",
      "13487 Training Loss: tensor(0.3474)\n",
      "13488 Training Loss: tensor(0.3412)\n",
      "13489 Training Loss: tensor(0.3413)\n",
      "13490 Training Loss: tensor(0.3437)\n",
      "13491 Training Loss: tensor(0.3423)\n",
      "13492 Training Loss: tensor(0.3473)\n",
      "13493 Training Loss: tensor(0.3434)\n",
      "13494 Training Loss: tensor(0.3424)\n",
      "13495 Training Loss: tensor(0.3416)\n",
      "13496 Training Loss: tensor(0.3414)\n",
      "13497 Training Loss: tensor(0.3412)\n",
      "13498 Training Loss: tensor(0.3414)\n",
      "13499 Training Loss: tensor(0.3456)\n",
      "13500 Training Loss: tensor(0.3471)\n",
      "13501 Training Loss: tensor(0.3409)\n",
      "13502 Training Loss: tensor(0.3419)\n",
      "13503 Training Loss: tensor(0.3432)\n",
      "13504 Training Loss: tensor(0.3435)\n",
      "13505 Training Loss: tensor(0.3427)\n",
      "13506 Training Loss: tensor(0.3434)\n",
      "13507 Training Loss: tensor(0.3425)\n",
      "13508 Training Loss: tensor(0.3429)\n",
      "13509 Training Loss: tensor(0.3455)\n",
      "13510 Training Loss: tensor(0.3436)\n",
      "13511 Training Loss: tensor(0.3417)\n",
      "13512 Training Loss: tensor(0.3414)\n",
      "13513 Training Loss: tensor(0.3427)\n",
      "13514 Training Loss: tensor(0.3420)\n",
      "13515 Training Loss: tensor(0.3472)\n",
      "13516 Training Loss: tensor(0.3431)\n",
      "13517 Training Loss: tensor(0.3421)\n",
      "13518 Training Loss: tensor(0.3430)\n",
      "13519 Training Loss: tensor(0.3497)\n",
      "13520 Training Loss: tensor(0.3424)\n",
      "13521 Training Loss: tensor(0.3421)\n",
      "13522 Training Loss: tensor(0.3447)\n",
      "13523 Training Loss: tensor(0.3462)\n",
      "13524 Training Loss: tensor(0.3513)\n",
      "13525 Training Loss: tensor(0.3435)\n",
      "13526 Training Loss: tensor(0.3431)\n",
      "13527 Training Loss: tensor(0.3421)\n",
      "13528 Training Loss: tensor(0.3429)\n",
      "13529 Training Loss: tensor(0.3418)\n",
      "13530 Training Loss: tensor(0.3424)\n",
      "13531 Training Loss: tensor(0.3433)\n",
      "13532 Training Loss: tensor(0.3438)\n",
      "13533 Training Loss: tensor(0.3432)\n",
      "13534 Training Loss: tensor(0.3422)\n",
      "13535 Training Loss: tensor(0.3432)\n",
      "13536 Training Loss: tensor(0.3410)\n",
      "13537 Training Loss: tensor(0.3431)\n",
      "13538 Training Loss: tensor(0.3414)\n",
      "13539 Training Loss: tensor(0.3436)\n",
      "13540 Training Loss: tensor(0.3440)\n",
      "13541 Training Loss: tensor(0.3454)\n",
      "13542 Training Loss: tensor(0.3428)\n",
      "13543 Training Loss: tensor(0.3426)\n",
      "13544 Training Loss: tensor(0.3429)\n",
      "13545 Training Loss: tensor(0.3423)\n",
      "13546 Training Loss: tensor(0.3447)\n",
      "13547 Training Loss: tensor(0.3416)\n",
      "13548 Training Loss: tensor(0.3407)\n",
      "13549 Training Loss: tensor(0.3442)\n",
      "13550 Training Loss: tensor(0.3407)\n",
      "13551 Training Loss: tensor(0.3415)\n",
      "13552 Training Loss: tensor(0.3416)\n",
      "13553 Training Loss: tensor(0.3413)\n",
      "13554 Training Loss: tensor(0.3413)\n",
      "13555 Training Loss: tensor(0.3408)\n",
      "13556 Training Loss: tensor(0.3445)\n",
      "13557 Training Loss: tensor(0.3414)\n",
      "13558 Training Loss: tensor(0.3422)\n",
      "13559 Training Loss: tensor(0.3413)\n",
      "13560 Training Loss: tensor(0.3484)\n",
      "13561 Training Loss: tensor(0.3415)\n",
      "13562 Training Loss: tensor(0.3518)\n",
      "13563 Training Loss: tensor(0.3423)\n",
      "13564 Training Loss: tensor(0.3417)\n",
      "13565 Training Loss: tensor(0.3471)\n",
      "13566 Training Loss: tensor(0.3416)\n",
      "13567 Training Loss: tensor(0.3419)\n",
      "13568 Training Loss: tensor(0.3470)\n",
      "13569 Training Loss: tensor(0.3413)\n",
      "13570 Training Loss: tensor(0.3413)\n",
      "13571 Training Loss: tensor(0.3417)\n",
      "13572 Training Loss: tensor(0.3425)\n",
      "13573 Training Loss: tensor(0.3424)\n",
      "13574 Training Loss: tensor(0.3414)\n",
      "13575 Training Loss: tensor(0.3423)\n",
      "13576 Training Loss: tensor(0.3473)\n",
      "13577 Training Loss: tensor(0.3417)\n",
      "13578 Training Loss: tensor(0.3414)\n",
      "13579 Training Loss: tensor(0.3418)\n",
      "13580 Training Loss: tensor(0.3434)\n",
      "13581 Training Loss: tensor(0.3437)\n",
      "13582 Training Loss: tensor(0.3414)\n",
      "13583 Training Loss: tensor(0.3449)\n",
      "13584 Training Loss: tensor(0.3412)\n",
      "13585 Training Loss: tensor(0.3412)\n",
      "13586 Training Loss: tensor(0.3425)\n",
      "13587 Training Loss: tensor(0.3417)\n",
      "13588 Training Loss: tensor(0.3411)\n",
      "13589 Training Loss: tensor(0.3433)\n",
      "13590 Training Loss: tensor(0.3402)\n",
      "13591 Training Loss: tensor(0.3406)\n",
      "13592 Training Loss: tensor(0.3411)\n",
      "13593 Training Loss: tensor(0.3490)\n",
      "13594 Training Loss: tensor(0.3406)\n",
      "13595 Training Loss: tensor(0.3492)\n",
      "13596 Training Loss: tensor(0.3419)\n",
      "13597 Training Loss: tensor(0.3406)\n",
      "13598 Training Loss: tensor(0.3416)\n",
      "13599 Training Loss: tensor(0.3404)\n",
      "13600 Training Loss: tensor(0.3427)\n",
      "13601 Training Loss: tensor(0.3415)\n",
      "13602 Training Loss: tensor(0.3409)\n",
      "13603 Training Loss: tensor(0.3416)\n",
      "13604 Training Loss: tensor(0.3406)\n",
      "13605 Training Loss: tensor(0.3425)\n",
      "13606 Training Loss: tensor(0.3407)\n",
      "13607 Training Loss: tensor(0.3546)\n",
      "13608 Training Loss: tensor(0.3492)\n",
      "13609 Training Loss: tensor(0.3414)\n",
      "13610 Training Loss: tensor(0.3410)\n",
      "13611 Training Loss: tensor(0.3414)\n",
      "13612 Training Loss: tensor(0.3406)\n",
      "13613 Training Loss: tensor(0.3405)\n",
      "13614 Training Loss: tensor(0.3408)\n",
      "13615 Training Loss: tensor(0.3453)\n",
      "13616 Training Loss: tensor(0.3412)\n",
      "13617 Training Loss: tensor(0.3442)\n",
      "13618 Training Loss: tensor(0.3417)\n",
      "13619 Training Loss: tensor(0.3420)\n",
      "13620 Training Loss: tensor(0.3483)\n",
      "13621 Training Loss: tensor(0.3437)\n",
      "13622 Training Loss: tensor(0.3429)\n",
      "13623 Training Loss: tensor(0.3409)\n",
      "13624 Training Loss: tensor(0.3426)\n",
      "13625 Training Loss: tensor(0.3434)\n",
      "13626 Training Loss: tensor(0.3404)\n",
      "13627 Training Loss: tensor(0.3435)\n",
      "13628 Training Loss: tensor(0.3427)\n",
      "13629 Training Loss: tensor(0.3427)\n",
      "13630 Training Loss: tensor(0.3425)\n",
      "13631 Training Loss: tensor(0.3417)\n",
      "13632 Training Loss: tensor(0.3415)\n",
      "13633 Training Loss: tensor(0.3429)\n",
      "13634 Training Loss: tensor(0.3411)\n",
      "13635 Training Loss: tensor(0.3418)\n",
      "13636 Training Loss: tensor(0.3405)\n",
      "13637 Training Loss: tensor(0.3408)\n",
      "13638 Training Loss: tensor(0.3407)\n",
      "13639 Training Loss: tensor(0.3423)\n",
      "13640 Training Loss: tensor(0.3403)\n",
      "13641 Training Loss: tensor(0.3420)\n",
      "13642 Training Loss: tensor(0.3414)\n",
      "13643 Training Loss: tensor(0.3425)\n",
      "13644 Training Loss: tensor(0.3427)\n",
      "13645 Training Loss: tensor(0.3595)\n",
      "13646 Training Loss: tensor(0.3406)\n",
      "13647 Training Loss: tensor(0.3429)\n",
      "13648 Training Loss: tensor(0.3424)\n",
      "13649 Training Loss: tensor(0.3418)\n",
      "13650 Training Loss: tensor(0.3424)\n",
      "13651 Training Loss: tensor(0.3460)\n",
      "13652 Training Loss: tensor(0.3414)\n",
      "13653 Training Loss: tensor(0.3415)\n",
      "13654 Training Loss: tensor(0.3418)\n",
      "13655 Training Loss: tensor(0.3426)\n",
      "13656 Training Loss: tensor(0.3421)\n",
      "13657 Training Loss: tensor(0.3427)\n",
      "13658 Training Loss: tensor(0.3404)\n",
      "13659 Training Loss: tensor(0.3405)\n",
      "13660 Training Loss: tensor(0.3410)\n",
      "13661 Training Loss: tensor(0.3414)\n",
      "13662 Training Loss: tensor(0.3405)\n",
      "13663 Training Loss: tensor(0.3415)\n",
      "13664 Training Loss: tensor(0.3422)\n",
      "13665 Training Loss: tensor(0.3434)\n",
      "13666 Training Loss: tensor(0.3507)\n",
      "13667 Training Loss: tensor(0.3417)\n",
      "13668 Training Loss: tensor(0.3436)\n",
      "13669 Training Loss: tensor(0.3413)\n",
      "13670 Training Loss: tensor(0.3395)\n",
      "13671 Training Loss: tensor(0.3400)\n",
      "13672 Training Loss: tensor(0.3426)\n",
      "13673 Training Loss: tensor(0.3472)\n",
      "13674 Training Loss: tensor(0.3414)\n",
      "13675 Training Loss: tensor(0.3417)\n",
      "13676 Training Loss: tensor(0.3438)\n",
      "13677 Training Loss: tensor(0.3416)\n",
      "13678 Training Loss: tensor(0.3418)\n",
      "13679 Training Loss: tensor(0.3415)\n",
      "13680 Training Loss: tensor(0.3405)\n",
      "13681 Training Loss: tensor(0.3434)\n",
      "13682 Training Loss: tensor(0.3401)\n",
      "13683 Training Loss: tensor(0.3423)\n",
      "13684 Training Loss: tensor(0.3414)\n",
      "13685 Training Loss: tensor(0.3416)\n",
      "13686 Training Loss: tensor(0.3417)\n",
      "13687 Training Loss: tensor(0.3433)\n",
      "13688 Training Loss: tensor(0.3475)\n",
      "13689 Training Loss: tensor(0.3414)\n",
      "13690 Training Loss: tensor(0.3426)\n",
      "13691 Training Loss: tensor(0.3404)\n",
      "13692 Training Loss: tensor(0.3451)\n",
      "13693 Training Loss: tensor(0.3429)\n",
      "13694 Training Loss: tensor(0.3409)\n",
      "13695 Training Loss: tensor(0.3420)\n",
      "13696 Training Loss: tensor(0.3423)\n",
      "13697 Training Loss: tensor(0.3418)\n",
      "13698 Training Loss: tensor(0.3425)\n",
      "13699 Training Loss: tensor(0.3406)\n",
      "13700 Training Loss: tensor(0.3428)\n",
      "13701 Training Loss: tensor(0.3401)\n",
      "13702 Training Loss: tensor(0.3409)\n",
      "13703 Training Loss: tensor(0.3415)\n",
      "13704 Training Loss: tensor(0.3406)\n",
      "13705 Training Loss: tensor(0.3651)\n",
      "13706 Training Loss: tensor(0.3399)\n",
      "13707 Training Loss: tensor(0.3417)\n",
      "13708 Training Loss: tensor(0.3419)\n",
      "13709 Training Loss: tensor(0.3422)\n",
      "13710 Training Loss: tensor(0.3416)\n",
      "13711 Training Loss: tensor(0.3425)\n",
      "13712 Training Loss: tensor(0.3429)\n",
      "13713 Training Loss: tensor(0.3430)\n",
      "13714 Training Loss: tensor(0.3423)\n",
      "13715 Training Loss: tensor(0.3408)\n",
      "13716 Training Loss: tensor(0.3414)\n",
      "13717 Training Loss: tensor(0.3412)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13718 Training Loss: tensor(0.3529)\n",
      "13719 Training Loss: tensor(0.3508)\n",
      "13720 Training Loss: tensor(0.3434)\n",
      "13721 Training Loss: tensor(0.3458)\n",
      "13722 Training Loss: tensor(0.3408)\n",
      "13723 Training Loss: tensor(0.3408)\n",
      "13724 Training Loss: tensor(0.3425)\n",
      "13725 Training Loss: tensor(0.3414)\n",
      "13726 Training Loss: tensor(0.3426)\n",
      "13727 Training Loss: tensor(0.3419)\n",
      "13728 Training Loss: tensor(0.3467)\n",
      "13729 Training Loss: tensor(0.3410)\n",
      "13730 Training Loss: tensor(0.3423)\n",
      "13731 Training Loss: tensor(0.3430)\n",
      "13732 Training Loss: tensor(0.3424)\n",
      "13733 Training Loss: tensor(0.3413)\n",
      "13734 Training Loss: tensor(0.3451)\n",
      "13735 Training Loss: tensor(0.3433)\n",
      "13736 Training Loss: tensor(0.3408)\n",
      "13737 Training Loss: tensor(0.3427)\n",
      "13738 Training Loss: tensor(0.3418)\n",
      "13739 Training Loss: tensor(0.3425)\n",
      "13740 Training Loss: tensor(0.3409)\n",
      "13741 Training Loss: tensor(0.3418)\n",
      "13742 Training Loss: tensor(0.3435)\n",
      "13743 Training Loss: tensor(0.3418)\n",
      "13744 Training Loss: tensor(0.3414)\n",
      "13745 Training Loss: tensor(0.3442)\n",
      "13746 Training Loss: tensor(0.3413)\n",
      "13747 Training Loss: tensor(0.3414)\n",
      "13748 Training Loss: tensor(0.3411)\n",
      "13749 Training Loss: tensor(0.3401)\n",
      "13750 Training Loss: tensor(0.3423)\n",
      "13751 Training Loss: tensor(0.3404)\n",
      "13752 Training Loss: tensor(0.3465)\n",
      "13753 Training Loss: tensor(0.3487)\n",
      "13754 Training Loss: tensor(0.3424)\n",
      "13755 Training Loss: tensor(0.3412)\n",
      "13756 Training Loss: tensor(0.3418)\n",
      "13757 Training Loss: tensor(0.3413)\n",
      "13758 Training Loss: tensor(0.3411)\n",
      "13759 Training Loss: tensor(0.3403)\n",
      "13760 Training Loss: tensor(0.3405)\n",
      "13761 Training Loss: tensor(0.3417)\n",
      "13762 Training Loss: tensor(0.3409)\n",
      "13763 Training Loss: tensor(0.3415)\n",
      "13764 Training Loss: tensor(0.3417)\n",
      "13765 Training Loss: tensor(0.3417)\n",
      "13766 Training Loss: tensor(0.3408)\n",
      "13767 Training Loss: tensor(0.3404)\n",
      "13768 Training Loss: tensor(0.3405)\n",
      "13769 Training Loss: tensor(0.3423)\n",
      "13770 Training Loss: tensor(0.3402)\n",
      "13771 Training Loss: tensor(0.3439)\n",
      "13772 Training Loss: tensor(0.3414)\n",
      "13773 Training Loss: tensor(0.3424)\n",
      "13774 Training Loss: tensor(0.3434)\n",
      "13775 Training Loss: tensor(0.3401)\n",
      "13776 Training Loss: tensor(0.3395)\n",
      "13777 Training Loss: tensor(0.3412)\n",
      "13778 Training Loss: tensor(0.3405)\n",
      "13779 Training Loss: tensor(0.3410)\n",
      "13780 Training Loss: tensor(0.3396)\n",
      "13781 Training Loss: tensor(0.3408)\n",
      "13782 Training Loss: tensor(0.3450)\n",
      "13783 Training Loss: tensor(0.3412)\n",
      "13784 Training Loss: tensor(0.3393)\n",
      "13785 Training Loss: tensor(0.3477)\n",
      "13786 Training Loss: tensor(0.3497)\n",
      "13787 Training Loss: tensor(0.3420)\n",
      "13788 Training Loss: tensor(0.3412)\n",
      "13789 Training Loss: tensor(0.3424)\n",
      "13790 Training Loss: tensor(0.3441)\n",
      "13791 Training Loss: tensor(0.3416)\n",
      "13792 Training Loss: tensor(0.3414)\n",
      "13793 Training Loss: tensor(0.3407)\n",
      "13794 Training Loss: tensor(0.3434)\n",
      "13795 Training Loss: tensor(0.3408)\n",
      "13796 Training Loss: tensor(0.3418)\n",
      "13797 Training Loss: tensor(0.3443)\n",
      "13798 Training Loss: tensor(0.3427)\n",
      "13799 Training Loss: tensor(0.3429)\n",
      "13800 Training Loss: tensor(0.3486)\n",
      "13801 Training Loss: tensor(0.3416)\n",
      "13802 Training Loss: tensor(0.3430)\n",
      "13803 Training Loss: tensor(0.3412)\n",
      "13804 Training Loss: tensor(0.3407)\n",
      "13805 Training Loss: tensor(0.3426)\n",
      "13806 Training Loss: tensor(0.3425)\n",
      "13807 Training Loss: tensor(0.3421)\n",
      "13808 Training Loss: tensor(0.3409)\n",
      "13809 Training Loss: tensor(0.3432)\n",
      "13810 Training Loss: tensor(0.3425)\n",
      "13811 Training Loss: tensor(0.3421)\n",
      "13812 Training Loss: tensor(0.3410)\n",
      "13813 Training Loss: tensor(0.3408)\n",
      "13814 Training Loss: tensor(0.3468)\n",
      "13815 Training Loss: tensor(0.3425)\n",
      "13816 Training Loss: tensor(0.3416)\n",
      "13817 Training Loss: tensor(0.3405)\n",
      "13818 Training Loss: tensor(0.3401)\n",
      "13819 Training Loss: tensor(0.3410)\n",
      "13820 Training Loss: tensor(0.3401)\n",
      "13821 Training Loss: tensor(0.3426)\n",
      "13822 Training Loss: tensor(0.3409)\n",
      "13823 Training Loss: tensor(0.3455)\n",
      "13824 Training Loss: tensor(0.3414)\n",
      "13825 Training Loss: tensor(0.3415)\n",
      "13826 Training Loss: tensor(0.3427)\n",
      "13827 Training Loss: tensor(0.3420)\n",
      "13828 Training Loss: tensor(0.3409)\n",
      "13829 Training Loss: tensor(0.3428)\n",
      "13830 Training Loss: tensor(0.3431)\n",
      "13831 Training Loss: tensor(0.3427)\n",
      "13832 Training Loss: tensor(0.3426)\n",
      "13833 Training Loss: tensor(0.3408)\n",
      "13834 Training Loss: tensor(0.3397)\n",
      "13835 Training Loss: tensor(0.3410)\n",
      "13836 Training Loss: tensor(0.3438)\n",
      "13837 Training Loss: tensor(0.3414)\n",
      "13838 Training Loss: tensor(0.3471)\n",
      "13839 Training Loss: tensor(0.3433)\n",
      "13840 Training Loss: tensor(0.3429)\n",
      "13841 Training Loss: tensor(0.3407)\n",
      "13842 Training Loss: tensor(0.3405)\n",
      "13843 Training Loss: tensor(0.3409)\n",
      "13844 Training Loss: tensor(0.3401)\n",
      "13845 Training Loss: tensor(0.3430)\n",
      "13846 Training Loss: tensor(0.3420)\n",
      "13847 Training Loss: tensor(0.3416)\n",
      "13848 Training Loss: tensor(0.3398)\n",
      "13849 Training Loss: tensor(0.3436)\n",
      "13850 Training Loss: tensor(0.3403)\n",
      "13851 Training Loss: tensor(0.3446)\n",
      "13852 Training Loss: tensor(0.3493)\n",
      "13853 Training Loss: tensor(0.3415)\n",
      "13854 Training Loss: tensor(0.3397)\n",
      "13855 Training Loss: tensor(0.3489)\n",
      "13856 Training Loss: tensor(0.3409)\n",
      "13857 Training Loss: tensor(0.3445)\n",
      "13858 Training Loss: tensor(0.3430)\n",
      "13859 Training Loss: tensor(0.3400)\n",
      "13860 Training Loss: tensor(0.3407)\n",
      "13861 Training Loss: tensor(0.3408)\n",
      "13862 Training Loss: tensor(0.3412)\n",
      "13863 Training Loss: tensor(0.3400)\n",
      "13864 Training Loss: tensor(0.3412)\n",
      "13865 Training Loss: tensor(0.3426)\n",
      "13866 Training Loss: tensor(0.3436)\n",
      "13867 Training Loss: tensor(0.3414)\n",
      "13868 Training Loss: tensor(0.3418)\n",
      "13869 Training Loss: tensor(0.3399)\n",
      "13870 Training Loss: tensor(0.3394)\n",
      "13871 Training Loss: tensor(0.3405)\n",
      "13872 Training Loss: tensor(0.3408)\n",
      "13873 Training Loss: tensor(0.3452)\n",
      "13874 Training Loss: tensor(0.3508)\n",
      "13875 Training Loss: tensor(0.3456)\n",
      "13876 Training Loss: tensor(0.3394)\n",
      "13877 Training Loss: tensor(0.3413)\n",
      "13878 Training Loss: tensor(0.3402)\n",
      "13879 Training Loss: tensor(0.3404)\n",
      "13880 Training Loss: tensor(0.3466)\n",
      "13881 Training Loss: tensor(0.3413)\n",
      "13882 Training Loss: tensor(0.3406)\n",
      "13883 Training Loss: tensor(0.3408)\n",
      "13884 Training Loss: tensor(0.3408)\n",
      "13885 Training Loss: tensor(0.3412)\n",
      "13886 Training Loss: tensor(0.3405)\n",
      "13887 Training Loss: tensor(0.3410)\n",
      "13888 Training Loss: tensor(0.3416)\n",
      "13889 Training Loss: tensor(0.3403)\n",
      "13890 Training Loss: tensor(0.3429)\n",
      "13891 Training Loss: tensor(0.3407)\n",
      "13892 Training Loss: tensor(0.3420)\n",
      "13893 Training Loss: tensor(0.3410)\n",
      "13894 Training Loss: tensor(0.3531)\n",
      "13895 Training Loss: tensor(0.3431)\n",
      "13896 Training Loss: tensor(0.3419)\n",
      "13897 Training Loss: tensor(0.3411)\n",
      "13898 Training Loss: tensor(0.3423)\n",
      "13899 Training Loss: tensor(0.3425)\n",
      "13900 Training Loss: tensor(0.3396)\n",
      "13901 Training Loss: tensor(0.3413)\n",
      "13902 Training Loss: tensor(0.3486)\n",
      "13903 Training Loss: tensor(0.3427)\n",
      "13904 Training Loss: tensor(0.3423)\n",
      "13905 Training Loss: tensor(0.3434)\n",
      "13906 Training Loss: tensor(0.3410)\n",
      "13907 Training Loss: tensor(0.3426)\n",
      "13908 Training Loss: tensor(0.3440)\n",
      "13909 Training Loss: tensor(0.3544)\n",
      "13910 Training Loss: tensor(0.3432)\n",
      "13911 Training Loss: tensor(0.3418)\n",
      "13912 Training Loss: tensor(0.3422)\n",
      "13913 Training Loss: tensor(0.3441)\n",
      "13914 Training Loss: tensor(0.3444)\n",
      "13915 Training Loss: tensor(0.3422)\n",
      "13916 Training Loss: tensor(0.3423)\n",
      "13917 Training Loss: tensor(0.3498)\n",
      "13918 Training Loss: tensor(0.3411)\n",
      "13919 Training Loss: tensor(0.3429)\n",
      "13920 Training Loss: tensor(0.3440)\n",
      "13921 Training Loss: tensor(0.3427)\n",
      "13922 Training Loss: tensor(0.3436)\n",
      "13923 Training Loss: tensor(0.3429)\n",
      "13924 Training Loss: tensor(0.3419)\n",
      "13925 Training Loss: tensor(0.3405)\n",
      "13926 Training Loss: tensor(0.3427)\n",
      "13927 Training Loss: tensor(0.3417)\n",
      "13928 Training Loss: tensor(0.3430)\n",
      "13929 Training Loss: tensor(0.3409)\n",
      "13930 Training Loss: tensor(0.3474)\n",
      "13931 Training Loss: tensor(0.3418)\n",
      "13932 Training Loss: tensor(0.3401)\n",
      "13933 Training Loss: tensor(0.3402)\n",
      "13934 Training Loss: tensor(0.3402)\n",
      "13935 Training Loss: tensor(0.3404)\n",
      "13936 Training Loss: tensor(0.3405)\n",
      "13937 Training Loss: tensor(0.3402)\n",
      "13938 Training Loss: tensor(0.3445)\n",
      "13939 Training Loss: tensor(0.3411)\n",
      "13940 Training Loss: tensor(0.3394)\n",
      "13941 Training Loss: tensor(0.3404)\n",
      "13942 Training Loss: tensor(0.3409)\n",
      "13943 Training Loss: tensor(0.3402)\n",
      "13944 Training Loss: tensor(0.3416)\n",
      "13945 Training Loss: tensor(0.3418)\n",
      "13946 Training Loss: tensor(0.3426)\n",
      "13947 Training Loss: tensor(0.3397)\n",
      "13948 Training Loss: tensor(0.3404)\n",
      "13949 Training Loss: tensor(0.3420)\n",
      "13950 Training Loss: tensor(0.3427)\n",
      "13951 Training Loss: tensor(0.3401)\n",
      "13952 Training Loss: tensor(0.3407)\n",
      "13953 Training Loss: tensor(0.3448)\n",
      "13954 Training Loss: tensor(0.3418)\n",
      "13955 Training Loss: tensor(0.3418)\n",
      "13956 Training Loss: tensor(0.3400)\n",
      "13957 Training Loss: tensor(0.3396)\n",
      "13958 Training Loss: tensor(0.3423)\n",
      "13959 Training Loss: tensor(0.3397)\n",
      "13960 Training Loss: tensor(0.3443)\n",
      "13961 Training Loss: tensor(0.3443)\n",
      "13962 Training Loss: tensor(0.3453)\n",
      "13963 Training Loss: tensor(0.3412)\n",
      "13964 Training Loss: tensor(0.3419)\n",
      "13965 Training Loss: tensor(0.3414)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13966 Training Loss: tensor(0.3443)\n",
      "13967 Training Loss: tensor(0.3416)\n",
      "13968 Training Loss: tensor(0.3418)\n",
      "13969 Training Loss: tensor(0.3425)\n",
      "13970 Training Loss: tensor(0.3413)\n",
      "13971 Training Loss: tensor(0.3410)\n",
      "13972 Training Loss: tensor(0.3418)\n",
      "13973 Training Loss: tensor(0.3396)\n",
      "13974 Training Loss: tensor(0.3399)\n",
      "13975 Training Loss: tensor(0.3410)\n",
      "13976 Training Loss: tensor(0.3393)\n",
      "13977 Training Loss: tensor(0.3413)\n",
      "13978 Training Loss: tensor(0.3412)\n",
      "13979 Training Loss: tensor(0.3401)\n",
      "13980 Training Loss: tensor(0.3397)\n",
      "13981 Training Loss: tensor(0.3388)\n",
      "13982 Training Loss: tensor(0.3409)\n",
      "13983 Training Loss: tensor(0.3410)\n",
      "13984 Training Loss: tensor(0.3404)\n",
      "13985 Training Loss: tensor(0.3398)\n",
      "13986 Training Loss: tensor(0.3385)\n",
      "13987 Training Loss: tensor(0.3412)\n",
      "13988 Training Loss: tensor(0.3404)\n",
      "13989 Training Loss: tensor(0.3386)\n",
      "13990 Training Loss: tensor(0.3415)\n",
      "13991 Training Loss: tensor(0.3402)\n",
      "13992 Training Loss: tensor(0.3411)\n",
      "13993 Training Loss: tensor(0.3399)\n",
      "13994 Training Loss: tensor(0.3453)\n",
      "13995 Training Loss: tensor(0.3510)\n",
      "13996 Training Loss: tensor(0.3488)\n",
      "13997 Training Loss: tensor(0.3516)\n",
      "13998 Training Loss: tensor(0.3403)\n",
      "13999 Training Loss: tensor(0.3478)\n",
      "14000 Training Loss: tensor(0.3430)\n",
      "14001 Training Loss: tensor(0.3409)\n",
      "14002 Training Loss: tensor(0.3453)\n",
      "14003 Training Loss: tensor(0.3427)\n",
      "14004 Training Loss: tensor(0.3413)\n",
      "14005 Training Loss: tensor(0.3460)\n",
      "14006 Training Loss: tensor(0.3427)\n",
      "14007 Training Loss: tensor(0.3411)\n",
      "14008 Training Loss: tensor(0.3432)\n",
      "14009 Training Loss: tensor(0.3429)\n",
      "14010 Training Loss: tensor(0.3405)\n",
      "14011 Training Loss: tensor(0.3430)\n",
      "14012 Training Loss: tensor(0.3406)\n",
      "14013 Training Loss: tensor(0.3406)\n",
      "14014 Training Loss: tensor(0.3450)\n",
      "14015 Training Loss: tensor(0.3406)\n",
      "14016 Training Loss: tensor(0.3424)\n",
      "14017 Training Loss: tensor(0.3401)\n",
      "14018 Training Loss: tensor(0.3456)\n",
      "14019 Training Loss: tensor(0.3396)\n",
      "14020 Training Loss: tensor(0.3408)\n",
      "14021 Training Loss: tensor(0.3508)\n",
      "14022 Training Loss: tensor(0.3413)\n",
      "14023 Training Loss: tensor(0.3408)\n",
      "14024 Training Loss: tensor(0.3390)\n",
      "14025 Training Loss: tensor(0.3414)\n",
      "14026 Training Loss: tensor(0.3475)\n",
      "14027 Training Loss: tensor(0.3405)\n",
      "14028 Training Loss: tensor(0.3402)\n",
      "14029 Training Loss: tensor(0.3421)\n",
      "14030 Training Loss: tensor(0.3404)\n",
      "14031 Training Loss: tensor(0.3389)\n",
      "14032 Training Loss: tensor(0.3409)\n",
      "14033 Training Loss: tensor(0.3407)\n",
      "14034 Training Loss: tensor(0.3406)\n",
      "14035 Training Loss: tensor(0.3489)\n",
      "14036 Training Loss: tensor(0.3408)\n",
      "14037 Training Loss: tensor(0.3395)\n",
      "14038 Training Loss: tensor(0.3405)\n",
      "14039 Training Loss: tensor(0.3397)\n",
      "14040 Training Loss: tensor(0.3402)\n",
      "14041 Training Loss: tensor(0.3423)\n",
      "14042 Training Loss: tensor(0.3432)\n",
      "14043 Training Loss: tensor(0.3404)\n",
      "14044 Training Loss: tensor(0.3431)\n",
      "14045 Training Loss: tensor(0.3479)\n",
      "14046 Training Loss: tensor(0.3413)\n",
      "14047 Training Loss: tensor(0.3433)\n",
      "14048 Training Loss: tensor(0.3417)\n",
      "14049 Training Loss: tensor(0.3400)\n",
      "14050 Training Loss: tensor(0.3409)\n",
      "14051 Training Loss: tensor(0.3398)\n",
      "14052 Training Loss: tensor(0.3406)\n",
      "14053 Training Loss: tensor(0.3418)\n",
      "14054 Training Loss: tensor(0.3396)\n",
      "14055 Training Loss: tensor(0.3412)\n",
      "14056 Training Loss: tensor(0.3395)\n",
      "14057 Training Loss: tensor(0.3385)\n",
      "14058 Training Loss: tensor(0.3432)\n",
      "14059 Training Loss: tensor(0.3401)\n",
      "14060 Training Loss: tensor(0.3486)\n",
      "14061 Training Loss: tensor(0.3398)\n",
      "14062 Training Loss: tensor(0.3414)\n",
      "14063 Training Loss: tensor(0.3387)\n",
      "14064 Training Loss: tensor(0.3404)\n",
      "14065 Training Loss: tensor(0.3415)\n",
      "14066 Training Loss: tensor(0.3435)\n",
      "14067 Training Loss: tensor(0.3406)\n",
      "14068 Training Loss: tensor(0.3407)\n",
      "14069 Training Loss: tensor(0.3394)\n",
      "14070 Training Loss: tensor(0.3403)\n",
      "14071 Training Loss: tensor(0.3421)\n",
      "14072 Training Loss: tensor(0.3386)\n",
      "14073 Training Loss: tensor(0.3389)\n",
      "14074 Training Loss: tensor(0.3412)\n",
      "14075 Training Loss: tensor(0.3423)\n",
      "14076 Training Loss: tensor(0.3430)\n",
      "14077 Training Loss: tensor(0.3422)\n",
      "14078 Training Loss: tensor(0.3403)\n",
      "14079 Training Loss: tensor(0.3404)\n",
      "14080 Training Loss: tensor(0.3461)\n",
      "14081 Training Loss: tensor(0.3462)\n",
      "14082 Training Loss: tensor(0.3393)\n",
      "14083 Training Loss: tensor(0.3392)\n",
      "14084 Training Loss: tensor(0.3440)\n",
      "14085 Training Loss: tensor(0.3449)\n",
      "14086 Training Loss: tensor(0.3413)\n",
      "14087 Training Loss: tensor(0.3387)\n",
      "14088 Training Loss: tensor(0.3392)\n",
      "14089 Training Loss: tensor(0.3402)\n",
      "14090 Training Loss: tensor(0.3414)\n",
      "14091 Training Loss: tensor(0.3417)\n",
      "14092 Training Loss: tensor(0.3425)\n",
      "14093 Training Loss: tensor(0.3420)\n",
      "14094 Training Loss: tensor(0.3398)\n",
      "14095 Training Loss: tensor(0.3410)\n",
      "14096 Training Loss: tensor(0.3402)\n",
      "14097 Training Loss: tensor(0.3417)\n",
      "14098 Training Loss: tensor(0.3393)\n",
      "14099 Training Loss: tensor(0.3398)\n",
      "14100 Training Loss: tensor(0.3464)\n",
      "14101 Training Loss: tensor(0.3386)\n",
      "14102 Training Loss: tensor(0.3396)\n",
      "14103 Training Loss: tensor(0.3404)\n",
      "14104 Training Loss: tensor(0.3455)\n",
      "14105 Training Loss: tensor(0.3462)\n",
      "14106 Training Loss: tensor(0.3392)\n",
      "14107 Training Loss: tensor(0.3412)\n",
      "14108 Training Loss: tensor(0.3393)\n",
      "14109 Training Loss: tensor(0.3410)\n",
      "14110 Training Loss: tensor(0.3425)\n",
      "14111 Training Loss: tensor(0.3438)\n",
      "14112 Training Loss: tensor(0.3410)\n",
      "14113 Training Loss: tensor(0.3531)\n",
      "14114 Training Loss: tensor(0.3386)\n",
      "14115 Training Loss: tensor(0.3393)\n",
      "14116 Training Loss: tensor(0.3427)\n",
      "14117 Training Loss: tensor(0.3412)\n",
      "14118 Training Loss: tensor(0.3407)\n",
      "14119 Training Loss: tensor(0.3407)\n",
      "14120 Training Loss: tensor(0.3411)\n",
      "14121 Training Loss: tensor(0.3421)\n",
      "14122 Training Loss: tensor(0.3428)\n",
      "14123 Training Loss: tensor(0.3398)\n",
      "14124 Training Loss: tensor(0.3426)\n",
      "14125 Training Loss: tensor(0.3414)\n",
      "14126 Training Loss: tensor(0.3403)\n",
      "14127 Training Loss: tensor(0.3418)\n",
      "14128 Training Loss: tensor(0.3419)\n",
      "14129 Training Loss: tensor(0.3448)\n",
      "14130 Training Loss: tensor(0.3408)\n",
      "14131 Training Loss: tensor(0.3404)\n",
      "14132 Training Loss: tensor(0.3406)\n",
      "14133 Training Loss: tensor(0.3408)\n",
      "14134 Training Loss: tensor(0.3401)\n",
      "14135 Training Loss: tensor(0.3406)\n",
      "14136 Training Loss: tensor(0.3406)\n",
      "14137 Training Loss: tensor(0.3396)\n",
      "14138 Training Loss: tensor(0.3439)\n",
      "14139 Training Loss: tensor(0.3392)\n",
      "14140 Training Loss: tensor(0.3402)\n",
      "14141 Training Loss: tensor(0.3407)\n",
      "14142 Training Loss: tensor(0.3453)\n",
      "14143 Training Loss: tensor(0.3392)\n",
      "14144 Training Loss: tensor(0.3496)\n",
      "14145 Training Loss: tensor(0.3405)\n",
      "14146 Training Loss: tensor(0.3425)\n",
      "14147 Training Loss: tensor(0.3408)\n",
      "14148 Training Loss: tensor(0.3400)\n",
      "14149 Training Loss: tensor(0.3415)\n",
      "14150 Training Loss: tensor(0.3394)\n",
      "14151 Training Loss: tensor(0.3401)\n",
      "14152 Training Loss: tensor(0.3396)\n",
      "14153 Training Loss: tensor(0.3416)\n",
      "14154 Training Loss: tensor(0.3389)\n",
      "14155 Training Loss: tensor(0.3437)\n",
      "14156 Training Loss: tensor(0.3398)\n",
      "14157 Training Loss: tensor(0.3411)\n",
      "14158 Training Loss: tensor(0.3400)\n",
      "14159 Training Loss: tensor(0.3409)\n",
      "14160 Training Loss: tensor(0.3425)\n",
      "14161 Training Loss: tensor(0.3386)\n",
      "14162 Training Loss: tensor(0.3400)\n",
      "14163 Training Loss: tensor(0.3390)\n",
      "14164 Training Loss: tensor(0.3416)\n",
      "14165 Training Loss: tensor(0.3402)\n",
      "14166 Training Loss: tensor(0.3409)\n",
      "14167 Training Loss: tensor(0.3404)\n",
      "14168 Training Loss: tensor(0.3389)\n",
      "14169 Training Loss: tensor(0.3402)\n",
      "14170 Training Loss: tensor(0.3441)\n",
      "14171 Training Loss: tensor(0.3400)\n",
      "14172 Training Loss: tensor(0.3426)\n",
      "14173 Training Loss: tensor(0.3458)\n",
      "14174 Training Loss: tensor(0.3384)\n",
      "14175 Training Loss: tensor(0.3436)\n",
      "14176 Training Loss: tensor(0.3405)\n",
      "14177 Training Loss: tensor(0.3409)\n",
      "14178 Training Loss: tensor(0.3406)\n",
      "14179 Training Loss: tensor(0.3403)\n",
      "14180 Training Loss: tensor(0.3396)\n",
      "14181 Training Loss: tensor(0.3408)\n",
      "14182 Training Loss: tensor(0.3432)\n",
      "14183 Training Loss: tensor(0.3395)\n",
      "14184 Training Loss: tensor(0.3385)\n",
      "14185 Training Loss: tensor(0.3395)\n",
      "14186 Training Loss: tensor(0.3405)\n",
      "14187 Training Loss: tensor(0.3413)\n",
      "14188 Training Loss: tensor(0.3435)\n",
      "14189 Training Loss: tensor(0.3383)\n",
      "14190 Training Loss: tensor(0.3394)\n",
      "14191 Training Loss: tensor(0.3440)\n",
      "14192 Training Loss: tensor(0.3387)\n",
      "14193 Training Loss: tensor(0.3413)\n",
      "14194 Training Loss: tensor(0.3472)\n",
      "14195 Training Loss: tensor(0.3422)\n",
      "14196 Training Loss: tensor(0.3435)\n",
      "14197 Training Loss: tensor(0.3402)\n",
      "14198 Training Loss: tensor(0.3421)\n",
      "14199 Training Loss: tensor(0.3416)\n",
      "14200 Training Loss: tensor(0.3459)\n",
      "14201 Training Loss: tensor(0.3427)\n",
      "14202 Training Loss: tensor(0.3415)\n",
      "14203 Training Loss: tensor(0.3402)\n",
      "14204 Training Loss: tensor(0.3406)\n",
      "14205 Training Loss: tensor(0.3402)\n",
      "14206 Training Loss: tensor(0.3407)\n",
      "14207 Training Loss: tensor(0.3388)\n",
      "14208 Training Loss: tensor(0.3456)\n",
      "14209 Training Loss: tensor(0.3404)\n",
      "14210 Training Loss: tensor(0.3386)\n",
      "14211 Training Loss: tensor(0.3430)\n",
      "14212 Training Loss: tensor(0.3390)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14213 Training Loss: tensor(0.3472)\n",
      "14214 Training Loss: tensor(0.3397)\n",
      "14215 Training Loss: tensor(0.3454)\n",
      "14216 Training Loss: tensor(0.3387)\n",
      "14217 Training Loss: tensor(0.3520)\n",
      "14218 Training Loss: tensor(0.3407)\n",
      "14219 Training Loss: tensor(0.3398)\n",
      "14220 Training Loss: tensor(0.3458)\n",
      "14221 Training Loss: tensor(0.3409)\n",
      "14222 Training Loss: tensor(0.3403)\n",
      "14223 Training Loss: tensor(0.3409)\n",
      "14224 Training Loss: tensor(0.3402)\n",
      "14225 Training Loss: tensor(0.3416)\n",
      "14226 Training Loss: tensor(0.3410)\n",
      "14227 Training Loss: tensor(0.3403)\n",
      "14228 Training Loss: tensor(0.3412)\n",
      "14229 Training Loss: tensor(0.3420)\n",
      "14230 Training Loss: tensor(0.3400)\n",
      "14231 Training Loss: tensor(0.3397)\n",
      "14232 Training Loss: tensor(0.3413)\n",
      "14233 Training Loss: tensor(0.3418)\n",
      "14234 Training Loss: tensor(0.3404)\n",
      "14235 Training Loss: tensor(0.3412)\n",
      "14236 Training Loss: tensor(0.3473)\n",
      "14237 Training Loss: tensor(0.3423)\n",
      "14238 Training Loss: tensor(0.3440)\n",
      "14239 Training Loss: tensor(0.3422)\n",
      "14240 Training Loss: tensor(0.3448)\n",
      "14241 Training Loss: tensor(0.3426)\n",
      "14242 Training Loss: tensor(0.3450)\n",
      "14243 Training Loss: tensor(0.3419)\n",
      "14244 Training Loss: tensor(0.3431)\n",
      "14245 Training Loss: tensor(0.3429)\n",
      "14246 Training Loss: tensor(0.3422)\n",
      "14247 Training Loss: tensor(0.3449)\n",
      "14248 Training Loss: tensor(0.3393)\n",
      "14249 Training Loss: tensor(0.3406)\n",
      "14250 Training Loss: tensor(0.3426)\n",
      "14251 Training Loss: tensor(0.3424)\n",
      "14252 Training Loss: tensor(0.3410)\n",
      "14253 Training Loss: tensor(0.3392)\n",
      "14254 Training Loss: tensor(0.3419)\n",
      "14255 Training Loss: tensor(0.3431)\n",
      "14256 Training Loss: tensor(0.3401)\n",
      "14257 Training Loss: tensor(0.3409)\n",
      "14258 Training Loss: tensor(0.3397)\n",
      "14259 Training Loss: tensor(0.3391)\n",
      "14260 Training Loss: tensor(0.3433)\n",
      "14261 Training Loss: tensor(0.3398)\n",
      "14262 Training Loss: tensor(0.3400)\n",
      "14263 Training Loss: tensor(0.3386)\n",
      "14264 Training Loss: tensor(0.3427)\n",
      "14265 Training Loss: tensor(0.3433)\n",
      "14266 Training Loss: tensor(0.3407)\n",
      "14267 Training Loss: tensor(0.3404)\n",
      "14268 Training Loss: tensor(0.3457)\n",
      "14269 Training Loss: tensor(0.3407)\n",
      "14270 Training Loss: tensor(0.3416)\n",
      "14271 Training Loss: tensor(0.3402)\n",
      "14272 Training Loss: tensor(0.3401)\n",
      "14273 Training Loss: tensor(0.3401)\n",
      "14274 Training Loss: tensor(0.3422)\n",
      "14275 Training Loss: tensor(0.3396)\n",
      "14276 Training Loss: tensor(0.3413)\n",
      "14277 Training Loss: tensor(0.3402)\n",
      "14278 Training Loss: tensor(0.3440)\n",
      "14279 Training Loss: tensor(0.3468)\n",
      "14280 Training Loss: tensor(0.3406)\n",
      "14281 Training Loss: tensor(0.3412)\n",
      "14282 Training Loss: tensor(0.3417)\n",
      "14283 Training Loss: tensor(0.3398)\n",
      "14284 Training Loss: tensor(0.3399)\n",
      "14285 Training Loss: tensor(0.3416)\n",
      "14286 Training Loss: tensor(0.3407)\n",
      "14287 Training Loss: tensor(0.3440)\n",
      "14288 Training Loss: tensor(0.3422)\n",
      "14289 Training Loss: tensor(0.3424)\n",
      "14290 Training Loss: tensor(0.3413)\n",
      "14291 Training Loss: tensor(0.3429)\n",
      "14292 Training Loss: tensor(0.3398)\n",
      "14293 Training Loss: tensor(0.3400)\n",
      "14294 Training Loss: tensor(0.3390)\n",
      "14295 Training Loss: tensor(0.3409)\n",
      "14296 Training Loss: tensor(0.3419)\n",
      "14297 Training Loss: tensor(0.3406)\n",
      "14298 Training Loss: tensor(0.3400)\n",
      "14299 Training Loss: tensor(0.3402)\n",
      "14300 Training Loss: tensor(0.3385)\n",
      "14301 Training Loss: tensor(0.3402)\n",
      "14302 Training Loss: tensor(0.3455)\n",
      "14303 Training Loss: tensor(0.3409)\n",
      "14304 Training Loss: tensor(0.3408)\n",
      "14305 Training Loss: tensor(0.3483)\n",
      "14306 Training Loss: tensor(0.3400)\n",
      "14307 Training Loss: tensor(0.3411)\n",
      "14308 Training Loss: tensor(0.3392)\n",
      "14309 Training Loss: tensor(0.3387)\n",
      "14310 Training Loss: tensor(0.3416)\n",
      "14311 Training Loss: tensor(0.3449)\n",
      "14312 Training Loss: tensor(0.3409)\n",
      "14313 Training Loss: tensor(0.3406)\n",
      "14314 Training Loss: tensor(0.3411)\n",
      "14315 Training Loss: tensor(0.3401)\n",
      "14316 Training Loss: tensor(0.3407)\n",
      "14317 Training Loss: tensor(0.3390)\n",
      "14318 Training Loss: tensor(0.3416)\n",
      "14319 Training Loss: tensor(0.3416)\n",
      "14320 Training Loss: tensor(0.3403)\n",
      "14321 Training Loss: tensor(0.3437)\n",
      "14322 Training Loss: tensor(0.3516)\n",
      "14323 Training Loss: tensor(0.3392)\n",
      "14324 Training Loss: tensor(0.3438)\n",
      "14325 Training Loss: tensor(0.3409)\n",
      "14326 Training Loss: tensor(0.3423)\n",
      "14327 Training Loss: tensor(0.3415)\n",
      "14328 Training Loss: tensor(0.3403)\n",
      "14329 Training Loss: tensor(0.3428)\n",
      "14330 Training Loss: tensor(0.3401)\n",
      "14331 Training Loss: tensor(0.3402)\n",
      "14332 Training Loss: tensor(0.3406)\n",
      "14333 Training Loss: tensor(0.3420)\n",
      "14334 Training Loss: tensor(0.3404)\n",
      "14335 Training Loss: tensor(0.3406)\n",
      "14336 Training Loss: tensor(0.3440)\n",
      "14337 Training Loss: tensor(0.3403)\n",
      "14338 Training Loss: tensor(0.3389)\n",
      "14339 Training Loss: tensor(0.3434)\n",
      "14340 Training Loss: tensor(0.3417)\n",
      "14341 Training Loss: tensor(0.3400)\n",
      "14342 Training Loss: tensor(0.3391)\n",
      "14343 Training Loss: tensor(0.3393)\n",
      "14344 Training Loss: tensor(0.3445)\n",
      "14345 Training Loss: tensor(0.3404)\n",
      "14346 Training Loss: tensor(0.3418)\n",
      "14347 Training Loss: tensor(0.3405)\n",
      "14348 Training Loss: tensor(0.3383)\n",
      "14349 Training Loss: tensor(0.3398)\n",
      "14350 Training Loss: tensor(0.3398)\n",
      "14351 Training Loss: tensor(0.3392)\n",
      "14352 Training Loss: tensor(0.3457)\n",
      "14353 Training Loss: tensor(0.3393)\n",
      "14354 Training Loss: tensor(0.3392)\n",
      "14355 Training Loss: tensor(0.3412)\n",
      "14356 Training Loss: tensor(0.3411)\n",
      "14357 Training Loss: tensor(0.3399)\n",
      "14358 Training Loss: tensor(0.3397)\n",
      "14359 Training Loss: tensor(0.3396)\n",
      "14360 Training Loss: tensor(0.3397)\n",
      "14361 Training Loss: tensor(0.3394)\n",
      "14362 Training Loss: tensor(0.3384)\n",
      "14363 Training Loss: tensor(0.3427)\n",
      "14364 Training Loss: tensor(0.3391)\n",
      "14365 Training Loss: tensor(0.3388)\n",
      "14366 Training Loss: tensor(0.3395)\n",
      "14367 Training Loss: tensor(0.3429)\n",
      "14368 Training Loss: tensor(0.3387)\n",
      "14369 Training Loss: tensor(0.3394)\n",
      "14370 Training Loss: tensor(0.3390)\n",
      "14371 Training Loss: tensor(0.3388)\n",
      "14372 Training Loss: tensor(0.3393)\n",
      "14373 Training Loss: tensor(0.3476)\n",
      "14374 Training Loss: tensor(0.3399)\n",
      "14375 Training Loss: tensor(0.3389)\n",
      "14376 Training Loss: tensor(0.3378)\n",
      "14377 Training Loss: tensor(0.3390)\n",
      "14378 Training Loss: tensor(0.3398)\n",
      "14379 Training Loss: tensor(0.3444)\n",
      "14380 Training Loss: tensor(0.3379)\n",
      "14381 Training Loss: tensor(0.3394)\n",
      "14382 Training Loss: tensor(0.3425)\n",
      "14383 Training Loss: tensor(0.3382)\n",
      "14384 Training Loss: tensor(0.3402)\n",
      "14385 Training Loss: tensor(0.3479)\n",
      "14386 Training Loss: tensor(0.3407)\n",
      "14387 Training Loss: tensor(0.3382)\n",
      "14388 Training Loss: tensor(0.3395)\n",
      "14389 Training Loss: tensor(0.3393)\n",
      "14390 Training Loss: tensor(0.3412)\n",
      "14391 Training Loss: tensor(0.3432)\n",
      "14392 Training Loss: tensor(0.3421)\n",
      "14393 Training Loss: tensor(0.3396)\n",
      "14394 Training Loss: tensor(0.3434)\n",
      "14395 Training Loss: tensor(0.3436)\n",
      "14396 Training Loss: tensor(0.3400)\n",
      "14397 Training Loss: tensor(0.3405)\n",
      "14398 Training Loss: tensor(0.3408)\n",
      "14399 Training Loss: tensor(0.3395)\n",
      "14400 Training Loss: tensor(0.3433)\n",
      "14401 Training Loss: tensor(0.3394)\n",
      "14402 Training Loss: tensor(0.3405)\n",
      "14403 Training Loss: tensor(0.3435)\n",
      "14404 Training Loss: tensor(0.3471)\n",
      "14405 Training Loss: tensor(0.3417)\n",
      "14406 Training Loss: tensor(0.3410)\n",
      "14407 Training Loss: tensor(0.3421)\n",
      "14408 Training Loss: tensor(0.3385)\n",
      "14409 Training Loss: tensor(0.3419)\n",
      "14410 Training Loss: tensor(0.3422)\n",
      "14411 Training Loss: tensor(0.3386)\n",
      "14412 Training Loss: tensor(0.3459)\n",
      "14413 Training Loss: tensor(0.3378)\n",
      "14414 Training Loss: tensor(0.3405)\n",
      "14415 Training Loss: tensor(0.3397)\n",
      "14416 Training Loss: tensor(0.3390)\n",
      "14417 Training Loss: tensor(0.3390)\n",
      "14418 Training Loss: tensor(0.3412)\n",
      "14419 Training Loss: tensor(0.3402)\n",
      "14420 Training Loss: tensor(0.3388)\n",
      "14421 Training Loss: tensor(0.3392)\n",
      "14422 Training Loss: tensor(0.3420)\n",
      "14423 Training Loss: tensor(0.3407)\n",
      "14424 Training Loss: tensor(0.3386)\n",
      "14425 Training Loss: tensor(0.3403)\n",
      "14426 Training Loss: tensor(0.3415)\n",
      "14427 Training Loss: tensor(0.3397)\n",
      "14428 Training Loss: tensor(0.3441)\n",
      "14429 Training Loss: tensor(0.3406)\n",
      "14430 Training Loss: tensor(0.3401)\n",
      "14431 Training Loss: tensor(0.3437)\n",
      "14432 Training Loss: tensor(0.3412)\n",
      "14433 Training Loss: tensor(0.3390)\n",
      "14434 Training Loss: tensor(0.3403)\n",
      "14435 Training Loss: tensor(0.3422)\n",
      "14436 Training Loss: tensor(0.3415)\n",
      "14437 Training Loss: tensor(0.3411)\n",
      "14438 Training Loss: tensor(0.3402)\n",
      "14439 Training Loss: tensor(0.3402)\n",
      "14440 Training Loss: tensor(0.3399)\n",
      "14441 Training Loss: tensor(0.3385)\n",
      "14442 Training Loss: tensor(0.3383)\n",
      "14443 Training Loss: tensor(0.3391)\n",
      "14444 Training Loss: tensor(0.3434)\n",
      "14445 Training Loss: tensor(0.3417)\n",
      "14446 Training Loss: tensor(0.3402)\n",
      "14447 Training Loss: tensor(0.3418)\n",
      "14448 Training Loss: tensor(0.3375)\n",
      "14449 Training Loss: tensor(0.3404)\n",
      "14450 Training Loss: tensor(0.3410)\n",
      "14451 Training Loss: tensor(0.3394)\n",
      "14452 Training Loss: tensor(0.3396)\n",
      "14453 Training Loss: tensor(0.3378)\n",
      "14454 Training Loss: tensor(0.3395)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14455 Training Loss: tensor(0.3407)\n",
      "14456 Training Loss: tensor(0.3430)\n",
      "14457 Training Loss: tensor(0.3426)\n",
      "14458 Training Loss: tensor(0.3397)\n",
      "14459 Training Loss: tensor(0.3421)\n",
      "14460 Training Loss: tensor(0.3401)\n",
      "14461 Training Loss: tensor(0.3428)\n",
      "14462 Training Loss: tensor(0.3394)\n",
      "14463 Training Loss: tensor(0.3410)\n",
      "14464 Training Loss: tensor(0.3382)\n",
      "14465 Training Loss: tensor(0.3406)\n",
      "14466 Training Loss: tensor(0.3381)\n",
      "14467 Training Loss: tensor(0.3433)\n",
      "14468 Training Loss: tensor(0.3391)\n",
      "14469 Training Loss: tensor(0.3388)\n",
      "14470 Training Loss: tensor(0.3439)\n",
      "14471 Training Loss: tensor(0.3381)\n",
      "14472 Training Loss: tensor(0.3403)\n",
      "14473 Training Loss: tensor(0.3390)\n",
      "14474 Training Loss: tensor(0.3423)\n",
      "14475 Training Loss: tensor(0.3416)\n",
      "14476 Training Loss: tensor(0.3392)\n",
      "14477 Training Loss: tensor(0.3393)\n",
      "14478 Training Loss: tensor(0.3388)\n",
      "14479 Training Loss: tensor(0.3392)\n",
      "14480 Training Loss: tensor(0.3420)\n",
      "14481 Training Loss: tensor(0.3426)\n",
      "14482 Training Loss: tensor(0.3413)\n",
      "14483 Training Loss: tensor(0.3395)\n",
      "14484 Training Loss: tensor(0.3431)\n",
      "14485 Training Loss: tensor(0.3409)\n",
      "14486 Training Loss: tensor(0.3394)\n",
      "14487 Training Loss: tensor(0.3387)\n",
      "14488 Training Loss: tensor(0.3405)\n",
      "14489 Training Loss: tensor(0.3386)\n",
      "14490 Training Loss: tensor(0.3387)\n",
      "14491 Training Loss: tensor(0.3418)\n",
      "14492 Training Loss: tensor(0.3408)\n",
      "14493 Training Loss: tensor(0.3386)\n",
      "14494 Training Loss: tensor(0.3383)\n",
      "14495 Training Loss: tensor(0.3376)\n",
      "14496 Training Loss: tensor(0.3393)\n",
      "14497 Training Loss: tensor(0.3381)\n",
      "14498 Training Loss: tensor(0.3417)\n",
      "14499 Training Loss: tensor(0.3515)\n",
      "14500 Training Loss: tensor(0.3380)\n",
      "14501 Training Loss: tensor(0.3454)\n",
      "14502 Training Loss: tensor(0.3412)\n",
      "14503 Training Loss: tensor(0.3404)\n",
      "14504 Training Loss: tensor(0.3391)\n",
      "14505 Training Loss: tensor(0.3385)\n",
      "14506 Training Loss: tensor(0.3418)\n",
      "14507 Training Loss: tensor(0.3391)\n",
      "14508 Training Loss: tensor(0.3413)\n",
      "14509 Training Loss: tensor(0.3398)\n",
      "14510 Training Loss: tensor(0.3388)\n",
      "14511 Training Loss: tensor(0.3431)\n",
      "14512 Training Loss: tensor(0.3372)\n",
      "14513 Training Loss: tensor(0.3381)\n",
      "14514 Training Loss: tensor(0.3377)\n",
      "14515 Training Loss: tensor(0.3385)\n",
      "14516 Training Loss: tensor(0.3389)\n",
      "14517 Training Loss: tensor(0.3442)\n",
      "14518 Training Loss: tensor(0.3387)\n",
      "14519 Training Loss: tensor(0.3387)\n",
      "14520 Training Loss: tensor(0.3403)\n",
      "14521 Training Loss: tensor(0.3437)\n",
      "14522 Training Loss: tensor(0.3416)\n",
      "14523 Training Loss: tensor(0.3399)\n",
      "14524 Training Loss: tensor(0.3387)\n",
      "14525 Training Loss: tensor(0.3381)\n",
      "14526 Training Loss: tensor(0.3437)\n",
      "14527 Training Loss: tensor(0.3374)\n",
      "14528 Training Loss: tensor(0.3474)\n",
      "14529 Training Loss: tensor(0.3393)\n",
      "14530 Training Loss: tensor(0.3408)\n",
      "14531 Training Loss: tensor(0.3400)\n",
      "14532 Training Loss: tensor(0.3408)\n",
      "14533 Training Loss: tensor(0.3397)\n",
      "14534 Training Loss: tensor(0.3415)\n",
      "14535 Training Loss: tensor(0.3389)\n",
      "14536 Training Loss: tensor(0.3425)\n",
      "14537 Training Loss: tensor(0.3415)\n",
      "14538 Training Loss: tensor(0.3406)\n",
      "14539 Training Loss: tensor(0.3403)\n",
      "14540 Training Loss: tensor(0.3430)\n",
      "14541 Training Loss: tensor(0.3405)\n",
      "14542 Training Loss: tensor(0.3394)\n",
      "14543 Training Loss: tensor(0.3379)\n",
      "14544 Training Loss: tensor(0.3452)\n",
      "14545 Training Loss: tensor(0.3390)\n",
      "14546 Training Loss: tensor(0.3382)\n",
      "14547 Training Loss: tensor(0.3413)\n",
      "14548 Training Loss: tensor(0.3414)\n",
      "14549 Training Loss: tensor(0.3379)\n",
      "14550 Training Loss: tensor(0.3373)\n",
      "14551 Training Loss: tensor(0.3390)\n",
      "14552 Training Loss: tensor(0.3422)\n",
      "14553 Training Loss: tensor(0.3416)\n",
      "14554 Training Loss: tensor(0.3436)\n",
      "14555 Training Loss: tensor(0.3394)\n",
      "14556 Training Loss: tensor(0.3402)\n",
      "14557 Training Loss: tensor(0.3386)\n",
      "14558 Training Loss: tensor(0.3381)\n",
      "14559 Training Loss: tensor(0.3396)\n",
      "14560 Training Loss: tensor(0.3388)\n",
      "14561 Training Loss: tensor(0.3440)\n",
      "14562 Training Loss: tensor(0.3444)\n",
      "14563 Training Loss: tensor(0.3396)\n",
      "14564 Training Loss: tensor(0.3409)\n",
      "14565 Training Loss: tensor(0.3447)\n",
      "14566 Training Loss: tensor(0.3435)\n",
      "14567 Training Loss: tensor(0.3404)\n",
      "14568 Training Loss: tensor(0.3399)\n",
      "14569 Training Loss: tensor(0.3411)\n",
      "14570 Training Loss: tensor(0.3382)\n",
      "14571 Training Loss: tensor(0.3412)\n",
      "14572 Training Loss: tensor(0.3406)\n",
      "14573 Training Loss: tensor(0.3390)\n",
      "14574 Training Loss: tensor(0.3409)\n",
      "14575 Training Loss: tensor(0.3388)\n",
      "14576 Training Loss: tensor(0.3385)\n",
      "14577 Training Loss: tensor(0.3397)\n",
      "14578 Training Loss: tensor(0.3404)\n",
      "14579 Training Loss: tensor(0.3387)\n",
      "14580 Training Loss: tensor(0.3399)\n",
      "14581 Training Loss: tensor(0.3429)\n",
      "14582 Training Loss: tensor(0.3390)\n",
      "14583 Training Loss: tensor(0.3431)\n",
      "14584 Training Loss: tensor(0.3396)\n",
      "14585 Training Loss: tensor(0.3492)\n",
      "14586 Training Loss: tensor(0.3394)\n",
      "14587 Training Loss: tensor(0.3417)\n",
      "14588 Training Loss: tensor(0.3403)\n",
      "14589 Training Loss: tensor(0.3414)\n",
      "14590 Training Loss: tensor(0.3399)\n",
      "14591 Training Loss: tensor(0.3393)\n",
      "14592 Training Loss: tensor(0.3399)\n",
      "14593 Training Loss: tensor(0.3424)\n",
      "14594 Training Loss: tensor(0.3417)\n",
      "14595 Training Loss: tensor(0.3407)\n",
      "14596 Training Loss: tensor(0.3383)\n",
      "14597 Training Loss: tensor(0.3398)\n",
      "14598 Training Loss: tensor(0.3393)\n",
      "14599 Training Loss: tensor(0.3381)\n",
      "14600 Training Loss: tensor(0.3380)\n",
      "14601 Training Loss: tensor(0.3391)\n",
      "14602 Training Loss: tensor(0.3399)\n",
      "14603 Training Loss: tensor(0.3418)\n",
      "14604 Training Loss: tensor(0.3457)\n",
      "14605 Training Loss: tensor(0.3391)\n",
      "14606 Training Loss: tensor(0.3381)\n",
      "14607 Training Loss: tensor(0.3371)\n",
      "14608 Training Loss: tensor(0.3383)\n",
      "14609 Training Loss: tensor(0.3401)\n",
      "14610 Training Loss: tensor(0.3383)\n",
      "14611 Training Loss: tensor(0.3394)\n",
      "14612 Training Loss: tensor(0.3379)\n",
      "14613 Training Loss: tensor(0.3452)\n",
      "14614 Training Loss: tensor(0.3383)\n",
      "14615 Training Loss: tensor(0.3389)\n",
      "14616 Training Loss: tensor(0.3385)\n",
      "14617 Training Loss: tensor(0.3459)\n",
      "14618 Training Loss: tensor(0.3390)\n",
      "14619 Training Loss: tensor(0.3411)\n",
      "14620 Training Loss: tensor(0.3388)\n",
      "14621 Training Loss: tensor(0.3381)\n",
      "14622 Training Loss: tensor(0.3392)\n",
      "14623 Training Loss: tensor(0.3401)\n",
      "14624 Training Loss: tensor(0.3397)\n",
      "14625 Training Loss: tensor(0.3391)\n",
      "14626 Training Loss: tensor(0.3393)\n",
      "14627 Training Loss: tensor(0.3406)\n",
      "14628 Training Loss: tensor(0.3391)\n",
      "14629 Training Loss: tensor(0.3391)\n",
      "14630 Training Loss: tensor(0.3384)\n",
      "14631 Training Loss: tensor(0.3391)\n",
      "14632 Training Loss: tensor(0.3402)\n",
      "14633 Training Loss: tensor(0.3408)\n",
      "14634 Training Loss: tensor(0.3420)\n",
      "14635 Training Loss: tensor(0.3424)\n",
      "14636 Training Loss: tensor(0.3376)\n",
      "14637 Training Loss: tensor(0.3409)\n",
      "14638 Training Loss: tensor(0.3403)\n",
      "14639 Training Loss: tensor(0.3404)\n",
      "14640 Training Loss: tensor(0.3459)\n",
      "14641 Training Loss: tensor(0.3410)\n",
      "14642 Training Loss: tensor(0.3387)\n",
      "14643 Training Loss: tensor(0.3407)\n",
      "14644 Training Loss: tensor(0.3382)\n",
      "14645 Training Loss: tensor(0.3392)\n",
      "14646 Training Loss: tensor(0.3407)\n",
      "14647 Training Loss: tensor(0.3388)\n",
      "14648 Training Loss: tensor(0.3396)\n",
      "14649 Training Loss: tensor(0.3419)\n",
      "14650 Training Loss: tensor(0.3382)\n",
      "14651 Training Loss: tensor(0.3379)\n",
      "14652 Training Loss: tensor(0.3390)\n",
      "14653 Training Loss: tensor(0.3395)\n",
      "14654 Training Loss: tensor(0.3379)\n",
      "14655 Training Loss: tensor(0.3387)\n",
      "14656 Training Loss: tensor(0.3431)\n",
      "14657 Training Loss: tensor(0.3402)\n",
      "14658 Training Loss: tensor(0.3386)\n",
      "14659 Training Loss: tensor(0.3391)\n",
      "14660 Training Loss: tensor(0.3421)\n",
      "14661 Training Loss: tensor(0.3382)\n",
      "14662 Training Loss: tensor(0.3376)\n",
      "14663 Training Loss: tensor(0.3380)\n",
      "14664 Training Loss: tensor(0.3378)\n",
      "14665 Training Loss: tensor(0.3412)\n",
      "14666 Training Loss: tensor(0.3395)\n",
      "14667 Training Loss: tensor(0.3430)\n",
      "14668 Training Loss: tensor(0.3475)\n",
      "14669 Training Loss: tensor(0.3459)\n",
      "14670 Training Loss: tensor(0.3392)\n",
      "14671 Training Loss: tensor(0.3423)\n",
      "14672 Training Loss: tensor(0.3395)\n",
      "14673 Training Loss: tensor(0.3385)\n",
      "14674 Training Loss: tensor(0.3418)\n",
      "14675 Training Loss: tensor(0.3418)\n",
      "14676 Training Loss: tensor(0.3426)\n",
      "14677 Training Loss: tensor(0.3387)\n",
      "14678 Training Loss: tensor(0.3422)\n",
      "14679 Training Loss: tensor(0.3397)\n",
      "14680 Training Loss: tensor(0.3400)\n",
      "14681 Training Loss: tensor(0.3405)\n",
      "14682 Training Loss: tensor(0.3412)\n",
      "14683 Training Loss: tensor(0.3391)\n",
      "14684 Training Loss: tensor(0.3412)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14685 Training Loss: tensor(0.3403)\n",
      "14686 Training Loss: tensor(0.3383)\n",
      "14687 Training Loss: tensor(0.3388)\n",
      "14688 Training Loss: tensor(0.3424)\n",
      "14689 Training Loss: tensor(0.3437)\n",
      "14690 Training Loss: tensor(0.3391)\n",
      "14691 Training Loss: tensor(0.3382)\n",
      "14692 Training Loss: tensor(0.3426)\n",
      "14693 Training Loss: tensor(0.3392)\n",
      "14694 Training Loss: tensor(0.3387)\n",
      "14695 Training Loss: tensor(0.3403)\n",
      "14696 Training Loss: tensor(0.3400)\n",
      "14697 Training Loss: tensor(0.3384)\n",
      "14698 Training Loss: tensor(0.3408)\n",
      "14699 Training Loss: tensor(0.3386)\n",
      "14700 Training Loss: tensor(0.3388)\n",
      "14701 Training Loss: tensor(0.3398)\n",
      "14702 Training Loss: tensor(0.3391)\n",
      "14703 Training Loss: tensor(0.3375)\n",
      "14704 Training Loss: tensor(0.3373)\n",
      "14705 Training Loss: tensor(0.3393)\n",
      "14706 Training Loss: tensor(0.3411)\n",
      "14707 Training Loss: tensor(0.3390)\n",
      "14708 Training Loss: tensor(0.3372)\n",
      "14709 Training Loss: tensor(0.3380)\n",
      "14710 Training Loss: tensor(0.3390)\n",
      "14711 Training Loss: tensor(0.3387)\n",
      "14712 Training Loss: tensor(0.3372)\n",
      "14713 Training Loss: tensor(0.3368)\n",
      "14714 Training Loss: tensor(0.3385)\n",
      "14715 Training Loss: tensor(0.3403)\n",
      "14716 Training Loss: tensor(0.3412)\n",
      "14717 Training Loss: tensor(0.3378)\n",
      "14718 Training Loss: tensor(0.3376)\n",
      "14719 Training Loss: tensor(0.3397)\n",
      "14720 Training Loss: tensor(0.3433)\n",
      "14721 Training Loss: tensor(0.3395)\n",
      "14722 Training Loss: tensor(0.3370)\n",
      "14723 Training Loss: tensor(0.3400)\n",
      "14724 Training Loss: tensor(0.3398)\n",
      "14725 Training Loss: tensor(0.3377)\n",
      "14726 Training Loss: tensor(0.3405)\n",
      "14727 Training Loss: tensor(0.3478)\n",
      "14728 Training Loss: tensor(0.3388)\n",
      "14729 Training Loss: tensor(0.3413)\n",
      "14730 Training Loss: tensor(0.3405)\n",
      "14731 Training Loss: tensor(0.3379)\n",
      "14732 Training Loss: tensor(0.3399)\n",
      "14733 Training Loss: tensor(0.3387)\n",
      "14734 Training Loss: tensor(0.3397)\n",
      "14735 Training Loss: tensor(0.3400)\n",
      "14736 Training Loss: tensor(0.3373)\n",
      "14737 Training Loss: tensor(0.3401)\n",
      "14738 Training Loss: tensor(0.3373)\n",
      "14739 Training Loss: tensor(0.3388)\n",
      "14740 Training Loss: tensor(0.3401)\n",
      "14741 Training Loss: tensor(0.3400)\n",
      "14742 Training Loss: tensor(0.3382)\n",
      "14743 Training Loss: tensor(0.3394)\n",
      "14744 Training Loss: tensor(0.3390)\n",
      "14745 Training Loss: tensor(0.3403)\n",
      "14746 Training Loss: tensor(0.3443)\n",
      "14747 Training Loss: tensor(0.3378)\n",
      "14748 Training Loss: tensor(0.3449)\n",
      "14749 Training Loss: tensor(0.3393)\n",
      "14750 Training Loss: tensor(0.3413)\n",
      "14751 Training Loss: tensor(0.3422)\n",
      "14752 Training Loss: tensor(0.3409)\n",
      "14753 Training Loss: tensor(0.3401)\n",
      "14754 Training Loss: tensor(0.3387)\n",
      "14755 Training Loss: tensor(0.3375)\n",
      "14756 Training Loss: tensor(0.3379)\n",
      "14757 Training Loss: tensor(0.3378)\n",
      "14758 Training Loss: tensor(0.3391)\n",
      "14759 Training Loss: tensor(0.3484)\n",
      "14760 Training Loss: tensor(0.3373)\n",
      "14761 Training Loss: tensor(0.3392)\n",
      "14762 Training Loss: tensor(0.3387)\n",
      "14763 Training Loss: tensor(0.3388)\n",
      "14764 Training Loss: tensor(0.3373)\n",
      "14765 Training Loss: tensor(0.3432)\n",
      "14766 Training Loss: tensor(0.3399)\n",
      "14767 Training Loss: tensor(0.3376)\n",
      "14768 Training Loss: tensor(0.3400)\n",
      "14769 Training Loss: tensor(0.3413)\n",
      "14770 Training Loss: tensor(0.3386)\n",
      "14771 Training Loss: tensor(0.3513)\n",
      "14772 Training Loss: tensor(0.3376)\n",
      "14773 Training Loss: tensor(0.3414)\n",
      "14774 Training Loss: tensor(0.3390)\n",
      "14775 Training Loss: tensor(0.3400)\n",
      "14776 Training Loss: tensor(0.3409)\n",
      "14777 Training Loss: tensor(0.3385)\n",
      "14778 Training Loss: tensor(0.3388)\n",
      "14779 Training Loss: tensor(0.3398)\n",
      "14780 Training Loss: tensor(0.3373)\n",
      "14781 Training Loss: tensor(0.3405)\n",
      "14782 Training Loss: tensor(0.3377)\n",
      "14783 Training Loss: tensor(0.3385)\n",
      "14784 Training Loss: tensor(0.3381)\n",
      "14785 Training Loss: tensor(0.3400)\n",
      "14786 Training Loss: tensor(0.3414)\n",
      "14787 Training Loss: tensor(0.3402)\n",
      "14788 Training Loss: tensor(0.3400)\n",
      "14789 Training Loss: tensor(0.3383)\n",
      "14790 Training Loss: tensor(0.3381)\n",
      "14791 Training Loss: tensor(0.3384)\n",
      "14792 Training Loss: tensor(0.3383)\n",
      "14793 Training Loss: tensor(0.3379)\n",
      "14794 Training Loss: tensor(0.3399)\n",
      "14795 Training Loss: tensor(0.3431)\n",
      "14796 Training Loss: tensor(0.3389)\n",
      "14797 Training Loss: tensor(0.3381)\n",
      "14798 Training Loss: tensor(0.3360)\n",
      "14799 Training Loss: tensor(0.3387)\n",
      "14800 Training Loss: tensor(0.3362)\n",
      "14801 Training Loss: tensor(0.3388)\n",
      "14802 Training Loss: tensor(0.3431)\n",
      "14803 Training Loss: tensor(0.3400)\n",
      "14804 Training Loss: tensor(0.3424)\n",
      "14805 Training Loss: tensor(0.3410)\n",
      "14806 Training Loss: tensor(0.3375)\n",
      "14807 Training Loss: tensor(0.3395)\n",
      "14808 Training Loss: tensor(0.3373)\n",
      "14809 Training Loss: tensor(0.3368)\n",
      "14810 Training Loss: tensor(0.3374)\n",
      "14811 Training Loss: tensor(0.3371)\n",
      "14812 Training Loss: tensor(0.3386)\n",
      "14813 Training Loss: tensor(0.3375)\n",
      "14814 Training Loss: tensor(0.3373)\n",
      "14815 Training Loss: tensor(0.3380)\n",
      "14816 Training Loss: tensor(0.3377)\n",
      "14817 Training Loss: tensor(0.3385)\n",
      "14818 Training Loss: tensor(0.3423)\n",
      "14819 Training Loss: tensor(0.3375)\n",
      "14820 Training Loss: tensor(0.3393)\n",
      "14821 Training Loss: tensor(0.3395)\n",
      "14822 Training Loss: tensor(0.3407)\n",
      "14823 Training Loss: tensor(0.3374)\n",
      "14824 Training Loss: tensor(0.3398)\n",
      "14825 Training Loss: tensor(0.3377)\n",
      "14826 Training Loss: tensor(0.3379)\n",
      "14827 Training Loss: tensor(0.3407)\n",
      "14828 Training Loss: tensor(0.3371)\n",
      "14829 Training Loss: tensor(0.3419)\n",
      "14830 Training Loss: tensor(0.3373)\n",
      "14831 Training Loss: tensor(0.3374)\n",
      "14832 Training Loss: tensor(0.3429)\n",
      "14833 Training Loss: tensor(0.3428)\n",
      "14834 Training Loss: tensor(0.3379)\n",
      "14835 Training Loss: tensor(0.3380)\n",
      "14836 Training Loss: tensor(0.3370)\n",
      "14837 Training Loss: tensor(0.3382)\n",
      "14838 Training Loss: tensor(0.3379)\n",
      "14839 Training Loss: tensor(0.3396)\n",
      "14840 Training Loss: tensor(0.3376)\n",
      "14841 Training Loss: tensor(0.3381)\n",
      "14842 Training Loss: tensor(0.3400)\n",
      "14843 Training Loss: tensor(0.3385)\n",
      "14844 Training Loss: tensor(0.3393)\n",
      "14845 Training Loss: tensor(0.3501)\n",
      "14846 Training Loss: tensor(0.3406)\n",
      "14847 Training Loss: tensor(0.3376)\n",
      "14848 Training Loss: tensor(0.3376)\n",
      "14849 Training Loss: tensor(0.3449)\n",
      "14850 Training Loss: tensor(0.3400)\n",
      "14851 Training Loss: tensor(0.3402)\n",
      "14852 Training Loss: tensor(0.3371)\n",
      "14853 Training Loss: tensor(0.3384)\n",
      "14854 Training Loss: tensor(0.3382)\n",
      "14855 Training Loss: tensor(0.3424)\n",
      "14856 Training Loss: tensor(0.3417)\n",
      "14857 Training Loss: tensor(0.3387)\n",
      "14858 Training Loss: tensor(0.3386)\n",
      "14859 Training Loss: tensor(0.3403)\n",
      "14860 Training Loss: tensor(0.3381)\n",
      "14861 Training Loss: tensor(0.3437)\n",
      "14862 Training Loss: tensor(0.3371)\n",
      "14863 Training Loss: tensor(0.3407)\n",
      "14864 Training Loss: tensor(0.3380)\n",
      "14865 Training Loss: tensor(0.3379)\n",
      "14866 Training Loss: tensor(0.3385)\n",
      "14867 Training Loss: tensor(0.3382)\n",
      "14868 Training Loss: tensor(0.3379)\n",
      "14869 Training Loss: tensor(0.3395)\n",
      "14870 Training Loss: tensor(0.3389)\n",
      "14871 Training Loss: tensor(0.3388)\n",
      "14872 Training Loss: tensor(0.3392)\n",
      "14873 Training Loss: tensor(0.3377)\n",
      "14874 Training Loss: tensor(0.3395)\n",
      "14875 Training Loss: tensor(0.3386)\n",
      "14876 Training Loss: tensor(0.3438)\n",
      "14877 Training Loss: tensor(0.3379)\n",
      "14878 Training Loss: tensor(0.3420)\n",
      "14879 Training Loss: tensor(0.3363)\n",
      "14880 Training Loss: tensor(0.3367)\n",
      "14881 Training Loss: tensor(0.3414)\n",
      "14882 Training Loss: tensor(0.3451)\n",
      "14883 Training Loss: tensor(0.3370)\n",
      "14884 Training Loss: tensor(0.3483)\n",
      "14885 Training Loss: tensor(0.3392)\n",
      "14886 Training Loss: tensor(0.3373)\n",
      "14887 Training Loss: tensor(0.3391)\n",
      "14888 Training Loss: tensor(0.3383)\n",
      "14889 Training Loss: tensor(0.3367)\n",
      "14890 Training Loss: tensor(0.3410)\n",
      "14891 Training Loss: tensor(0.3393)\n",
      "14892 Training Loss: tensor(0.3451)\n",
      "14893 Training Loss: tensor(0.3387)\n",
      "14894 Training Loss: tensor(0.3380)\n",
      "14895 Training Loss: tensor(0.3378)\n",
      "14896 Training Loss: tensor(0.3398)\n",
      "14897 Training Loss: tensor(0.3393)\n",
      "14898 Training Loss: tensor(0.3390)\n",
      "14899 Training Loss: tensor(0.3394)\n",
      "14900 Training Loss: tensor(0.3369)\n",
      "14901 Training Loss: tensor(0.3401)\n",
      "14902 Training Loss: tensor(0.3407)\n",
      "14903 Training Loss: tensor(0.3372)\n",
      "14904 Training Loss: tensor(0.3422)\n",
      "14905 Training Loss: tensor(0.3368)\n",
      "14906 Training Loss: tensor(0.3373)\n",
      "14907 Training Loss: tensor(0.3412)\n",
      "14908 Training Loss: tensor(0.3376)\n",
      "14909 Training Loss: tensor(0.3389)\n",
      "14910 Training Loss: tensor(0.3401)\n",
      "14911 Training Loss: tensor(0.3373)\n",
      "14912 Training Loss: tensor(0.3390)\n",
      "14913 Training Loss: tensor(0.3391)\n",
      "14914 Training Loss: tensor(0.3407)\n",
      "14915 Training Loss: tensor(0.3378)\n",
      "14916 Training Loss: tensor(0.3379)\n",
      "14917 Training Loss: tensor(0.3428)\n",
      "14918 Training Loss: tensor(0.3403)\n",
      "14919 Training Loss: tensor(0.3395)\n",
      "14920 Training Loss: tensor(0.3407)\n",
      "14921 Training Loss: tensor(0.3399)\n",
      "14922 Training Loss: tensor(0.3377)\n",
      "14923 Training Loss: tensor(0.3379)\n",
      "14924 Training Loss: tensor(0.3379)\n",
      "14925 Training Loss: tensor(0.3380)\n",
      "14926 Training Loss: tensor(0.3365)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14927 Training Loss: tensor(0.3377)\n",
      "14928 Training Loss: tensor(0.3377)\n",
      "14929 Training Loss: tensor(0.3389)\n",
      "14930 Training Loss: tensor(0.3384)\n",
      "14931 Training Loss: tensor(0.3384)\n",
      "14932 Training Loss: tensor(0.3376)\n",
      "14933 Training Loss: tensor(0.3427)\n",
      "14934 Training Loss: tensor(0.3446)\n",
      "14935 Training Loss: tensor(0.3406)\n",
      "14936 Training Loss: tensor(0.3389)\n",
      "14937 Training Loss: tensor(0.3398)\n",
      "14938 Training Loss: tensor(0.3370)\n",
      "14939 Training Loss: tensor(0.3379)\n",
      "14940 Training Loss: tensor(0.3411)\n",
      "14941 Training Loss: tensor(0.3361)\n",
      "14942 Training Loss: tensor(0.3392)\n",
      "14943 Training Loss: tensor(0.3385)\n",
      "14944 Training Loss: tensor(0.3444)\n",
      "14945 Training Loss: tensor(0.3378)\n",
      "14946 Training Loss: tensor(0.3377)\n",
      "14947 Training Loss: tensor(0.3401)\n",
      "14948 Training Loss: tensor(0.3376)\n",
      "14949 Training Loss: tensor(0.3373)\n",
      "14950 Training Loss: tensor(0.3391)\n",
      "14951 Training Loss: tensor(0.3382)\n",
      "14952 Training Loss: tensor(0.3373)\n",
      "14953 Training Loss: tensor(0.3373)\n",
      "14954 Training Loss: tensor(0.3371)\n",
      "14955 Training Loss: tensor(0.3409)\n",
      "14956 Training Loss: tensor(0.3416)\n",
      "14957 Training Loss: tensor(0.3376)\n",
      "14958 Training Loss: tensor(0.3446)\n",
      "14959 Training Loss: tensor(0.3384)\n",
      "14960 Training Loss: tensor(0.3385)\n",
      "14961 Training Loss: tensor(0.3382)\n",
      "14962 Training Loss: tensor(0.3404)\n",
      "14963 Training Loss: tensor(0.3405)\n",
      "14964 Training Loss: tensor(0.3423)\n",
      "14965 Training Loss: tensor(0.3376)\n",
      "14966 Training Loss: tensor(0.3392)\n",
      "14967 Training Loss: tensor(0.3389)\n",
      "14968 Training Loss: tensor(0.3375)\n",
      "14969 Training Loss: tensor(0.3368)\n",
      "14970 Training Loss: tensor(0.3369)\n",
      "14971 Training Loss: tensor(0.3381)\n",
      "14972 Training Loss: tensor(0.3389)\n",
      "14973 Training Loss: tensor(0.3379)\n",
      "14974 Training Loss: tensor(0.3414)\n",
      "14975 Training Loss: tensor(0.3377)\n",
      "14976 Training Loss: tensor(0.3371)\n",
      "14977 Training Loss: tensor(0.3390)\n",
      "14978 Training Loss: tensor(0.3379)\n",
      "14979 Training Loss: tensor(0.3379)\n",
      "14980 Training Loss: tensor(0.3392)\n",
      "14981 Training Loss: tensor(0.3416)\n",
      "14982 Training Loss: tensor(0.3397)\n",
      "14983 Training Loss: tensor(0.3375)\n",
      "14984 Training Loss: tensor(0.3377)\n",
      "14985 Training Loss: tensor(0.3402)\n",
      "14986 Training Loss: tensor(0.3395)\n",
      "14987 Training Loss: tensor(0.3403)\n",
      "14988 Training Loss: tensor(0.3376)\n",
      "14989 Training Loss: tensor(0.3373)\n",
      "14990 Training Loss: tensor(0.3389)\n",
      "14991 Training Loss: tensor(0.3381)\n",
      "14992 Training Loss: tensor(0.3428)\n",
      "14993 Training Loss: tensor(0.3406)\n",
      "14994 Training Loss: tensor(0.3369)\n",
      "14995 Training Loss: tensor(0.3364)\n",
      "14996 Training Loss: tensor(0.3374)\n",
      "14997 Training Loss: tensor(0.3390)\n",
      "14998 Training Loss: tensor(0.3390)\n",
      "14999 Training Loss: tensor(0.3391)\n",
      "15000 Training Loss: tensor(0.3376)\n",
      "15001 Training Loss: tensor(0.3365)\n",
      "15002 Training Loss: tensor(0.3370)\n",
      "15003 Training Loss: tensor(0.3375)\n",
      "15004 Training Loss: tensor(0.3363)\n",
      "15005 Training Loss: tensor(0.3371)\n",
      "15006 Training Loss: tensor(0.3382)\n",
      "15007 Training Loss: tensor(0.3480)\n",
      "15008 Training Loss: tensor(0.3358)\n",
      "15009 Training Loss: tensor(0.3507)\n",
      "15010 Training Loss: tensor(0.3393)\n",
      "15011 Training Loss: tensor(0.3433)\n",
      "15012 Training Loss: tensor(0.3364)\n",
      "15013 Training Loss: tensor(0.3375)\n",
      "15014 Training Loss: tensor(0.3388)\n",
      "15015 Training Loss: tensor(0.3398)\n",
      "15016 Training Loss: tensor(0.3411)\n",
      "15017 Training Loss: tensor(0.3449)\n",
      "15018 Training Loss: tensor(0.3378)\n",
      "15019 Training Loss: tensor(0.3534)\n",
      "15020 Training Loss: tensor(0.3397)\n",
      "15021 Training Loss: tensor(0.3402)\n",
      "15022 Training Loss: tensor(0.3396)\n",
      "15023 Training Loss: tensor(0.3407)\n",
      "15024 Training Loss: tensor(0.3427)\n",
      "15025 Training Loss: tensor(0.3437)\n",
      "15026 Training Loss: tensor(0.3426)\n",
      "15027 Training Loss: tensor(0.3433)\n",
      "15028 Training Loss: tensor(0.3385)\n",
      "15029 Training Loss: tensor(0.3388)\n",
      "15030 Training Loss: tensor(0.3389)\n",
      "15031 Training Loss: tensor(0.3384)\n",
      "15032 Training Loss: tensor(0.3389)\n",
      "15033 Training Loss: tensor(0.3395)\n",
      "15034 Training Loss: tensor(0.3393)\n",
      "15035 Training Loss: tensor(0.3387)\n",
      "15036 Training Loss: tensor(0.3388)\n",
      "15037 Training Loss: tensor(0.3409)\n",
      "15038 Training Loss: tensor(0.3401)\n",
      "15039 Training Loss: tensor(0.3377)\n",
      "15040 Training Loss: tensor(0.3382)\n",
      "15041 Training Loss: tensor(0.3389)\n",
      "15042 Training Loss: tensor(0.3367)\n",
      "15043 Training Loss: tensor(0.3375)\n",
      "15044 Training Loss: tensor(0.3436)\n",
      "15045 Training Loss: tensor(0.3356)\n",
      "15046 Training Loss: tensor(0.3368)\n",
      "15047 Training Loss: tensor(0.3373)\n",
      "15048 Training Loss: tensor(0.3400)\n",
      "15049 Training Loss: tensor(0.3449)\n",
      "15050 Training Loss: tensor(0.3390)\n",
      "15051 Training Loss: tensor(0.3378)\n",
      "15052 Training Loss: tensor(0.3383)\n",
      "15053 Training Loss: tensor(0.3372)\n",
      "15054 Training Loss: tensor(0.3385)\n",
      "15055 Training Loss: tensor(0.3395)\n",
      "15056 Training Loss: tensor(0.3387)\n",
      "15057 Training Loss: tensor(0.3372)\n",
      "15058 Training Loss: tensor(0.3370)\n",
      "15059 Training Loss: tensor(0.3374)\n",
      "15060 Training Loss: tensor(0.3371)\n",
      "15061 Training Loss: tensor(0.3371)\n",
      "15062 Training Loss: tensor(0.3411)\n",
      "15063 Training Loss: tensor(0.3384)\n",
      "15064 Training Loss: tensor(0.3404)\n",
      "15065 Training Loss: tensor(0.3401)\n",
      "15066 Training Loss: tensor(0.3384)\n",
      "15067 Training Loss: tensor(0.3395)\n",
      "15068 Training Loss: tensor(0.3374)\n",
      "15069 Training Loss: tensor(0.3390)\n",
      "15070 Training Loss: tensor(0.3371)\n",
      "15071 Training Loss: tensor(0.3373)\n",
      "15072 Training Loss: tensor(0.3382)\n",
      "15073 Training Loss: tensor(0.3374)\n",
      "15074 Training Loss: tensor(0.3381)\n",
      "15075 Training Loss: tensor(0.3450)\n",
      "15076 Training Loss: tensor(0.3372)\n",
      "15077 Training Loss: tensor(0.3396)\n",
      "15078 Training Loss: tensor(0.3395)\n",
      "15079 Training Loss: tensor(0.3362)\n",
      "15080 Training Loss: tensor(0.3441)\n",
      "15081 Training Loss: tensor(0.3387)\n",
      "15082 Training Loss: tensor(0.3377)\n",
      "15083 Training Loss: tensor(0.3383)\n",
      "15084 Training Loss: tensor(0.3374)\n",
      "15085 Training Loss: tensor(0.3452)\n",
      "15086 Training Loss: tensor(0.3385)\n",
      "15087 Training Loss: tensor(0.3421)\n",
      "15088 Training Loss: tensor(0.3377)\n",
      "15089 Training Loss: tensor(0.3384)\n",
      "15090 Training Loss: tensor(0.3441)\n",
      "15091 Training Loss: tensor(0.3391)\n",
      "15092 Training Loss: tensor(0.3373)\n",
      "15093 Training Loss: tensor(0.3394)\n",
      "15094 Training Loss: tensor(0.3389)\n",
      "15095 Training Loss: tensor(0.3366)\n",
      "15096 Training Loss: tensor(0.3395)\n",
      "15097 Training Loss: tensor(0.3380)\n",
      "15098 Training Loss: tensor(0.3398)\n",
      "15099 Training Loss: tensor(0.3376)\n",
      "15100 Training Loss: tensor(0.3365)\n",
      "15101 Training Loss: tensor(0.3388)\n",
      "15102 Training Loss: tensor(0.3385)\n",
      "15103 Training Loss: tensor(0.3366)\n",
      "15104 Training Loss: tensor(0.3380)\n",
      "15105 Training Loss: tensor(0.3400)\n",
      "15106 Training Loss: tensor(0.3364)\n",
      "15107 Training Loss: tensor(0.3369)\n",
      "15108 Training Loss: tensor(0.3417)\n",
      "15109 Training Loss: tensor(0.3396)\n",
      "15110 Training Loss: tensor(0.3369)\n",
      "15111 Training Loss: tensor(0.3372)\n",
      "15112 Training Loss: tensor(0.3377)\n",
      "15113 Training Loss: tensor(0.3383)\n",
      "15114 Training Loss: tensor(0.3356)\n",
      "15115 Training Loss: tensor(0.3378)\n",
      "15116 Training Loss: tensor(0.3403)\n",
      "15117 Training Loss: tensor(0.3380)\n",
      "15118 Training Loss: tensor(0.3414)\n",
      "15119 Training Loss: tensor(0.3366)\n",
      "15120 Training Loss: tensor(0.3398)\n",
      "15121 Training Loss: tensor(0.3395)\n",
      "15122 Training Loss: tensor(0.3387)\n",
      "15123 Training Loss: tensor(0.3385)\n",
      "15124 Training Loss: tensor(0.3388)\n",
      "15125 Training Loss: tensor(0.3376)\n",
      "15126 Training Loss: tensor(0.3382)\n",
      "15127 Training Loss: tensor(0.3375)\n",
      "15128 Training Loss: tensor(0.3366)\n",
      "15129 Training Loss: tensor(0.3401)\n",
      "15130 Training Loss: tensor(0.3379)\n",
      "15131 Training Loss: tensor(0.3395)\n",
      "15132 Training Loss: tensor(0.3411)\n",
      "15133 Training Loss: tensor(0.3400)\n",
      "15134 Training Loss: tensor(0.3370)\n",
      "15135 Training Loss: tensor(0.3361)\n",
      "15136 Training Loss: tensor(0.3391)\n",
      "15137 Training Loss: tensor(0.3424)\n",
      "15138 Training Loss: tensor(0.3365)\n",
      "15139 Training Loss: tensor(0.3380)\n",
      "15140 Training Loss: tensor(0.3387)\n",
      "15141 Training Loss: tensor(0.3461)\n",
      "15142 Training Loss: tensor(0.3365)\n",
      "15143 Training Loss: tensor(0.3375)\n",
      "15144 Training Loss: tensor(0.3408)\n",
      "15145 Training Loss: tensor(0.3370)\n",
      "15146 Training Loss: tensor(0.3386)\n",
      "15147 Training Loss: tensor(0.3384)\n",
      "15148 Training Loss: tensor(0.3393)\n",
      "15149 Training Loss: tensor(0.3386)\n",
      "15150 Training Loss: tensor(0.3402)\n",
      "15151 Training Loss: tensor(0.3372)\n",
      "15152 Training Loss: tensor(0.3381)\n",
      "15153 Training Loss: tensor(0.3375)\n",
      "15154 Training Loss: tensor(0.3366)\n",
      "15155 Training Loss: tensor(0.3364)\n",
      "15156 Training Loss: tensor(0.3397)\n",
      "15157 Training Loss: tensor(0.3434)\n",
      "15158 Training Loss: tensor(0.3400)\n",
      "15159 Training Loss: tensor(0.3413)\n",
      "15160 Training Loss: tensor(0.3372)\n",
      "15161 Training Loss: tensor(0.3384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15162 Training Loss: tensor(0.3377)\n",
      "15163 Training Loss: tensor(0.3376)\n",
      "15164 Training Loss: tensor(0.3387)\n",
      "15165 Training Loss: tensor(0.3393)\n",
      "15166 Training Loss: tensor(0.3387)\n",
      "15167 Training Loss: tensor(0.3368)\n",
      "15168 Training Loss: tensor(0.3422)\n",
      "15169 Training Loss: tensor(0.3374)\n",
      "15170 Training Loss: tensor(0.3363)\n",
      "15171 Training Loss: tensor(0.3455)\n",
      "15172 Training Loss: tensor(0.3390)\n",
      "15173 Training Loss: tensor(0.3384)\n",
      "15174 Training Loss: tensor(0.3387)\n",
      "15175 Training Loss: tensor(0.3363)\n",
      "15176 Training Loss: tensor(0.3383)\n",
      "15177 Training Loss: tensor(0.3400)\n",
      "15178 Training Loss: tensor(0.3373)\n",
      "15179 Training Loss: tensor(0.3370)\n",
      "15180 Training Loss: tensor(0.3388)\n",
      "15181 Training Loss: tensor(0.3398)\n",
      "15182 Training Loss: tensor(0.3378)\n",
      "15183 Training Loss: tensor(0.3379)\n",
      "15184 Training Loss: tensor(0.3458)\n",
      "15185 Training Loss: tensor(0.3375)\n",
      "15186 Training Loss: tensor(0.3393)\n",
      "15187 Training Loss: tensor(0.3379)\n",
      "15188 Training Loss: tensor(0.3359)\n",
      "15189 Training Loss: tensor(0.3359)\n",
      "15190 Training Loss: tensor(0.3382)\n",
      "15191 Training Loss: tensor(0.3379)\n",
      "15192 Training Loss: tensor(0.3435)\n",
      "15193 Training Loss: tensor(0.3469)\n",
      "15194 Training Loss: tensor(0.3377)\n",
      "15195 Training Loss: tensor(0.3366)\n",
      "15196 Training Loss: tensor(0.3368)\n",
      "15197 Training Loss: tensor(0.3374)\n",
      "15198 Training Loss: tensor(0.3371)\n",
      "15199 Training Loss: tensor(0.3376)\n",
      "15200 Training Loss: tensor(0.3394)\n",
      "15201 Training Loss: tensor(0.3365)\n",
      "15202 Training Loss: tensor(0.3383)\n",
      "15203 Training Loss: tensor(0.3382)\n",
      "15204 Training Loss: tensor(0.3364)\n",
      "15205 Training Loss: tensor(0.3403)\n",
      "15206 Training Loss: tensor(0.3412)\n",
      "15207 Training Loss: tensor(0.3405)\n",
      "15208 Training Loss: tensor(0.3376)\n",
      "15209 Training Loss: tensor(0.3384)\n",
      "15210 Training Loss: tensor(0.3374)\n",
      "15211 Training Loss: tensor(0.3382)\n",
      "15212 Training Loss: tensor(0.3417)\n",
      "15213 Training Loss: tensor(0.3374)\n",
      "15214 Training Loss: tensor(0.3388)\n",
      "15215 Training Loss: tensor(0.3366)\n",
      "15216 Training Loss: tensor(0.3366)\n",
      "15217 Training Loss: tensor(0.3374)\n",
      "15218 Training Loss: tensor(0.3370)\n",
      "15219 Training Loss: tensor(0.3361)\n",
      "15220 Training Loss: tensor(0.3361)\n",
      "15221 Training Loss: tensor(0.3377)\n",
      "15222 Training Loss: tensor(0.3400)\n",
      "15223 Training Loss: tensor(0.3382)\n",
      "15224 Training Loss: tensor(0.3366)\n",
      "15225 Training Loss: tensor(0.3398)\n",
      "15226 Training Loss: tensor(0.3443)\n",
      "15227 Training Loss: tensor(0.3366)\n",
      "15228 Training Loss: tensor(0.3369)\n",
      "15229 Training Loss: tensor(0.3419)\n",
      "15230 Training Loss: tensor(0.3389)\n",
      "15231 Training Loss: tensor(0.3366)\n",
      "15232 Training Loss: tensor(0.3401)\n",
      "15233 Training Loss: tensor(0.3402)\n",
      "15234 Training Loss: tensor(0.3374)\n",
      "15235 Training Loss: tensor(0.3416)\n",
      "15236 Training Loss: tensor(0.3382)\n",
      "15237 Training Loss: tensor(0.3412)\n",
      "15238 Training Loss: tensor(0.3428)\n",
      "15239 Training Loss: tensor(0.3386)\n",
      "15240 Training Loss: tensor(0.3369)\n",
      "15241 Training Loss: tensor(0.3381)\n",
      "15242 Training Loss: tensor(0.3434)\n",
      "15243 Training Loss: tensor(0.3390)\n",
      "15244 Training Loss: tensor(0.3377)\n",
      "15245 Training Loss: tensor(0.3379)\n",
      "15246 Training Loss: tensor(0.3356)\n",
      "15247 Training Loss: tensor(0.3380)\n",
      "15248 Training Loss: tensor(0.3409)\n",
      "15249 Training Loss: tensor(0.3362)\n",
      "15250 Training Loss: tensor(0.3358)\n",
      "15251 Training Loss: tensor(0.3358)\n",
      "15252 Training Loss: tensor(0.3356)\n",
      "15253 Training Loss: tensor(0.3373)\n",
      "15254 Training Loss: tensor(0.3351)\n",
      "15255 Training Loss: tensor(0.3360)\n",
      "15256 Training Loss: tensor(0.3375)\n",
      "15257 Training Loss: tensor(0.3369)\n",
      "15258 Training Loss: tensor(0.3369)\n",
      "15259 Training Loss: tensor(0.3363)\n",
      "15260 Training Loss: tensor(0.3374)\n",
      "15261 Training Loss: tensor(0.3402)\n",
      "15262 Training Loss: tensor(0.3480)\n",
      "15263 Training Loss: tensor(0.3382)\n",
      "15264 Training Loss: tensor(0.3368)\n",
      "15265 Training Loss: tensor(0.3407)\n",
      "15266 Training Loss: tensor(0.3419)\n",
      "15267 Training Loss: tensor(0.3391)\n",
      "15268 Training Loss: tensor(0.3361)\n",
      "15269 Training Loss: tensor(0.3372)\n",
      "15270 Training Loss: tensor(0.3377)\n",
      "15271 Training Loss: tensor(0.3433)\n",
      "15272 Training Loss: tensor(0.3376)\n",
      "15273 Training Loss: tensor(0.3378)\n",
      "15274 Training Loss: tensor(0.3410)\n",
      "15275 Training Loss: tensor(0.3383)\n",
      "15276 Training Loss: tensor(0.3374)\n",
      "15277 Training Loss: tensor(0.3393)\n",
      "15278 Training Loss: tensor(0.3394)\n",
      "15279 Training Loss: tensor(0.3376)\n",
      "15280 Training Loss: tensor(0.3365)\n",
      "15281 Training Loss: tensor(0.3422)\n",
      "15282 Training Loss: tensor(0.3396)\n",
      "15283 Training Loss: tensor(0.3374)\n",
      "15284 Training Loss: tensor(0.3366)\n",
      "15285 Training Loss: tensor(0.3361)\n",
      "15286 Training Loss: tensor(0.3375)\n",
      "15287 Training Loss: tensor(0.3371)\n",
      "15288 Training Loss: tensor(0.3356)\n",
      "15289 Training Loss: tensor(0.3386)\n",
      "15290 Training Loss: tensor(0.3355)\n",
      "15291 Training Loss: tensor(0.3379)\n",
      "15292 Training Loss: tensor(0.3366)\n",
      "15293 Training Loss: tensor(0.3379)\n",
      "15294 Training Loss: tensor(0.3367)\n",
      "15295 Training Loss: tensor(0.3388)\n",
      "15296 Training Loss: tensor(0.3405)\n",
      "15297 Training Loss: tensor(0.3352)\n",
      "15298 Training Loss: tensor(0.3378)\n",
      "15299 Training Loss: tensor(0.3359)\n",
      "15300 Training Loss: tensor(0.3411)\n",
      "15301 Training Loss: tensor(0.3408)\n",
      "15302 Training Loss: tensor(0.3376)\n",
      "15303 Training Loss: tensor(0.3361)\n",
      "15304 Training Loss: tensor(0.3362)\n",
      "15305 Training Loss: tensor(0.3359)\n",
      "15306 Training Loss: tensor(0.3383)\n",
      "15307 Training Loss: tensor(0.3393)\n",
      "15308 Training Loss: tensor(0.3358)\n",
      "15309 Training Loss: tensor(0.3449)\n",
      "15310 Training Loss: tensor(0.3389)\n",
      "15311 Training Loss: tensor(0.3396)\n",
      "15312 Training Loss: tensor(0.3380)\n",
      "15313 Training Loss: tensor(0.3379)\n",
      "15314 Training Loss: tensor(0.3396)\n",
      "15315 Training Loss: tensor(0.3375)\n",
      "15316 Training Loss: tensor(0.3369)\n",
      "15317 Training Loss: tensor(0.3361)\n",
      "15318 Training Loss: tensor(0.3356)\n",
      "15319 Training Loss: tensor(0.3396)\n",
      "15320 Training Loss: tensor(0.3373)\n",
      "15321 Training Loss: tensor(0.3373)\n",
      "15322 Training Loss: tensor(0.3378)\n",
      "15323 Training Loss: tensor(0.3360)\n",
      "15324 Training Loss: tensor(0.3373)\n",
      "15325 Training Loss: tensor(0.3349)\n",
      "15326 Training Loss: tensor(0.3372)\n",
      "15327 Training Loss: tensor(0.3416)\n",
      "15328 Training Loss: tensor(0.3353)\n",
      "15329 Training Loss: tensor(0.3368)\n",
      "15330 Training Loss: tensor(0.3358)\n",
      "15331 Training Loss: tensor(0.3475)\n",
      "15332 Training Loss: tensor(0.3442)\n",
      "15333 Training Loss: tensor(0.3376)\n",
      "15334 Training Loss: tensor(0.3362)\n",
      "15335 Training Loss: tensor(0.3373)\n",
      "15336 Training Loss: tensor(0.3380)\n",
      "15337 Training Loss: tensor(0.3376)\n",
      "15338 Training Loss: tensor(0.3385)\n",
      "15339 Training Loss: tensor(0.3373)\n",
      "15340 Training Loss: tensor(0.3368)\n",
      "15341 Training Loss: tensor(0.3406)\n",
      "15342 Training Loss: tensor(0.3390)\n",
      "15343 Training Loss: tensor(0.3441)\n",
      "15344 Training Loss: tensor(0.3387)\n",
      "15345 Training Loss: tensor(0.3396)\n",
      "15346 Training Loss: tensor(0.3374)\n",
      "15347 Training Loss: tensor(0.3369)\n",
      "15348 Training Loss: tensor(0.3385)\n",
      "15349 Training Loss: tensor(0.3376)\n",
      "15350 Training Loss: tensor(0.3363)\n",
      "15351 Training Loss: tensor(0.3377)\n",
      "15352 Training Loss: tensor(0.3392)\n",
      "15353 Training Loss: tensor(0.3370)\n",
      "15354 Training Loss: tensor(0.3450)\n",
      "15355 Training Loss: tensor(0.3381)\n",
      "15356 Training Loss: tensor(0.3366)\n",
      "15357 Training Loss: tensor(0.3363)\n",
      "15358 Training Loss: tensor(0.3354)\n",
      "15359 Training Loss: tensor(0.3366)\n",
      "15360 Training Loss: tensor(0.3371)\n",
      "15361 Training Loss: tensor(0.3418)\n",
      "15362 Training Loss: tensor(0.3367)\n",
      "15363 Training Loss: tensor(0.3360)\n",
      "15364 Training Loss: tensor(0.3364)\n",
      "15365 Training Loss: tensor(0.3383)\n",
      "15366 Training Loss: tensor(0.3370)\n",
      "15367 Training Loss: tensor(0.3359)\n",
      "15368 Training Loss: tensor(0.3373)\n",
      "15369 Training Loss: tensor(0.3377)\n",
      "15370 Training Loss: tensor(0.3363)\n",
      "15371 Training Loss: tensor(0.3389)\n",
      "15372 Training Loss: tensor(0.3370)\n",
      "15373 Training Loss: tensor(0.3368)\n",
      "15374 Training Loss: tensor(0.3375)\n",
      "15375 Training Loss: tensor(0.3385)\n",
      "15376 Training Loss: tensor(0.3386)\n",
      "15377 Training Loss: tensor(0.3368)\n",
      "15378 Training Loss: tensor(0.3385)\n",
      "15379 Training Loss: tensor(0.3376)\n",
      "15380 Training Loss: tensor(0.3359)\n",
      "15381 Training Loss: tensor(0.3374)\n",
      "15382 Training Loss: tensor(0.3372)\n",
      "15383 Training Loss: tensor(0.3373)\n",
      "15384 Training Loss: tensor(0.3363)\n",
      "15385 Training Loss: tensor(0.3379)\n",
      "15386 Training Loss: tensor(0.3370)\n",
      "15387 Training Loss: tensor(0.3374)\n",
      "15388 Training Loss: tensor(0.3364)\n",
      "15389 Training Loss: tensor(0.3379)\n",
      "15390 Training Loss: tensor(0.3364)\n",
      "15391 Training Loss: tensor(0.3364)\n",
      "15392 Training Loss: tensor(0.3367)\n",
      "15393 Training Loss: tensor(0.3406)\n",
      "15394 Training Loss: tensor(0.3373)\n",
      "15395 Training Loss: tensor(0.3406)\n",
      "15396 Training Loss: tensor(0.3366)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15397 Training Loss: tensor(0.3430)\n",
      "15398 Training Loss: tensor(0.3401)\n",
      "15399 Training Loss: tensor(0.3439)\n",
      "15400 Training Loss: tensor(0.3416)\n",
      "15401 Training Loss: tensor(0.3392)\n",
      "15402 Training Loss: tensor(0.3373)\n",
      "15403 Training Loss: tensor(0.3405)\n",
      "15404 Training Loss: tensor(0.3383)\n",
      "15405 Training Loss: tensor(0.3407)\n",
      "15406 Training Loss: tensor(0.3378)\n",
      "15407 Training Loss: tensor(0.3409)\n",
      "15408 Training Loss: tensor(0.3385)\n",
      "15409 Training Loss: tensor(0.3365)\n",
      "15410 Training Loss: tensor(0.3383)\n",
      "15411 Training Loss: tensor(0.3380)\n",
      "15412 Training Loss: tensor(0.3379)\n",
      "15413 Training Loss: tensor(0.3381)\n",
      "15414 Training Loss: tensor(0.3394)\n",
      "15415 Training Loss: tensor(0.3373)\n",
      "15416 Training Loss: tensor(0.3353)\n",
      "15417 Training Loss: tensor(0.3484)\n",
      "15418 Training Loss: tensor(0.3383)\n",
      "15419 Training Loss: tensor(0.3369)\n",
      "15420 Training Loss: tensor(0.3368)\n",
      "15421 Training Loss: tensor(0.3409)\n",
      "15422 Training Loss: tensor(0.3366)\n",
      "15423 Training Loss: tensor(0.3424)\n",
      "15424 Training Loss: tensor(0.3379)\n",
      "15425 Training Loss: tensor(0.3374)\n",
      "15426 Training Loss: tensor(0.3382)\n",
      "15427 Training Loss: tensor(0.3386)\n",
      "15428 Training Loss: tensor(0.3393)\n",
      "15429 Training Loss: tensor(0.3395)\n",
      "15430 Training Loss: tensor(0.3397)\n",
      "15431 Training Loss: tensor(0.3378)\n",
      "15432 Training Loss: tensor(0.3407)\n",
      "15433 Training Loss: tensor(0.3431)\n",
      "15434 Training Loss: tensor(0.3385)\n",
      "15435 Training Loss: tensor(0.3361)\n",
      "15436 Training Loss: tensor(0.3412)\n",
      "15437 Training Loss: tensor(0.3366)\n",
      "15438 Training Loss: tensor(0.3364)\n",
      "15439 Training Loss: tensor(0.3381)\n",
      "15440 Training Loss: tensor(0.3376)\n",
      "15441 Training Loss: tensor(0.3370)\n",
      "15442 Training Loss: tensor(0.3386)\n",
      "15443 Training Loss: tensor(0.3363)\n",
      "15444 Training Loss: tensor(0.3379)\n",
      "15445 Training Loss: tensor(0.3447)\n",
      "15446 Training Loss: tensor(0.3368)\n",
      "15447 Training Loss: tensor(0.3399)\n",
      "15448 Training Loss: tensor(0.3412)\n",
      "15449 Training Loss: tensor(0.3394)\n",
      "15450 Training Loss: tensor(0.3375)\n",
      "15451 Training Loss: tensor(0.3384)\n",
      "15452 Training Loss: tensor(0.3368)\n",
      "15453 Training Loss: tensor(0.3368)\n",
      "15454 Training Loss: tensor(0.3377)\n",
      "15455 Training Loss: tensor(0.3417)\n",
      "15456 Training Loss: tensor(0.3365)\n",
      "15457 Training Loss: tensor(0.3365)\n",
      "15458 Training Loss: tensor(0.3390)\n",
      "15459 Training Loss: tensor(0.3367)\n",
      "15460 Training Loss: tensor(0.3388)\n",
      "15461 Training Loss: tensor(0.3376)\n",
      "15462 Training Loss: tensor(0.3368)\n",
      "15463 Training Loss: tensor(0.3350)\n",
      "15464 Training Loss: tensor(0.3359)\n",
      "15465 Training Loss: tensor(0.3383)\n",
      "15466 Training Loss: tensor(0.3437)\n",
      "15467 Training Loss: tensor(0.3395)\n",
      "15468 Training Loss: tensor(0.3417)\n",
      "15469 Training Loss: tensor(0.3398)\n",
      "15470 Training Loss: tensor(0.3362)\n",
      "15471 Training Loss: tensor(0.3358)\n",
      "15472 Training Loss: tensor(0.3391)\n",
      "15473 Training Loss: tensor(0.3390)\n",
      "15474 Training Loss: tensor(0.3371)\n",
      "15475 Training Loss: tensor(0.3374)\n",
      "15476 Training Loss: tensor(0.3415)\n",
      "15477 Training Loss: tensor(0.3403)\n",
      "15478 Training Loss: tensor(0.3368)\n",
      "15479 Training Loss: tensor(0.3377)\n",
      "15480 Training Loss: tensor(0.3361)\n",
      "15481 Training Loss: tensor(0.3377)\n",
      "15482 Training Loss: tensor(0.3363)\n",
      "15483 Training Loss: tensor(0.3382)\n",
      "15484 Training Loss: tensor(0.3396)\n",
      "15485 Training Loss: tensor(0.3379)\n",
      "15486 Training Loss: tensor(0.3410)\n",
      "15487 Training Loss: tensor(0.3365)\n",
      "15488 Training Loss: tensor(0.3396)\n",
      "15489 Training Loss: tensor(0.3362)\n",
      "15490 Training Loss: tensor(0.3379)\n",
      "15491 Training Loss: tensor(0.3371)\n",
      "15492 Training Loss: tensor(0.3372)\n",
      "15493 Training Loss: tensor(0.3348)\n",
      "15494 Training Loss: tensor(0.3364)\n",
      "15495 Training Loss: tensor(0.3379)\n",
      "15496 Training Loss: tensor(0.3385)\n",
      "15497 Training Loss: tensor(0.3369)\n",
      "15498 Training Loss: tensor(0.3397)\n",
      "15499 Training Loss: tensor(0.3366)\n",
      "15500 Training Loss: tensor(0.3378)\n",
      "15501 Training Loss: tensor(0.3365)\n",
      "15502 Training Loss: tensor(0.3365)\n",
      "15503 Training Loss: tensor(0.3358)\n",
      "15504 Training Loss: tensor(0.3376)\n",
      "15505 Training Loss: tensor(0.3374)\n",
      "15506 Training Loss: tensor(0.3371)\n",
      "15507 Training Loss: tensor(0.3362)\n",
      "15508 Training Loss: tensor(0.3496)\n",
      "15509 Training Loss: tensor(0.3381)\n",
      "15510 Training Loss: tensor(0.3366)\n",
      "15511 Training Loss: tensor(0.3389)\n",
      "15512 Training Loss: tensor(0.3377)\n",
      "15513 Training Loss: tensor(0.3354)\n",
      "15514 Training Loss: tensor(0.3358)\n",
      "15515 Training Loss: tensor(0.3357)\n",
      "15516 Training Loss: tensor(0.3441)\n",
      "15517 Training Loss: tensor(0.3363)\n",
      "15518 Training Loss: tensor(0.3407)\n",
      "15519 Training Loss: tensor(0.3369)\n",
      "15520 Training Loss: tensor(0.3364)\n",
      "15521 Training Loss: tensor(0.3371)\n",
      "15522 Training Loss: tensor(0.3392)\n",
      "15523 Training Loss: tensor(0.3478)\n",
      "15524 Training Loss: tensor(0.3370)\n",
      "15525 Training Loss: tensor(0.3386)\n",
      "15526 Training Loss: tensor(0.3373)\n",
      "15527 Training Loss: tensor(0.3388)\n",
      "15528 Training Loss: tensor(0.3386)\n",
      "15529 Training Loss: tensor(0.3437)\n",
      "15530 Training Loss: tensor(0.3376)\n",
      "15531 Training Loss: tensor(0.3385)\n",
      "15532 Training Loss: tensor(0.3370)\n",
      "15533 Training Loss: tensor(0.3420)\n",
      "15534 Training Loss: tensor(0.3381)\n",
      "15535 Training Loss: tensor(0.3366)\n",
      "15536 Training Loss: tensor(0.3381)\n",
      "15537 Training Loss: tensor(0.3349)\n",
      "15538 Training Loss: tensor(0.3371)\n",
      "15539 Training Loss: tensor(0.3427)\n",
      "15540 Training Loss: tensor(0.3350)\n",
      "15541 Training Loss: tensor(0.3372)\n",
      "15542 Training Loss: tensor(0.3360)\n",
      "15543 Training Loss: tensor(0.3367)\n",
      "15544 Training Loss: tensor(0.3358)\n",
      "15545 Training Loss: tensor(0.3351)\n",
      "15546 Training Loss: tensor(0.3404)\n",
      "15547 Training Loss: tensor(0.3358)\n",
      "15548 Training Loss: tensor(0.3369)\n",
      "15549 Training Loss: tensor(0.3370)\n",
      "15550 Training Loss: tensor(0.3368)\n",
      "15551 Training Loss: tensor(0.3370)\n",
      "15552 Training Loss: tensor(0.3374)\n",
      "15553 Training Loss: tensor(0.3355)\n",
      "15554 Training Loss: tensor(0.3363)\n",
      "15555 Training Loss: tensor(0.3361)\n",
      "15556 Training Loss: tensor(0.3398)\n",
      "15557 Training Loss: tensor(0.3366)\n",
      "15558 Training Loss: tensor(0.3407)\n",
      "15559 Training Loss: tensor(0.3354)\n",
      "15560 Training Loss: tensor(0.3401)\n",
      "15561 Training Loss: tensor(0.3365)\n",
      "15562 Training Loss: tensor(0.3352)\n",
      "15563 Training Loss: tensor(0.3368)\n",
      "15564 Training Loss: tensor(0.3382)\n",
      "15565 Training Loss: tensor(0.3355)\n",
      "15566 Training Loss: tensor(0.3373)\n",
      "15567 Training Loss: tensor(0.3370)\n",
      "15568 Training Loss: tensor(0.3380)\n",
      "15569 Training Loss: tensor(0.3349)\n",
      "15570 Training Loss: tensor(0.3377)\n",
      "15571 Training Loss: tensor(0.3406)\n",
      "15572 Training Loss: tensor(0.3393)\n",
      "15573 Training Loss: tensor(0.3350)\n",
      "15574 Training Loss: tensor(0.3358)\n",
      "15575 Training Loss: tensor(0.3349)\n",
      "15576 Training Loss: tensor(0.3368)\n",
      "15577 Training Loss: tensor(0.3359)\n",
      "15578 Training Loss: tensor(0.3374)\n",
      "15579 Training Loss: tensor(0.3359)\n",
      "15580 Training Loss: tensor(0.3451)\n",
      "15581 Training Loss: tensor(0.3386)\n",
      "15582 Training Loss: tensor(0.3383)\n",
      "15583 Training Loss: tensor(0.3420)\n",
      "15584 Training Loss: tensor(0.3412)\n",
      "15585 Training Loss: tensor(0.3366)\n",
      "15586 Training Loss: tensor(0.3392)\n",
      "15587 Training Loss: tensor(0.3350)\n",
      "15588 Training Loss: tensor(0.3384)\n",
      "15589 Training Loss: tensor(0.3360)\n",
      "15590 Training Loss: tensor(0.3363)\n",
      "15591 Training Loss: tensor(0.3353)\n",
      "15592 Training Loss: tensor(0.3379)\n",
      "15593 Training Loss: tensor(0.3372)\n",
      "15594 Training Loss: tensor(0.3366)\n",
      "15595 Training Loss: tensor(0.3377)\n",
      "15596 Training Loss: tensor(0.3369)\n",
      "15597 Training Loss: tensor(0.3353)\n",
      "15598 Training Loss: tensor(0.3365)\n",
      "15599 Training Loss: tensor(0.3378)\n",
      "15600 Training Loss: tensor(0.3420)\n",
      "15601 Training Loss: tensor(0.3422)\n",
      "15602 Training Loss: tensor(0.3404)\n",
      "15603 Training Loss: tensor(0.3385)\n",
      "15604 Training Loss: tensor(0.3347)\n",
      "15605 Training Loss: tensor(0.3363)\n",
      "15606 Training Loss: tensor(0.3381)\n",
      "15607 Training Loss: tensor(0.3372)\n",
      "15608 Training Loss: tensor(0.3370)\n",
      "15609 Training Loss: tensor(0.3356)\n",
      "15610 Training Loss: tensor(0.3354)\n",
      "15611 Training Loss: tensor(0.3385)\n",
      "15612 Training Loss: tensor(0.3365)\n",
      "15613 Training Loss: tensor(0.3354)\n",
      "15614 Training Loss: tensor(0.3517)\n",
      "15615 Training Loss: tensor(0.3372)\n",
      "15616 Training Loss: tensor(0.3354)\n",
      "15617 Training Loss: tensor(0.3382)\n",
      "15618 Training Loss: tensor(0.3465)\n",
      "15619 Training Loss: tensor(0.3368)\n",
      "15620 Training Loss: tensor(0.3394)\n",
      "15621 Training Loss: tensor(0.3375)\n",
      "15622 Training Loss: tensor(0.3370)\n",
      "15623 Training Loss: tensor(0.3377)\n",
      "15624 Training Loss: tensor(0.3377)\n",
      "15625 Training Loss: tensor(0.3432)\n",
      "15626 Training Loss: tensor(0.3386)\n",
      "15627 Training Loss: tensor(0.3386)\n",
      "15628 Training Loss: tensor(0.3362)\n",
      "15629 Training Loss: tensor(0.3354)\n",
      "15630 Training Loss: tensor(0.3360)\n",
      "15631 Training Loss: tensor(0.3417)\n",
      "15632 Training Loss: tensor(0.3441)\n",
      "15633 Training Loss: tensor(0.3391)\n",
      "15634 Training Loss: tensor(0.3355)\n",
      "15635 Training Loss: tensor(0.3363)\n",
      "15636 Training Loss: tensor(0.3380)\n",
      "15637 Training Loss: tensor(0.3360)\n",
      "15638 Training Loss: tensor(0.3384)\n",
      "15639 Training Loss: tensor(0.3376)\n",
      "15640 Training Loss: tensor(0.3355)\n",
      "15641 Training Loss: tensor(0.3390)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15642 Training Loss: tensor(0.3392)\n",
      "15643 Training Loss: tensor(0.3367)\n",
      "15644 Training Loss: tensor(0.3361)\n",
      "15645 Training Loss: tensor(0.3388)\n",
      "15646 Training Loss: tensor(0.3398)\n",
      "15647 Training Loss: tensor(0.3406)\n",
      "15648 Training Loss: tensor(0.3378)\n",
      "15649 Training Loss: tensor(0.3405)\n",
      "15650 Training Loss: tensor(0.3368)\n",
      "15651 Training Loss: tensor(0.3496)\n",
      "15652 Training Loss: tensor(0.3359)\n",
      "15653 Training Loss: tensor(0.3347)\n",
      "15654 Training Loss: tensor(0.3382)\n",
      "15655 Training Loss: tensor(0.3353)\n",
      "15656 Training Loss: tensor(0.3387)\n",
      "15657 Training Loss: tensor(0.3367)\n",
      "15658 Training Loss: tensor(0.3384)\n",
      "15659 Training Loss: tensor(0.3392)\n",
      "15660 Training Loss: tensor(0.3370)\n",
      "15661 Training Loss: tensor(0.3371)\n",
      "15662 Training Loss: tensor(0.3379)\n",
      "15663 Training Loss: tensor(0.3373)\n",
      "15664 Training Loss: tensor(0.3365)\n",
      "15665 Training Loss: tensor(0.3374)\n",
      "15666 Training Loss: tensor(0.3389)\n",
      "15667 Training Loss: tensor(0.3368)\n",
      "15668 Training Loss: tensor(0.3424)\n",
      "15669 Training Loss: tensor(0.3368)\n",
      "15670 Training Loss: tensor(0.3374)\n",
      "15671 Training Loss: tensor(0.3382)\n",
      "15672 Training Loss: tensor(0.3350)\n",
      "15673 Training Loss: tensor(0.3357)\n",
      "15674 Training Loss: tensor(0.3379)\n",
      "15675 Training Loss: tensor(0.3379)\n",
      "15676 Training Loss: tensor(0.3416)\n",
      "15677 Training Loss: tensor(0.3378)\n",
      "15678 Training Loss: tensor(0.3419)\n",
      "15679 Training Loss: tensor(0.3362)\n",
      "15680 Training Loss: tensor(0.3353)\n",
      "15681 Training Loss: tensor(0.3346)\n",
      "15682 Training Loss: tensor(0.3370)\n",
      "15683 Training Loss: tensor(0.3346)\n",
      "15684 Training Loss: tensor(0.3365)\n",
      "15685 Training Loss: tensor(0.3388)\n",
      "15686 Training Loss: tensor(0.3395)\n",
      "15687 Training Loss: tensor(0.3357)\n",
      "15688 Training Loss: tensor(0.3401)\n",
      "15689 Training Loss: tensor(0.3360)\n",
      "15690 Training Loss: tensor(0.3376)\n",
      "15691 Training Loss: tensor(0.3392)\n",
      "15692 Training Loss: tensor(0.3372)\n",
      "15693 Training Loss: tensor(0.3357)\n",
      "15694 Training Loss: tensor(0.3349)\n",
      "15695 Training Loss: tensor(0.3378)\n",
      "15696 Training Loss: tensor(0.3355)\n",
      "15697 Training Loss: tensor(0.3396)\n",
      "15698 Training Loss: tensor(0.3358)\n",
      "15699 Training Loss: tensor(0.3418)\n",
      "15700 Training Loss: tensor(0.3434)\n",
      "15701 Training Loss: tensor(0.3376)\n",
      "15702 Training Loss: tensor(0.3390)\n",
      "15703 Training Loss: tensor(0.3358)\n",
      "15704 Training Loss: tensor(0.3377)\n",
      "15705 Training Loss: tensor(0.3349)\n",
      "15706 Training Loss: tensor(0.3385)\n",
      "15707 Training Loss: tensor(0.3389)\n",
      "15708 Training Loss: tensor(0.3359)\n",
      "15709 Training Loss: tensor(0.3391)\n",
      "15710 Training Loss: tensor(0.3381)\n",
      "15711 Training Loss: tensor(0.3397)\n",
      "15712 Training Loss: tensor(0.3380)\n",
      "15713 Training Loss: tensor(0.3369)\n",
      "15714 Training Loss: tensor(0.3393)\n",
      "15715 Training Loss: tensor(0.3349)\n",
      "15716 Training Loss: tensor(0.3356)\n",
      "15717 Training Loss: tensor(0.3406)\n",
      "15718 Training Loss: tensor(0.3354)\n",
      "15719 Training Loss: tensor(0.3357)\n",
      "15720 Training Loss: tensor(0.3347)\n",
      "15721 Training Loss: tensor(0.3369)\n",
      "15722 Training Loss: tensor(0.3352)\n",
      "15723 Training Loss: tensor(0.3373)\n",
      "15724 Training Loss: tensor(0.3390)\n",
      "15725 Training Loss: tensor(0.3356)\n",
      "15726 Training Loss: tensor(0.3366)\n",
      "15727 Training Loss: tensor(0.3370)\n",
      "15728 Training Loss: tensor(0.3377)\n",
      "15729 Training Loss: tensor(0.3374)\n",
      "15730 Training Loss: tensor(0.3396)\n",
      "15731 Training Loss: tensor(0.3424)\n",
      "15732 Training Loss: tensor(0.3353)\n",
      "15733 Training Loss: tensor(0.3350)\n",
      "15734 Training Loss: tensor(0.3357)\n",
      "15735 Training Loss: tensor(0.3374)\n",
      "15736 Training Loss: tensor(0.3360)\n",
      "15737 Training Loss: tensor(0.3413)\n",
      "15738 Training Loss: tensor(0.3380)\n",
      "15739 Training Loss: tensor(0.3381)\n",
      "15740 Training Loss: tensor(0.3346)\n",
      "15741 Training Loss: tensor(0.3402)\n",
      "15742 Training Loss: tensor(0.3378)\n",
      "15743 Training Loss: tensor(0.3379)\n",
      "15744 Training Loss: tensor(0.3367)\n",
      "15745 Training Loss: tensor(0.3354)\n",
      "15746 Training Loss: tensor(0.3362)\n",
      "15747 Training Loss: tensor(0.3362)\n",
      "15748 Training Loss: tensor(0.3377)\n",
      "15749 Training Loss: tensor(0.3358)\n",
      "15750 Training Loss: tensor(0.3346)\n",
      "15751 Training Loss: tensor(0.3371)\n",
      "15752 Training Loss: tensor(0.3359)\n",
      "15753 Training Loss: tensor(0.3396)\n",
      "15754 Training Loss: tensor(0.3369)\n",
      "15755 Training Loss: tensor(0.3348)\n",
      "15756 Training Loss: tensor(0.3377)\n",
      "15757 Training Loss: tensor(0.3355)\n",
      "15758 Training Loss: tensor(0.3371)\n",
      "15759 Training Loss: tensor(0.3361)\n",
      "15760 Training Loss: tensor(0.3352)\n",
      "15761 Training Loss: tensor(0.3375)\n",
      "15762 Training Loss: tensor(0.3379)\n",
      "15763 Training Loss: tensor(0.3373)\n",
      "15764 Training Loss: tensor(0.3355)\n",
      "15765 Training Loss: tensor(0.3355)\n",
      "15766 Training Loss: tensor(0.3408)\n",
      "15767 Training Loss: tensor(0.3397)\n",
      "15768 Training Loss: tensor(0.3385)\n",
      "15769 Training Loss: tensor(0.3357)\n",
      "15770 Training Loss: tensor(0.3347)\n",
      "15771 Training Loss: tensor(0.3365)\n",
      "15772 Training Loss: tensor(0.3374)\n",
      "15773 Training Loss: tensor(0.3376)\n",
      "15774 Training Loss: tensor(0.3369)\n",
      "15775 Training Loss: tensor(0.3360)\n",
      "15776 Training Loss: tensor(0.3360)\n",
      "15777 Training Loss: tensor(0.3377)\n",
      "15778 Training Loss: tensor(0.3367)\n",
      "15779 Training Loss: tensor(0.3368)\n",
      "15780 Training Loss: tensor(0.3354)\n",
      "15781 Training Loss: tensor(0.3402)\n",
      "15782 Training Loss: tensor(0.3358)\n",
      "15783 Training Loss: tensor(0.3351)\n",
      "15784 Training Loss: tensor(0.3381)\n",
      "15785 Training Loss: tensor(0.3353)\n",
      "15786 Training Loss: tensor(0.3354)\n",
      "15787 Training Loss: tensor(0.3397)\n",
      "15788 Training Loss: tensor(0.3356)\n",
      "15789 Training Loss: tensor(0.3360)\n",
      "15790 Training Loss: tensor(0.3350)\n",
      "15791 Training Loss: tensor(0.3380)\n",
      "15792 Training Loss: tensor(0.3411)\n",
      "15793 Training Loss: tensor(0.3370)\n",
      "15794 Training Loss: tensor(0.3395)\n",
      "15795 Training Loss: tensor(0.3427)\n",
      "15796 Training Loss: tensor(0.3389)\n",
      "15797 Training Loss: tensor(0.3357)\n",
      "15798 Training Loss: tensor(0.3368)\n",
      "15799 Training Loss: tensor(0.3370)\n",
      "15800 Training Loss: tensor(0.3364)\n",
      "15801 Training Loss: tensor(0.3372)\n",
      "15802 Training Loss: tensor(0.3409)\n",
      "15803 Training Loss: tensor(0.3381)\n",
      "15804 Training Loss: tensor(0.3373)\n",
      "15805 Training Loss: tensor(0.3373)\n",
      "15806 Training Loss: tensor(0.3362)\n",
      "15807 Training Loss: tensor(0.3363)\n",
      "15808 Training Loss: tensor(0.3353)\n",
      "15809 Training Loss: tensor(0.3370)\n",
      "15810 Training Loss: tensor(0.3360)\n",
      "15811 Training Loss: tensor(0.3353)\n",
      "15812 Training Loss: tensor(0.3404)\n",
      "15813 Training Loss: tensor(0.3377)\n",
      "15814 Training Loss: tensor(0.3361)\n",
      "15815 Training Loss: tensor(0.3353)\n",
      "15816 Training Loss: tensor(0.3412)\n",
      "15817 Training Loss: tensor(0.3347)\n",
      "15818 Training Loss: tensor(0.3395)\n",
      "15819 Training Loss: tensor(0.3396)\n",
      "15820 Training Loss: tensor(0.3360)\n",
      "15821 Training Loss: tensor(0.3420)\n",
      "15822 Training Loss: tensor(0.3358)\n",
      "15823 Training Loss: tensor(0.3361)\n",
      "15824 Training Loss: tensor(0.3353)\n",
      "15825 Training Loss: tensor(0.3363)\n",
      "15826 Training Loss: tensor(0.3366)\n",
      "15827 Training Loss: tensor(0.3368)\n",
      "15828 Training Loss: tensor(0.3416)\n",
      "15829 Training Loss: tensor(0.3467)\n",
      "15830 Training Loss: tensor(0.3361)\n",
      "15831 Training Loss: tensor(0.3376)\n",
      "15832 Training Loss: tensor(0.3355)\n",
      "15833 Training Loss: tensor(0.3398)\n",
      "15834 Training Loss: tensor(0.3393)\n",
      "15835 Training Loss: tensor(0.3383)\n",
      "15836 Training Loss: tensor(0.3360)\n",
      "15837 Training Loss: tensor(0.3386)\n",
      "15838 Training Loss: tensor(0.3352)\n",
      "15839 Training Loss: tensor(0.3363)\n",
      "15840 Training Loss: tensor(0.3348)\n",
      "15841 Training Loss: tensor(0.3361)\n",
      "15842 Training Loss: tensor(0.3366)\n",
      "15843 Training Loss: tensor(0.3376)\n",
      "15844 Training Loss: tensor(0.3371)\n",
      "15845 Training Loss: tensor(0.3435)\n",
      "15846 Training Loss: tensor(0.3360)\n",
      "15847 Training Loss: tensor(0.3375)\n",
      "15848 Training Loss: tensor(0.3426)\n",
      "15849 Training Loss: tensor(0.3357)\n",
      "15850 Training Loss: tensor(0.3345)\n",
      "15851 Training Loss: tensor(0.3353)\n",
      "15852 Training Loss: tensor(0.3359)\n",
      "15853 Training Loss: tensor(0.3378)\n",
      "15854 Training Loss: tensor(0.3351)\n",
      "15855 Training Loss: tensor(0.3378)\n",
      "15856 Training Loss: tensor(0.3343)\n",
      "15857 Training Loss: tensor(0.3366)\n",
      "15858 Training Loss: tensor(0.3379)\n",
      "15859 Training Loss: tensor(0.3359)\n",
      "15860 Training Loss: tensor(0.3388)\n",
      "15861 Training Loss: tensor(0.3348)\n",
      "15862 Training Loss: tensor(0.3388)\n",
      "15863 Training Loss: tensor(0.3458)\n",
      "15864 Training Loss: tensor(0.3349)\n",
      "15865 Training Loss: tensor(0.3396)\n",
      "15866 Training Loss: tensor(0.3364)\n",
      "15867 Training Loss: tensor(0.3358)\n",
      "15868 Training Loss: tensor(0.3355)\n",
      "15869 Training Loss: tensor(0.3366)\n",
      "15870 Training Loss: tensor(0.3396)\n",
      "15871 Training Loss: tensor(0.3421)\n",
      "15872 Training Loss: tensor(0.3452)\n",
      "15873 Training Loss: tensor(0.3350)\n",
      "15874 Training Loss: tensor(0.3350)\n",
      "15875 Training Loss: tensor(0.3343)\n",
      "15876 Training Loss: tensor(0.3385)\n",
      "15877 Training Loss: tensor(0.3370)\n",
      "15878 Training Loss: tensor(0.3347)\n",
      "15879 Training Loss: tensor(0.3391)\n",
      "15880 Training Loss: tensor(0.3350)\n",
      "15881 Training Loss: tensor(0.3386)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15882 Training Loss: tensor(0.3375)\n",
      "15883 Training Loss: tensor(0.3371)\n",
      "15884 Training Loss: tensor(0.3363)\n",
      "15885 Training Loss: tensor(0.3444)\n",
      "15886 Training Loss: tensor(0.3366)\n",
      "15887 Training Loss: tensor(0.3365)\n",
      "15888 Training Loss: tensor(0.3356)\n",
      "15889 Training Loss: tensor(0.3373)\n",
      "15890 Training Loss: tensor(0.3373)\n",
      "15891 Training Loss: tensor(0.3349)\n",
      "15892 Training Loss: tensor(0.3347)\n",
      "15893 Training Loss: tensor(0.3382)\n",
      "15894 Training Loss: tensor(0.3344)\n",
      "15895 Training Loss: tensor(0.3371)\n",
      "15896 Training Loss: tensor(0.3385)\n",
      "15897 Training Loss: tensor(0.3387)\n",
      "15898 Training Loss: tensor(0.3388)\n",
      "15899 Training Loss: tensor(0.3384)\n",
      "15900 Training Loss: tensor(0.3364)\n",
      "15901 Training Loss: tensor(0.3379)\n",
      "15902 Training Loss: tensor(0.3411)\n",
      "15903 Training Loss: tensor(0.3353)\n",
      "15904 Training Loss: tensor(0.3351)\n",
      "15905 Training Loss: tensor(0.3374)\n",
      "15906 Training Loss: tensor(0.3364)\n",
      "15907 Training Loss: tensor(0.3339)\n",
      "15908 Training Loss: tensor(0.3355)\n",
      "15909 Training Loss: tensor(0.3358)\n",
      "15910 Training Loss: tensor(0.3373)\n",
      "15911 Training Loss: tensor(0.3436)\n",
      "15912 Training Loss: tensor(0.3392)\n",
      "15913 Training Loss: tensor(0.3380)\n",
      "15914 Training Loss: tensor(0.3364)\n",
      "15915 Training Loss: tensor(0.3359)\n",
      "15916 Training Loss: tensor(0.3386)\n",
      "15917 Training Loss: tensor(0.3395)\n",
      "15918 Training Loss: tensor(0.3376)\n",
      "15919 Training Loss: tensor(0.3372)\n",
      "15920 Training Loss: tensor(0.3362)\n",
      "15921 Training Loss: tensor(0.3342)\n",
      "15922 Training Loss: tensor(0.3390)\n",
      "15923 Training Loss: tensor(0.3360)\n",
      "15924 Training Loss: tensor(0.3373)\n",
      "15925 Training Loss: tensor(0.3377)\n",
      "15926 Training Loss: tensor(0.3349)\n",
      "15927 Training Loss: tensor(0.3358)\n",
      "15928 Training Loss: tensor(0.3371)\n",
      "15929 Training Loss: tensor(0.3380)\n",
      "15930 Training Loss: tensor(0.3373)\n",
      "15931 Training Loss: tensor(0.3422)\n",
      "15932 Training Loss: tensor(0.3346)\n",
      "15933 Training Loss: tensor(0.3396)\n",
      "15934 Training Loss: tensor(0.3345)\n",
      "15935 Training Loss: tensor(0.3358)\n",
      "15936 Training Loss: tensor(0.3410)\n",
      "15937 Training Loss: tensor(0.3384)\n",
      "15938 Training Loss: tensor(0.3380)\n",
      "15939 Training Loss: tensor(0.3380)\n",
      "15940 Training Loss: tensor(0.3380)\n",
      "15941 Training Loss: tensor(0.3383)\n",
      "15942 Training Loss: tensor(0.3387)\n",
      "15943 Training Loss: tensor(0.3372)\n",
      "15944 Training Loss: tensor(0.3368)\n",
      "15945 Training Loss: tensor(0.3375)\n",
      "15946 Training Loss: tensor(0.3404)\n",
      "15947 Training Loss: tensor(0.3362)\n",
      "15948 Training Loss: tensor(0.3373)\n",
      "15949 Training Loss: tensor(0.3351)\n",
      "15950 Training Loss: tensor(0.3354)\n",
      "15951 Training Loss: tensor(0.3349)\n",
      "15952 Training Loss: tensor(0.3352)\n",
      "15953 Training Loss: tensor(0.3362)\n",
      "15954 Training Loss: tensor(0.3422)\n",
      "15955 Training Loss: tensor(0.3356)\n",
      "15956 Training Loss: tensor(0.3379)\n",
      "15957 Training Loss: tensor(0.3359)\n",
      "15958 Training Loss: tensor(0.3390)\n",
      "15959 Training Loss: tensor(0.3366)\n",
      "15960 Training Loss: tensor(0.3374)\n",
      "15961 Training Loss: tensor(0.3404)\n",
      "15962 Training Loss: tensor(0.3368)\n",
      "15963 Training Loss: tensor(0.3377)\n",
      "15964 Training Loss: tensor(0.3394)\n",
      "15965 Training Loss: tensor(0.3378)\n",
      "15966 Training Loss: tensor(0.3381)\n",
      "15967 Training Loss: tensor(0.3359)\n",
      "15968 Training Loss: tensor(0.3355)\n",
      "15969 Training Loss: tensor(0.3366)\n",
      "15970 Training Loss: tensor(0.3394)\n",
      "15971 Training Loss: tensor(0.3409)\n",
      "15972 Training Loss: tensor(0.3408)\n",
      "15973 Training Loss: tensor(0.3414)\n",
      "15974 Training Loss: tensor(0.3358)\n",
      "15975 Training Loss: tensor(0.3357)\n",
      "15976 Training Loss: tensor(0.3350)\n",
      "15977 Training Loss: tensor(0.3373)\n",
      "15978 Training Loss: tensor(0.3370)\n",
      "15979 Training Loss: tensor(0.3353)\n",
      "15980 Training Loss: tensor(0.3384)\n",
      "15981 Training Loss: tensor(0.3357)\n",
      "15982 Training Loss: tensor(0.3358)\n",
      "15983 Training Loss: tensor(0.3364)\n",
      "15984 Training Loss: tensor(0.3382)\n",
      "15985 Training Loss: tensor(0.3376)\n",
      "15986 Training Loss: tensor(0.3369)\n",
      "15987 Training Loss: tensor(0.3393)\n",
      "15988 Training Loss: tensor(0.3364)\n",
      "15989 Training Loss: tensor(0.3359)\n",
      "15990 Training Loss: tensor(0.3357)\n",
      "15991 Training Loss: tensor(0.3420)\n",
      "15992 Training Loss: tensor(0.3378)\n",
      "15993 Training Loss: tensor(0.3382)\n",
      "15994 Training Loss: tensor(0.3386)\n",
      "15995 Training Loss: tensor(0.3344)\n",
      "15996 Training Loss: tensor(0.3359)\n",
      "15997 Training Loss: tensor(0.3397)\n",
      "15998 Training Loss: tensor(0.3343)\n",
      "15999 Training Loss: tensor(0.3368)\n",
      "16000 Training Loss: tensor(0.3387)\n",
      "16001 Training Loss: tensor(0.3374)\n",
      "16002 Training Loss: tensor(0.3380)\n",
      "16003 Training Loss: tensor(0.3370)\n",
      "16004 Training Loss: tensor(0.3354)\n",
      "16005 Training Loss: tensor(0.3341)\n",
      "16006 Training Loss: tensor(0.3362)\n",
      "16007 Training Loss: tensor(0.3367)\n",
      "16008 Training Loss: tensor(0.3339)\n",
      "16009 Training Loss: tensor(0.3356)\n",
      "16010 Training Loss: tensor(0.3357)\n",
      "16011 Training Loss: tensor(0.3343)\n",
      "16012 Training Loss: tensor(0.3355)\n",
      "16013 Training Loss: tensor(0.3343)\n",
      "16014 Training Loss: tensor(0.3385)\n",
      "16015 Training Loss: tensor(0.3367)\n",
      "16016 Training Loss: tensor(0.3442)\n",
      "16017 Training Loss: tensor(0.3344)\n",
      "16018 Training Loss: tensor(0.3409)\n",
      "16019 Training Loss: tensor(0.3360)\n",
      "16020 Training Loss: tensor(0.3358)\n",
      "16021 Training Loss: tensor(0.3356)\n",
      "16022 Training Loss: tensor(0.3405)\n",
      "16023 Training Loss: tensor(0.3364)\n",
      "16024 Training Loss: tensor(0.3376)\n",
      "16025 Training Loss: tensor(0.3386)\n",
      "16026 Training Loss: tensor(0.3367)\n",
      "16027 Training Loss: tensor(0.3346)\n",
      "16028 Training Loss: tensor(0.3349)\n",
      "16029 Training Loss: tensor(0.3350)\n",
      "16030 Training Loss: tensor(0.3404)\n",
      "16031 Training Loss: tensor(0.3351)\n",
      "16032 Training Loss: tensor(0.3405)\n",
      "16033 Training Loss: tensor(0.3369)\n",
      "16034 Training Loss: tensor(0.3381)\n",
      "16035 Training Loss: tensor(0.3352)\n",
      "16036 Training Loss: tensor(0.3371)\n",
      "16037 Training Loss: tensor(0.3373)\n",
      "16038 Training Loss: tensor(0.3367)\n",
      "16039 Training Loss: tensor(0.3380)\n",
      "16040 Training Loss: tensor(0.3375)\n",
      "16041 Training Loss: tensor(0.3376)\n",
      "16042 Training Loss: tensor(0.3368)\n",
      "16043 Training Loss: tensor(0.3360)\n",
      "16044 Training Loss: tensor(0.3345)\n",
      "16045 Training Loss: tensor(0.3354)\n",
      "16046 Training Loss: tensor(0.3373)\n",
      "16047 Training Loss: tensor(0.3355)\n",
      "16048 Training Loss: tensor(0.3353)\n",
      "16049 Training Loss: tensor(0.3379)\n",
      "16050 Training Loss: tensor(0.3365)\n",
      "16051 Training Loss: tensor(0.3388)\n",
      "16052 Training Loss: tensor(0.3369)\n",
      "16053 Training Loss: tensor(0.3355)\n",
      "16054 Training Loss: tensor(0.3340)\n",
      "16055 Training Loss: tensor(0.3358)\n",
      "16056 Training Loss: tensor(0.3369)\n",
      "16057 Training Loss: tensor(0.3405)\n",
      "16058 Training Loss: tensor(0.3367)\n",
      "16059 Training Loss: tensor(0.3361)\n",
      "16060 Training Loss: tensor(0.3350)\n",
      "16061 Training Loss: tensor(0.3373)\n",
      "16062 Training Loss: tensor(0.3424)\n",
      "16063 Training Loss: tensor(0.3373)\n",
      "16064 Training Loss: tensor(0.3372)\n",
      "16065 Training Loss: tensor(0.3401)\n",
      "16066 Training Loss: tensor(0.3355)\n",
      "16067 Training Loss: tensor(0.3358)\n",
      "16068 Training Loss: tensor(0.3355)\n",
      "16069 Training Loss: tensor(0.3399)\n",
      "16070 Training Loss: tensor(0.3366)\n",
      "16071 Training Loss: tensor(0.3337)\n",
      "16072 Training Loss: tensor(0.3386)\n",
      "16073 Training Loss: tensor(0.3357)\n",
      "16074 Training Loss: tensor(0.3377)\n",
      "16075 Training Loss: tensor(0.3422)\n",
      "16076 Training Loss: tensor(0.3377)\n",
      "16077 Training Loss: tensor(0.3385)\n",
      "16078 Training Loss: tensor(0.3345)\n",
      "16079 Training Loss: tensor(0.3376)\n",
      "16080 Training Loss: tensor(0.3370)\n",
      "16081 Training Loss: tensor(0.3373)\n",
      "16082 Training Loss: tensor(0.3365)\n",
      "16083 Training Loss: tensor(0.3370)\n",
      "16084 Training Loss: tensor(0.3355)\n",
      "16085 Training Loss: tensor(0.3369)\n",
      "16086 Training Loss: tensor(0.3356)\n",
      "16087 Training Loss: tensor(0.3368)\n",
      "16088 Training Loss: tensor(0.3361)\n",
      "16089 Training Loss: tensor(0.3365)\n",
      "16090 Training Loss: tensor(0.3395)\n",
      "16091 Training Loss: tensor(0.3343)\n",
      "16092 Training Loss: tensor(0.3402)\n",
      "16093 Training Loss: tensor(0.3361)\n",
      "16094 Training Loss: tensor(0.3370)\n",
      "16095 Training Loss: tensor(0.3374)\n",
      "16096 Training Loss: tensor(0.3358)\n",
      "16097 Training Loss: tensor(0.3360)\n",
      "16098 Training Loss: tensor(0.3358)\n",
      "16099 Training Loss: tensor(0.3388)\n",
      "16100 Training Loss: tensor(0.3355)\n",
      "16101 Training Loss: tensor(0.3370)\n",
      "16102 Training Loss: tensor(0.3339)\n",
      "16103 Training Loss: tensor(0.3379)\n",
      "16104 Training Loss: tensor(0.3385)\n",
      "16105 Training Loss: tensor(0.3373)\n",
      "16106 Training Loss: tensor(0.3383)\n",
      "16107 Training Loss: tensor(0.3361)\n",
      "16108 Training Loss: tensor(0.3357)\n",
      "16109 Training Loss: tensor(0.3355)\n",
      "16110 Training Loss: tensor(0.3350)\n",
      "16111 Training Loss: tensor(0.3360)\n",
      "16112 Training Loss: tensor(0.3342)\n",
      "16113 Training Loss: tensor(0.3376)\n",
      "16114 Training Loss: tensor(0.3357)\n",
      "16115 Training Loss: tensor(0.3372)\n",
      "16116 Training Loss: tensor(0.3370)\n",
      "16117 Training Loss: tensor(0.3378)\n",
      "16118 Training Loss: tensor(0.3353)\n",
      "16119 Training Loss: tensor(0.3375)\n",
      "16120 Training Loss: tensor(0.3354)\n",
      "16121 Training Loss: tensor(0.3356)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16122 Training Loss: tensor(0.3341)\n",
      "16123 Training Loss: tensor(0.3383)\n",
      "16124 Training Loss: tensor(0.3359)\n",
      "16125 Training Loss: tensor(0.3335)\n",
      "16126 Training Loss: tensor(0.3338)\n",
      "16127 Training Loss: tensor(0.3350)\n",
      "16128 Training Loss: tensor(0.3367)\n",
      "16129 Training Loss: tensor(0.3349)\n",
      "16130 Training Loss: tensor(0.3336)\n",
      "16131 Training Loss: tensor(0.3363)\n",
      "16132 Training Loss: tensor(0.3392)\n",
      "16133 Training Loss: tensor(0.3436)\n",
      "16134 Training Loss: tensor(0.3433)\n",
      "16135 Training Loss: tensor(0.3371)\n",
      "16136 Training Loss: tensor(0.3354)\n",
      "16137 Training Loss: tensor(0.3352)\n",
      "16138 Training Loss: tensor(0.3439)\n",
      "16139 Training Loss: tensor(0.3348)\n",
      "16140 Training Loss: tensor(0.3365)\n",
      "16141 Training Loss: tensor(0.3344)\n",
      "16142 Training Loss: tensor(0.3346)\n",
      "16143 Training Loss: tensor(0.3365)\n",
      "16144 Training Loss: tensor(0.3376)\n",
      "16145 Training Loss: tensor(0.3356)\n",
      "16146 Training Loss: tensor(0.3448)\n",
      "16147 Training Loss: tensor(0.3356)\n",
      "16148 Training Loss: tensor(0.3359)\n",
      "16149 Training Loss: tensor(0.3370)\n",
      "16150 Training Loss: tensor(0.3390)\n",
      "16151 Training Loss: tensor(0.3371)\n",
      "16152 Training Loss: tensor(0.3373)\n",
      "16153 Training Loss: tensor(0.3365)\n",
      "16154 Training Loss: tensor(0.3364)\n",
      "16155 Training Loss: tensor(0.3380)\n",
      "16156 Training Loss: tensor(0.3353)\n",
      "16157 Training Loss: tensor(0.3370)\n",
      "16158 Training Loss: tensor(0.3352)\n",
      "16159 Training Loss: tensor(0.3352)\n",
      "16160 Training Loss: tensor(0.3352)\n",
      "16161 Training Loss: tensor(0.3372)\n",
      "16162 Training Loss: tensor(0.3341)\n",
      "16163 Training Loss: tensor(0.3444)\n",
      "16164 Training Loss: tensor(0.3352)\n",
      "16165 Training Loss: tensor(0.3344)\n",
      "16166 Training Loss: tensor(0.3342)\n",
      "16167 Training Loss: tensor(0.3382)\n",
      "16168 Training Loss: tensor(0.3370)\n",
      "16169 Training Loss: tensor(0.3361)\n",
      "16170 Training Loss: tensor(0.3389)\n",
      "16171 Training Loss: tensor(0.3364)\n",
      "16172 Training Loss: tensor(0.3441)\n",
      "16173 Training Loss: tensor(0.3360)\n",
      "16174 Training Loss: tensor(0.3392)\n",
      "16175 Training Loss: tensor(0.3349)\n",
      "16176 Training Loss: tensor(0.3390)\n",
      "16177 Training Loss: tensor(0.3359)\n",
      "16178 Training Loss: tensor(0.3359)\n",
      "16179 Training Loss: tensor(0.3363)\n",
      "16180 Training Loss: tensor(0.3391)\n",
      "16181 Training Loss: tensor(0.3371)\n",
      "16182 Training Loss: tensor(0.3353)\n",
      "16183 Training Loss: tensor(0.3355)\n",
      "16184 Training Loss: tensor(0.3344)\n",
      "16185 Training Loss: tensor(0.3360)\n",
      "16186 Training Loss: tensor(0.3377)\n",
      "16187 Training Loss: tensor(0.3365)\n",
      "16188 Training Loss: tensor(0.3357)\n",
      "16189 Training Loss: tensor(0.3352)\n",
      "16190 Training Loss: tensor(0.3402)\n",
      "16191 Training Loss: tensor(0.3354)\n",
      "16192 Training Loss: tensor(0.3403)\n",
      "16193 Training Loss: tensor(0.3371)\n",
      "16194 Training Loss: tensor(0.3340)\n",
      "16195 Training Loss: tensor(0.3402)\n",
      "16196 Training Loss: tensor(0.3358)\n",
      "16197 Training Loss: tensor(0.3353)\n",
      "16198 Training Loss: tensor(0.3381)\n",
      "16199 Training Loss: tensor(0.3387)\n",
      "16200 Training Loss: tensor(0.3386)\n",
      "16201 Training Loss: tensor(0.3361)\n",
      "16202 Training Loss: tensor(0.3357)\n",
      "16203 Training Loss: tensor(0.3375)\n",
      "16204 Training Loss: tensor(0.3363)\n",
      "16205 Training Loss: tensor(0.3400)\n",
      "16206 Training Loss: tensor(0.3359)\n",
      "16207 Training Loss: tensor(0.3356)\n",
      "16208 Training Loss: tensor(0.3382)\n",
      "16209 Training Loss: tensor(0.3366)\n",
      "16210 Training Loss: tensor(0.3367)\n",
      "16211 Training Loss: tensor(0.3367)\n",
      "16212 Training Loss: tensor(0.3373)\n",
      "16213 Training Loss: tensor(0.3368)\n",
      "16214 Training Loss: tensor(0.3347)\n",
      "16215 Training Loss: tensor(0.3375)\n",
      "16216 Training Loss: tensor(0.3359)\n",
      "16217 Training Loss: tensor(0.3363)\n",
      "16218 Training Loss: tensor(0.3355)\n",
      "16219 Training Loss: tensor(0.3338)\n",
      "16220 Training Loss: tensor(0.3352)\n",
      "16221 Training Loss: tensor(0.3356)\n",
      "16222 Training Loss: tensor(0.3351)\n",
      "16223 Training Loss: tensor(0.3373)\n",
      "16224 Training Loss: tensor(0.3354)\n",
      "16225 Training Loss: tensor(0.3363)\n",
      "16226 Training Loss: tensor(0.3345)\n",
      "16227 Training Loss: tensor(0.3346)\n",
      "16228 Training Loss: tensor(0.3409)\n",
      "16229 Training Loss: tensor(0.3338)\n",
      "16230 Training Loss: tensor(0.3346)\n",
      "16231 Training Loss: tensor(0.3334)\n",
      "16232 Training Loss: tensor(0.3363)\n",
      "16233 Training Loss: tensor(0.3347)\n",
      "16234 Training Loss: tensor(0.3355)\n",
      "16235 Training Loss: tensor(0.3349)\n",
      "16236 Training Loss: tensor(0.3346)\n",
      "16237 Training Loss: tensor(0.3346)\n",
      "16238 Training Loss: tensor(0.3345)\n",
      "16239 Training Loss: tensor(0.3356)\n",
      "16240 Training Loss: tensor(0.3348)\n",
      "16241 Training Loss: tensor(0.3332)\n",
      "16242 Training Loss: tensor(0.3342)\n",
      "16243 Training Loss: tensor(0.3346)\n",
      "16244 Training Loss: tensor(0.3330)\n",
      "16245 Training Loss: tensor(0.3340)\n",
      "16246 Training Loss: tensor(0.3347)\n",
      "16247 Training Loss: tensor(0.3346)\n",
      "16248 Training Loss: tensor(0.3437)\n",
      "16249 Training Loss: tensor(0.3377)\n",
      "16250 Training Loss: tensor(0.3350)\n",
      "16251 Training Loss: tensor(0.3344)\n",
      "16252 Training Loss: tensor(0.3391)\n",
      "16253 Training Loss: tensor(0.3337)\n",
      "16254 Training Loss: tensor(0.3373)\n",
      "16255 Training Loss: tensor(0.3345)\n",
      "16256 Training Loss: tensor(0.3356)\n",
      "16257 Training Loss: tensor(0.3340)\n",
      "16258 Training Loss: tensor(0.3359)\n",
      "16259 Training Loss: tensor(0.3442)\n",
      "16260 Training Loss: tensor(0.3411)\n",
      "16261 Training Loss: tensor(0.3382)\n",
      "16262 Training Loss: tensor(0.3373)\n",
      "16263 Training Loss: tensor(0.3342)\n",
      "16264 Training Loss: tensor(0.3347)\n",
      "16265 Training Loss: tensor(0.3337)\n",
      "16266 Training Loss: tensor(0.3346)\n",
      "16267 Training Loss: tensor(0.3358)\n",
      "16268 Training Loss: tensor(0.3358)\n",
      "16269 Training Loss: tensor(0.3337)\n",
      "16270 Training Loss: tensor(0.3351)\n",
      "16271 Training Loss: tensor(0.3340)\n",
      "16272 Training Loss: tensor(0.3403)\n",
      "16273 Training Loss: tensor(0.3369)\n",
      "16274 Training Loss: tensor(0.3365)\n",
      "16275 Training Loss: tensor(0.3336)\n",
      "16276 Training Loss: tensor(0.3403)\n",
      "16277 Training Loss: tensor(0.3331)\n",
      "16278 Training Loss: tensor(0.3351)\n",
      "16279 Training Loss: tensor(0.3347)\n",
      "16280 Training Loss: tensor(0.3350)\n",
      "16281 Training Loss: tensor(0.3334)\n",
      "16282 Training Loss: tensor(0.3375)\n",
      "16283 Training Loss: tensor(0.3375)\n",
      "16284 Training Loss: tensor(0.3351)\n",
      "16285 Training Loss: tensor(0.3376)\n",
      "16286 Training Loss: tensor(0.3380)\n",
      "16287 Training Loss: tensor(0.3333)\n",
      "16288 Training Loss: tensor(0.3393)\n",
      "16289 Training Loss: tensor(0.3367)\n",
      "16290 Training Loss: tensor(0.3328)\n",
      "16291 Training Loss: tensor(0.3451)\n",
      "16292 Training Loss: tensor(0.3395)\n",
      "16293 Training Loss: tensor(0.3368)\n",
      "16294 Training Loss: tensor(0.3363)\n",
      "16295 Training Loss: tensor(0.3417)\n",
      "16296 Training Loss: tensor(0.3381)\n",
      "16297 Training Loss: tensor(0.3368)\n",
      "16298 Training Loss: tensor(0.3353)\n",
      "16299 Training Loss: tensor(0.3371)\n",
      "16300 Training Loss: tensor(0.3358)\n",
      "16301 Training Loss: tensor(0.3366)\n",
      "16302 Training Loss: tensor(0.3356)\n",
      "16303 Training Loss: tensor(0.3364)\n",
      "16304 Training Loss: tensor(0.3378)\n",
      "16305 Training Loss: tensor(0.3359)\n",
      "16306 Training Loss: tensor(0.3371)\n",
      "16307 Training Loss: tensor(0.3351)\n",
      "16308 Training Loss: tensor(0.3352)\n",
      "16309 Training Loss: tensor(0.3335)\n",
      "16310 Training Loss: tensor(0.3362)\n",
      "16311 Training Loss: tensor(0.3332)\n",
      "16312 Training Loss: tensor(0.3338)\n",
      "16313 Training Loss: tensor(0.3342)\n",
      "16314 Training Loss: tensor(0.3422)\n",
      "16315 Training Loss: tensor(0.3424)\n",
      "16316 Training Loss: tensor(0.3392)\n",
      "16317 Training Loss: tensor(0.3345)\n",
      "16318 Training Loss: tensor(0.3335)\n",
      "16319 Training Loss: tensor(0.3391)\n",
      "16320 Training Loss: tensor(0.3374)\n",
      "16321 Training Loss: tensor(0.3402)\n",
      "16322 Training Loss: tensor(0.3435)\n",
      "16323 Training Loss: tensor(0.3391)\n",
      "16324 Training Loss: tensor(0.3366)\n",
      "16325 Training Loss: tensor(0.3370)\n",
      "16326 Training Loss: tensor(0.3378)\n",
      "16327 Training Loss: tensor(0.3390)\n",
      "16328 Training Loss: tensor(0.3356)\n",
      "16329 Training Loss: tensor(0.3371)\n",
      "16330 Training Loss: tensor(0.3380)\n",
      "16331 Training Loss: tensor(0.3354)\n",
      "16332 Training Loss: tensor(0.3370)\n",
      "16333 Training Loss: tensor(0.3346)\n",
      "16334 Training Loss: tensor(0.3365)\n",
      "16335 Training Loss: tensor(0.3370)\n",
      "16336 Training Loss: tensor(0.3360)\n",
      "16337 Training Loss: tensor(0.3361)\n",
      "16338 Training Loss: tensor(0.3356)\n",
      "16339 Training Loss: tensor(0.3349)\n",
      "16340 Training Loss: tensor(0.3382)\n",
      "16341 Training Loss: tensor(0.3348)\n",
      "16342 Training Loss: tensor(0.3342)\n",
      "16343 Training Loss: tensor(0.3358)\n",
      "16344 Training Loss: tensor(0.3351)\n",
      "16345 Training Loss: tensor(0.3358)\n",
      "16346 Training Loss: tensor(0.3354)\n",
      "16347 Training Loss: tensor(0.3343)\n",
      "16348 Training Loss: tensor(0.3377)\n",
      "16349 Training Loss: tensor(0.3376)\n",
      "16350 Training Loss: tensor(0.3389)\n",
      "16351 Training Loss: tensor(0.3353)\n",
      "16352 Training Loss: tensor(0.3341)\n",
      "16353 Training Loss: tensor(0.3383)\n",
      "16354 Training Loss: tensor(0.3348)\n",
      "16355 Training Loss: tensor(0.3380)\n",
      "16356 Training Loss: tensor(0.3343)\n",
      "16357 Training Loss: tensor(0.3337)\n",
      "16358 Training Loss: tensor(0.3342)\n",
      "16359 Training Loss: tensor(0.3338)\n",
      "16360 Training Loss: tensor(0.3440)\n",
      "16361 Training Loss: tensor(0.3358)\n",
      "16362 Training Loss: tensor(0.3354)\n",
      "16363 Training Loss: tensor(0.3360)\n",
      "16364 Training Loss: tensor(0.3366)\n",
      "16365 Training Loss: tensor(0.3390)\n",
      "16366 Training Loss: tensor(0.3360)\n",
      "16367 Training Loss: tensor(0.3341)\n",
      "16368 Training Loss: tensor(0.3376)\n",
      "16369 Training Loss: tensor(0.3355)\n",
      "16370 Training Loss: tensor(0.3353)\n",
      "16371 Training Loss: tensor(0.3341)\n",
      "16372 Training Loss: tensor(0.3376)\n",
      "16373 Training Loss: tensor(0.3353)\n",
      "16374 Training Loss: tensor(0.3355)\n",
      "16375 Training Loss: tensor(0.3371)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16376 Training Loss: tensor(0.3352)\n",
      "16377 Training Loss: tensor(0.3347)\n",
      "16378 Training Loss: tensor(0.3365)\n",
      "16379 Training Loss: tensor(0.3350)\n",
      "16380 Training Loss: tensor(0.3343)\n",
      "16381 Training Loss: tensor(0.3348)\n",
      "16382 Training Loss: tensor(0.3348)\n",
      "16383 Training Loss: tensor(0.3351)\n",
      "16384 Training Loss: tensor(0.3400)\n",
      "16385 Training Loss: tensor(0.3361)\n",
      "16386 Training Loss: tensor(0.3361)\n",
      "16387 Training Loss: tensor(0.3390)\n",
      "16388 Training Loss: tensor(0.3344)\n",
      "16389 Training Loss: tensor(0.3386)\n",
      "16390 Training Loss: tensor(0.3353)\n",
      "16391 Training Loss: tensor(0.3350)\n",
      "16392 Training Loss: tensor(0.3365)\n",
      "16393 Training Loss: tensor(0.3355)\n",
      "16394 Training Loss: tensor(0.3382)\n",
      "16395 Training Loss: tensor(0.3370)\n",
      "16396 Training Loss: tensor(0.3348)\n",
      "16397 Training Loss: tensor(0.3339)\n",
      "16398 Training Loss: tensor(0.3339)\n",
      "16399 Training Loss: tensor(0.3367)\n",
      "16400 Training Loss: tensor(0.3370)\n",
      "16401 Training Loss: tensor(0.3365)\n",
      "16402 Training Loss: tensor(0.3357)\n",
      "16403 Training Loss: tensor(0.3393)\n",
      "16404 Training Loss: tensor(0.3339)\n",
      "16405 Training Loss: tensor(0.3370)\n",
      "16406 Training Loss: tensor(0.3368)\n",
      "16407 Training Loss: tensor(0.3360)\n",
      "16408 Training Loss: tensor(0.3344)\n",
      "16409 Training Loss: tensor(0.3349)\n",
      "16410 Training Loss: tensor(0.3375)\n",
      "16411 Training Loss: tensor(0.3372)\n",
      "16412 Training Loss: tensor(0.3352)\n",
      "16413 Training Loss: tensor(0.3350)\n",
      "16414 Training Loss: tensor(0.3359)\n",
      "16415 Training Loss: tensor(0.3440)\n",
      "16416 Training Loss: tensor(0.3380)\n",
      "16417 Training Loss: tensor(0.3359)\n",
      "16418 Training Loss: tensor(0.3352)\n",
      "16419 Training Loss: tensor(0.3380)\n",
      "16420 Training Loss: tensor(0.3366)\n",
      "16421 Training Loss: tensor(0.3367)\n",
      "16422 Training Loss: tensor(0.3365)\n",
      "16423 Training Loss: tensor(0.3348)\n",
      "16424 Training Loss: tensor(0.3351)\n",
      "16425 Training Loss: tensor(0.3355)\n",
      "16426 Training Loss: tensor(0.3364)\n",
      "16427 Training Loss: tensor(0.3349)\n",
      "16428 Training Loss: tensor(0.3481)\n",
      "16429 Training Loss: tensor(0.3341)\n",
      "16430 Training Loss: tensor(0.3348)\n",
      "16431 Training Loss: tensor(0.3354)\n",
      "16432 Training Loss: tensor(0.3379)\n",
      "16433 Training Loss: tensor(0.3351)\n",
      "16434 Training Loss: tensor(0.3356)\n",
      "16435 Training Loss: tensor(0.3360)\n",
      "16436 Training Loss: tensor(0.3346)\n",
      "16437 Training Loss: tensor(0.3359)\n",
      "16438 Training Loss: tensor(0.3368)\n",
      "16439 Training Loss: tensor(0.3385)\n",
      "16440 Training Loss: tensor(0.3362)\n",
      "16441 Training Loss: tensor(0.3338)\n",
      "16442 Training Loss: tensor(0.3347)\n",
      "16443 Training Loss: tensor(0.3350)\n",
      "16444 Training Loss: tensor(0.3341)\n",
      "16445 Training Loss: tensor(0.3334)\n",
      "16446 Training Loss: tensor(0.3365)\n",
      "16447 Training Loss: tensor(0.3348)\n",
      "16448 Training Loss: tensor(0.3347)\n",
      "16449 Training Loss: tensor(0.3367)\n",
      "16450 Training Loss: tensor(0.3400)\n",
      "16451 Training Loss: tensor(0.3346)\n",
      "16452 Training Loss: tensor(0.3352)\n",
      "16453 Training Loss: tensor(0.3348)\n",
      "16454 Training Loss: tensor(0.3371)\n",
      "16455 Training Loss: tensor(0.3366)\n",
      "16456 Training Loss: tensor(0.3371)\n",
      "16457 Training Loss: tensor(0.3330)\n",
      "16458 Training Loss: tensor(0.3339)\n",
      "16459 Training Loss: tensor(0.3345)\n",
      "16460 Training Loss: tensor(0.3334)\n",
      "16461 Training Loss: tensor(0.3341)\n",
      "16462 Training Loss: tensor(0.3342)\n",
      "16463 Training Loss: tensor(0.3333)\n",
      "16464 Training Loss: tensor(0.3337)\n",
      "16465 Training Loss: tensor(0.3332)\n",
      "16466 Training Loss: tensor(0.3334)\n",
      "16467 Training Loss: tensor(0.3358)\n",
      "16468 Training Loss: tensor(0.3353)\n",
      "16469 Training Loss: tensor(0.3386)\n",
      "16470 Training Loss: tensor(0.3336)\n",
      "16471 Training Loss: tensor(0.3350)\n",
      "16472 Training Loss: tensor(0.3380)\n",
      "16473 Training Loss: tensor(0.3395)\n",
      "16474 Training Loss: tensor(0.3384)\n",
      "16475 Training Loss: tensor(0.3400)\n",
      "16476 Training Loss: tensor(0.3374)\n",
      "16477 Training Loss: tensor(0.3356)\n",
      "16478 Training Loss: tensor(0.3362)\n",
      "16479 Training Loss: tensor(0.3353)\n",
      "16480 Training Loss: tensor(0.3356)\n",
      "16481 Training Loss: tensor(0.3336)\n",
      "16482 Training Loss: tensor(0.3342)\n",
      "16483 Training Loss: tensor(0.3342)\n",
      "16484 Training Loss: tensor(0.3359)\n",
      "16485 Training Loss: tensor(0.3349)\n",
      "16486 Training Loss: tensor(0.3432)\n",
      "16487 Training Loss: tensor(0.3336)\n",
      "16488 Training Loss: tensor(0.3332)\n",
      "16489 Training Loss: tensor(0.3356)\n",
      "16490 Training Loss: tensor(0.3348)\n",
      "16491 Training Loss: tensor(0.3357)\n",
      "16492 Training Loss: tensor(0.3350)\n",
      "16493 Training Loss: tensor(0.3405)\n",
      "16494 Training Loss: tensor(0.3359)\n",
      "16495 Training Loss: tensor(0.3355)\n",
      "16496 Training Loss: tensor(0.3377)\n",
      "16497 Training Loss: tensor(0.3405)\n",
      "16498 Training Loss: tensor(0.3414)\n",
      "16499 Training Loss: tensor(0.3361)\n",
      "16500 Training Loss: tensor(0.3350)\n",
      "16501 Training Loss: tensor(0.3346)\n",
      "16502 Training Loss: tensor(0.3360)\n",
      "16503 Training Loss: tensor(0.3336)\n",
      "16504 Training Loss: tensor(0.3352)\n",
      "16505 Training Loss: tensor(0.3372)\n",
      "16506 Training Loss: tensor(0.3347)\n",
      "16507 Training Loss: tensor(0.3349)\n",
      "16508 Training Loss: tensor(0.3334)\n",
      "16509 Training Loss: tensor(0.3390)\n",
      "16510 Training Loss: tensor(0.3352)\n",
      "16511 Training Loss: tensor(0.3355)\n",
      "16512 Training Loss: tensor(0.3344)\n",
      "16513 Training Loss: tensor(0.3392)\n",
      "16514 Training Loss: tensor(0.3348)\n",
      "16515 Training Loss: tensor(0.3369)\n",
      "16516 Training Loss: tensor(0.3364)\n",
      "16517 Training Loss: tensor(0.3370)\n",
      "16518 Training Loss: tensor(0.3365)\n",
      "16519 Training Loss: tensor(0.3347)\n",
      "16520 Training Loss: tensor(0.3356)\n",
      "16521 Training Loss: tensor(0.3351)\n",
      "16522 Training Loss: tensor(0.3356)\n",
      "16523 Training Loss: tensor(0.3358)\n",
      "16524 Training Loss: tensor(0.3355)\n",
      "16525 Training Loss: tensor(0.3356)\n",
      "16526 Training Loss: tensor(0.3357)\n",
      "16527 Training Loss: tensor(0.3345)\n",
      "16528 Training Loss: tensor(0.3343)\n",
      "16529 Training Loss: tensor(0.3354)\n",
      "16530 Training Loss: tensor(0.3347)\n",
      "16531 Training Loss: tensor(0.3409)\n",
      "16532 Training Loss: tensor(0.3344)\n",
      "16533 Training Loss: tensor(0.3352)\n",
      "16534 Training Loss: tensor(0.3365)\n",
      "16535 Training Loss: tensor(0.3352)\n",
      "16536 Training Loss: tensor(0.3355)\n",
      "16537 Training Loss: tensor(0.3384)\n",
      "16538 Training Loss: tensor(0.3355)\n",
      "16539 Training Loss: tensor(0.3349)\n",
      "16540 Training Loss: tensor(0.3363)\n",
      "16541 Training Loss: tensor(0.3352)\n",
      "16542 Training Loss: tensor(0.3352)\n",
      "16543 Training Loss: tensor(0.3353)\n",
      "16544 Training Loss: tensor(0.3353)\n",
      "16545 Training Loss: tensor(0.3350)\n",
      "16546 Training Loss: tensor(0.3358)\n",
      "16547 Training Loss: tensor(0.3345)\n",
      "16548 Training Loss: tensor(0.3344)\n",
      "16549 Training Loss: tensor(0.3353)\n",
      "16550 Training Loss: tensor(0.3363)\n",
      "16551 Training Loss: tensor(0.3334)\n",
      "16552 Training Loss: tensor(0.3408)\n",
      "16553 Training Loss: tensor(0.3363)\n",
      "16554 Training Loss: tensor(0.3364)\n",
      "16555 Training Loss: tensor(0.3334)\n",
      "16556 Training Loss: tensor(0.3347)\n",
      "16557 Training Loss: tensor(0.3332)\n",
      "16558 Training Loss: tensor(0.3340)\n",
      "16559 Training Loss: tensor(0.3362)\n",
      "16560 Training Loss: tensor(0.3341)\n",
      "16561 Training Loss: tensor(0.3362)\n",
      "16562 Training Loss: tensor(0.3370)\n",
      "16563 Training Loss: tensor(0.3356)\n",
      "16564 Training Loss: tensor(0.3338)\n",
      "16565 Training Loss: tensor(0.3345)\n",
      "16566 Training Loss: tensor(0.3345)\n",
      "16567 Training Loss: tensor(0.3336)\n",
      "16568 Training Loss: tensor(0.3378)\n",
      "16569 Training Loss: tensor(0.3389)\n",
      "16570 Training Loss: tensor(0.3337)\n",
      "16571 Training Loss: tensor(0.3351)\n",
      "16572 Training Loss: tensor(0.3349)\n",
      "16573 Training Loss: tensor(0.3335)\n",
      "16574 Training Loss: tensor(0.3341)\n",
      "16575 Training Loss: tensor(0.3344)\n",
      "16576 Training Loss: tensor(0.3337)\n",
      "16577 Training Loss: tensor(0.3374)\n",
      "16578 Training Loss: tensor(0.3336)\n",
      "16579 Training Loss: tensor(0.3352)\n",
      "16580 Training Loss: tensor(0.3364)\n",
      "16581 Training Loss: tensor(0.3344)\n",
      "16582 Training Loss: tensor(0.3354)\n",
      "16583 Training Loss: tensor(0.3365)\n",
      "16584 Training Loss: tensor(0.3345)\n",
      "16585 Training Loss: tensor(0.3345)\n",
      "16586 Training Loss: tensor(0.3371)\n",
      "16587 Training Loss: tensor(0.3343)\n",
      "16588 Training Loss: tensor(0.3341)\n",
      "16589 Training Loss: tensor(0.3390)\n",
      "16590 Training Loss: tensor(0.3347)\n",
      "16591 Training Loss: tensor(0.3349)\n",
      "16592 Training Loss: tensor(0.3335)\n",
      "16593 Training Loss: tensor(0.3345)\n",
      "16594 Training Loss: tensor(0.3358)\n",
      "16595 Training Loss: tensor(0.3379)\n",
      "16596 Training Loss: tensor(0.3356)\n",
      "16597 Training Loss: tensor(0.3370)\n",
      "16598 Training Loss: tensor(0.3345)\n",
      "16599 Training Loss: tensor(0.3373)\n",
      "16600 Training Loss: tensor(0.3366)\n",
      "16601 Training Loss: tensor(0.3371)\n",
      "16602 Training Loss: tensor(0.3345)\n",
      "16603 Training Loss: tensor(0.3373)\n",
      "16604 Training Loss: tensor(0.3352)\n",
      "16605 Training Loss: tensor(0.3353)\n",
      "16606 Training Loss: tensor(0.3355)\n",
      "16607 Training Loss: tensor(0.3349)\n",
      "16608 Training Loss: tensor(0.3386)\n",
      "16609 Training Loss: tensor(0.3367)\n",
      "16610 Training Loss: tensor(0.3359)\n",
      "16611 Training Loss: tensor(0.3357)\n",
      "16612 Training Loss: tensor(0.3338)\n",
      "16613 Training Loss: tensor(0.3348)\n",
      "16614 Training Loss: tensor(0.3335)\n",
      "16615 Training Loss: tensor(0.3375)\n",
      "16616 Training Loss: tensor(0.3374)\n",
      "16617 Training Loss: tensor(0.3345)\n",
      "16618 Training Loss: tensor(0.3352)\n",
      "16619 Training Loss: tensor(0.3343)\n",
      "16620 Training Loss: tensor(0.3365)\n",
      "16621 Training Loss: tensor(0.3358)\n",
      "16622 Training Loss: tensor(0.3350)\n",
      "16623 Training Loss: tensor(0.3344)\n",
      "16624 Training Loss: tensor(0.3328)\n",
      "16625 Training Loss: tensor(0.3334)\n",
      "16626 Training Loss: tensor(0.3337)\n",
      "16627 Training Loss: tensor(0.3419)\n",
      "16628 Training Loss: tensor(0.3385)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16629 Training Loss: tensor(0.3344)\n",
      "16630 Training Loss: tensor(0.3344)\n",
      "16631 Training Loss: tensor(0.3363)\n",
      "16632 Training Loss: tensor(0.3337)\n",
      "16633 Training Loss: tensor(0.3352)\n",
      "16634 Training Loss: tensor(0.3340)\n",
      "16635 Training Loss: tensor(0.3354)\n",
      "16636 Training Loss: tensor(0.3354)\n",
      "16637 Training Loss: tensor(0.3380)\n",
      "16638 Training Loss: tensor(0.3358)\n",
      "16639 Training Loss: tensor(0.3345)\n",
      "16640 Training Loss: tensor(0.3352)\n",
      "16641 Training Loss: tensor(0.3339)\n",
      "16642 Training Loss: tensor(0.3346)\n",
      "16643 Training Loss: tensor(0.3331)\n",
      "16644 Training Loss: tensor(0.3371)\n",
      "16645 Training Loss: tensor(0.3421)\n",
      "16646 Training Loss: tensor(0.3331)\n",
      "16647 Training Loss: tensor(0.3345)\n",
      "16648 Training Loss: tensor(0.3369)\n",
      "16649 Training Loss: tensor(0.3380)\n",
      "16650 Training Loss: tensor(0.3334)\n",
      "16651 Training Loss: tensor(0.3359)\n",
      "16652 Training Loss: tensor(0.3349)\n",
      "16653 Training Loss: tensor(0.3336)\n",
      "16654 Training Loss: tensor(0.3346)\n",
      "16655 Training Loss: tensor(0.3361)\n",
      "16656 Training Loss: tensor(0.3336)\n",
      "16657 Training Loss: tensor(0.3339)\n",
      "16658 Training Loss: tensor(0.3345)\n",
      "16659 Training Loss: tensor(0.3405)\n",
      "16660 Training Loss: tensor(0.3376)\n",
      "16661 Training Loss: tensor(0.3333)\n",
      "16662 Training Loss: tensor(0.3399)\n",
      "16663 Training Loss: tensor(0.3353)\n",
      "16664 Training Loss: tensor(0.3329)\n",
      "16665 Training Loss: tensor(0.3380)\n",
      "16666 Training Loss: tensor(0.3344)\n",
      "16667 Training Loss: tensor(0.3361)\n",
      "16668 Training Loss: tensor(0.3356)\n",
      "16669 Training Loss: tensor(0.3357)\n",
      "16670 Training Loss: tensor(0.3377)\n",
      "16671 Training Loss: tensor(0.3332)\n",
      "16672 Training Loss: tensor(0.3413)\n",
      "16673 Training Loss: tensor(0.3353)\n",
      "16674 Training Loss: tensor(0.3376)\n",
      "16675 Training Loss: tensor(0.3348)\n",
      "16676 Training Loss: tensor(0.3361)\n",
      "16677 Training Loss: tensor(0.3356)\n",
      "16678 Training Loss: tensor(0.3333)\n",
      "16679 Training Loss: tensor(0.3358)\n",
      "16680 Training Loss: tensor(0.3345)\n",
      "16681 Training Loss: tensor(0.3355)\n",
      "16682 Training Loss: tensor(0.3366)\n",
      "16683 Training Loss: tensor(0.3394)\n",
      "16684 Training Loss: tensor(0.3363)\n",
      "16685 Training Loss: tensor(0.3342)\n",
      "16686 Training Loss: tensor(0.3352)\n",
      "16687 Training Loss: tensor(0.3374)\n",
      "16688 Training Loss: tensor(0.3353)\n",
      "16689 Training Loss: tensor(0.3361)\n",
      "16690 Training Loss: tensor(0.3386)\n",
      "16691 Training Loss: tensor(0.3345)\n",
      "16692 Training Loss: tensor(0.3386)\n",
      "16693 Training Loss: tensor(0.3346)\n",
      "16694 Training Loss: tensor(0.3374)\n",
      "16695 Training Loss: tensor(0.3334)\n",
      "16696 Training Loss: tensor(0.3344)\n",
      "16697 Training Loss: tensor(0.3345)\n",
      "16698 Training Loss: tensor(0.3340)\n",
      "16699 Training Loss: tensor(0.3350)\n",
      "16700 Training Loss: tensor(0.3337)\n",
      "16701 Training Loss: tensor(0.3335)\n",
      "16702 Training Loss: tensor(0.3337)\n",
      "16703 Training Loss: tensor(0.3335)\n",
      "16704 Training Loss: tensor(0.3360)\n",
      "16705 Training Loss: tensor(0.3350)\n",
      "16706 Training Loss: tensor(0.3348)\n",
      "16707 Training Loss: tensor(0.3350)\n",
      "16708 Training Loss: tensor(0.3401)\n",
      "16709 Training Loss: tensor(0.3339)\n",
      "16710 Training Loss: tensor(0.3341)\n",
      "16711 Training Loss: tensor(0.3344)\n",
      "16712 Training Loss: tensor(0.3332)\n",
      "16713 Training Loss: tensor(0.3358)\n",
      "16714 Training Loss: tensor(0.3359)\n",
      "16715 Training Loss: tensor(0.3348)\n",
      "16716 Training Loss: tensor(0.3345)\n",
      "16717 Training Loss: tensor(0.3362)\n",
      "16718 Training Loss: tensor(0.3367)\n",
      "16719 Training Loss: tensor(0.3328)\n",
      "16720 Training Loss: tensor(0.3359)\n",
      "16721 Training Loss: tensor(0.3354)\n",
      "16722 Training Loss: tensor(0.3343)\n",
      "16723 Training Loss: tensor(0.3349)\n",
      "16724 Training Loss: tensor(0.3340)\n",
      "16725 Training Loss: tensor(0.3334)\n",
      "16726 Training Loss: tensor(0.3369)\n",
      "16727 Training Loss: tensor(0.3377)\n",
      "16728 Training Loss: tensor(0.3327)\n",
      "16729 Training Loss: tensor(0.3345)\n",
      "16730 Training Loss: tensor(0.3343)\n",
      "16731 Training Loss: tensor(0.3350)\n",
      "16732 Training Loss: tensor(0.3359)\n",
      "16733 Training Loss: tensor(0.3327)\n",
      "16734 Training Loss: tensor(0.3328)\n",
      "16735 Training Loss: tensor(0.3345)\n",
      "16736 Training Loss: tensor(0.3370)\n",
      "16737 Training Loss: tensor(0.3352)\n",
      "16738 Training Loss: tensor(0.3337)\n",
      "16739 Training Loss: tensor(0.3330)\n",
      "16740 Training Loss: tensor(0.3347)\n",
      "16741 Training Loss: tensor(0.3349)\n",
      "16742 Training Loss: tensor(0.3356)\n",
      "16743 Training Loss: tensor(0.3353)\n",
      "16744 Training Loss: tensor(0.3346)\n",
      "16745 Training Loss: tensor(0.3357)\n",
      "16746 Training Loss: tensor(0.3335)\n",
      "16747 Training Loss: tensor(0.3334)\n",
      "16748 Training Loss: tensor(0.3333)\n",
      "16749 Training Loss: tensor(0.3418)\n",
      "16750 Training Loss: tensor(0.3331)\n",
      "16751 Training Loss: tensor(0.3364)\n",
      "16752 Training Loss: tensor(0.3346)\n",
      "16753 Training Loss: tensor(0.3357)\n",
      "16754 Training Loss: tensor(0.3365)\n",
      "16755 Training Loss: tensor(0.3371)\n",
      "16756 Training Loss: tensor(0.3359)\n",
      "16757 Training Loss: tensor(0.3360)\n",
      "16758 Training Loss: tensor(0.3434)\n",
      "16759 Training Loss: tensor(0.3348)\n",
      "16760 Training Loss: tensor(0.3347)\n",
      "16761 Training Loss: tensor(0.3353)\n",
      "16762 Training Loss: tensor(0.3348)\n",
      "16763 Training Loss: tensor(0.3443)\n",
      "16764 Training Loss: tensor(0.3378)\n",
      "16765 Training Loss: tensor(0.3406)\n",
      "16766 Training Loss: tensor(0.3340)\n",
      "16767 Training Loss: tensor(0.3337)\n",
      "16768 Training Loss: tensor(0.3376)\n",
      "16769 Training Loss: tensor(0.3380)\n",
      "16770 Training Loss: tensor(0.3357)\n",
      "16771 Training Loss: tensor(0.3348)\n",
      "16772 Training Loss: tensor(0.3343)\n",
      "16773 Training Loss: tensor(0.3355)\n",
      "16774 Training Loss: tensor(0.3351)\n",
      "16775 Training Loss: tensor(0.3346)\n",
      "16776 Training Loss: tensor(0.3362)\n",
      "16777 Training Loss: tensor(0.3340)\n",
      "16778 Training Loss: tensor(0.3362)\n",
      "16779 Training Loss: tensor(0.3384)\n",
      "16780 Training Loss: tensor(0.3340)\n",
      "16781 Training Loss: tensor(0.3344)\n",
      "16782 Training Loss: tensor(0.3335)\n",
      "16783 Training Loss: tensor(0.3337)\n",
      "16784 Training Loss: tensor(0.3348)\n",
      "16785 Training Loss: tensor(0.3366)\n",
      "16786 Training Loss: tensor(0.3338)\n",
      "16787 Training Loss: tensor(0.3334)\n",
      "16788 Training Loss: tensor(0.3362)\n",
      "16789 Training Loss: tensor(0.3358)\n",
      "16790 Training Loss: tensor(0.3362)\n",
      "16791 Training Loss: tensor(0.3384)\n",
      "16792 Training Loss: tensor(0.3338)\n",
      "16793 Training Loss: tensor(0.3462)\n",
      "16794 Training Loss: tensor(0.3359)\n",
      "16795 Training Loss: tensor(0.3326)\n",
      "16796 Training Loss: tensor(0.3357)\n",
      "16797 Training Loss: tensor(0.3368)\n",
      "16798 Training Loss: tensor(0.3388)\n",
      "16799 Training Loss: tensor(0.3369)\n",
      "16800 Training Loss: tensor(0.3373)\n",
      "16801 Training Loss: tensor(0.3357)\n",
      "16802 Training Loss: tensor(0.3355)\n",
      "16803 Training Loss: tensor(0.3371)\n",
      "16804 Training Loss: tensor(0.3348)\n",
      "16805 Training Loss: tensor(0.3360)\n",
      "16806 Training Loss: tensor(0.3357)\n",
      "16807 Training Loss: tensor(0.3369)\n",
      "16808 Training Loss: tensor(0.3347)\n",
      "16809 Training Loss: tensor(0.3357)\n",
      "16810 Training Loss: tensor(0.3425)\n",
      "16811 Training Loss: tensor(0.3361)\n",
      "16812 Training Loss: tensor(0.3338)\n",
      "16813 Training Loss: tensor(0.3347)\n",
      "16814 Training Loss: tensor(0.3334)\n",
      "16815 Training Loss: tensor(0.3344)\n",
      "16816 Training Loss: tensor(0.3336)\n",
      "16817 Training Loss: tensor(0.3352)\n",
      "16818 Training Loss: tensor(0.3326)\n",
      "16819 Training Loss: tensor(0.3374)\n",
      "16820 Training Loss: tensor(0.3323)\n",
      "16821 Training Loss: tensor(0.3348)\n",
      "16822 Training Loss: tensor(0.3358)\n",
      "16823 Training Loss: tensor(0.3351)\n",
      "16824 Training Loss: tensor(0.3356)\n",
      "16825 Training Loss: tensor(0.3352)\n",
      "16826 Training Loss: tensor(0.3334)\n",
      "16827 Training Loss: tensor(0.3339)\n",
      "16828 Training Loss: tensor(0.3338)\n",
      "16829 Training Loss: tensor(0.3337)\n",
      "16830 Training Loss: tensor(0.3332)\n",
      "16831 Training Loss: tensor(0.3346)\n",
      "16832 Training Loss: tensor(0.3365)\n",
      "16833 Training Loss: tensor(0.3372)\n",
      "16834 Training Loss: tensor(0.3343)\n",
      "16835 Training Loss: tensor(0.3347)\n",
      "16836 Training Loss: tensor(0.3343)\n",
      "16837 Training Loss: tensor(0.3351)\n",
      "16838 Training Loss: tensor(0.3342)\n",
      "16839 Training Loss: tensor(0.3353)\n",
      "16840 Training Loss: tensor(0.3341)\n",
      "16841 Training Loss: tensor(0.3338)\n",
      "16842 Training Loss: tensor(0.3334)\n",
      "16843 Training Loss: tensor(0.3400)\n",
      "16844 Training Loss: tensor(0.3373)\n",
      "16845 Training Loss: tensor(0.3347)\n",
      "16846 Training Loss: tensor(0.3364)\n",
      "16847 Training Loss: tensor(0.3344)\n",
      "16848 Training Loss: tensor(0.3346)\n",
      "16849 Training Loss: tensor(0.3346)\n",
      "16850 Training Loss: tensor(0.3325)\n",
      "16851 Training Loss: tensor(0.3358)\n",
      "16852 Training Loss: tensor(0.3408)\n",
      "16853 Training Loss: tensor(0.3359)\n",
      "16854 Training Loss: tensor(0.3362)\n",
      "16855 Training Loss: tensor(0.3360)\n",
      "16856 Training Loss: tensor(0.3367)\n",
      "16857 Training Loss: tensor(0.3376)\n",
      "16858 Training Loss: tensor(0.3351)\n",
      "16859 Training Loss: tensor(0.3361)\n",
      "16860 Training Loss: tensor(0.3368)\n",
      "16861 Training Loss: tensor(0.3393)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16862 Training Loss: tensor(0.3341)\n",
      "16863 Training Loss: tensor(0.3337)\n",
      "16864 Training Loss: tensor(0.3346)\n",
      "16865 Training Loss: tensor(0.3385)\n",
      "16866 Training Loss: tensor(0.3351)\n",
      "16867 Training Loss: tensor(0.3322)\n",
      "16868 Training Loss: tensor(0.3331)\n",
      "16869 Training Loss: tensor(0.3355)\n",
      "16870 Training Loss: tensor(0.3373)\n",
      "16871 Training Loss: tensor(0.3333)\n",
      "16872 Training Loss: tensor(0.3382)\n",
      "16873 Training Loss: tensor(0.3343)\n",
      "16874 Training Loss: tensor(0.3373)\n",
      "16875 Training Loss: tensor(0.3364)\n",
      "16876 Training Loss: tensor(0.3428)\n",
      "16877 Training Loss: tensor(0.3349)\n",
      "16878 Training Loss: tensor(0.3341)\n",
      "16879 Training Loss: tensor(0.3345)\n",
      "16880 Training Loss: tensor(0.3399)\n",
      "16881 Training Loss: tensor(0.3352)\n",
      "16882 Training Loss: tensor(0.3366)\n",
      "16883 Training Loss: tensor(0.3347)\n",
      "16884 Training Loss: tensor(0.3366)\n",
      "16885 Training Loss: tensor(0.3353)\n",
      "16886 Training Loss: tensor(0.3340)\n",
      "16887 Training Loss: tensor(0.3356)\n",
      "16888 Training Loss: tensor(0.3346)\n",
      "16889 Training Loss: tensor(0.3392)\n",
      "16890 Training Loss: tensor(0.3369)\n",
      "16891 Training Loss: tensor(0.3343)\n",
      "16892 Training Loss: tensor(0.3360)\n",
      "16893 Training Loss: tensor(0.3334)\n",
      "16894 Training Loss: tensor(0.3355)\n",
      "16895 Training Loss: tensor(0.3347)\n",
      "16896 Training Loss: tensor(0.3342)\n",
      "16897 Training Loss: tensor(0.3332)\n",
      "16898 Training Loss: tensor(0.3352)\n",
      "16899 Training Loss: tensor(0.3351)\n",
      "16900 Training Loss: tensor(0.3361)\n",
      "16901 Training Loss: tensor(0.3341)\n",
      "16902 Training Loss: tensor(0.3332)\n",
      "16903 Training Loss: tensor(0.3360)\n",
      "16904 Training Loss: tensor(0.3338)\n",
      "16905 Training Loss: tensor(0.3346)\n",
      "16906 Training Loss: tensor(0.3402)\n",
      "16907 Training Loss: tensor(0.3357)\n",
      "16908 Training Loss: tensor(0.3340)\n",
      "16909 Training Loss: tensor(0.3333)\n",
      "16910 Training Loss: tensor(0.3367)\n",
      "16911 Training Loss: tensor(0.3361)\n",
      "16912 Training Loss: tensor(0.3338)\n",
      "16913 Training Loss: tensor(0.3362)\n",
      "16914 Training Loss: tensor(0.3389)\n",
      "16915 Training Loss: tensor(0.3386)\n",
      "16916 Training Loss: tensor(0.3370)\n",
      "16917 Training Loss: tensor(0.3376)\n",
      "16918 Training Loss: tensor(0.3348)\n",
      "16919 Training Loss: tensor(0.3340)\n",
      "16920 Training Loss: tensor(0.3341)\n",
      "16921 Training Loss: tensor(0.3341)\n",
      "16922 Training Loss: tensor(0.3350)\n",
      "16923 Training Loss: tensor(0.3364)\n",
      "16924 Training Loss: tensor(0.3369)\n",
      "16925 Training Loss: tensor(0.3349)\n",
      "16926 Training Loss: tensor(0.3350)\n",
      "16927 Training Loss: tensor(0.3333)\n",
      "16928 Training Loss: tensor(0.3337)\n",
      "16929 Training Loss: tensor(0.3336)\n",
      "16930 Training Loss: tensor(0.3381)\n",
      "16931 Training Loss: tensor(0.3355)\n",
      "16932 Training Loss: tensor(0.3347)\n",
      "16933 Training Loss: tensor(0.3413)\n",
      "16934 Training Loss: tensor(0.3329)\n",
      "16935 Training Loss: tensor(0.3343)\n",
      "16936 Training Loss: tensor(0.3332)\n",
      "16937 Training Loss: tensor(0.3329)\n",
      "16938 Training Loss: tensor(0.3371)\n",
      "16939 Training Loss: tensor(0.3353)\n",
      "16940 Training Loss: tensor(0.3382)\n",
      "16941 Training Loss: tensor(0.3342)\n",
      "16942 Training Loss: tensor(0.3335)\n",
      "16943 Training Loss: tensor(0.3372)\n",
      "16944 Training Loss: tensor(0.3350)\n",
      "16945 Training Loss: tensor(0.3382)\n",
      "16946 Training Loss: tensor(0.3344)\n",
      "16947 Training Loss: tensor(0.3379)\n",
      "16948 Training Loss: tensor(0.3412)\n",
      "16949 Training Loss: tensor(0.3365)\n",
      "16950 Training Loss: tensor(0.3346)\n",
      "16951 Training Loss: tensor(0.3342)\n",
      "16952 Training Loss: tensor(0.3334)\n",
      "16953 Training Loss: tensor(0.3396)\n",
      "16954 Training Loss: tensor(0.3386)\n",
      "16955 Training Loss: tensor(0.3340)\n",
      "16956 Training Loss: tensor(0.3354)\n",
      "16957 Training Loss: tensor(0.3351)\n",
      "16958 Training Loss: tensor(0.3333)\n",
      "16959 Training Loss: tensor(0.3344)\n",
      "16960 Training Loss: tensor(0.3361)\n",
      "16961 Training Loss: tensor(0.3347)\n",
      "16962 Training Loss: tensor(0.3325)\n",
      "16963 Training Loss: tensor(0.3352)\n",
      "16964 Training Loss: tensor(0.3337)\n",
      "16965 Training Loss: tensor(0.3397)\n",
      "16966 Training Loss: tensor(0.3345)\n",
      "16967 Training Loss: tensor(0.3325)\n",
      "16968 Training Loss: tensor(0.3337)\n",
      "16969 Training Loss: tensor(0.3344)\n",
      "16970 Training Loss: tensor(0.3344)\n",
      "16971 Training Loss: tensor(0.3340)\n",
      "16972 Training Loss: tensor(0.3368)\n",
      "16973 Training Loss: tensor(0.3354)\n",
      "16974 Training Loss: tensor(0.3348)\n",
      "16975 Training Loss: tensor(0.3334)\n",
      "16976 Training Loss: tensor(0.3344)\n",
      "16977 Training Loss: tensor(0.3324)\n",
      "16978 Training Loss: tensor(0.3386)\n",
      "16979 Training Loss: tensor(0.3383)\n",
      "16980 Training Loss: tensor(0.3345)\n",
      "16981 Training Loss: tensor(0.3398)\n",
      "16982 Training Loss: tensor(0.3358)\n",
      "16983 Training Loss: tensor(0.3417)\n",
      "16984 Training Loss: tensor(0.3365)\n",
      "16985 Training Loss: tensor(0.3335)\n",
      "16986 Training Loss: tensor(0.3355)\n",
      "16987 Training Loss: tensor(0.3343)\n",
      "16988 Training Loss: tensor(0.3350)\n",
      "16989 Training Loss: tensor(0.3365)\n",
      "16990 Training Loss: tensor(0.3352)\n",
      "16991 Training Loss: tensor(0.3351)\n",
      "16992 Training Loss: tensor(0.3361)\n",
      "16993 Training Loss: tensor(0.3334)\n",
      "16994 Training Loss: tensor(0.3349)\n",
      "16995 Training Loss: tensor(0.3368)\n",
      "16996 Training Loss: tensor(0.3361)\n",
      "16997 Training Loss: tensor(0.3357)\n",
      "16998 Training Loss: tensor(0.3363)\n",
      "16999 Training Loss: tensor(0.3349)\n",
      "17000 Training Loss: tensor(0.3328)\n",
      "17001 Training Loss: tensor(0.3364)\n",
      "17002 Training Loss: tensor(0.3347)\n",
      "17003 Training Loss: tensor(0.3336)\n",
      "17004 Training Loss: tensor(0.3341)\n",
      "17005 Training Loss: tensor(0.3402)\n",
      "17006 Training Loss: tensor(0.3333)\n",
      "17007 Training Loss: tensor(0.3340)\n",
      "17008 Training Loss: tensor(0.3336)\n",
      "17009 Training Loss: tensor(0.3382)\n",
      "17010 Training Loss: tensor(0.3347)\n",
      "17011 Training Loss: tensor(0.3327)\n",
      "17012 Training Loss: tensor(0.3357)\n",
      "17013 Training Loss: tensor(0.3329)\n",
      "17014 Training Loss: tensor(0.3331)\n",
      "17015 Training Loss: tensor(0.3346)\n",
      "17016 Training Loss: tensor(0.3335)\n",
      "17017 Training Loss: tensor(0.3339)\n",
      "17018 Training Loss: tensor(0.3343)\n",
      "17019 Training Loss: tensor(0.3329)\n",
      "17020 Training Loss: tensor(0.3328)\n",
      "17021 Training Loss: tensor(0.3383)\n",
      "17022 Training Loss: tensor(0.3341)\n",
      "17023 Training Loss: tensor(0.3339)\n",
      "17024 Training Loss: tensor(0.3336)\n",
      "17025 Training Loss: tensor(0.3339)\n",
      "17026 Training Loss: tensor(0.3329)\n",
      "17027 Training Loss: tensor(0.3360)\n",
      "17028 Training Loss: tensor(0.3357)\n",
      "17029 Training Loss: tensor(0.3338)\n",
      "17030 Training Loss: tensor(0.3356)\n",
      "17031 Training Loss: tensor(0.3350)\n",
      "17032 Training Loss: tensor(0.3349)\n",
      "17033 Training Loss: tensor(0.3333)\n",
      "17034 Training Loss: tensor(0.3351)\n",
      "17035 Training Loss: tensor(0.3357)\n",
      "17036 Training Loss: tensor(0.3375)\n",
      "17037 Training Loss: tensor(0.3356)\n",
      "17038 Training Loss: tensor(0.3346)\n",
      "17039 Training Loss: tensor(0.3378)\n",
      "17040 Training Loss: tensor(0.3365)\n",
      "17041 Training Loss: tensor(0.3355)\n",
      "17042 Training Loss: tensor(0.3364)\n",
      "17043 Training Loss: tensor(0.3369)\n",
      "17044 Training Loss: tensor(0.3348)\n",
      "17045 Training Loss: tensor(0.3386)\n",
      "17046 Training Loss: tensor(0.3337)\n",
      "17047 Training Loss: tensor(0.3341)\n",
      "17048 Training Loss: tensor(0.3332)\n",
      "17049 Training Loss: tensor(0.3359)\n",
      "17050 Training Loss: tensor(0.3353)\n",
      "17051 Training Loss: tensor(0.3366)\n",
      "17052 Training Loss: tensor(0.3361)\n",
      "17053 Training Loss: tensor(0.3347)\n",
      "17054 Training Loss: tensor(0.3407)\n",
      "17055 Training Loss: tensor(0.3354)\n",
      "17056 Training Loss: tensor(0.3326)\n",
      "17057 Training Loss: tensor(0.3341)\n",
      "17058 Training Loss: tensor(0.3331)\n",
      "17059 Training Loss: tensor(0.3330)\n",
      "17060 Training Loss: tensor(0.3334)\n",
      "17061 Training Loss: tensor(0.3357)\n",
      "17062 Training Loss: tensor(0.3341)\n",
      "17063 Training Loss: tensor(0.3391)\n",
      "17064 Training Loss: tensor(0.3369)\n",
      "17065 Training Loss: tensor(0.3339)\n",
      "17066 Training Loss: tensor(0.3356)\n",
      "17067 Training Loss: tensor(0.3331)\n",
      "17068 Training Loss: tensor(0.3322)\n",
      "17069 Training Loss: tensor(0.3333)\n",
      "17070 Training Loss: tensor(0.3319)\n",
      "17071 Training Loss: tensor(0.3347)\n",
      "17072 Training Loss: tensor(0.3389)\n",
      "17073 Training Loss: tensor(0.3335)\n",
      "17074 Training Loss: tensor(0.3341)\n",
      "17075 Training Loss: tensor(0.3350)\n",
      "17076 Training Loss: tensor(0.3337)\n",
      "17077 Training Loss: tensor(0.3358)\n",
      "17078 Training Loss: tensor(0.3358)\n",
      "17079 Training Loss: tensor(0.3384)\n",
      "17080 Training Loss: tensor(0.3347)\n",
      "17081 Training Loss: tensor(0.3336)\n",
      "17082 Training Loss: tensor(0.3345)\n",
      "17083 Training Loss: tensor(0.3347)\n",
      "17084 Training Loss: tensor(0.3332)\n",
      "17085 Training Loss: tensor(0.3353)\n",
      "17086 Training Loss: tensor(0.3347)\n",
      "17087 Training Loss: tensor(0.3336)\n",
      "17088 Training Loss: tensor(0.3319)\n",
      "17089 Training Loss: tensor(0.3353)\n",
      "17090 Training Loss: tensor(0.3331)\n",
      "17091 Training Loss: tensor(0.3333)\n",
      "17092 Training Loss: tensor(0.3331)\n",
      "17093 Training Loss: tensor(0.3328)\n",
      "17094 Training Loss: tensor(0.3378)\n",
      "17095 Training Loss: tensor(0.3322)\n",
      "17096 Training Loss: tensor(0.3384)\n",
      "17097 Training Loss: tensor(0.3355)\n",
      "17098 Training Loss: tensor(0.3423)\n",
      "17099 Training Loss: tensor(0.3335)\n",
      "17100 Training Loss: tensor(0.3334)\n",
      "17101 Training Loss: tensor(0.3364)\n",
      "17102 Training Loss: tensor(0.3334)\n",
      "17103 Training Loss: tensor(0.3359)\n",
      "17104 Training Loss: tensor(0.3343)\n",
      "17105 Training Loss: tensor(0.3333)\n",
      "17106 Training Loss: tensor(0.3364)\n",
      "17107 Training Loss: tensor(0.3363)\n",
      "17108 Training Loss: tensor(0.3348)\n",
      "17109 Training Loss: tensor(0.3356)\n",
      "17110 Training Loss: tensor(0.3332)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17111 Training Loss: tensor(0.3346)\n",
      "17112 Training Loss: tensor(0.3339)\n",
      "17113 Training Loss: tensor(0.3347)\n",
      "17114 Training Loss: tensor(0.3328)\n",
      "17115 Training Loss: tensor(0.3357)\n",
      "17116 Training Loss: tensor(0.3339)\n",
      "17117 Training Loss: tensor(0.3414)\n",
      "17118 Training Loss: tensor(0.3377)\n",
      "17119 Training Loss: tensor(0.3335)\n",
      "17120 Training Loss: tensor(0.3351)\n",
      "17121 Training Loss: tensor(0.3374)\n",
      "17122 Training Loss: tensor(0.3366)\n",
      "17123 Training Loss: tensor(0.3337)\n",
      "17124 Training Loss: tensor(0.3355)\n",
      "17125 Training Loss: tensor(0.3358)\n",
      "17126 Training Loss: tensor(0.3337)\n",
      "17127 Training Loss: tensor(0.3333)\n",
      "17128 Training Loss: tensor(0.3366)\n",
      "17129 Training Loss: tensor(0.3375)\n",
      "17130 Training Loss: tensor(0.3342)\n",
      "17131 Training Loss: tensor(0.3345)\n",
      "17132 Training Loss: tensor(0.3336)\n",
      "17133 Training Loss: tensor(0.3336)\n",
      "17134 Training Loss: tensor(0.3329)\n",
      "17135 Training Loss: tensor(0.3347)\n",
      "17136 Training Loss: tensor(0.3350)\n",
      "17137 Training Loss: tensor(0.3395)\n",
      "17138 Training Loss: tensor(0.3346)\n",
      "17139 Training Loss: tensor(0.3352)\n",
      "17140 Training Loss: tensor(0.3367)\n",
      "17141 Training Loss: tensor(0.3356)\n",
      "17142 Training Loss: tensor(0.3346)\n",
      "17143 Training Loss: tensor(0.3351)\n",
      "17144 Training Loss: tensor(0.3349)\n",
      "17145 Training Loss: tensor(0.3339)\n",
      "17146 Training Loss: tensor(0.3344)\n",
      "17147 Training Loss: tensor(0.3360)\n",
      "17148 Training Loss: tensor(0.3375)\n",
      "17149 Training Loss: tensor(0.3336)\n",
      "17150 Training Loss: tensor(0.3351)\n",
      "17151 Training Loss: tensor(0.3335)\n",
      "17152 Training Loss: tensor(0.3361)\n",
      "17153 Training Loss: tensor(0.3334)\n",
      "17154 Training Loss: tensor(0.3373)\n",
      "17155 Training Loss: tensor(0.3333)\n",
      "17156 Training Loss: tensor(0.3338)\n",
      "17157 Training Loss: tensor(0.3335)\n",
      "17158 Training Loss: tensor(0.3342)\n",
      "17159 Training Loss: tensor(0.3349)\n",
      "17160 Training Loss: tensor(0.3342)\n",
      "17161 Training Loss: tensor(0.3339)\n",
      "17162 Training Loss: tensor(0.3338)\n",
      "17163 Training Loss: tensor(0.3379)\n",
      "17164 Training Loss: tensor(0.3364)\n",
      "17165 Training Loss: tensor(0.3403)\n",
      "17166 Training Loss: tensor(0.3337)\n",
      "17167 Training Loss: tensor(0.3360)\n",
      "17168 Training Loss: tensor(0.3329)\n",
      "17169 Training Loss: tensor(0.3348)\n",
      "17170 Training Loss: tensor(0.3335)\n",
      "17171 Training Loss: tensor(0.3351)\n",
      "17172 Training Loss: tensor(0.3360)\n",
      "17173 Training Loss: tensor(0.3341)\n",
      "17174 Training Loss: tensor(0.3363)\n",
      "17175 Training Loss: tensor(0.3358)\n",
      "17176 Training Loss: tensor(0.3358)\n",
      "17177 Training Loss: tensor(0.3346)\n",
      "17178 Training Loss: tensor(0.3327)\n",
      "17179 Training Loss: tensor(0.3335)\n",
      "17180 Training Loss: tensor(0.3414)\n",
      "17181 Training Loss: tensor(0.3330)\n",
      "17182 Training Loss: tensor(0.3349)\n",
      "17183 Training Loss: tensor(0.3372)\n",
      "17184 Training Loss: tensor(0.3342)\n",
      "17185 Training Loss: tensor(0.3334)\n",
      "17186 Training Loss: tensor(0.3331)\n",
      "17187 Training Loss: tensor(0.3338)\n",
      "17188 Training Loss: tensor(0.3411)\n",
      "17189 Training Loss: tensor(0.3342)\n",
      "17190 Training Loss: tensor(0.3331)\n",
      "17191 Training Loss: tensor(0.3397)\n",
      "17192 Training Loss: tensor(0.3343)\n",
      "17193 Training Loss: tensor(0.3335)\n",
      "17194 Training Loss: tensor(0.3330)\n",
      "17195 Training Loss: tensor(0.3325)\n",
      "17196 Training Loss: tensor(0.3337)\n",
      "17197 Training Loss: tensor(0.3354)\n",
      "17198 Training Loss: tensor(0.3339)\n",
      "17199 Training Loss: tensor(0.3386)\n",
      "17200 Training Loss: tensor(0.3357)\n",
      "17201 Training Loss: tensor(0.3346)\n",
      "17202 Training Loss: tensor(0.3327)\n",
      "17203 Training Loss: tensor(0.3345)\n",
      "17204 Training Loss: tensor(0.3341)\n",
      "17205 Training Loss: tensor(0.3353)\n",
      "17206 Training Loss: tensor(0.3323)\n",
      "17207 Training Loss: tensor(0.3352)\n",
      "17208 Training Loss: tensor(0.3357)\n",
      "17209 Training Loss: tensor(0.3329)\n",
      "17210 Training Loss: tensor(0.3408)\n",
      "17211 Training Loss: tensor(0.3339)\n",
      "17212 Training Loss: tensor(0.3365)\n",
      "17213 Training Loss: tensor(0.3338)\n",
      "17214 Training Loss: tensor(0.3349)\n",
      "17215 Training Loss: tensor(0.3330)\n",
      "17216 Training Loss: tensor(0.3327)\n",
      "17217 Training Loss: tensor(0.3380)\n",
      "17218 Training Loss: tensor(0.3395)\n",
      "17219 Training Loss: tensor(0.3355)\n",
      "17220 Training Loss: tensor(0.3341)\n",
      "17221 Training Loss: tensor(0.3343)\n",
      "17222 Training Loss: tensor(0.3357)\n",
      "17223 Training Loss: tensor(0.3347)\n",
      "17224 Training Loss: tensor(0.3362)\n",
      "17225 Training Loss: tensor(0.3341)\n",
      "17226 Training Loss: tensor(0.3357)\n",
      "17227 Training Loss: tensor(0.3373)\n",
      "17228 Training Loss: tensor(0.3353)\n",
      "17229 Training Loss: tensor(0.3388)\n",
      "17230 Training Loss: tensor(0.3359)\n",
      "17231 Training Loss: tensor(0.3340)\n",
      "17232 Training Loss: tensor(0.3328)\n",
      "17233 Training Loss: tensor(0.3343)\n",
      "17234 Training Loss: tensor(0.3349)\n",
      "17235 Training Loss: tensor(0.3336)\n",
      "17236 Training Loss: tensor(0.3363)\n",
      "17237 Training Loss: tensor(0.3365)\n",
      "17238 Training Loss: tensor(0.3372)\n",
      "17239 Training Loss: tensor(0.3361)\n",
      "17240 Training Loss: tensor(0.3335)\n",
      "17241 Training Loss: tensor(0.3361)\n",
      "17242 Training Loss: tensor(0.3346)\n",
      "17243 Training Loss: tensor(0.3341)\n",
      "17244 Training Loss: tensor(0.3341)\n",
      "17245 Training Loss: tensor(0.3351)\n",
      "17246 Training Loss: tensor(0.3335)\n",
      "17247 Training Loss: tensor(0.3350)\n",
      "17248 Training Loss: tensor(0.3343)\n",
      "17249 Training Loss: tensor(0.3373)\n",
      "17250 Training Loss: tensor(0.3365)\n",
      "17251 Training Loss: tensor(0.3346)\n",
      "17252 Training Loss: tensor(0.3332)\n",
      "17253 Training Loss: tensor(0.3339)\n",
      "17254 Training Loss: tensor(0.3362)\n",
      "17255 Training Loss: tensor(0.3342)\n",
      "17256 Training Loss: tensor(0.3364)\n",
      "17257 Training Loss: tensor(0.3341)\n",
      "17258 Training Loss: tensor(0.3339)\n",
      "17259 Training Loss: tensor(0.3341)\n",
      "17260 Training Loss: tensor(0.3345)\n",
      "17261 Training Loss: tensor(0.3330)\n",
      "17262 Training Loss: tensor(0.3322)\n",
      "17263 Training Loss: tensor(0.3326)\n",
      "17264 Training Loss: tensor(0.3351)\n",
      "17265 Training Loss: tensor(0.3328)\n",
      "17266 Training Loss: tensor(0.3331)\n",
      "17267 Training Loss: tensor(0.3324)\n",
      "17268 Training Loss: tensor(0.3338)\n",
      "17269 Training Loss: tensor(0.3361)\n",
      "17270 Training Loss: tensor(0.3350)\n",
      "17271 Training Loss: tensor(0.3336)\n",
      "17272 Training Loss: tensor(0.3354)\n",
      "17273 Training Loss: tensor(0.3338)\n",
      "17274 Training Loss: tensor(0.3330)\n",
      "17275 Training Loss: tensor(0.3323)\n",
      "17276 Training Loss: tensor(0.3335)\n",
      "17277 Training Loss: tensor(0.3313)\n",
      "17278 Training Loss: tensor(0.3331)\n",
      "17279 Training Loss: tensor(0.3329)\n",
      "17280 Training Loss: tensor(0.3346)\n",
      "17281 Training Loss: tensor(0.3328)\n",
      "17282 Training Loss: tensor(0.3348)\n",
      "17283 Training Loss: tensor(0.3326)\n",
      "17284 Training Loss: tensor(0.3333)\n",
      "17285 Training Loss: tensor(0.3334)\n",
      "17286 Training Loss: tensor(0.3394)\n",
      "17287 Training Loss: tensor(0.3340)\n",
      "17288 Training Loss: tensor(0.3341)\n",
      "17289 Training Loss: tensor(0.3334)\n",
      "17290 Training Loss: tensor(0.3338)\n",
      "17291 Training Loss: tensor(0.3352)\n",
      "17292 Training Loss: tensor(0.3346)\n",
      "17293 Training Loss: tensor(0.3350)\n",
      "17294 Training Loss: tensor(0.3348)\n",
      "17295 Training Loss: tensor(0.3332)\n",
      "17296 Training Loss: tensor(0.3368)\n",
      "17297 Training Loss: tensor(0.3328)\n",
      "17298 Training Loss: tensor(0.3360)\n",
      "17299 Training Loss: tensor(0.3348)\n",
      "17300 Training Loss: tensor(0.3332)\n",
      "17301 Training Loss: tensor(0.3363)\n",
      "17302 Training Loss: tensor(0.3354)\n",
      "17303 Training Loss: tensor(0.3400)\n",
      "17304 Training Loss: tensor(0.3346)\n",
      "17305 Training Loss: tensor(0.3391)\n",
      "17306 Training Loss: tensor(0.3331)\n",
      "17307 Training Loss: tensor(0.3377)\n",
      "17308 Training Loss: tensor(0.3358)\n",
      "17309 Training Loss: tensor(0.3338)\n",
      "17310 Training Loss: tensor(0.3373)\n",
      "17311 Training Loss: tensor(0.3361)\n",
      "17312 Training Loss: tensor(0.3361)\n",
      "17313 Training Loss: tensor(0.3344)\n",
      "17314 Training Loss: tensor(0.3351)\n",
      "17315 Training Loss: tensor(0.3355)\n",
      "17316 Training Loss: tensor(0.3368)\n",
      "17317 Training Loss: tensor(0.3379)\n",
      "17318 Training Loss: tensor(0.3328)\n",
      "17319 Training Loss: tensor(0.3326)\n",
      "17320 Training Loss: tensor(0.3333)\n",
      "17321 Training Loss: tensor(0.3357)\n",
      "17322 Training Loss: tensor(0.3338)\n",
      "17323 Training Loss: tensor(0.3348)\n",
      "17324 Training Loss: tensor(0.3345)\n",
      "17325 Training Loss: tensor(0.3336)\n",
      "17326 Training Loss: tensor(0.3328)\n",
      "17327 Training Loss: tensor(0.3338)\n",
      "17328 Training Loss: tensor(0.3341)\n",
      "17329 Training Loss: tensor(0.3328)\n",
      "17330 Training Loss: tensor(0.3320)\n",
      "17331 Training Loss: tensor(0.3329)\n",
      "17332 Training Loss: tensor(0.3338)\n",
      "17333 Training Loss: tensor(0.3340)\n",
      "17334 Training Loss: tensor(0.3335)\n",
      "17335 Training Loss: tensor(0.3343)\n",
      "17336 Training Loss: tensor(0.3329)\n",
      "17337 Training Loss: tensor(0.3325)\n",
      "17338 Training Loss: tensor(0.3343)\n",
      "17339 Training Loss: tensor(0.3392)\n",
      "17340 Training Loss: tensor(0.3330)\n",
      "17341 Training Loss: tensor(0.3345)\n",
      "17342 Training Loss: tensor(0.3347)\n",
      "17343 Training Loss: tensor(0.3348)\n",
      "17344 Training Loss: tensor(0.3327)\n",
      "17345 Training Loss: tensor(0.3319)\n",
      "17346 Training Loss: tensor(0.3354)\n",
      "17347 Training Loss: tensor(0.3325)\n",
      "17348 Training Loss: tensor(0.3416)\n",
      "17349 Training Loss: tensor(0.3354)\n",
      "17350 Training Loss: tensor(0.3340)\n",
      "17351 Training Loss: tensor(0.3330)\n",
      "17352 Training Loss: tensor(0.3345)\n",
      "17353 Training Loss: tensor(0.3338)\n",
      "17354 Training Loss: tensor(0.3351)\n",
      "17355 Training Loss: tensor(0.3320)\n",
      "17356 Training Loss: tensor(0.3333)\n",
      "17357 Training Loss: tensor(0.3342)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17358 Training Loss: tensor(0.3416)\n",
      "17359 Training Loss: tensor(0.3352)\n",
      "17360 Training Loss: tensor(0.3338)\n",
      "17361 Training Loss: tensor(0.3331)\n",
      "17362 Training Loss: tensor(0.3336)\n",
      "17363 Training Loss: tensor(0.3346)\n",
      "17364 Training Loss: tensor(0.3335)\n",
      "17365 Training Loss: tensor(0.3353)\n",
      "17366 Training Loss: tensor(0.3351)\n",
      "17367 Training Loss: tensor(0.3327)\n",
      "17368 Training Loss: tensor(0.3350)\n",
      "17369 Training Loss: tensor(0.3358)\n",
      "17370 Training Loss: tensor(0.3333)\n",
      "17371 Training Loss: tensor(0.3346)\n",
      "17372 Training Loss: tensor(0.3337)\n",
      "17373 Training Loss: tensor(0.3370)\n",
      "17374 Training Loss: tensor(0.3336)\n",
      "17375 Training Loss: tensor(0.3335)\n",
      "17376 Training Loss: tensor(0.3330)\n",
      "17377 Training Loss: tensor(0.3328)\n",
      "17378 Training Loss: tensor(0.3344)\n",
      "17379 Training Loss: tensor(0.3332)\n",
      "17380 Training Loss: tensor(0.3322)\n",
      "17381 Training Loss: tensor(0.3375)\n",
      "17382 Training Loss: tensor(0.3356)\n",
      "17383 Training Loss: tensor(0.3361)\n",
      "17384 Training Loss: tensor(0.3366)\n",
      "17385 Training Loss: tensor(0.3354)\n",
      "17386 Training Loss: tensor(0.3341)\n",
      "17387 Training Loss: tensor(0.3329)\n",
      "17388 Training Loss: tensor(0.3373)\n",
      "17389 Training Loss: tensor(0.3341)\n",
      "17390 Training Loss: tensor(0.3337)\n",
      "17391 Training Loss: tensor(0.3338)\n",
      "17392 Training Loss: tensor(0.3348)\n",
      "17393 Training Loss: tensor(0.3322)\n",
      "17394 Training Loss: tensor(0.3363)\n",
      "17395 Training Loss: tensor(0.3367)\n",
      "17396 Training Loss: tensor(0.3363)\n",
      "17397 Training Loss: tensor(0.3334)\n",
      "17398 Training Loss: tensor(0.3355)\n",
      "17399 Training Loss: tensor(0.3334)\n",
      "17400 Training Loss: tensor(0.3344)\n",
      "17401 Training Loss: tensor(0.3322)\n",
      "17402 Training Loss: tensor(0.3347)\n",
      "17403 Training Loss: tensor(0.3339)\n",
      "17404 Training Loss: tensor(0.3354)\n",
      "17405 Training Loss: tensor(0.3358)\n",
      "17406 Training Loss: tensor(0.3327)\n",
      "17407 Training Loss: tensor(0.3348)\n",
      "17408 Training Loss: tensor(0.3341)\n",
      "17409 Training Loss: tensor(0.3323)\n",
      "17410 Training Loss: tensor(0.3341)\n",
      "17411 Training Loss: tensor(0.3361)\n",
      "17412 Training Loss: tensor(0.3329)\n",
      "17413 Training Loss: tensor(0.3351)\n",
      "17414 Training Loss: tensor(0.3322)\n",
      "17415 Training Loss: tensor(0.3338)\n",
      "17416 Training Loss: tensor(0.3358)\n",
      "17417 Training Loss: tensor(0.3349)\n",
      "17418 Training Loss: tensor(0.3345)\n",
      "17419 Training Loss: tensor(0.3330)\n",
      "17420 Training Loss: tensor(0.3379)\n",
      "17421 Training Loss: tensor(0.3364)\n",
      "17422 Training Loss: tensor(0.3357)\n",
      "17423 Training Loss: tensor(0.3343)\n",
      "17424 Training Loss: tensor(0.3352)\n",
      "17425 Training Loss: tensor(0.3345)\n",
      "17426 Training Loss: tensor(0.3345)\n",
      "17427 Training Loss: tensor(0.3323)\n",
      "17428 Training Loss: tensor(0.3352)\n",
      "17429 Training Loss: tensor(0.3349)\n",
      "17430 Training Loss: tensor(0.3335)\n",
      "17431 Training Loss: tensor(0.3346)\n",
      "17432 Training Loss: tensor(0.3457)\n",
      "17433 Training Loss: tensor(0.3333)\n",
      "17434 Training Loss: tensor(0.3349)\n",
      "17435 Training Loss: tensor(0.3337)\n",
      "17436 Training Loss: tensor(0.3368)\n",
      "17437 Training Loss: tensor(0.3347)\n",
      "17438 Training Loss: tensor(0.3360)\n",
      "17439 Training Loss: tensor(0.3358)\n",
      "17440 Training Loss: tensor(0.3348)\n",
      "17441 Training Loss: tensor(0.3363)\n",
      "17442 Training Loss: tensor(0.3375)\n",
      "17443 Training Loss: tensor(0.3327)\n",
      "17444 Training Loss: tensor(0.3342)\n",
      "17445 Training Loss: tensor(0.3323)\n",
      "17446 Training Loss: tensor(0.3330)\n",
      "17447 Training Loss: tensor(0.3332)\n",
      "17448 Training Loss: tensor(0.3352)\n",
      "17449 Training Loss: tensor(0.3347)\n",
      "17450 Training Loss: tensor(0.3332)\n",
      "17451 Training Loss: tensor(0.3321)\n",
      "17452 Training Loss: tensor(0.3335)\n",
      "17453 Training Loss: tensor(0.3334)\n",
      "17454 Training Loss: tensor(0.3345)\n",
      "17455 Training Loss: tensor(0.3347)\n",
      "17456 Training Loss: tensor(0.3319)\n",
      "17457 Training Loss: tensor(0.3349)\n",
      "17458 Training Loss: tensor(0.3357)\n",
      "17459 Training Loss: tensor(0.3329)\n",
      "17460 Training Loss: tensor(0.3337)\n",
      "17461 Training Loss: tensor(0.3441)\n",
      "17462 Training Loss: tensor(0.3323)\n",
      "17463 Training Loss: tensor(0.3328)\n",
      "17464 Training Loss: tensor(0.3344)\n",
      "17465 Training Loss: tensor(0.3348)\n",
      "17466 Training Loss: tensor(0.3353)\n",
      "17467 Training Loss: tensor(0.3355)\n",
      "17468 Training Loss: tensor(0.3367)\n",
      "17469 Training Loss: tensor(0.3339)\n",
      "17470 Training Loss: tensor(0.3351)\n",
      "17471 Training Loss: tensor(0.3325)\n",
      "17472 Training Loss: tensor(0.3386)\n",
      "17473 Training Loss: tensor(0.3357)\n",
      "17474 Training Loss: tensor(0.3336)\n",
      "17475 Training Loss: tensor(0.3366)\n",
      "17476 Training Loss: tensor(0.3335)\n",
      "17477 Training Loss: tensor(0.3334)\n",
      "17478 Training Loss: tensor(0.3383)\n",
      "17479 Training Loss: tensor(0.3346)\n",
      "17480 Training Loss: tensor(0.3353)\n",
      "17481 Training Loss: tensor(0.3338)\n",
      "17482 Training Loss: tensor(0.3323)\n",
      "17483 Training Loss: tensor(0.3324)\n",
      "17484 Training Loss: tensor(0.3345)\n",
      "17485 Training Loss: tensor(0.3352)\n",
      "17486 Training Loss: tensor(0.3351)\n",
      "17487 Training Loss: tensor(0.3339)\n",
      "17488 Training Loss: tensor(0.3341)\n",
      "17489 Training Loss: tensor(0.3328)\n",
      "17490 Training Loss: tensor(0.3362)\n",
      "17491 Training Loss: tensor(0.3352)\n",
      "17492 Training Loss: tensor(0.3339)\n",
      "17493 Training Loss: tensor(0.3316)\n",
      "17494 Training Loss: tensor(0.3344)\n",
      "17495 Training Loss: tensor(0.3324)\n",
      "17496 Training Loss: tensor(0.3319)\n",
      "17497 Training Loss: tensor(0.3347)\n",
      "17498 Training Loss: tensor(0.3364)\n",
      "17499 Training Loss: tensor(0.3325)\n",
      "17500 Training Loss: tensor(0.3364)\n",
      "17501 Training Loss: tensor(0.3345)\n",
      "17502 Training Loss: tensor(0.3342)\n",
      "17503 Training Loss: tensor(0.3346)\n",
      "17504 Training Loss: tensor(0.3345)\n",
      "17505 Training Loss: tensor(0.3349)\n",
      "17506 Training Loss: tensor(0.3326)\n",
      "17507 Training Loss: tensor(0.3345)\n",
      "17508 Training Loss: tensor(0.3338)\n",
      "17509 Training Loss: tensor(0.3376)\n",
      "17510 Training Loss: tensor(0.3384)\n",
      "17511 Training Loss: tensor(0.3332)\n",
      "17512 Training Loss: tensor(0.3351)\n",
      "17513 Training Loss: tensor(0.3370)\n",
      "17514 Training Loss: tensor(0.3319)\n",
      "17515 Training Loss: tensor(0.3332)\n",
      "17516 Training Loss: tensor(0.3374)\n",
      "17517 Training Loss: tensor(0.3344)\n",
      "17518 Training Loss: tensor(0.3342)\n",
      "17519 Training Loss: tensor(0.3373)\n",
      "17520 Training Loss: tensor(0.3327)\n",
      "17521 Training Loss: tensor(0.3383)\n",
      "17522 Training Loss: tensor(0.3355)\n",
      "17523 Training Loss: tensor(0.3357)\n",
      "17524 Training Loss: tensor(0.3341)\n",
      "17525 Training Loss: tensor(0.3337)\n",
      "17526 Training Loss: tensor(0.3346)\n",
      "17527 Training Loss: tensor(0.3329)\n",
      "17528 Training Loss: tensor(0.3329)\n",
      "17529 Training Loss: tensor(0.3357)\n",
      "17530 Training Loss: tensor(0.3345)\n",
      "17531 Training Loss: tensor(0.3322)\n",
      "17532 Training Loss: tensor(0.3326)\n",
      "17533 Training Loss: tensor(0.3333)\n",
      "17534 Training Loss: tensor(0.3337)\n",
      "17535 Training Loss: tensor(0.3347)\n",
      "17536 Training Loss: tensor(0.3331)\n",
      "17537 Training Loss: tensor(0.3325)\n",
      "17538 Training Loss: tensor(0.3374)\n",
      "17539 Training Loss: tensor(0.3372)\n",
      "17540 Training Loss: tensor(0.3312)\n",
      "17541 Training Loss: tensor(0.3326)\n",
      "17542 Training Loss: tensor(0.3368)\n",
      "17543 Training Loss: tensor(0.3340)\n",
      "17544 Training Loss: tensor(0.3330)\n",
      "17545 Training Loss: tensor(0.3340)\n",
      "17546 Training Loss: tensor(0.3327)\n",
      "17547 Training Loss: tensor(0.3392)\n",
      "17548 Training Loss: tensor(0.3330)\n",
      "17549 Training Loss: tensor(0.3334)\n",
      "17550 Training Loss: tensor(0.3335)\n",
      "17551 Training Loss: tensor(0.3341)\n",
      "17552 Training Loss: tensor(0.3320)\n",
      "17553 Training Loss: tensor(0.3338)\n",
      "17554 Training Loss: tensor(0.3325)\n",
      "17555 Training Loss: tensor(0.3332)\n",
      "17556 Training Loss: tensor(0.3329)\n",
      "17557 Training Loss: tensor(0.3349)\n",
      "17558 Training Loss: tensor(0.3379)\n",
      "17559 Training Loss: tensor(0.3355)\n",
      "17560 Training Loss: tensor(0.3328)\n",
      "17561 Training Loss: tensor(0.3334)\n",
      "17562 Training Loss: tensor(0.3339)\n",
      "17563 Training Loss: tensor(0.3329)\n",
      "17564 Training Loss: tensor(0.3398)\n",
      "17565 Training Loss: tensor(0.3350)\n",
      "17566 Training Loss: tensor(0.3342)\n",
      "17567 Training Loss: tensor(0.3325)\n",
      "17568 Training Loss: tensor(0.3336)\n",
      "17569 Training Loss: tensor(0.3350)\n",
      "17570 Training Loss: tensor(0.3328)\n",
      "17571 Training Loss: tensor(0.3333)\n",
      "17572 Training Loss: tensor(0.3339)\n",
      "17573 Training Loss: tensor(0.3349)\n",
      "17574 Training Loss: tensor(0.3332)\n",
      "17575 Training Loss: tensor(0.3322)\n",
      "17576 Training Loss: tensor(0.3327)\n",
      "17577 Training Loss: tensor(0.3325)\n",
      "17578 Training Loss: tensor(0.3379)\n",
      "17579 Training Loss: tensor(0.3349)\n",
      "17580 Training Loss: tensor(0.3395)\n",
      "17581 Training Loss: tensor(0.3371)\n",
      "17582 Training Loss: tensor(0.3334)\n",
      "17583 Training Loss: tensor(0.3366)\n",
      "17584 Training Loss: tensor(0.3335)\n",
      "17585 Training Loss: tensor(0.3326)\n",
      "17586 Training Loss: tensor(0.3328)\n",
      "17587 Training Loss: tensor(0.3356)\n",
      "17588 Training Loss: tensor(0.3326)\n",
      "17589 Training Loss: tensor(0.3342)\n",
      "17590 Training Loss: tensor(0.3324)\n",
      "17591 Training Loss: tensor(0.3345)\n",
      "17592 Training Loss: tensor(0.3325)\n",
      "17593 Training Loss: tensor(0.3320)\n",
      "17594 Training Loss: tensor(0.3349)\n",
      "17595 Training Loss: tensor(0.3336)\n",
      "17596 Training Loss: tensor(0.3333)\n",
      "17597 Training Loss: tensor(0.3348)\n",
      "17598 Training Loss: tensor(0.3332)\n",
      "17599 Training Loss: tensor(0.3368)\n",
      "17600 Training Loss: tensor(0.3393)\n",
      "17601 Training Loss: tensor(0.3338)\n",
      "17602 Training Loss: tensor(0.3359)\n",
      "17603 Training Loss: tensor(0.3335)\n",
      "17604 Training Loss: tensor(0.3335)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17605 Training Loss: tensor(0.3338)\n",
      "17606 Training Loss: tensor(0.3353)\n",
      "17607 Training Loss: tensor(0.3348)\n",
      "17608 Training Loss: tensor(0.3360)\n",
      "17609 Training Loss: tensor(0.3334)\n",
      "17610 Training Loss: tensor(0.3330)\n",
      "17611 Training Loss: tensor(0.3372)\n",
      "17612 Training Loss: tensor(0.3357)\n",
      "17613 Training Loss: tensor(0.3362)\n",
      "17614 Training Loss: tensor(0.3325)\n",
      "17615 Training Loss: tensor(0.3336)\n",
      "17616 Training Loss: tensor(0.3326)\n",
      "17617 Training Loss: tensor(0.3369)\n",
      "17618 Training Loss: tensor(0.3327)\n",
      "17619 Training Loss: tensor(0.3320)\n",
      "17620 Training Loss: tensor(0.3350)\n",
      "17621 Training Loss: tensor(0.3321)\n",
      "17622 Training Loss: tensor(0.3391)\n",
      "17623 Training Loss: tensor(0.3339)\n",
      "17624 Training Loss: tensor(0.3323)\n",
      "17625 Training Loss: tensor(0.3336)\n",
      "17626 Training Loss: tensor(0.3352)\n",
      "17627 Training Loss: tensor(0.3350)\n",
      "17628 Training Loss: tensor(0.3343)\n",
      "17629 Training Loss: tensor(0.3334)\n",
      "17630 Training Loss: tensor(0.3327)\n",
      "17631 Training Loss: tensor(0.3330)\n",
      "17632 Training Loss: tensor(0.3389)\n",
      "17633 Training Loss: tensor(0.3355)\n",
      "17634 Training Loss: tensor(0.3329)\n",
      "17635 Training Loss: tensor(0.3348)\n",
      "17636 Training Loss: tensor(0.3345)\n",
      "17637 Training Loss: tensor(0.3335)\n",
      "17638 Training Loss: tensor(0.3356)\n",
      "17639 Training Loss: tensor(0.3327)\n",
      "17640 Training Loss: tensor(0.3327)\n",
      "17641 Training Loss: tensor(0.3331)\n",
      "17642 Training Loss: tensor(0.3373)\n",
      "17643 Training Loss: tensor(0.3326)\n",
      "17644 Training Loss: tensor(0.3336)\n",
      "17645 Training Loss: tensor(0.3322)\n",
      "17646 Training Loss: tensor(0.3365)\n",
      "17647 Training Loss: tensor(0.3341)\n",
      "17648 Training Loss: tensor(0.3363)\n",
      "17649 Training Loss: tensor(0.3322)\n",
      "17650 Training Loss: tensor(0.3331)\n",
      "17651 Training Loss: tensor(0.3343)\n",
      "17652 Training Loss: tensor(0.3326)\n",
      "17653 Training Loss: tensor(0.3373)\n",
      "17654 Training Loss: tensor(0.3334)\n",
      "17655 Training Loss: tensor(0.3331)\n",
      "17656 Training Loss: tensor(0.3337)\n",
      "17657 Training Loss: tensor(0.3333)\n",
      "17658 Training Loss: tensor(0.3330)\n",
      "17659 Training Loss: tensor(0.3335)\n",
      "17660 Training Loss: tensor(0.3330)\n",
      "17661 Training Loss: tensor(0.3341)\n",
      "17662 Training Loss: tensor(0.3361)\n",
      "17663 Training Loss: tensor(0.3333)\n",
      "17664 Training Loss: tensor(0.3325)\n",
      "17665 Training Loss: tensor(0.3338)\n",
      "17666 Training Loss: tensor(0.3332)\n",
      "17667 Training Loss: tensor(0.3326)\n",
      "17668 Training Loss: tensor(0.3328)\n",
      "17669 Training Loss: tensor(0.3345)\n",
      "17670 Training Loss: tensor(0.3333)\n",
      "17671 Training Loss: tensor(0.3326)\n",
      "17672 Training Loss: tensor(0.3342)\n",
      "17673 Training Loss: tensor(0.3356)\n",
      "17674 Training Loss: tensor(0.3355)\n",
      "17675 Training Loss: tensor(0.3321)\n",
      "17676 Training Loss: tensor(0.3343)\n",
      "17677 Training Loss: tensor(0.3340)\n",
      "17678 Training Loss: tensor(0.3331)\n",
      "17679 Training Loss: tensor(0.3348)\n",
      "17680 Training Loss: tensor(0.3315)\n",
      "17681 Training Loss: tensor(0.3337)\n",
      "17682 Training Loss: tensor(0.3328)\n",
      "17683 Training Loss: tensor(0.3345)\n",
      "17684 Training Loss: tensor(0.3326)\n",
      "17685 Training Loss: tensor(0.3365)\n",
      "17686 Training Loss: tensor(0.3349)\n",
      "17687 Training Loss: tensor(0.3326)\n",
      "17688 Training Loss: tensor(0.3321)\n",
      "17689 Training Loss: tensor(0.3338)\n",
      "17690 Training Loss: tensor(0.3380)\n",
      "17691 Training Loss: tensor(0.3321)\n",
      "17692 Training Loss: tensor(0.3338)\n",
      "17693 Training Loss: tensor(0.3336)\n",
      "17694 Training Loss: tensor(0.3335)\n",
      "17695 Training Loss: tensor(0.3333)\n",
      "17696 Training Loss: tensor(0.3348)\n",
      "17697 Training Loss: tensor(0.3329)\n",
      "17698 Training Loss: tensor(0.3324)\n",
      "17699 Training Loss: tensor(0.3323)\n",
      "17700 Training Loss: tensor(0.3354)\n",
      "17701 Training Loss: tensor(0.3322)\n",
      "17702 Training Loss: tensor(0.3409)\n",
      "17703 Training Loss: tensor(0.3315)\n",
      "17704 Training Loss: tensor(0.3363)\n",
      "17705 Training Loss: tensor(0.3335)\n",
      "17706 Training Loss: tensor(0.3369)\n",
      "17707 Training Loss: tensor(0.3330)\n",
      "17708 Training Loss: tensor(0.3321)\n",
      "17709 Training Loss: tensor(0.3334)\n",
      "17710 Training Loss: tensor(0.3360)\n",
      "17711 Training Loss: tensor(0.3327)\n",
      "17712 Training Loss: tensor(0.3341)\n",
      "17713 Training Loss: tensor(0.3333)\n",
      "17714 Training Loss: tensor(0.3336)\n",
      "17715 Training Loss: tensor(0.3367)\n",
      "17716 Training Loss: tensor(0.3322)\n",
      "17717 Training Loss: tensor(0.3335)\n",
      "17718 Training Loss: tensor(0.3338)\n",
      "17719 Training Loss: tensor(0.3333)\n",
      "17720 Training Loss: tensor(0.3356)\n",
      "17721 Training Loss: tensor(0.3340)\n",
      "17722 Training Loss: tensor(0.3316)\n",
      "17723 Training Loss: tensor(0.3334)\n",
      "17724 Training Loss: tensor(0.3344)\n",
      "17725 Training Loss: tensor(0.3375)\n",
      "17726 Training Loss: tensor(0.3322)\n",
      "17727 Training Loss: tensor(0.3351)\n",
      "17728 Training Loss: tensor(0.3334)\n",
      "17729 Training Loss: tensor(0.3341)\n",
      "17730 Training Loss: tensor(0.3367)\n",
      "17731 Training Loss: tensor(0.3342)\n",
      "17732 Training Loss: tensor(0.3342)\n",
      "17733 Training Loss: tensor(0.3341)\n",
      "17734 Training Loss: tensor(0.3327)\n",
      "17735 Training Loss: tensor(0.3339)\n",
      "17736 Training Loss: tensor(0.3385)\n",
      "17737 Training Loss: tensor(0.3353)\n",
      "17738 Training Loss: tensor(0.3355)\n",
      "17739 Training Loss: tensor(0.3349)\n",
      "17740 Training Loss: tensor(0.3330)\n",
      "17741 Training Loss: tensor(0.3329)\n",
      "17742 Training Loss: tensor(0.3338)\n",
      "17743 Training Loss: tensor(0.3337)\n",
      "17744 Training Loss: tensor(0.3330)\n",
      "17745 Training Loss: tensor(0.3329)\n",
      "17746 Training Loss: tensor(0.3329)\n",
      "17747 Training Loss: tensor(0.3329)\n",
      "17748 Training Loss: tensor(0.3338)\n",
      "17749 Training Loss: tensor(0.3372)\n",
      "17750 Training Loss: tensor(0.3312)\n",
      "17751 Training Loss: tensor(0.3331)\n",
      "17752 Training Loss: tensor(0.3322)\n",
      "17753 Training Loss: tensor(0.3316)\n",
      "17754 Training Loss: tensor(0.3363)\n",
      "17755 Training Loss: tensor(0.3330)\n",
      "17756 Training Loss: tensor(0.3357)\n",
      "17757 Training Loss: tensor(0.3332)\n",
      "17758 Training Loss: tensor(0.3389)\n",
      "17759 Training Loss: tensor(0.3332)\n",
      "17760 Training Loss: tensor(0.3358)\n",
      "17761 Training Loss: tensor(0.3362)\n",
      "17762 Training Loss: tensor(0.3331)\n",
      "17763 Training Loss: tensor(0.3337)\n",
      "17764 Training Loss: tensor(0.3329)\n",
      "17765 Training Loss: tensor(0.3326)\n",
      "17766 Training Loss: tensor(0.3324)\n",
      "17767 Training Loss: tensor(0.3328)\n",
      "17768 Training Loss: tensor(0.3327)\n",
      "17769 Training Loss: tensor(0.3371)\n",
      "17770 Training Loss: tensor(0.3329)\n",
      "17771 Training Loss: tensor(0.3342)\n",
      "17772 Training Loss: tensor(0.3328)\n",
      "17773 Training Loss: tensor(0.3341)\n",
      "17774 Training Loss: tensor(0.3313)\n",
      "17775 Training Loss: tensor(0.3367)\n",
      "17776 Training Loss: tensor(0.3327)\n",
      "17777 Training Loss: tensor(0.3328)\n",
      "17778 Training Loss: tensor(0.3387)\n",
      "17779 Training Loss: tensor(0.3352)\n",
      "17780 Training Loss: tensor(0.3335)\n",
      "17781 Training Loss: tensor(0.3337)\n",
      "17782 Training Loss: tensor(0.3344)\n",
      "17783 Training Loss: tensor(0.3374)\n",
      "17784 Training Loss: tensor(0.3320)\n",
      "17785 Training Loss: tensor(0.3334)\n",
      "17786 Training Loss: tensor(0.3365)\n",
      "17787 Training Loss: tensor(0.3362)\n",
      "17788 Training Loss: tensor(0.3333)\n",
      "17789 Training Loss: tensor(0.3321)\n",
      "17790 Training Loss: tensor(0.3317)\n",
      "17791 Training Loss: tensor(0.3327)\n",
      "17792 Training Loss: tensor(0.3338)\n",
      "17793 Training Loss: tensor(0.3330)\n",
      "17794 Training Loss: tensor(0.3337)\n",
      "17795 Training Loss: tensor(0.3329)\n",
      "17796 Training Loss: tensor(0.3363)\n",
      "17797 Training Loss: tensor(0.3326)\n",
      "17798 Training Loss: tensor(0.3350)\n",
      "17799 Training Loss: tensor(0.3319)\n",
      "17800 Training Loss: tensor(0.3341)\n",
      "17801 Training Loss: tensor(0.3353)\n",
      "17802 Training Loss: tensor(0.3342)\n",
      "17803 Training Loss: tensor(0.3318)\n",
      "17804 Training Loss: tensor(0.3378)\n",
      "17805 Training Loss: tensor(0.3325)\n",
      "17806 Training Loss: tensor(0.3366)\n",
      "17807 Training Loss: tensor(0.3319)\n",
      "17808 Training Loss: tensor(0.3341)\n",
      "17809 Training Loss: tensor(0.3360)\n",
      "17810 Training Loss: tensor(0.3340)\n",
      "17811 Training Loss: tensor(0.3337)\n",
      "17812 Training Loss: tensor(0.3341)\n",
      "17813 Training Loss: tensor(0.3319)\n",
      "17814 Training Loss: tensor(0.3340)\n",
      "17815 Training Loss: tensor(0.3347)\n",
      "17816 Training Loss: tensor(0.3317)\n",
      "17817 Training Loss: tensor(0.3340)\n",
      "17818 Training Loss: tensor(0.3350)\n",
      "17819 Training Loss: tensor(0.3338)\n",
      "17820 Training Loss: tensor(0.3354)\n",
      "17821 Training Loss: tensor(0.3319)\n",
      "17822 Training Loss: tensor(0.3323)\n",
      "17823 Training Loss: tensor(0.3331)\n",
      "17824 Training Loss: tensor(0.3320)\n",
      "17825 Training Loss: tensor(0.3376)\n",
      "17826 Training Loss: tensor(0.3357)\n",
      "17827 Training Loss: tensor(0.3327)\n",
      "17828 Training Loss: tensor(0.3330)\n",
      "17829 Training Loss: tensor(0.3336)\n",
      "17830 Training Loss: tensor(0.3322)\n",
      "17831 Training Loss: tensor(0.3328)\n",
      "17832 Training Loss: tensor(0.3352)\n",
      "17833 Training Loss: tensor(0.3326)\n",
      "17834 Training Loss: tensor(0.3346)\n",
      "17835 Training Loss: tensor(0.3340)\n",
      "17836 Training Loss: tensor(0.3333)\n",
      "17837 Training Loss: tensor(0.3344)\n",
      "17838 Training Loss: tensor(0.3328)\n",
      "17839 Training Loss: tensor(0.3351)\n",
      "17840 Training Loss: tensor(0.3321)\n",
      "17841 Training Loss: tensor(0.3330)\n",
      "17842 Training Loss: tensor(0.3330)\n",
      "17843 Training Loss: tensor(0.3342)\n",
      "17844 Training Loss: tensor(0.3340)\n",
      "17845 Training Loss: tensor(0.3349)\n",
      "17846 Training Loss: tensor(0.3328)\n",
      "17847 Training Loss: tensor(0.3316)\n",
      "17848 Training Loss: tensor(0.3325)\n",
      "17849 Training Loss: tensor(0.3328)\n",
      "17850 Training Loss: tensor(0.3343)\n",
      "17851 Training Loss: tensor(0.3398)\n",
      "17852 Training Loss: tensor(0.3326)\n",
      "17853 Training Loss: tensor(0.3413)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17854 Training Loss: tensor(0.3320)\n",
      "17855 Training Loss: tensor(0.3356)\n",
      "17856 Training Loss: tensor(0.3351)\n",
      "17857 Training Loss: tensor(0.3342)\n",
      "17858 Training Loss: tensor(0.3365)\n",
      "17859 Training Loss: tensor(0.3332)\n",
      "17860 Training Loss: tensor(0.3351)\n",
      "17861 Training Loss: tensor(0.3335)\n",
      "17862 Training Loss: tensor(0.3358)\n",
      "17863 Training Loss: tensor(0.3342)\n",
      "17864 Training Loss: tensor(0.3345)\n",
      "17865 Training Loss: tensor(0.3328)\n",
      "17866 Training Loss: tensor(0.3330)\n",
      "17867 Training Loss: tensor(0.3390)\n",
      "17868 Training Loss: tensor(0.3333)\n",
      "17869 Training Loss: tensor(0.3323)\n",
      "17870 Training Loss: tensor(0.3346)\n",
      "17871 Training Loss: tensor(0.3326)\n",
      "17872 Training Loss: tensor(0.3355)\n",
      "17873 Training Loss: tensor(0.3324)\n",
      "17874 Training Loss: tensor(0.3327)\n",
      "17875 Training Loss: tensor(0.3335)\n",
      "17876 Training Loss: tensor(0.3328)\n",
      "17877 Training Loss: tensor(0.3321)\n",
      "17878 Training Loss: tensor(0.3324)\n",
      "17879 Training Loss: tensor(0.3328)\n",
      "17880 Training Loss: tensor(0.3327)\n",
      "17881 Training Loss: tensor(0.3319)\n",
      "17882 Training Loss: tensor(0.3333)\n",
      "17883 Training Loss: tensor(0.3348)\n",
      "17884 Training Loss: tensor(0.3329)\n",
      "17885 Training Loss: tensor(0.3327)\n",
      "17886 Training Loss: tensor(0.3329)\n",
      "17887 Training Loss: tensor(0.3321)\n",
      "17888 Training Loss: tensor(0.3332)\n",
      "17889 Training Loss: tensor(0.3324)\n",
      "17890 Training Loss: tensor(0.3321)\n",
      "17891 Training Loss: tensor(0.3346)\n",
      "17892 Training Loss: tensor(0.3333)\n",
      "17893 Training Loss: tensor(0.3323)\n",
      "17894 Training Loss: tensor(0.3354)\n",
      "17895 Training Loss: tensor(0.3337)\n",
      "17896 Training Loss: tensor(0.3321)\n",
      "17897 Training Loss: tensor(0.3333)\n",
      "17898 Training Loss: tensor(0.3323)\n",
      "17899 Training Loss: tensor(0.3319)\n",
      "17900 Training Loss: tensor(0.3355)\n",
      "17901 Training Loss: tensor(0.3321)\n",
      "17902 Training Loss: tensor(0.3331)\n",
      "17903 Training Loss: tensor(0.3335)\n",
      "17904 Training Loss: tensor(0.3331)\n",
      "17905 Training Loss: tensor(0.3362)\n",
      "17906 Training Loss: tensor(0.3345)\n",
      "17907 Training Loss: tensor(0.3322)\n",
      "17908 Training Loss: tensor(0.3365)\n",
      "17909 Training Loss: tensor(0.3365)\n",
      "17910 Training Loss: tensor(0.3351)\n",
      "17911 Training Loss: tensor(0.3331)\n",
      "17912 Training Loss: tensor(0.3361)\n",
      "17913 Training Loss: tensor(0.3347)\n",
      "17914 Training Loss: tensor(0.3378)\n",
      "17915 Training Loss: tensor(0.3325)\n",
      "17916 Training Loss: tensor(0.3323)\n",
      "17917 Training Loss: tensor(0.3327)\n",
      "17918 Training Loss: tensor(0.3327)\n",
      "17919 Training Loss: tensor(0.3331)\n",
      "17920 Training Loss: tensor(0.3384)\n",
      "17921 Training Loss: tensor(0.3322)\n",
      "17922 Training Loss: tensor(0.3328)\n",
      "17923 Training Loss: tensor(0.3336)\n",
      "17924 Training Loss: tensor(0.3343)\n",
      "17925 Training Loss: tensor(0.3342)\n",
      "17926 Training Loss: tensor(0.3353)\n",
      "17927 Training Loss: tensor(0.3330)\n",
      "17928 Training Loss: tensor(0.3338)\n",
      "17929 Training Loss: tensor(0.3313)\n",
      "17930 Training Loss: tensor(0.3359)\n",
      "17931 Training Loss: tensor(0.3328)\n",
      "17932 Training Loss: tensor(0.3332)\n",
      "17933 Training Loss: tensor(0.3328)\n",
      "17934 Training Loss: tensor(0.3332)\n",
      "17935 Training Loss: tensor(0.3332)\n",
      "17936 Training Loss: tensor(0.3331)\n",
      "17937 Training Loss: tensor(0.3386)\n",
      "17938 Training Loss: tensor(0.3368)\n",
      "17939 Training Loss: tensor(0.3321)\n",
      "17940 Training Loss: tensor(0.3319)\n",
      "17941 Training Loss: tensor(0.3331)\n",
      "17942 Training Loss: tensor(0.3349)\n",
      "17943 Training Loss: tensor(0.3328)\n",
      "17944 Training Loss: tensor(0.3343)\n",
      "17945 Training Loss: tensor(0.3357)\n",
      "17946 Training Loss: tensor(0.3331)\n",
      "17947 Training Loss: tensor(0.3327)\n",
      "17948 Training Loss: tensor(0.3312)\n",
      "17949 Training Loss: tensor(0.3347)\n",
      "17950 Training Loss: tensor(0.3328)\n",
      "17951 Training Loss: tensor(0.3340)\n",
      "17952 Training Loss: tensor(0.3349)\n",
      "17953 Training Loss: tensor(0.3332)\n",
      "17954 Training Loss: tensor(0.3337)\n",
      "17955 Training Loss: tensor(0.3333)\n",
      "17956 Training Loss: tensor(0.3372)\n",
      "17957 Training Loss: tensor(0.3332)\n",
      "17958 Training Loss: tensor(0.3328)\n",
      "17959 Training Loss: tensor(0.3338)\n",
      "17960 Training Loss: tensor(0.3341)\n",
      "17961 Training Loss: tensor(0.3341)\n",
      "17962 Training Loss: tensor(0.3318)\n",
      "17963 Training Loss: tensor(0.3319)\n",
      "17964 Training Loss: tensor(0.3329)\n",
      "17965 Training Loss: tensor(0.3337)\n",
      "17966 Training Loss: tensor(0.3326)\n",
      "17967 Training Loss: tensor(0.3326)\n",
      "17968 Training Loss: tensor(0.3324)\n",
      "17969 Training Loss: tensor(0.3369)\n",
      "17970 Training Loss: tensor(0.3326)\n",
      "17971 Training Loss: tensor(0.3372)\n",
      "17972 Training Loss: tensor(0.3324)\n",
      "17973 Training Loss: tensor(0.3379)\n",
      "17974 Training Loss: tensor(0.3328)\n",
      "17975 Training Loss: tensor(0.3334)\n",
      "17976 Training Loss: tensor(0.3319)\n",
      "17977 Training Loss: tensor(0.3321)\n",
      "17978 Training Loss: tensor(0.3343)\n",
      "17979 Training Loss: tensor(0.3329)\n",
      "17980 Training Loss: tensor(0.3347)\n",
      "17981 Training Loss: tensor(0.3329)\n",
      "17982 Training Loss: tensor(0.3341)\n",
      "17983 Training Loss: tensor(0.3371)\n",
      "17984 Training Loss: tensor(0.3317)\n",
      "17985 Training Loss: tensor(0.3351)\n",
      "17986 Training Loss: tensor(0.3355)\n",
      "17987 Training Loss: tensor(0.3337)\n",
      "17988 Training Loss: tensor(0.3318)\n",
      "17989 Training Loss: tensor(0.3338)\n",
      "17990 Training Loss: tensor(0.3337)\n",
      "17991 Training Loss: tensor(0.3340)\n",
      "17992 Training Loss: tensor(0.3345)\n",
      "17993 Training Loss: tensor(0.3359)\n",
      "17994 Training Loss: tensor(0.3324)\n",
      "17995 Training Loss: tensor(0.3375)\n",
      "17996 Training Loss: tensor(0.3325)\n",
      "17997 Training Loss: tensor(0.3325)\n",
      "17998 Training Loss: tensor(0.3340)\n",
      "17999 Training Loss: tensor(0.3337)\n",
      "18000 Training Loss: tensor(0.3357)\n",
      "18001 Training Loss: tensor(0.3324)\n",
      "18002 Training Loss: tensor(0.3337)\n",
      "18003 Training Loss: tensor(0.3355)\n",
      "18004 Training Loss: tensor(0.3341)\n",
      "18005 Training Loss: tensor(0.3328)\n",
      "18006 Training Loss: tensor(0.3325)\n",
      "18007 Training Loss: tensor(0.3312)\n",
      "18008 Training Loss: tensor(0.3356)\n",
      "18009 Training Loss: tensor(0.3364)\n",
      "18010 Training Loss: tensor(0.3321)\n",
      "18011 Training Loss: tensor(0.3336)\n",
      "18012 Training Loss: tensor(0.3320)\n",
      "18013 Training Loss: tensor(0.3333)\n",
      "18014 Training Loss: tensor(0.3331)\n",
      "18015 Training Loss: tensor(0.3349)\n",
      "18016 Training Loss: tensor(0.3333)\n",
      "18017 Training Loss: tensor(0.3336)\n",
      "18018 Training Loss: tensor(0.3321)\n",
      "18019 Training Loss: tensor(0.3329)\n",
      "18020 Training Loss: tensor(0.3329)\n",
      "18021 Training Loss: tensor(0.3390)\n",
      "18022 Training Loss: tensor(0.3341)\n",
      "18023 Training Loss: tensor(0.3337)\n",
      "18024 Training Loss: tensor(0.3332)\n",
      "18025 Training Loss: tensor(0.3360)\n",
      "18026 Training Loss: tensor(0.3318)\n",
      "18027 Training Loss: tensor(0.3333)\n",
      "18028 Training Loss: tensor(0.3313)\n",
      "18029 Training Loss: tensor(0.3317)\n",
      "18030 Training Loss: tensor(0.3356)\n",
      "18031 Training Loss: tensor(0.3323)\n",
      "18032 Training Loss: tensor(0.3333)\n",
      "18033 Training Loss: tensor(0.3342)\n",
      "18034 Training Loss: tensor(0.3324)\n",
      "18035 Training Loss: tensor(0.3415)\n",
      "18036 Training Loss: tensor(0.3333)\n",
      "18037 Training Loss: tensor(0.3347)\n",
      "18038 Training Loss: tensor(0.3321)\n",
      "18039 Training Loss: tensor(0.3341)\n",
      "18040 Training Loss: tensor(0.3316)\n",
      "18041 Training Loss: tensor(0.3347)\n",
      "18042 Training Loss: tensor(0.3334)\n",
      "18043 Training Loss: tensor(0.3330)\n",
      "18044 Training Loss: tensor(0.3323)\n",
      "18045 Training Loss: tensor(0.3331)\n",
      "18046 Training Loss: tensor(0.3323)\n",
      "18047 Training Loss: tensor(0.3341)\n",
      "18048 Training Loss: tensor(0.3333)\n",
      "18049 Training Loss: tensor(0.3340)\n",
      "18050 Training Loss: tensor(0.3337)\n",
      "18051 Training Loss: tensor(0.3319)\n",
      "18052 Training Loss: tensor(0.3324)\n",
      "18053 Training Loss: tensor(0.3337)\n",
      "18054 Training Loss: tensor(0.3339)\n",
      "18055 Training Loss: tensor(0.3347)\n",
      "18056 Training Loss: tensor(0.3324)\n",
      "18057 Training Loss: tensor(0.3343)\n",
      "18058 Training Loss: tensor(0.3324)\n",
      "18059 Training Loss: tensor(0.3348)\n",
      "18060 Training Loss: tensor(0.3322)\n",
      "18061 Training Loss: tensor(0.3338)\n",
      "18062 Training Loss: tensor(0.3316)\n",
      "18063 Training Loss: tensor(0.3340)\n",
      "18064 Training Loss: tensor(0.3329)\n",
      "18065 Training Loss: tensor(0.3340)\n",
      "18066 Training Loss: tensor(0.3392)\n",
      "18067 Training Loss: tensor(0.3320)\n",
      "18068 Training Loss: tensor(0.3326)\n",
      "18069 Training Loss: tensor(0.3315)\n",
      "18070 Training Loss: tensor(0.3328)\n",
      "18071 Training Loss: tensor(0.3336)\n",
      "18072 Training Loss: tensor(0.3331)\n",
      "18073 Training Loss: tensor(0.3323)\n",
      "18074 Training Loss: tensor(0.3336)\n",
      "18075 Training Loss: tensor(0.3328)\n",
      "18076 Training Loss: tensor(0.3364)\n",
      "18077 Training Loss: tensor(0.3344)\n",
      "18078 Training Loss: tensor(0.3317)\n",
      "18079 Training Loss: tensor(0.3335)\n",
      "18080 Training Loss: tensor(0.3334)\n",
      "18081 Training Loss: tensor(0.3331)\n",
      "18082 Training Loss: tensor(0.3356)\n",
      "18083 Training Loss: tensor(0.3309)\n",
      "18084 Training Loss: tensor(0.3323)\n",
      "18085 Training Loss: tensor(0.3324)\n",
      "18086 Training Loss: tensor(0.3370)\n",
      "18087 Training Loss: tensor(0.3320)\n",
      "18088 Training Loss: tensor(0.3331)\n",
      "18089 Training Loss: tensor(0.3328)\n",
      "18090 Training Loss: tensor(0.3325)\n",
      "18091 Training Loss: tensor(0.3329)\n",
      "18092 Training Loss: tensor(0.3359)\n",
      "18093 Training Loss: tensor(0.3325)\n",
      "18094 Training Loss: tensor(0.3336)\n",
      "18095 Training Loss: tensor(0.3327)\n",
      "18096 Training Loss: tensor(0.3331)\n",
      "18097 Training Loss: tensor(0.3321)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18098 Training Loss: tensor(0.3320)\n",
      "18099 Training Loss: tensor(0.3312)\n",
      "18100 Training Loss: tensor(0.3348)\n",
      "18101 Training Loss: tensor(0.3342)\n",
      "18102 Training Loss: tensor(0.3335)\n",
      "18103 Training Loss: tensor(0.3317)\n",
      "18104 Training Loss: tensor(0.3331)\n",
      "18105 Training Loss: tensor(0.3340)\n",
      "18106 Training Loss: tensor(0.3335)\n",
      "18107 Training Loss: tensor(0.3332)\n",
      "18108 Training Loss: tensor(0.3322)\n",
      "18109 Training Loss: tensor(0.3380)\n",
      "18110 Training Loss: tensor(0.3331)\n",
      "18111 Training Loss: tensor(0.3343)\n",
      "18112 Training Loss: tensor(0.3373)\n",
      "18113 Training Loss: tensor(0.3325)\n",
      "18114 Training Loss: tensor(0.3345)\n",
      "18115 Training Loss: tensor(0.3357)\n",
      "18116 Training Loss: tensor(0.3354)\n",
      "18117 Training Loss: tensor(0.3337)\n",
      "18118 Training Loss: tensor(0.3316)\n",
      "18119 Training Loss: tensor(0.3316)\n",
      "18120 Training Loss: tensor(0.3327)\n",
      "18121 Training Loss: tensor(0.3311)\n",
      "18122 Training Loss: tensor(0.3323)\n",
      "18123 Training Loss: tensor(0.3359)\n",
      "18124 Training Loss: tensor(0.3316)\n",
      "18125 Training Loss: tensor(0.3396)\n",
      "18126 Training Loss: tensor(0.3391)\n",
      "18127 Training Loss: tensor(0.3375)\n",
      "18128 Training Loss: tensor(0.3330)\n",
      "18129 Training Loss: tensor(0.3328)\n",
      "18130 Training Loss: tensor(0.3342)\n",
      "18131 Training Loss: tensor(0.3355)\n",
      "18132 Training Loss: tensor(0.3345)\n",
      "18133 Training Loss: tensor(0.3361)\n",
      "18134 Training Loss: tensor(0.3326)\n",
      "18135 Training Loss: tensor(0.3333)\n",
      "18136 Training Loss: tensor(0.3354)\n",
      "18137 Training Loss: tensor(0.3360)\n",
      "18138 Training Loss: tensor(0.3352)\n",
      "18139 Training Loss: tensor(0.3337)\n",
      "18140 Training Loss: tensor(0.3322)\n",
      "18141 Training Loss: tensor(0.3329)\n",
      "18142 Training Loss: tensor(0.3358)\n",
      "18143 Training Loss: tensor(0.3340)\n",
      "18144 Training Loss: tensor(0.3335)\n",
      "18145 Training Loss: tensor(0.3332)\n",
      "18146 Training Loss: tensor(0.3339)\n",
      "18147 Training Loss: tensor(0.3335)\n",
      "18148 Training Loss: tensor(0.3330)\n",
      "18149 Training Loss: tensor(0.3355)\n",
      "18150 Training Loss: tensor(0.3330)\n",
      "18151 Training Loss: tensor(0.3324)\n",
      "18152 Training Loss: tensor(0.3330)\n",
      "18153 Training Loss: tensor(0.3323)\n",
      "18154 Training Loss: tensor(0.3342)\n",
      "18155 Training Loss: tensor(0.3331)\n",
      "18156 Training Loss: tensor(0.3331)\n",
      "18157 Training Loss: tensor(0.3340)\n",
      "18158 Training Loss: tensor(0.3313)\n",
      "18159 Training Loss: tensor(0.3360)\n",
      "18160 Training Loss: tensor(0.3308)\n",
      "18161 Training Loss: tensor(0.3338)\n",
      "18162 Training Loss: tensor(0.3317)\n",
      "18163 Training Loss: tensor(0.3321)\n",
      "18164 Training Loss: tensor(0.3353)\n",
      "18165 Training Loss: tensor(0.3357)\n",
      "18166 Training Loss: tensor(0.3318)\n",
      "18167 Training Loss: tensor(0.3321)\n",
      "18168 Training Loss: tensor(0.3371)\n",
      "18169 Training Loss: tensor(0.3329)\n",
      "18170 Training Loss: tensor(0.3326)\n",
      "18171 Training Loss: tensor(0.3331)\n",
      "18172 Training Loss: tensor(0.3326)\n",
      "18173 Training Loss: tensor(0.3321)\n",
      "18174 Training Loss: tensor(0.3323)\n",
      "18175 Training Loss: tensor(0.3312)\n",
      "18176 Training Loss: tensor(0.3322)\n",
      "18177 Training Loss: tensor(0.3355)\n",
      "18178 Training Loss: tensor(0.3322)\n",
      "18179 Training Loss: tensor(0.3322)\n",
      "18180 Training Loss: tensor(0.3311)\n",
      "18181 Training Loss: tensor(0.3322)\n",
      "18182 Training Loss: tensor(0.3310)\n",
      "18183 Training Loss: tensor(0.3321)\n",
      "18184 Training Loss: tensor(0.3329)\n",
      "18185 Training Loss: tensor(0.3318)\n",
      "18186 Training Loss: tensor(0.3334)\n",
      "18187 Training Loss: tensor(0.3338)\n",
      "18188 Training Loss: tensor(0.3311)\n",
      "18189 Training Loss: tensor(0.3325)\n",
      "18190 Training Loss: tensor(0.3320)\n",
      "18191 Training Loss: tensor(0.3409)\n",
      "18192 Training Loss: tensor(0.3414)\n",
      "18193 Training Loss: tensor(0.3320)\n",
      "18194 Training Loss: tensor(0.3318)\n",
      "18195 Training Loss: tensor(0.3342)\n",
      "18196 Training Loss: tensor(0.3359)\n",
      "18197 Training Loss: tensor(0.3321)\n",
      "18198 Training Loss: tensor(0.3327)\n",
      "18199 Training Loss: tensor(0.3367)\n",
      "18200 Training Loss: tensor(0.3322)\n",
      "18201 Training Loss: tensor(0.3337)\n",
      "18202 Training Loss: tensor(0.3334)\n",
      "18203 Training Loss: tensor(0.3342)\n",
      "18204 Training Loss: tensor(0.3331)\n",
      "18205 Training Loss: tensor(0.3340)\n",
      "18206 Training Loss: tensor(0.3328)\n",
      "18207 Training Loss: tensor(0.3352)\n",
      "18208 Training Loss: tensor(0.3339)\n",
      "18209 Training Loss: tensor(0.3342)\n",
      "18210 Training Loss: tensor(0.3318)\n",
      "18211 Training Loss: tensor(0.3344)\n",
      "18212 Training Loss: tensor(0.3313)\n",
      "18213 Training Loss: tensor(0.3333)\n",
      "18214 Training Loss: tensor(0.3318)\n",
      "18215 Training Loss: tensor(0.3410)\n",
      "18216 Training Loss: tensor(0.3351)\n",
      "18217 Training Loss: tensor(0.3322)\n",
      "18218 Training Loss: tensor(0.3342)\n",
      "18219 Training Loss: tensor(0.3320)\n",
      "18220 Training Loss: tensor(0.3354)\n",
      "18221 Training Loss: tensor(0.3339)\n",
      "18222 Training Loss: tensor(0.3331)\n",
      "18223 Training Loss: tensor(0.3372)\n",
      "18224 Training Loss: tensor(0.3356)\n",
      "18225 Training Loss: tensor(0.3328)\n",
      "18226 Training Loss: tensor(0.3327)\n",
      "18227 Training Loss: tensor(0.3350)\n",
      "18228 Training Loss: tensor(0.3336)\n",
      "18229 Training Loss: tensor(0.3346)\n",
      "18230 Training Loss: tensor(0.3340)\n",
      "18231 Training Loss: tensor(0.3350)\n",
      "18232 Training Loss: tensor(0.3350)\n",
      "18233 Training Loss: tensor(0.3352)\n",
      "18234 Training Loss: tensor(0.3327)\n",
      "18235 Training Loss: tensor(0.3321)\n",
      "18236 Training Loss: tensor(0.3362)\n",
      "18237 Training Loss: tensor(0.3321)\n",
      "18238 Training Loss: tensor(0.3332)\n",
      "18239 Training Loss: tensor(0.3330)\n",
      "18240 Training Loss: tensor(0.3327)\n",
      "18241 Training Loss: tensor(0.3323)\n",
      "18242 Training Loss: tensor(0.3325)\n",
      "18243 Training Loss: tensor(0.3331)\n",
      "18244 Training Loss: tensor(0.3330)\n",
      "18245 Training Loss: tensor(0.3387)\n",
      "18246 Training Loss: tensor(0.3319)\n",
      "18247 Training Loss: tensor(0.3311)\n",
      "18248 Training Loss: tensor(0.3350)\n",
      "18249 Training Loss: tensor(0.3329)\n",
      "18250 Training Loss: tensor(0.3332)\n",
      "18251 Training Loss: tensor(0.3338)\n",
      "18252 Training Loss: tensor(0.3319)\n",
      "18253 Training Loss: tensor(0.3331)\n",
      "18254 Training Loss: tensor(0.3356)\n",
      "18255 Training Loss: tensor(0.3341)\n",
      "18256 Training Loss: tensor(0.3329)\n",
      "18257 Training Loss: tensor(0.3338)\n",
      "18258 Training Loss: tensor(0.3367)\n",
      "18259 Training Loss: tensor(0.3337)\n",
      "18260 Training Loss: tensor(0.3347)\n",
      "18261 Training Loss: tensor(0.3330)\n",
      "18262 Training Loss: tensor(0.3319)\n",
      "18263 Training Loss: tensor(0.3320)\n",
      "18264 Training Loss: tensor(0.3355)\n",
      "18265 Training Loss: tensor(0.3331)\n",
      "18266 Training Loss: tensor(0.3311)\n",
      "18267 Training Loss: tensor(0.3330)\n",
      "18268 Training Loss: tensor(0.3317)\n",
      "18269 Training Loss: tensor(0.3333)\n",
      "18270 Training Loss: tensor(0.3322)\n",
      "18271 Training Loss: tensor(0.3337)\n",
      "18272 Training Loss: tensor(0.3321)\n",
      "18273 Training Loss: tensor(0.3353)\n",
      "18274 Training Loss: tensor(0.3325)\n",
      "18275 Training Loss: tensor(0.3327)\n",
      "18276 Training Loss: tensor(0.3322)\n",
      "18277 Training Loss: tensor(0.3353)\n",
      "18278 Training Loss: tensor(0.3318)\n",
      "18279 Training Loss: tensor(0.3307)\n",
      "18280 Training Loss: tensor(0.3336)\n",
      "18281 Training Loss: tensor(0.3316)\n",
      "18282 Training Loss: tensor(0.3345)\n",
      "18283 Training Loss: tensor(0.3315)\n",
      "18284 Training Loss: tensor(0.3325)\n",
      "18285 Training Loss: tensor(0.3340)\n",
      "18286 Training Loss: tensor(0.3329)\n",
      "18287 Training Loss: tensor(0.3365)\n",
      "18288 Training Loss: tensor(0.3322)\n",
      "18289 Training Loss: tensor(0.3326)\n",
      "18290 Training Loss: tensor(0.3329)\n",
      "18291 Training Loss: tensor(0.3319)\n",
      "18292 Training Loss: tensor(0.3315)\n",
      "18293 Training Loss: tensor(0.3321)\n",
      "18294 Training Loss: tensor(0.3334)\n",
      "18295 Training Loss: tensor(0.3318)\n",
      "18296 Training Loss: tensor(0.3353)\n",
      "18297 Training Loss: tensor(0.3343)\n",
      "18298 Training Loss: tensor(0.3317)\n",
      "18299 Training Loss: tensor(0.3353)\n",
      "18300 Training Loss: tensor(0.3318)\n",
      "18301 Training Loss: tensor(0.3328)\n",
      "18302 Training Loss: tensor(0.3406)\n",
      "18303 Training Loss: tensor(0.3330)\n",
      "18304 Training Loss: tensor(0.3345)\n",
      "18305 Training Loss: tensor(0.3319)\n",
      "18306 Training Loss: tensor(0.3339)\n",
      "18307 Training Loss: tensor(0.3313)\n",
      "18308 Training Loss: tensor(0.3314)\n",
      "18309 Training Loss: tensor(0.3319)\n",
      "18310 Training Loss: tensor(0.3327)\n",
      "18311 Training Loss: tensor(0.3342)\n",
      "18312 Training Loss: tensor(0.3318)\n",
      "18313 Training Loss: tensor(0.3329)\n",
      "18314 Training Loss: tensor(0.3342)\n",
      "18315 Training Loss: tensor(0.3323)\n",
      "18316 Training Loss: tensor(0.3319)\n",
      "18317 Training Loss: tensor(0.3327)\n",
      "18318 Training Loss: tensor(0.3354)\n",
      "18319 Training Loss: tensor(0.3325)\n",
      "18320 Training Loss: tensor(0.3321)\n",
      "18321 Training Loss: tensor(0.3321)\n",
      "18322 Training Loss: tensor(0.3345)\n",
      "18323 Training Loss: tensor(0.3324)\n",
      "18324 Training Loss: tensor(0.3325)\n",
      "18325 Training Loss: tensor(0.3329)\n",
      "18326 Training Loss: tensor(0.3350)\n",
      "18327 Training Loss: tensor(0.3327)\n",
      "18328 Training Loss: tensor(0.3313)\n",
      "18329 Training Loss: tensor(0.3327)\n",
      "18330 Training Loss: tensor(0.3317)\n",
      "18331 Training Loss: tensor(0.3344)\n",
      "18332 Training Loss: tensor(0.3314)\n",
      "18333 Training Loss: tensor(0.3319)\n",
      "18334 Training Loss: tensor(0.3330)\n",
      "18335 Training Loss: tensor(0.3373)\n",
      "18336 Training Loss: tensor(0.3339)\n",
      "18337 Training Loss: tensor(0.3352)\n",
      "18338 Training Loss: tensor(0.3342)\n",
      "18339 Training Loss: tensor(0.3327)\n",
      "18340 Training Loss: tensor(0.3373)\n",
      "18341 Training Loss: tensor(0.3325)\n",
      "18342 Training Loss: tensor(0.3326)\n",
      "18343 Training Loss: tensor(0.3333)\n",
      "18344 Training Loss: tensor(0.3338)\n",
      "18345 Training Loss: tensor(0.3321)\n",
      "18346 Training Loss: tensor(0.3312)\n",
      "18347 Training Loss: tensor(0.3330)\n",
      "18348 Training Loss: tensor(0.3316)\n",
      "18349 Training Loss: tensor(0.3342)\n",
      "18350 Training Loss: tensor(0.3412)\n",
      "18351 Training Loss: tensor(0.3324)\n",
      "18352 Training Loss: tensor(0.3327)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18353 Training Loss: tensor(0.3323)\n",
      "18354 Training Loss: tensor(0.3345)\n",
      "18355 Training Loss: tensor(0.3330)\n",
      "18356 Training Loss: tensor(0.3352)\n",
      "18357 Training Loss: tensor(0.3335)\n",
      "18358 Training Loss: tensor(0.3313)\n",
      "18359 Training Loss: tensor(0.3367)\n",
      "18360 Training Loss: tensor(0.3347)\n",
      "18361 Training Loss: tensor(0.3372)\n",
      "18362 Training Loss: tensor(0.3360)\n",
      "18363 Training Loss: tensor(0.3339)\n",
      "18364 Training Loss: tensor(0.3342)\n",
      "18365 Training Loss: tensor(0.3326)\n",
      "18366 Training Loss: tensor(0.3343)\n",
      "18367 Training Loss: tensor(0.3353)\n",
      "18368 Training Loss: tensor(0.3320)\n",
      "18369 Training Loss: tensor(0.3320)\n",
      "18370 Training Loss: tensor(0.3336)\n",
      "18371 Training Loss: tensor(0.3309)\n",
      "18372 Training Loss: tensor(0.3339)\n",
      "18373 Training Loss: tensor(0.3338)\n",
      "18374 Training Loss: tensor(0.3331)\n",
      "18375 Training Loss: tensor(0.3319)\n",
      "18376 Training Loss: tensor(0.3322)\n",
      "18377 Training Loss: tensor(0.3340)\n",
      "18378 Training Loss: tensor(0.3347)\n",
      "18379 Training Loss: tensor(0.3325)\n",
      "18380 Training Loss: tensor(0.3350)\n",
      "18381 Training Loss: tensor(0.3332)\n",
      "18382 Training Loss: tensor(0.3320)\n",
      "18383 Training Loss: tensor(0.3325)\n",
      "18384 Training Loss: tensor(0.3331)\n",
      "18385 Training Loss: tensor(0.3325)\n",
      "18386 Training Loss: tensor(0.3340)\n",
      "18387 Training Loss: tensor(0.3324)\n",
      "18388 Training Loss: tensor(0.3311)\n",
      "18389 Training Loss: tensor(0.3338)\n",
      "18390 Training Loss: tensor(0.3317)\n",
      "18391 Training Loss: tensor(0.3337)\n",
      "18392 Training Loss: tensor(0.3339)\n",
      "18393 Training Loss: tensor(0.3328)\n",
      "18394 Training Loss: tensor(0.3343)\n",
      "18395 Training Loss: tensor(0.3381)\n",
      "18396 Training Loss: tensor(0.3328)\n",
      "18397 Training Loss: tensor(0.3337)\n",
      "18398 Training Loss: tensor(0.3345)\n",
      "18399 Training Loss: tensor(0.3319)\n",
      "18400 Training Loss: tensor(0.3330)\n",
      "18401 Training Loss: tensor(0.3342)\n",
      "18402 Training Loss: tensor(0.3331)\n",
      "18403 Training Loss: tensor(0.3346)\n",
      "18404 Training Loss: tensor(0.3321)\n",
      "18405 Training Loss: tensor(0.3314)\n",
      "18406 Training Loss: tensor(0.3330)\n",
      "18407 Training Loss: tensor(0.3320)\n",
      "18408 Training Loss: tensor(0.3325)\n",
      "18409 Training Loss: tensor(0.3330)\n",
      "18410 Training Loss: tensor(0.3347)\n",
      "18411 Training Loss: tensor(0.3366)\n",
      "18412 Training Loss: tensor(0.3325)\n",
      "18413 Training Loss: tensor(0.3329)\n",
      "18414 Training Loss: tensor(0.3321)\n",
      "18415 Training Loss: tensor(0.3327)\n",
      "18416 Training Loss: tensor(0.3355)\n",
      "18417 Training Loss: tensor(0.3313)\n",
      "18418 Training Loss: tensor(0.3317)\n",
      "18419 Training Loss: tensor(0.3328)\n",
      "18420 Training Loss: tensor(0.3318)\n",
      "18421 Training Loss: tensor(0.3336)\n",
      "18422 Training Loss: tensor(0.3319)\n",
      "18423 Training Loss: tensor(0.3336)\n",
      "18424 Training Loss: tensor(0.3329)\n",
      "18425 Training Loss: tensor(0.3338)\n",
      "18426 Training Loss: tensor(0.3336)\n",
      "18427 Training Loss: tensor(0.3328)\n",
      "18428 Training Loss: tensor(0.3318)\n",
      "18429 Training Loss: tensor(0.3328)\n",
      "18430 Training Loss: tensor(0.3313)\n",
      "18431 Training Loss: tensor(0.3375)\n",
      "18432 Training Loss: tensor(0.3323)\n",
      "18433 Training Loss: tensor(0.3358)\n",
      "18434 Training Loss: tensor(0.3326)\n",
      "18435 Training Loss: tensor(0.3330)\n",
      "18436 Training Loss: tensor(0.3331)\n",
      "18437 Training Loss: tensor(0.3327)\n",
      "18438 Training Loss: tensor(0.3334)\n",
      "18439 Training Loss: tensor(0.3359)\n",
      "18440 Training Loss: tensor(0.3326)\n",
      "18441 Training Loss: tensor(0.3331)\n",
      "18442 Training Loss: tensor(0.3329)\n",
      "18443 Training Loss: tensor(0.3317)\n",
      "18444 Training Loss: tensor(0.3341)\n",
      "18445 Training Loss: tensor(0.3324)\n",
      "18446 Training Loss: tensor(0.3329)\n",
      "18447 Training Loss: tensor(0.3350)\n",
      "18448 Training Loss: tensor(0.3328)\n",
      "18449 Training Loss: tensor(0.3320)\n",
      "18450 Training Loss: tensor(0.3317)\n",
      "18451 Training Loss: tensor(0.3334)\n",
      "18452 Training Loss: tensor(0.3318)\n",
      "18453 Training Loss: tensor(0.3321)\n",
      "18454 Training Loss: tensor(0.3326)\n",
      "18455 Training Loss: tensor(0.3320)\n",
      "18456 Training Loss: tensor(0.3358)\n",
      "18457 Training Loss: tensor(0.3323)\n",
      "18458 Training Loss: tensor(0.3311)\n",
      "18459 Training Loss: tensor(0.3312)\n",
      "18460 Training Loss: tensor(0.3307)\n",
      "18461 Training Loss: tensor(0.3314)\n",
      "18462 Training Loss: tensor(0.3349)\n",
      "18463 Training Loss: tensor(0.3328)\n",
      "18464 Training Loss: tensor(0.3329)\n",
      "18465 Training Loss: tensor(0.3333)\n",
      "18466 Training Loss: tensor(0.3338)\n",
      "18467 Training Loss: tensor(0.3326)\n",
      "18468 Training Loss: tensor(0.3309)\n",
      "18469 Training Loss: tensor(0.3342)\n",
      "18470 Training Loss: tensor(0.3332)\n",
      "18471 Training Loss: tensor(0.3329)\n",
      "18472 Training Loss: tensor(0.3318)\n",
      "18473 Training Loss: tensor(0.3334)\n",
      "18474 Training Loss: tensor(0.3331)\n",
      "18475 Training Loss: tensor(0.3390)\n",
      "18476 Training Loss: tensor(0.3342)\n",
      "18477 Training Loss: tensor(0.3321)\n",
      "18478 Training Loss: tensor(0.3321)\n",
      "18479 Training Loss: tensor(0.3319)\n",
      "18480 Training Loss: tensor(0.3327)\n",
      "18481 Training Loss: tensor(0.3332)\n",
      "18482 Training Loss: tensor(0.3308)\n",
      "18483 Training Loss: tensor(0.3323)\n",
      "18484 Training Loss: tensor(0.3321)\n",
      "18485 Training Loss: tensor(0.3351)\n",
      "18486 Training Loss: tensor(0.3321)\n",
      "18487 Training Loss: tensor(0.3322)\n",
      "18488 Training Loss: tensor(0.3387)\n",
      "18489 Training Loss: tensor(0.3317)\n",
      "18490 Training Loss: tensor(0.3315)\n",
      "18491 Training Loss: tensor(0.3357)\n",
      "18492 Training Loss: tensor(0.3315)\n",
      "18493 Training Loss: tensor(0.3319)\n",
      "18494 Training Loss: tensor(0.3316)\n",
      "18495 Training Loss: tensor(0.3330)\n",
      "18496 Training Loss: tensor(0.3301)\n",
      "18497 Training Loss: tensor(0.3374)\n",
      "18498 Training Loss: tensor(0.3320)\n",
      "18499 Training Loss: tensor(0.3325)\n",
      "18500 Training Loss: tensor(0.3319)\n",
      "18501 Training Loss: tensor(0.3338)\n",
      "18502 Training Loss: tensor(0.3325)\n",
      "18503 Training Loss: tensor(0.3340)\n",
      "18504 Training Loss: tensor(0.3356)\n",
      "18505 Training Loss: tensor(0.3319)\n",
      "18506 Training Loss: tensor(0.3320)\n",
      "18507 Training Loss: tensor(0.3318)\n",
      "18508 Training Loss: tensor(0.3322)\n",
      "18509 Training Loss: tensor(0.3318)\n",
      "18510 Training Loss: tensor(0.3346)\n",
      "18511 Training Loss: tensor(0.3331)\n",
      "18512 Training Loss: tensor(0.3331)\n",
      "18513 Training Loss: tensor(0.3317)\n",
      "18514 Training Loss: tensor(0.3357)\n",
      "18515 Training Loss: tensor(0.3323)\n",
      "18516 Training Loss: tensor(0.3324)\n",
      "18517 Training Loss: tensor(0.3346)\n",
      "18518 Training Loss: tensor(0.3321)\n",
      "18519 Training Loss: tensor(0.3344)\n",
      "18520 Training Loss: tensor(0.3352)\n",
      "18521 Training Loss: tensor(0.3358)\n",
      "18522 Training Loss: tensor(0.3322)\n",
      "18523 Training Loss: tensor(0.3321)\n",
      "18524 Training Loss: tensor(0.3345)\n",
      "18525 Training Loss: tensor(0.3327)\n",
      "18526 Training Loss: tensor(0.3331)\n",
      "18527 Training Loss: tensor(0.3321)\n",
      "18528 Training Loss: tensor(0.3332)\n",
      "18529 Training Loss: tensor(0.3323)\n",
      "18530 Training Loss: tensor(0.3361)\n",
      "18531 Training Loss: tensor(0.3328)\n",
      "18532 Training Loss: tensor(0.3311)\n",
      "18533 Training Loss: tensor(0.3324)\n",
      "18534 Training Loss: tensor(0.3313)\n",
      "18535 Training Loss: tensor(0.3305)\n",
      "18536 Training Loss: tensor(0.3307)\n",
      "18537 Training Loss: tensor(0.3326)\n",
      "18538 Training Loss: tensor(0.3324)\n",
      "18539 Training Loss: tensor(0.3329)\n",
      "18540 Training Loss: tensor(0.3342)\n",
      "18541 Training Loss: tensor(0.3313)\n",
      "18542 Training Loss: tensor(0.3327)\n",
      "18543 Training Loss: tensor(0.3331)\n",
      "18544 Training Loss: tensor(0.3337)\n",
      "18545 Training Loss: tensor(0.3415)\n",
      "18546 Training Loss: tensor(0.3325)\n",
      "18547 Training Loss: tensor(0.3336)\n",
      "18548 Training Loss: tensor(0.3343)\n",
      "18549 Training Loss: tensor(0.3315)\n",
      "18550 Training Loss: tensor(0.3417)\n",
      "18551 Training Loss: tensor(0.3327)\n",
      "18552 Training Loss: tensor(0.3312)\n",
      "18553 Training Loss: tensor(0.3334)\n",
      "18554 Training Loss: tensor(0.3345)\n",
      "18555 Training Loss: tensor(0.3340)\n",
      "18556 Training Loss: tensor(0.3340)\n",
      "18557 Training Loss: tensor(0.3327)\n",
      "18558 Training Loss: tensor(0.3365)\n",
      "18559 Training Loss: tensor(0.3343)\n",
      "18560 Training Loss: tensor(0.3346)\n",
      "18561 Training Loss: tensor(0.3324)\n",
      "18562 Training Loss: tensor(0.3335)\n",
      "18563 Training Loss: tensor(0.3317)\n",
      "18564 Training Loss: tensor(0.3324)\n",
      "18565 Training Loss: tensor(0.3327)\n",
      "18566 Training Loss: tensor(0.3328)\n",
      "18567 Training Loss: tensor(0.3422)\n",
      "18568 Training Loss: tensor(0.3309)\n",
      "18569 Training Loss: tensor(0.3320)\n",
      "18570 Training Loss: tensor(0.3379)\n",
      "18571 Training Loss: tensor(0.3375)\n",
      "18572 Training Loss: tensor(0.3320)\n",
      "18573 Training Loss: tensor(0.3341)\n",
      "18574 Training Loss: tensor(0.3335)\n",
      "18575 Training Loss: tensor(0.3311)\n",
      "18576 Training Loss: tensor(0.3334)\n",
      "18577 Training Loss: tensor(0.3339)\n",
      "18578 Training Loss: tensor(0.3343)\n",
      "18579 Training Loss: tensor(0.3352)\n",
      "18580 Training Loss: tensor(0.3326)\n",
      "18581 Training Loss: tensor(0.3351)\n",
      "18582 Training Loss: tensor(0.3314)\n",
      "18583 Training Loss: tensor(0.3333)\n",
      "18584 Training Loss: tensor(0.3314)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18585 Training Loss: tensor(0.3316)\n",
      "18586 Training Loss: tensor(0.3345)\n",
      "18587 Training Loss: tensor(0.3317)\n",
      "18588 Training Loss: tensor(0.3323)\n",
      "18589 Training Loss: tensor(0.3357)\n",
      "18590 Training Loss: tensor(0.3323)\n",
      "18591 Training Loss: tensor(0.3368)\n",
      "18592 Training Loss: tensor(0.3322)\n",
      "18593 Training Loss: tensor(0.3325)\n",
      "18594 Training Loss: tensor(0.3351)\n",
      "18595 Training Loss: tensor(0.3321)\n",
      "18596 Training Loss: tensor(0.3368)\n",
      "18597 Training Loss: tensor(0.3333)\n",
      "18598 Training Loss: tensor(0.3327)\n",
      "18599 Training Loss: tensor(0.3341)\n",
      "18600 Training Loss: tensor(0.3326)\n",
      "18601 Training Loss: tensor(0.3324)\n",
      "18602 Training Loss: tensor(0.3314)\n",
      "18603 Training Loss: tensor(0.3347)\n",
      "18604 Training Loss: tensor(0.3328)\n",
      "18605 Training Loss: tensor(0.3315)\n",
      "18606 Training Loss: tensor(0.3316)\n",
      "18607 Training Loss: tensor(0.3329)\n",
      "18608 Training Loss: tensor(0.3318)\n",
      "18609 Training Loss: tensor(0.3325)\n",
      "18610 Training Loss: tensor(0.3341)\n",
      "18611 Training Loss: tensor(0.3343)\n",
      "18612 Training Loss: tensor(0.3326)\n",
      "18613 Training Loss: tensor(0.3308)\n",
      "18614 Training Loss: tensor(0.3346)\n",
      "18615 Training Loss: tensor(0.3354)\n",
      "18616 Training Loss: tensor(0.3383)\n",
      "18617 Training Loss: tensor(0.3327)\n",
      "18618 Training Loss: tensor(0.3405)\n",
      "18619 Training Loss: tensor(0.3327)\n",
      "18620 Training Loss: tensor(0.3418)\n",
      "18621 Training Loss: tensor(0.3340)\n",
      "18622 Training Loss: tensor(0.3329)\n",
      "18623 Training Loss: tensor(0.3326)\n",
      "18624 Training Loss: tensor(0.3326)\n",
      "18625 Training Loss: tensor(0.3323)\n",
      "18626 Training Loss: tensor(0.3340)\n",
      "18627 Training Loss: tensor(0.3328)\n",
      "18628 Training Loss: tensor(0.3375)\n",
      "18629 Training Loss: tensor(0.3330)\n",
      "18630 Training Loss: tensor(0.3328)\n",
      "18631 Training Loss: tensor(0.3364)\n",
      "18632 Training Loss: tensor(0.3321)\n",
      "18633 Training Loss: tensor(0.3344)\n",
      "18634 Training Loss: tensor(0.3322)\n",
      "18635 Training Loss: tensor(0.3345)\n",
      "18636 Training Loss: tensor(0.3324)\n",
      "18637 Training Loss: tensor(0.3347)\n",
      "18638 Training Loss: tensor(0.3394)\n",
      "18639 Training Loss: tensor(0.3338)\n",
      "18640 Training Loss: tensor(0.3357)\n",
      "18641 Training Loss: tensor(0.3324)\n",
      "18642 Training Loss: tensor(0.3346)\n",
      "18643 Training Loss: tensor(0.3351)\n",
      "18644 Training Loss: tensor(0.3343)\n",
      "18645 Training Loss: tensor(0.3351)\n",
      "18646 Training Loss: tensor(0.3363)\n",
      "18647 Training Loss: tensor(0.3334)\n",
      "18648 Training Loss: tensor(0.3336)\n",
      "18649 Training Loss: tensor(0.3330)\n",
      "18650 Training Loss: tensor(0.3336)\n",
      "18651 Training Loss: tensor(0.3327)\n",
      "18652 Training Loss: tensor(0.3322)\n",
      "18653 Training Loss: tensor(0.3316)\n",
      "18654 Training Loss: tensor(0.3334)\n",
      "18655 Training Loss: tensor(0.3330)\n",
      "18656 Training Loss: tensor(0.3317)\n",
      "18657 Training Loss: tensor(0.3323)\n",
      "18658 Training Loss: tensor(0.3339)\n",
      "18659 Training Loss: tensor(0.3317)\n",
      "18660 Training Loss: tensor(0.3324)\n",
      "18661 Training Loss: tensor(0.3335)\n",
      "18662 Training Loss: tensor(0.3341)\n",
      "18663 Training Loss: tensor(0.3366)\n",
      "18664 Training Loss: tensor(0.3317)\n",
      "18665 Training Loss: tensor(0.3315)\n",
      "18666 Training Loss: tensor(0.3328)\n",
      "18667 Training Loss: tensor(0.3370)\n",
      "18668 Training Loss: tensor(0.3340)\n",
      "18669 Training Loss: tensor(0.3332)\n",
      "18670 Training Loss: tensor(0.3318)\n",
      "18671 Training Loss: tensor(0.3316)\n",
      "18672 Training Loss: tensor(0.3332)\n",
      "18673 Training Loss: tensor(0.3342)\n",
      "18674 Training Loss: tensor(0.3321)\n",
      "18675 Training Loss: tensor(0.3353)\n",
      "18676 Training Loss: tensor(0.3325)\n",
      "18677 Training Loss: tensor(0.3339)\n",
      "18678 Training Loss: tensor(0.3320)\n",
      "18679 Training Loss: tensor(0.3382)\n",
      "18680 Training Loss: tensor(0.3328)\n",
      "18681 Training Loss: tensor(0.3309)\n",
      "18682 Training Loss: tensor(0.3329)\n",
      "18683 Training Loss: tensor(0.3323)\n",
      "18684 Training Loss: tensor(0.3343)\n",
      "18685 Training Loss: tensor(0.3330)\n",
      "18686 Training Loss: tensor(0.3319)\n",
      "18687 Training Loss: tensor(0.3303)\n",
      "18688 Training Loss: tensor(0.3339)\n",
      "18689 Training Loss: tensor(0.3336)\n",
      "18690 Training Loss: tensor(0.3334)\n",
      "18691 Training Loss: tensor(0.3312)\n",
      "18692 Training Loss: tensor(0.3320)\n",
      "18693 Training Loss: tensor(0.3317)\n",
      "18694 Training Loss: tensor(0.3331)\n",
      "18695 Training Loss: tensor(0.3306)\n",
      "18696 Training Loss: tensor(0.3311)\n",
      "18697 Training Loss: tensor(0.3328)\n",
      "18698 Training Loss: tensor(0.3313)\n",
      "18699 Training Loss: tensor(0.3323)\n",
      "18700 Training Loss: tensor(0.3333)\n",
      "18701 Training Loss: tensor(0.3335)\n",
      "18702 Training Loss: tensor(0.3328)\n",
      "18703 Training Loss: tensor(0.3329)\n",
      "18704 Training Loss: tensor(0.3321)\n",
      "18705 Training Loss: tensor(0.3315)\n",
      "18706 Training Loss: tensor(0.3327)\n",
      "18707 Training Loss: tensor(0.3309)\n",
      "18708 Training Loss: tensor(0.3330)\n",
      "18709 Training Loss: tensor(0.3314)\n",
      "18710 Training Loss: tensor(0.3350)\n",
      "18711 Training Loss: tensor(0.3321)\n",
      "18712 Training Loss: tensor(0.3315)\n",
      "18713 Training Loss: tensor(0.3315)\n",
      "18714 Training Loss: tensor(0.3346)\n",
      "18715 Training Loss: tensor(0.3330)\n",
      "18716 Training Loss: tensor(0.3318)\n",
      "18717 Training Loss: tensor(0.3321)\n",
      "18718 Training Loss: tensor(0.3316)\n",
      "18719 Training Loss: tensor(0.3349)\n",
      "18720 Training Loss: tensor(0.3317)\n",
      "18721 Training Loss: tensor(0.3330)\n",
      "18722 Training Loss: tensor(0.3312)\n",
      "18723 Training Loss: tensor(0.3320)\n",
      "18724 Training Loss: tensor(0.3334)\n",
      "18725 Training Loss: tensor(0.3320)\n",
      "18726 Training Loss: tensor(0.3312)\n",
      "18727 Training Loss: tensor(0.3320)\n",
      "18728 Training Loss: tensor(0.3314)\n",
      "18729 Training Loss: tensor(0.3305)\n",
      "18730 Training Loss: tensor(0.3315)\n",
      "18731 Training Loss: tensor(0.3309)\n",
      "18732 Training Loss: tensor(0.3322)\n",
      "18733 Training Loss: tensor(0.3332)\n",
      "18734 Training Loss: tensor(0.3317)\n",
      "18735 Training Loss: tensor(0.3301)\n",
      "18736 Training Loss: tensor(0.3306)\n",
      "18737 Training Loss: tensor(0.3395)\n",
      "18738 Training Loss: tensor(0.3336)\n",
      "18739 Training Loss: tensor(0.3315)\n",
      "18740 Training Loss: tensor(0.3298)\n",
      "18741 Training Loss: tensor(0.3351)\n",
      "18742 Training Loss: tensor(0.3330)\n",
      "18743 Training Loss: tensor(0.3355)\n",
      "18744 Training Loss: tensor(0.3312)\n",
      "18745 Training Loss: tensor(0.3328)\n",
      "18746 Training Loss: tensor(0.3375)\n",
      "18747 Training Loss: tensor(0.3332)\n",
      "18748 Training Loss: tensor(0.3340)\n",
      "18749 Training Loss: tensor(0.3366)\n",
      "18750 Training Loss: tensor(0.3317)\n",
      "18751 Training Loss: tensor(0.3317)\n",
      "18752 Training Loss: tensor(0.3335)\n",
      "18753 Training Loss: tensor(0.3318)\n",
      "18754 Training Loss: tensor(0.3356)\n",
      "18755 Training Loss: tensor(0.3328)\n",
      "18756 Training Loss: tensor(0.3335)\n",
      "18757 Training Loss: tensor(0.3367)\n",
      "18758 Training Loss: tensor(0.3330)\n",
      "18759 Training Loss: tensor(0.3349)\n",
      "18760 Training Loss: tensor(0.3339)\n",
      "18761 Training Loss: tensor(0.3317)\n",
      "18762 Training Loss: tensor(0.3312)\n",
      "18763 Training Loss: tensor(0.3326)\n",
      "18764 Training Loss: tensor(0.3331)\n",
      "18765 Training Loss: tensor(0.3321)\n",
      "18766 Training Loss: tensor(0.3340)\n",
      "18767 Training Loss: tensor(0.3339)\n",
      "18768 Training Loss: tensor(0.3317)\n",
      "18769 Training Loss: tensor(0.3358)\n",
      "18770 Training Loss: tensor(0.3347)\n",
      "18771 Training Loss: tensor(0.3351)\n",
      "18772 Training Loss: tensor(0.3322)\n",
      "18773 Training Loss: tensor(0.3338)\n",
      "18774 Training Loss: tensor(0.3313)\n",
      "18775 Training Loss: tensor(0.3323)\n",
      "18776 Training Loss: tensor(0.3328)\n",
      "18777 Training Loss: tensor(0.3310)\n",
      "18778 Training Loss: tensor(0.3312)\n",
      "18779 Training Loss: tensor(0.3318)\n",
      "18780 Training Loss: tensor(0.3316)\n",
      "18781 Training Loss: tensor(0.3306)\n",
      "18782 Training Loss: tensor(0.3325)\n",
      "18783 Training Loss: tensor(0.3320)\n",
      "18784 Training Loss: tensor(0.3389)\n",
      "18785 Training Loss: tensor(0.3327)\n",
      "18786 Training Loss: tensor(0.3399)\n",
      "18787 Training Loss: tensor(0.3327)\n",
      "18788 Training Loss: tensor(0.3305)\n",
      "18789 Training Loss: tensor(0.3344)\n",
      "18790 Training Loss: tensor(0.3315)\n",
      "18791 Training Loss: tensor(0.3324)\n",
      "18792 Training Loss: tensor(0.3319)\n",
      "18793 Training Loss: tensor(0.3324)\n",
      "18794 Training Loss: tensor(0.3326)\n",
      "18795 Training Loss: tensor(0.3328)\n",
      "18796 Training Loss: tensor(0.3330)\n",
      "18797 Training Loss: tensor(0.3335)\n",
      "18798 Training Loss: tensor(0.3368)\n",
      "18799 Training Loss: tensor(0.3314)\n",
      "18800 Training Loss: tensor(0.3353)\n",
      "18801 Training Loss: tensor(0.3325)\n",
      "18802 Training Loss: tensor(0.3330)\n",
      "18803 Training Loss: tensor(0.3342)\n",
      "18804 Training Loss: tensor(0.3325)\n",
      "18805 Training Loss: tensor(0.3370)\n",
      "18806 Training Loss: tensor(0.3324)\n",
      "18807 Training Loss: tensor(0.3336)\n",
      "18808 Training Loss: tensor(0.3313)\n",
      "18809 Training Loss: tensor(0.3355)\n",
      "18810 Training Loss: tensor(0.3333)\n",
      "18811 Training Loss: tensor(0.3335)\n",
      "18812 Training Loss: tensor(0.3311)\n",
      "18813 Training Loss: tensor(0.3331)\n",
      "18814 Training Loss: tensor(0.3315)\n",
      "18815 Training Loss: tensor(0.3347)\n",
      "18816 Training Loss: tensor(0.3309)\n",
      "18817 Training Loss: tensor(0.3344)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18818 Training Loss: tensor(0.3377)\n",
      "18819 Training Loss: tensor(0.3351)\n",
      "18820 Training Loss: tensor(0.3320)\n",
      "18821 Training Loss: tensor(0.3340)\n",
      "18822 Training Loss: tensor(0.3337)\n",
      "18823 Training Loss: tensor(0.3328)\n",
      "18824 Training Loss: tensor(0.3347)\n",
      "18825 Training Loss: tensor(0.3329)\n",
      "18826 Training Loss: tensor(0.3335)\n",
      "18827 Training Loss: tensor(0.3348)\n",
      "18828 Training Loss: tensor(0.3373)\n",
      "18829 Training Loss: tensor(0.3353)\n",
      "18830 Training Loss: tensor(0.3360)\n",
      "18831 Training Loss: tensor(0.3336)\n",
      "18832 Training Loss: tensor(0.3333)\n",
      "18833 Training Loss: tensor(0.3330)\n",
      "18834 Training Loss: tensor(0.3330)\n",
      "18835 Training Loss: tensor(0.3323)\n",
      "18836 Training Loss: tensor(0.3317)\n",
      "18837 Training Loss: tensor(0.3328)\n",
      "18838 Training Loss: tensor(0.3328)\n",
      "18839 Training Loss: tensor(0.3318)\n",
      "18840 Training Loss: tensor(0.3335)\n",
      "18841 Training Loss: tensor(0.3328)\n",
      "18842 Training Loss: tensor(0.3319)\n",
      "18843 Training Loss: tensor(0.3325)\n",
      "18844 Training Loss: tensor(0.3330)\n",
      "18845 Training Loss: tensor(0.3332)\n",
      "18846 Training Loss: tensor(0.3348)\n",
      "18847 Training Loss: tensor(0.3338)\n",
      "18848 Training Loss: tensor(0.3323)\n",
      "18849 Training Loss: tensor(0.3334)\n",
      "18850 Training Loss: tensor(0.3318)\n",
      "18851 Training Loss: tensor(0.3322)\n",
      "18852 Training Loss: tensor(0.3315)\n",
      "18853 Training Loss: tensor(0.3331)\n",
      "18854 Training Loss: tensor(0.3326)\n",
      "18855 Training Loss: tensor(0.3352)\n",
      "18856 Training Loss: tensor(0.3327)\n",
      "18857 Training Loss: tensor(0.3313)\n",
      "18858 Training Loss: tensor(0.3313)\n",
      "18859 Training Loss: tensor(0.3338)\n",
      "18860 Training Loss: tensor(0.3336)\n",
      "18861 Training Loss: tensor(0.3328)\n",
      "18862 Training Loss: tensor(0.3351)\n",
      "18863 Training Loss: tensor(0.3350)\n",
      "18864 Training Loss: tensor(0.3318)\n",
      "18865 Training Loss: tensor(0.3318)\n",
      "18866 Training Loss: tensor(0.3357)\n",
      "18867 Training Loss: tensor(0.3343)\n",
      "18868 Training Loss: tensor(0.3336)\n",
      "18869 Training Loss: tensor(0.3347)\n",
      "18870 Training Loss: tensor(0.3350)\n",
      "18871 Training Loss: tensor(0.3320)\n",
      "18872 Training Loss: tensor(0.3345)\n",
      "18873 Training Loss: tensor(0.3337)\n",
      "18874 Training Loss: tensor(0.3321)\n",
      "18875 Training Loss: tensor(0.3317)\n",
      "18876 Training Loss: tensor(0.3336)\n",
      "18877 Training Loss: tensor(0.3329)\n",
      "18878 Training Loss: tensor(0.3329)\n",
      "18879 Training Loss: tensor(0.3319)\n",
      "18880 Training Loss: tensor(0.3329)\n",
      "18881 Training Loss: tensor(0.3318)\n",
      "18882 Training Loss: tensor(0.3324)\n",
      "18883 Training Loss: tensor(0.3336)\n",
      "18884 Training Loss: tensor(0.3330)\n",
      "18885 Training Loss: tensor(0.3321)\n",
      "18886 Training Loss: tensor(0.3308)\n",
      "18887 Training Loss: tensor(0.3330)\n",
      "18888 Training Loss: tensor(0.3327)\n",
      "18889 Training Loss: tensor(0.3314)\n",
      "18890 Training Loss: tensor(0.3335)\n",
      "18891 Training Loss: tensor(0.3347)\n",
      "18892 Training Loss: tensor(0.3343)\n",
      "18893 Training Loss: tensor(0.3315)\n",
      "18894 Training Loss: tensor(0.3317)\n",
      "18895 Training Loss: tensor(0.3340)\n",
      "18896 Training Loss: tensor(0.3335)\n",
      "18897 Training Loss: tensor(0.3314)\n",
      "18898 Training Loss: tensor(0.3308)\n",
      "18899 Training Loss: tensor(0.3342)\n",
      "18900 Training Loss: tensor(0.3333)\n",
      "18901 Training Loss: tensor(0.3321)\n",
      "18902 Training Loss: tensor(0.3336)\n",
      "18903 Training Loss: tensor(0.3319)\n",
      "18904 Training Loss: tensor(0.3310)\n",
      "18905 Training Loss: tensor(0.3324)\n",
      "18906 Training Loss: tensor(0.3314)\n",
      "18907 Training Loss: tensor(0.3315)\n",
      "18908 Training Loss: tensor(0.3321)\n",
      "18909 Training Loss: tensor(0.3355)\n",
      "18910 Training Loss: tensor(0.3330)\n",
      "18911 Training Loss: tensor(0.3327)\n",
      "18912 Training Loss: tensor(0.3322)\n",
      "18913 Training Loss: tensor(0.3314)\n",
      "18914 Training Loss: tensor(0.3316)\n",
      "18915 Training Loss: tensor(0.3382)\n",
      "18916 Training Loss: tensor(0.3369)\n",
      "18917 Training Loss: tensor(0.3320)\n",
      "18918 Training Loss: tensor(0.3318)\n",
      "18919 Training Loss: tensor(0.3336)\n",
      "18920 Training Loss: tensor(0.3316)\n",
      "18921 Training Loss: tensor(0.3363)\n",
      "18922 Training Loss: tensor(0.3349)\n",
      "18923 Training Loss: tensor(0.3359)\n",
      "18924 Training Loss: tensor(0.3337)\n",
      "18925 Training Loss: tensor(0.3336)\n",
      "18926 Training Loss: tensor(0.3329)\n",
      "18927 Training Loss: tensor(0.3324)\n",
      "18928 Training Loss: tensor(0.3331)\n",
      "18929 Training Loss: tensor(0.3377)\n",
      "18930 Training Loss: tensor(0.3321)\n",
      "18931 Training Loss: tensor(0.3368)\n",
      "18932 Training Loss: tensor(0.3336)\n",
      "18933 Training Loss: tensor(0.3348)\n",
      "18934 Training Loss: tensor(0.3335)\n",
      "18935 Training Loss: tensor(0.3322)\n",
      "18936 Training Loss: tensor(0.3320)\n",
      "18937 Training Loss: tensor(0.3350)\n",
      "18938 Training Loss: tensor(0.3312)\n",
      "18939 Training Loss: tensor(0.3335)\n",
      "18940 Training Loss: tensor(0.3316)\n",
      "18941 Training Loss: tensor(0.3319)\n",
      "18942 Training Loss: tensor(0.3337)\n",
      "18943 Training Loss: tensor(0.3319)\n",
      "18944 Training Loss: tensor(0.3355)\n",
      "18945 Training Loss: tensor(0.3329)\n",
      "18946 Training Loss: tensor(0.3370)\n",
      "18947 Training Loss: tensor(0.3318)\n",
      "18948 Training Loss: tensor(0.3321)\n",
      "18949 Training Loss: tensor(0.3318)\n",
      "18950 Training Loss: tensor(0.3331)\n",
      "18951 Training Loss: tensor(0.3339)\n",
      "18952 Training Loss: tensor(0.3320)\n",
      "18953 Training Loss: tensor(0.3390)\n",
      "18954 Training Loss: tensor(0.3320)\n",
      "18955 Training Loss: tensor(0.3328)\n",
      "18956 Training Loss: tensor(0.3327)\n",
      "18957 Training Loss: tensor(0.3328)\n",
      "18958 Training Loss: tensor(0.3325)\n",
      "18959 Training Loss: tensor(0.3309)\n",
      "18960 Training Loss: tensor(0.3310)\n",
      "18961 Training Loss: tensor(0.3318)\n",
      "18962 Training Loss: tensor(0.3330)\n",
      "18963 Training Loss: tensor(0.3320)\n",
      "18964 Training Loss: tensor(0.3316)\n",
      "18965 Training Loss: tensor(0.3316)\n",
      "18966 Training Loss: tensor(0.3389)\n",
      "18967 Training Loss: tensor(0.3324)\n",
      "18968 Training Loss: tensor(0.3328)\n",
      "18969 Training Loss: tensor(0.3329)\n",
      "18970 Training Loss: tensor(0.3315)\n",
      "18971 Training Loss: tensor(0.3338)\n",
      "18972 Training Loss: tensor(0.3329)\n",
      "18973 Training Loss: tensor(0.3343)\n",
      "18974 Training Loss: tensor(0.3351)\n",
      "18975 Training Loss: tensor(0.3305)\n",
      "18976 Training Loss: tensor(0.3343)\n",
      "18977 Training Loss: tensor(0.3331)\n",
      "18978 Training Loss: tensor(0.3313)\n",
      "18979 Training Loss: tensor(0.3322)\n",
      "18980 Training Loss: tensor(0.3352)\n",
      "18981 Training Loss: tensor(0.3357)\n",
      "18982 Training Loss: tensor(0.3317)\n",
      "18983 Training Loss: tensor(0.3337)\n",
      "18984 Training Loss: tensor(0.3342)\n",
      "18985 Training Loss: tensor(0.3339)\n",
      "18986 Training Loss: tensor(0.3317)\n",
      "18987 Training Loss: tensor(0.3325)\n",
      "18988 Training Loss: tensor(0.3340)\n",
      "18989 Training Loss: tensor(0.3348)\n",
      "18990 Training Loss: tensor(0.3313)\n",
      "18991 Training Loss: tensor(0.3332)\n",
      "18992 Training Loss: tensor(0.3393)\n",
      "18993 Training Loss: tensor(0.3315)\n",
      "18994 Training Loss: tensor(0.3323)\n",
      "18995 Training Loss: tensor(0.3326)\n",
      "18996 Training Loss: tensor(0.3402)\n",
      "18997 Training Loss: tensor(0.3355)\n",
      "18998 Training Loss: tensor(0.3347)\n",
      "18999 Training Loss: tensor(0.3352)\n",
      "19000 Training Loss: tensor(0.3339)\n",
      "19001 Training Loss: tensor(0.3337)\n",
      "19002 Training Loss: tensor(0.3333)\n",
      "19003 Training Loss: tensor(0.3322)\n",
      "19004 Training Loss: tensor(0.3357)\n",
      "19005 Training Loss: tensor(0.3339)\n",
      "19006 Training Loss: tensor(0.3343)\n",
      "19007 Training Loss: tensor(0.3320)\n",
      "19008 Training Loss: tensor(0.3320)\n",
      "19009 Training Loss: tensor(0.3324)\n",
      "19010 Training Loss: tensor(0.3325)\n",
      "19011 Training Loss: tensor(0.3321)\n",
      "19012 Training Loss: tensor(0.3334)\n",
      "19013 Training Loss: tensor(0.3335)\n",
      "19014 Training Loss: tensor(0.3345)\n",
      "19015 Training Loss: tensor(0.3328)\n",
      "19016 Training Loss: tensor(0.3356)\n",
      "19017 Training Loss: tensor(0.3325)\n",
      "19018 Training Loss: tensor(0.3317)\n",
      "19019 Training Loss: tensor(0.3317)\n",
      "19020 Training Loss: tensor(0.3307)\n",
      "19021 Training Loss: tensor(0.3330)\n",
      "19022 Training Loss: tensor(0.3313)\n",
      "19023 Training Loss: tensor(0.3311)\n",
      "19024 Training Loss: tensor(0.3330)\n",
      "19025 Training Loss: tensor(0.3341)\n",
      "19026 Training Loss: tensor(0.3322)\n",
      "19027 Training Loss: tensor(0.3318)\n",
      "19028 Training Loss: tensor(0.3329)\n",
      "19029 Training Loss: tensor(0.3318)\n",
      "19030 Training Loss: tensor(0.3326)\n",
      "19031 Training Loss: tensor(0.3326)\n",
      "19032 Training Loss: tensor(0.3333)\n",
      "19033 Training Loss: tensor(0.3320)\n",
      "19034 Training Loss: tensor(0.3348)\n",
      "19035 Training Loss: tensor(0.3339)\n",
      "19036 Training Loss: tensor(0.3343)\n",
      "19037 Training Loss: tensor(0.3327)\n",
      "19038 Training Loss: tensor(0.3306)\n",
      "19039 Training Loss: tensor(0.3311)\n",
      "19040 Training Loss: tensor(0.3325)\n",
      "19041 Training Loss: tensor(0.3326)\n",
      "19042 Training Loss: tensor(0.3380)\n",
      "19043 Training Loss: tensor(0.3374)\n",
      "19044 Training Loss: tensor(0.3309)\n",
      "19045 Training Loss: tensor(0.3325)\n",
      "19046 Training Loss: tensor(0.3333)\n",
      "19047 Training Loss: tensor(0.3322)\n",
      "19048 Training Loss: tensor(0.3328)\n",
      "19049 Training Loss: tensor(0.3337)\n",
      "19050 Training Loss: tensor(0.3319)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19051 Training Loss: tensor(0.3319)\n",
      "19052 Training Loss: tensor(0.3322)\n",
      "19053 Training Loss: tensor(0.3334)\n",
      "19054 Training Loss: tensor(0.3315)\n",
      "19055 Training Loss: tensor(0.3321)\n",
      "19056 Training Loss: tensor(0.3331)\n",
      "19057 Training Loss: tensor(0.3369)\n",
      "19058 Training Loss: tensor(0.3321)\n",
      "19059 Training Loss: tensor(0.3316)\n",
      "19060 Training Loss: tensor(0.3333)\n",
      "19061 Training Loss: tensor(0.3323)\n",
      "19062 Training Loss: tensor(0.3310)\n",
      "19063 Training Loss: tensor(0.3316)\n",
      "19064 Training Loss: tensor(0.3313)\n",
      "19065 Training Loss: tensor(0.3315)\n",
      "19066 Training Loss: tensor(0.3319)\n",
      "19067 Training Loss: tensor(0.3400)\n",
      "19068 Training Loss: tensor(0.3333)\n",
      "19069 Training Loss: tensor(0.3312)\n",
      "19070 Training Loss: tensor(0.3311)\n",
      "19071 Training Loss: tensor(0.3342)\n",
      "19072 Training Loss: tensor(0.3333)\n",
      "19073 Training Loss: tensor(0.3315)\n",
      "19074 Training Loss: tensor(0.3343)\n",
      "19075 Training Loss: tensor(0.3320)\n",
      "19076 Training Loss: tensor(0.3318)\n",
      "19077 Training Loss: tensor(0.3363)\n",
      "19078 Training Loss: tensor(0.3325)\n",
      "19079 Training Loss: tensor(0.3323)\n",
      "19080 Training Loss: tensor(0.3366)\n",
      "19081 Training Loss: tensor(0.3323)\n",
      "19082 Training Loss: tensor(0.3317)\n",
      "19083 Training Loss: tensor(0.3337)\n",
      "19084 Training Loss: tensor(0.3327)\n",
      "19085 Training Loss: tensor(0.3323)\n",
      "19086 Training Loss: tensor(0.3327)\n",
      "19087 Training Loss: tensor(0.3343)\n",
      "19088 Training Loss: tensor(0.3318)\n",
      "19089 Training Loss: tensor(0.3319)\n",
      "19090 Training Loss: tensor(0.3324)\n",
      "19091 Training Loss: tensor(0.3318)\n",
      "19092 Training Loss: tensor(0.3321)\n",
      "19093 Training Loss: tensor(0.3310)\n",
      "19094 Training Loss: tensor(0.3332)\n",
      "19095 Training Loss: tensor(0.3406)\n",
      "19096 Training Loss: tensor(0.3316)\n",
      "19097 Training Loss: tensor(0.3344)\n",
      "19098 Training Loss: tensor(0.3327)\n",
      "19099 Training Loss: tensor(0.3324)\n",
      "19100 Training Loss: tensor(0.3321)\n",
      "19101 Training Loss: tensor(0.3322)\n",
      "19102 Training Loss: tensor(0.3325)\n",
      "19103 Training Loss: tensor(0.3319)\n",
      "19104 Training Loss: tensor(0.3334)\n",
      "19105 Training Loss: tensor(0.3313)\n",
      "19106 Training Loss: tensor(0.3320)\n",
      "19107 Training Loss: tensor(0.3322)\n",
      "19108 Training Loss: tensor(0.3337)\n",
      "19109 Training Loss: tensor(0.3304)\n",
      "19110 Training Loss: tensor(0.3345)\n",
      "19111 Training Loss: tensor(0.3335)\n",
      "19112 Training Loss: tensor(0.3313)\n",
      "19113 Training Loss: tensor(0.3342)\n",
      "19114 Training Loss: tensor(0.3321)\n",
      "19115 Training Loss: tensor(0.3331)\n",
      "19116 Training Loss: tensor(0.3311)\n",
      "19117 Training Loss: tensor(0.3323)\n",
      "19118 Training Loss: tensor(0.3341)\n",
      "19119 Training Loss: tensor(0.3331)\n",
      "19120 Training Loss: tensor(0.3318)\n",
      "19121 Training Loss: tensor(0.3306)\n",
      "19122 Training Loss: tensor(0.3326)\n",
      "19123 Training Loss: tensor(0.3320)\n",
      "19124 Training Loss: tensor(0.3329)\n",
      "19125 Training Loss: tensor(0.3301)\n",
      "19126 Training Loss: tensor(0.3321)\n",
      "19127 Training Loss: tensor(0.3320)\n",
      "19128 Training Loss: tensor(0.3345)\n",
      "19129 Training Loss: tensor(0.3322)\n",
      "19130 Training Loss: tensor(0.3326)\n",
      "19131 Training Loss: tensor(0.3317)\n",
      "19132 Training Loss: tensor(0.3312)\n",
      "19133 Training Loss: tensor(0.3323)\n",
      "19134 Training Loss: tensor(0.3309)\n",
      "19135 Training Loss: tensor(0.3310)\n",
      "19136 Training Loss: tensor(0.3332)\n",
      "19137 Training Loss: tensor(0.3309)\n",
      "19138 Training Loss: tensor(0.3310)\n",
      "19139 Training Loss: tensor(0.3315)\n",
      "19140 Training Loss: tensor(0.3330)\n",
      "19141 Training Loss: tensor(0.3317)\n",
      "19142 Training Loss: tensor(0.3335)\n",
      "19143 Training Loss: tensor(0.3321)\n",
      "19144 Training Loss: tensor(0.3299)\n",
      "19145 Training Loss: tensor(0.3308)\n",
      "19146 Training Loss: tensor(0.3313)\n",
      "19147 Training Loss: tensor(0.3354)\n",
      "19148 Training Loss: tensor(0.3344)\n",
      "19149 Training Loss: tensor(0.3326)\n",
      "19150 Training Loss: tensor(0.3343)\n",
      "19151 Training Loss: tensor(0.3320)\n",
      "19152 Training Loss: tensor(0.3320)\n",
      "19153 Training Loss: tensor(0.3313)\n",
      "19154 Training Loss: tensor(0.3318)\n",
      "19155 Training Loss: tensor(0.3324)\n",
      "19156 Training Loss: tensor(0.3310)\n",
      "19157 Training Loss: tensor(0.3335)\n",
      "19158 Training Loss: tensor(0.3343)\n",
      "19159 Training Loss: tensor(0.3318)\n",
      "19160 Training Loss: tensor(0.3331)\n",
      "19161 Training Loss: tensor(0.3310)\n",
      "19162 Training Loss: tensor(0.3362)\n",
      "19163 Training Loss: tensor(0.3327)\n",
      "19164 Training Loss: tensor(0.3343)\n",
      "19165 Training Loss: tensor(0.3315)\n",
      "19166 Training Loss: tensor(0.3311)\n",
      "19167 Training Loss: tensor(0.3308)\n",
      "19168 Training Loss: tensor(0.3328)\n",
      "19169 Training Loss: tensor(0.3321)\n",
      "19170 Training Loss: tensor(0.3358)\n",
      "19171 Training Loss: tensor(0.3315)\n",
      "19172 Training Loss: tensor(0.3383)\n",
      "19173 Training Loss: tensor(0.3320)\n",
      "19174 Training Loss: tensor(0.3316)\n",
      "19175 Training Loss: tensor(0.3326)\n",
      "19176 Training Loss: tensor(0.3330)\n",
      "19177 Training Loss: tensor(0.3310)\n",
      "19178 Training Loss: tensor(0.3326)\n",
      "19179 Training Loss: tensor(0.3317)\n",
      "19180 Training Loss: tensor(0.3343)\n",
      "19181 Training Loss: tensor(0.3305)\n",
      "19182 Training Loss: tensor(0.3323)\n",
      "19183 Training Loss: tensor(0.3332)\n",
      "19184 Training Loss: tensor(0.3332)\n",
      "19185 Training Loss: tensor(0.3332)\n",
      "19186 Training Loss: tensor(0.3329)\n",
      "19187 Training Loss: tensor(0.3316)\n",
      "19188 Training Loss: tensor(0.3314)\n",
      "19189 Training Loss: tensor(0.3311)\n",
      "19190 Training Loss: tensor(0.3324)\n",
      "19191 Training Loss: tensor(0.3335)\n",
      "19192 Training Loss: tensor(0.3311)\n",
      "19193 Training Loss: tensor(0.3313)\n",
      "19194 Training Loss: tensor(0.3312)\n",
      "19195 Training Loss: tensor(0.3336)\n",
      "19196 Training Loss: tensor(0.3317)\n",
      "19197 Training Loss: tensor(0.3318)\n",
      "19198 Training Loss: tensor(0.3323)\n",
      "19199 Training Loss: tensor(0.3306)\n",
      "19200 Training Loss: tensor(0.3303)\n",
      "19201 Training Loss: tensor(0.3315)\n",
      "19202 Training Loss: tensor(0.3337)\n",
      "19203 Training Loss: tensor(0.3362)\n",
      "19204 Training Loss: tensor(0.3325)\n",
      "19205 Training Loss: tensor(0.3333)\n",
      "19206 Training Loss: tensor(0.3338)\n",
      "19207 Training Loss: tensor(0.3355)\n",
      "19208 Training Loss: tensor(0.3335)\n",
      "19209 Training Loss: tensor(0.3318)\n",
      "19210 Training Loss: tensor(0.3329)\n",
      "19211 Training Loss: tensor(0.3351)\n",
      "19212 Training Loss: tensor(0.3361)\n",
      "19213 Training Loss: tensor(0.3351)\n",
      "19214 Training Loss: tensor(0.3343)\n",
      "19215 Training Loss: tensor(0.3334)\n",
      "19216 Training Loss: tensor(0.3331)\n",
      "19217 Training Loss: tensor(0.3319)\n",
      "19218 Training Loss: tensor(0.3330)\n",
      "19219 Training Loss: tensor(0.3316)\n",
      "19220 Training Loss: tensor(0.3315)\n",
      "19221 Training Loss: tensor(0.3325)\n",
      "19222 Training Loss: tensor(0.3325)\n",
      "19223 Training Loss: tensor(0.3315)\n",
      "19224 Training Loss: tensor(0.3321)\n",
      "19225 Training Loss: tensor(0.3314)\n",
      "19226 Training Loss: tensor(0.3318)\n",
      "19227 Training Loss: tensor(0.3347)\n",
      "19228 Training Loss: tensor(0.3324)\n",
      "19229 Training Loss: tensor(0.3327)\n",
      "19230 Training Loss: tensor(0.3308)\n",
      "19231 Training Loss: tensor(0.3311)\n",
      "19232 Training Loss: tensor(0.3320)\n",
      "19233 Training Loss: tensor(0.3322)\n",
      "19234 Training Loss: tensor(0.3356)\n",
      "19235 Training Loss: tensor(0.3363)\n",
      "19236 Training Loss: tensor(0.3320)\n",
      "19237 Training Loss: tensor(0.3342)\n",
      "19238 Training Loss: tensor(0.3329)\n",
      "19239 Training Loss: tensor(0.3321)\n",
      "19240 Training Loss: tensor(0.3330)\n",
      "19241 Training Loss: tensor(0.3327)\n",
      "19242 Training Loss: tensor(0.3310)\n",
      "19243 Training Loss: tensor(0.3360)\n",
      "19244 Training Loss: tensor(0.3321)\n",
      "19245 Training Loss: tensor(0.3331)\n",
      "19246 Training Loss: tensor(0.3310)\n",
      "19247 Training Loss: tensor(0.3311)\n",
      "19248 Training Loss: tensor(0.3327)\n",
      "19249 Training Loss: tensor(0.3334)\n",
      "19250 Training Loss: tensor(0.3319)\n",
      "19251 Training Loss: tensor(0.3350)\n",
      "19252 Training Loss: tensor(0.3343)\n",
      "19253 Training Loss: tensor(0.3328)\n",
      "19254 Training Loss: tensor(0.3318)\n",
      "19255 Training Loss: tensor(0.3355)\n",
      "19256 Training Loss: tensor(0.3345)\n",
      "19257 Training Loss: tensor(0.3331)\n",
      "19258 Training Loss: tensor(0.3336)\n",
      "19259 Training Loss: tensor(0.3341)\n",
      "19260 Training Loss: tensor(0.3359)\n",
      "19261 Training Loss: tensor(0.3314)\n",
      "19262 Training Loss: tensor(0.3312)\n",
      "19263 Training Loss: tensor(0.3335)\n",
      "19264 Training Loss: tensor(0.3334)\n",
      "19265 Training Loss: tensor(0.3318)\n",
      "19266 Training Loss: tensor(0.3332)\n",
      "19267 Training Loss: tensor(0.3354)\n",
      "19268 Training Loss: tensor(0.3329)\n",
      "19269 Training Loss: tensor(0.3303)\n",
      "19270 Training Loss: tensor(0.3322)\n",
      "19271 Training Loss: tensor(0.3319)\n",
      "19272 Training Loss: tensor(0.3310)\n",
      "19273 Training Loss: tensor(0.3332)\n",
      "19274 Training Loss: tensor(0.3358)\n",
      "19275 Training Loss: tensor(0.3325)\n",
      "19276 Training Loss: tensor(0.3332)\n",
      "19277 Training Loss: tensor(0.3316)\n",
      "19278 Training Loss: tensor(0.3327)\n",
      "19279 Training Loss: tensor(0.3344)\n",
      "19280 Training Loss: tensor(0.3341)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19281 Training Loss: tensor(0.3317)\n",
      "19282 Training Loss: tensor(0.3341)\n",
      "19283 Training Loss: tensor(0.3333)\n",
      "19284 Training Loss: tensor(0.3315)\n",
      "19285 Training Loss: tensor(0.3319)\n",
      "19286 Training Loss: tensor(0.3334)\n",
      "19287 Training Loss: tensor(0.3324)\n",
      "19288 Training Loss: tensor(0.3313)\n",
      "19289 Training Loss: tensor(0.3339)\n",
      "19290 Training Loss: tensor(0.3325)\n",
      "19291 Training Loss: tensor(0.3339)\n",
      "19292 Training Loss: tensor(0.3321)\n",
      "19293 Training Loss: tensor(0.3314)\n",
      "19294 Training Loss: tensor(0.3356)\n",
      "19295 Training Loss: tensor(0.3324)\n",
      "19296 Training Loss: tensor(0.3349)\n",
      "19297 Training Loss: tensor(0.3312)\n",
      "19298 Training Loss: tensor(0.3320)\n",
      "19299 Training Loss: tensor(0.3326)\n",
      "19300 Training Loss: tensor(0.3343)\n",
      "19301 Training Loss: tensor(0.3349)\n",
      "19302 Training Loss: tensor(0.3319)\n",
      "19303 Training Loss: tensor(0.3341)\n",
      "19304 Training Loss: tensor(0.3307)\n",
      "19305 Training Loss: tensor(0.3314)\n",
      "19306 Training Loss: tensor(0.3349)\n",
      "19307 Training Loss: tensor(0.3305)\n",
      "19308 Training Loss: tensor(0.3351)\n",
      "19309 Training Loss: tensor(0.3314)\n",
      "19310 Training Loss: tensor(0.3316)\n",
      "19311 Training Loss: tensor(0.3313)\n",
      "19312 Training Loss: tensor(0.3354)\n",
      "19313 Training Loss: tensor(0.3311)\n",
      "19314 Training Loss: tensor(0.3328)\n",
      "19315 Training Loss: tensor(0.3314)\n",
      "19316 Training Loss: tensor(0.3314)\n",
      "19317 Training Loss: tensor(0.3328)\n",
      "19318 Training Loss: tensor(0.3328)\n",
      "19319 Training Loss: tensor(0.3324)\n",
      "19320 Training Loss: tensor(0.3334)\n",
      "19321 Training Loss: tensor(0.3318)\n",
      "19322 Training Loss: tensor(0.3335)\n",
      "19323 Training Loss: tensor(0.3338)\n",
      "19324 Training Loss: tensor(0.3312)\n",
      "19325 Training Loss: tensor(0.3353)\n",
      "19326 Training Loss: tensor(0.3317)\n",
      "19327 Training Loss: tensor(0.3348)\n",
      "19328 Training Loss: tensor(0.3323)\n",
      "19329 Training Loss: tensor(0.3311)\n",
      "19330 Training Loss: tensor(0.3315)\n",
      "19331 Training Loss: tensor(0.3314)\n",
      "19332 Training Loss: tensor(0.3309)\n",
      "19333 Training Loss: tensor(0.3345)\n",
      "19334 Training Loss: tensor(0.3311)\n",
      "19335 Training Loss: tensor(0.3341)\n",
      "19336 Training Loss: tensor(0.3331)\n",
      "19337 Training Loss: tensor(0.3320)\n",
      "19338 Training Loss: tensor(0.3333)\n",
      "19339 Training Loss: tensor(0.3347)\n",
      "19340 Training Loss: tensor(0.3318)\n",
      "19341 Training Loss: tensor(0.3307)\n",
      "19342 Training Loss: tensor(0.3318)\n",
      "19343 Training Loss: tensor(0.3318)\n",
      "19344 Training Loss: tensor(0.3333)\n",
      "19345 Training Loss: tensor(0.3314)\n",
      "19346 Training Loss: tensor(0.3352)\n",
      "19347 Training Loss: tensor(0.3319)\n",
      "19348 Training Loss: tensor(0.3329)\n",
      "19349 Training Loss: tensor(0.3309)\n",
      "19350 Training Loss: tensor(0.3311)\n",
      "19351 Training Loss: tensor(0.3337)\n",
      "19352 Training Loss: tensor(0.3314)\n",
      "19353 Training Loss: tensor(0.3317)\n",
      "19354 Training Loss: tensor(0.3329)\n",
      "19355 Training Loss: tensor(0.3356)\n",
      "19356 Training Loss: tensor(0.3311)\n",
      "19357 Training Loss: tensor(0.3306)\n",
      "19358 Training Loss: tensor(0.3325)\n",
      "19359 Training Loss: tensor(0.3321)\n",
      "19360 Training Loss: tensor(0.3313)\n",
      "19361 Training Loss: tensor(0.3360)\n",
      "19362 Training Loss: tensor(0.3317)\n",
      "19363 Training Loss: tensor(0.3369)\n",
      "19364 Training Loss: tensor(0.3316)\n",
      "19365 Training Loss: tensor(0.3314)\n",
      "19366 Training Loss: tensor(0.3320)\n",
      "19367 Training Loss: tensor(0.3330)\n",
      "19368 Training Loss: tensor(0.3333)\n",
      "19369 Training Loss: tensor(0.3390)\n",
      "19370 Training Loss: tensor(0.3316)\n",
      "19371 Training Loss: tensor(0.3312)\n",
      "19372 Training Loss: tensor(0.3309)\n",
      "19373 Training Loss: tensor(0.3321)\n",
      "19374 Training Loss: tensor(0.3315)\n",
      "19375 Training Loss: tensor(0.3332)\n",
      "19376 Training Loss: tensor(0.3312)\n",
      "19377 Training Loss: tensor(0.3325)\n",
      "19378 Training Loss: tensor(0.3322)\n",
      "19379 Training Loss: tensor(0.3339)\n",
      "19380 Training Loss: tensor(0.3319)\n",
      "19381 Training Loss: tensor(0.3308)\n",
      "19382 Training Loss: tensor(0.3325)\n",
      "19383 Training Loss: tensor(0.3355)\n",
      "19384 Training Loss: tensor(0.3306)\n",
      "19385 Training Loss: tensor(0.3308)\n",
      "19386 Training Loss: tensor(0.3318)\n",
      "19387 Training Loss: tensor(0.3306)\n",
      "19388 Training Loss: tensor(0.3305)\n",
      "19389 Training Loss: tensor(0.3304)\n",
      "19390 Training Loss: tensor(0.3314)\n",
      "19391 Training Loss: tensor(0.3304)\n",
      "19392 Training Loss: tensor(0.3350)\n",
      "19393 Training Loss: tensor(0.3305)\n",
      "19394 Training Loss: tensor(0.3335)\n",
      "19395 Training Loss: tensor(0.3329)\n",
      "19396 Training Loss: tensor(0.3325)\n",
      "19397 Training Loss: tensor(0.3328)\n",
      "19398 Training Loss: tensor(0.3316)\n",
      "19399 Training Loss: tensor(0.3325)\n",
      "19400 Training Loss: tensor(0.3324)\n",
      "19401 Training Loss: tensor(0.3320)\n",
      "19402 Training Loss: tensor(0.3317)\n",
      "19403 Training Loss: tensor(0.3306)\n",
      "19404 Training Loss: tensor(0.3326)\n",
      "19405 Training Loss: tensor(0.3320)\n",
      "19406 Training Loss: tensor(0.3339)\n",
      "19407 Training Loss: tensor(0.3345)\n",
      "19408 Training Loss: tensor(0.3324)\n",
      "19409 Training Loss: tensor(0.3319)\n",
      "19410 Training Loss: tensor(0.3329)\n",
      "19411 Training Loss: tensor(0.3316)\n",
      "19412 Training Loss: tensor(0.3307)\n",
      "19413 Training Loss: tensor(0.3337)\n",
      "19414 Training Loss: tensor(0.3322)\n",
      "19415 Training Loss: tensor(0.3373)\n",
      "19416 Training Loss: tensor(0.3310)\n",
      "19417 Training Loss: tensor(0.3325)\n",
      "19418 Training Loss: tensor(0.3370)\n",
      "19419 Training Loss: tensor(0.3320)\n",
      "19420 Training Loss: tensor(0.3357)\n",
      "19421 Training Loss: tensor(0.3324)\n",
      "19422 Training Loss: tensor(0.3317)\n",
      "19423 Training Loss: tensor(0.3351)\n",
      "19424 Training Loss: tensor(0.3315)\n",
      "19425 Training Loss: tensor(0.3319)\n",
      "19426 Training Loss: tensor(0.3323)\n",
      "19427 Training Loss: tensor(0.3333)\n",
      "19428 Training Loss: tensor(0.3311)\n",
      "19429 Training Loss: tensor(0.3325)\n",
      "19430 Training Loss: tensor(0.3326)\n",
      "19431 Training Loss: tensor(0.3315)\n",
      "19432 Training Loss: tensor(0.3310)\n",
      "19433 Training Loss: tensor(0.3319)\n",
      "19434 Training Loss: tensor(0.3377)\n",
      "19435 Training Loss: tensor(0.3316)\n",
      "19436 Training Loss: tensor(0.3329)\n",
      "19437 Training Loss: tensor(0.3318)\n",
      "19438 Training Loss: tensor(0.3314)\n",
      "19439 Training Loss: tensor(0.3312)\n",
      "19440 Training Loss: tensor(0.3342)\n",
      "19441 Training Loss: tensor(0.3350)\n",
      "19442 Training Loss: tensor(0.3322)\n",
      "19443 Training Loss: tensor(0.3327)\n",
      "19444 Training Loss: tensor(0.3331)\n",
      "19445 Training Loss: tensor(0.3324)\n",
      "19446 Training Loss: tensor(0.3353)\n",
      "19447 Training Loss: tensor(0.3314)\n",
      "19448 Training Loss: tensor(0.3324)\n",
      "19449 Training Loss: tensor(0.3318)\n",
      "19450 Training Loss: tensor(0.3369)\n",
      "19451 Training Loss: tensor(0.3335)\n",
      "19452 Training Loss: tensor(0.3328)\n",
      "19453 Training Loss: tensor(0.3321)\n",
      "19454 Training Loss: tensor(0.3318)\n",
      "19455 Training Loss: tensor(0.3340)\n",
      "19456 Training Loss: tensor(0.3322)\n",
      "19457 Training Loss: tensor(0.3327)\n",
      "19458 Training Loss: tensor(0.3309)\n",
      "19459 Training Loss: tensor(0.3320)\n",
      "19460 Training Loss: tensor(0.3317)\n",
      "19461 Training Loss: tensor(0.3337)\n",
      "19462 Training Loss: tensor(0.3317)\n",
      "19463 Training Loss: tensor(0.3324)\n",
      "19464 Training Loss: tensor(0.3323)\n",
      "19465 Training Loss: tensor(0.3305)\n",
      "19466 Training Loss: tensor(0.3338)\n",
      "19467 Training Loss: tensor(0.3317)\n",
      "19468 Training Loss: tensor(0.3307)\n",
      "19469 Training Loss: tensor(0.3312)\n",
      "19470 Training Loss: tensor(0.3321)\n",
      "19471 Training Loss: tensor(0.3305)\n",
      "19472 Training Loss: tensor(0.3313)\n",
      "19473 Training Loss: tensor(0.3324)\n",
      "19474 Training Loss: tensor(0.3336)\n",
      "19475 Training Loss: tensor(0.3321)\n",
      "19476 Training Loss: tensor(0.3338)\n",
      "19477 Training Loss: tensor(0.3306)\n",
      "19478 Training Loss: tensor(0.3319)\n",
      "19479 Training Loss: tensor(0.3317)\n",
      "19480 Training Loss: tensor(0.3331)\n",
      "19481 Training Loss: tensor(0.3333)\n",
      "19482 Training Loss: tensor(0.3301)\n",
      "19483 Training Loss: tensor(0.3305)\n",
      "19484 Training Loss: tensor(0.3333)\n",
      "19485 Training Loss: tensor(0.3348)\n",
      "19486 Training Loss: tensor(0.3344)\n",
      "19487 Training Loss: tensor(0.3331)\n",
      "19488 Training Loss: tensor(0.3321)\n",
      "19489 Training Loss: tensor(0.3320)\n",
      "19490 Training Loss: tensor(0.3349)\n",
      "19491 Training Loss: tensor(0.3356)\n",
      "19492 Training Loss: tensor(0.3311)\n",
      "19493 Training Loss: tensor(0.3314)\n",
      "19494 Training Loss: tensor(0.3315)\n",
      "19495 Training Loss: tensor(0.3313)\n",
      "19496 Training Loss: tensor(0.3339)\n",
      "19497 Training Loss: tensor(0.3319)\n",
      "19498 Training Loss: tensor(0.3331)\n",
      "19499 Training Loss: tensor(0.3303)\n",
      "19500 Training Loss: tensor(0.3332)\n",
      "19501 Training Loss: tensor(0.3310)\n",
      "19502 Training Loss: tensor(0.3333)\n",
      "19503 Training Loss: tensor(0.3317)\n",
      "19504 Training Loss: tensor(0.3310)\n",
      "19505 Training Loss: tensor(0.3316)\n",
      "19506 Training Loss: tensor(0.3349)\n",
      "19507 Training Loss: tensor(0.3322)\n",
      "19508 Training Loss: tensor(0.3296)\n",
      "19509 Training Loss: tensor(0.3308)\n",
      "19510 Training Loss: tensor(0.3368)\n",
      "19511 Training Loss: tensor(0.3342)\n",
      "19512 Training Loss: tensor(0.3305)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19513 Training Loss: tensor(0.3340)\n",
      "19514 Training Loss: tensor(0.3356)\n",
      "19515 Training Loss: tensor(0.3327)\n",
      "19516 Training Loss: tensor(0.3328)\n",
      "19517 Training Loss: tensor(0.3327)\n",
      "19518 Training Loss: tensor(0.3320)\n",
      "19519 Training Loss: tensor(0.3337)\n",
      "19520 Training Loss: tensor(0.3334)\n",
      "19521 Training Loss: tensor(0.3324)\n",
      "19522 Training Loss: tensor(0.3323)\n",
      "19523 Training Loss: tensor(0.3330)\n",
      "19524 Training Loss: tensor(0.3311)\n",
      "19525 Training Loss: tensor(0.3314)\n",
      "19526 Training Loss: tensor(0.3365)\n",
      "19527 Training Loss: tensor(0.3324)\n",
      "19528 Training Loss: tensor(0.3348)\n",
      "19529 Training Loss: tensor(0.3322)\n",
      "19530 Training Loss: tensor(0.3334)\n",
      "19531 Training Loss: tensor(0.3324)\n",
      "19532 Training Loss: tensor(0.3341)\n",
      "19533 Training Loss: tensor(0.3310)\n",
      "19534 Training Loss: tensor(0.3313)\n",
      "19535 Training Loss: tensor(0.3355)\n",
      "19536 Training Loss: tensor(0.3329)\n",
      "19537 Training Loss: tensor(0.3351)\n",
      "19538 Training Loss: tensor(0.3322)\n",
      "19539 Training Loss: tensor(0.3332)\n",
      "19540 Training Loss: tensor(0.3333)\n",
      "19541 Training Loss: tensor(0.3320)\n",
      "19542 Training Loss: tensor(0.3319)\n",
      "19543 Training Loss: tensor(0.3316)\n",
      "19544 Training Loss: tensor(0.3312)\n",
      "19545 Training Loss: tensor(0.3318)\n",
      "19546 Training Loss: tensor(0.3338)\n",
      "19547 Training Loss: tensor(0.3319)\n",
      "19548 Training Loss: tensor(0.3336)\n",
      "19549 Training Loss: tensor(0.3311)\n",
      "19550 Training Loss: tensor(0.3342)\n",
      "19551 Training Loss: tensor(0.3327)\n",
      "19552 Training Loss: tensor(0.3313)\n",
      "19553 Training Loss: tensor(0.3310)\n",
      "19554 Training Loss: tensor(0.3326)\n",
      "19555 Training Loss: tensor(0.3309)\n",
      "19556 Training Loss: tensor(0.3319)\n",
      "19557 Training Loss: tensor(0.3337)\n",
      "19558 Training Loss: tensor(0.3323)\n",
      "19559 Training Loss: tensor(0.3334)\n",
      "19560 Training Loss: tensor(0.3322)\n",
      "19561 Training Loss: tensor(0.3314)\n",
      "19562 Training Loss: tensor(0.3346)\n",
      "19563 Training Loss: tensor(0.3310)\n",
      "19564 Training Loss: tensor(0.3305)\n",
      "19565 Training Loss: tensor(0.3309)\n",
      "19566 Training Loss: tensor(0.3329)\n",
      "19567 Training Loss: tensor(0.3309)\n",
      "19568 Training Loss: tensor(0.3312)\n",
      "19569 Training Loss: tensor(0.3321)\n",
      "19570 Training Loss: tensor(0.3355)\n",
      "19571 Training Loss: tensor(0.3312)\n",
      "19572 Training Loss: tensor(0.3334)\n",
      "19573 Training Loss: tensor(0.3325)\n",
      "19574 Training Loss: tensor(0.3308)\n",
      "19575 Training Loss: tensor(0.3439)\n",
      "19576 Training Loss: tensor(0.3327)\n",
      "19577 Training Loss: tensor(0.3304)\n",
      "19578 Training Loss: tensor(0.3331)\n",
      "19579 Training Loss: tensor(0.3316)\n",
      "19580 Training Loss: tensor(0.3317)\n",
      "19581 Training Loss: tensor(0.3327)\n",
      "19582 Training Loss: tensor(0.3320)\n",
      "19583 Training Loss: tensor(0.3328)\n",
      "19584 Training Loss: tensor(0.3320)\n",
      "19585 Training Loss: tensor(0.3307)\n",
      "19586 Training Loss: tensor(0.3363)\n",
      "19587 Training Loss: tensor(0.3322)\n",
      "19588 Training Loss: tensor(0.3315)\n",
      "19589 Training Loss: tensor(0.3316)\n",
      "19590 Training Loss: tensor(0.3319)\n",
      "19591 Training Loss: tensor(0.3334)\n",
      "19592 Training Loss: tensor(0.3331)\n",
      "19593 Training Loss: tensor(0.3304)\n",
      "19594 Training Loss: tensor(0.3304)\n",
      "19595 Training Loss: tensor(0.3341)\n",
      "19596 Training Loss: tensor(0.3311)\n",
      "19597 Training Loss: tensor(0.3305)\n",
      "19598 Training Loss: tensor(0.3320)\n",
      "19599 Training Loss: tensor(0.3326)\n",
      "19600 Training Loss: tensor(0.3315)\n",
      "19601 Training Loss: tensor(0.3342)\n",
      "19602 Training Loss: tensor(0.3306)\n",
      "19603 Training Loss: tensor(0.3303)\n",
      "19604 Training Loss: tensor(0.3330)\n",
      "19605 Training Loss: tensor(0.3345)\n",
      "19606 Training Loss: tensor(0.3316)\n",
      "19607 Training Loss: tensor(0.3310)\n",
      "19608 Training Loss: tensor(0.3314)\n",
      "19609 Training Loss: tensor(0.3308)\n",
      "19610 Training Loss: tensor(0.3311)\n",
      "19611 Training Loss: tensor(0.3317)\n",
      "19612 Training Loss: tensor(0.3316)\n",
      "19613 Training Loss: tensor(0.3313)\n",
      "19614 Training Loss: tensor(0.3306)\n",
      "19615 Training Loss: tensor(0.3316)\n",
      "19616 Training Loss: tensor(0.3317)\n",
      "19617 Training Loss: tensor(0.3315)\n",
      "19618 Training Loss: tensor(0.3314)\n",
      "19619 Training Loss: tensor(0.3363)\n",
      "19620 Training Loss: tensor(0.3305)\n",
      "19621 Training Loss: tensor(0.3317)\n",
      "19622 Training Loss: tensor(0.3318)\n",
      "19623 Training Loss: tensor(0.3337)\n",
      "19624 Training Loss: tensor(0.3324)\n",
      "19625 Training Loss: tensor(0.3335)\n",
      "19626 Training Loss: tensor(0.3324)\n",
      "19627 Training Loss: tensor(0.3306)\n",
      "19628 Training Loss: tensor(0.3315)\n",
      "19629 Training Loss: tensor(0.3312)\n",
      "19630 Training Loss: tensor(0.3334)\n",
      "19631 Training Loss: tensor(0.3307)\n",
      "19632 Training Loss: tensor(0.3307)\n",
      "19633 Training Loss: tensor(0.3318)\n",
      "19634 Training Loss: tensor(0.3318)\n",
      "19635 Training Loss: tensor(0.3320)\n",
      "19636 Training Loss: tensor(0.3340)\n",
      "19637 Training Loss: tensor(0.3323)\n",
      "19638 Training Loss: tensor(0.3303)\n",
      "19639 Training Loss: tensor(0.3313)\n",
      "19640 Training Loss: tensor(0.3344)\n",
      "19641 Training Loss: tensor(0.3311)\n",
      "19642 Training Loss: tensor(0.3334)\n",
      "19643 Training Loss: tensor(0.3329)\n",
      "19644 Training Loss: tensor(0.3317)\n",
      "19645 Training Loss: tensor(0.3321)\n",
      "19646 Training Loss: tensor(0.3335)\n",
      "19647 Training Loss: tensor(0.3316)\n",
      "19648 Training Loss: tensor(0.3321)\n",
      "19649 Training Loss: tensor(0.3332)\n",
      "19650 Training Loss: tensor(0.3327)\n",
      "19651 Training Loss: tensor(0.3323)\n",
      "19652 Training Loss: tensor(0.3323)\n",
      "19653 Training Loss: tensor(0.3320)\n",
      "19654 Training Loss: tensor(0.3325)\n",
      "19655 Training Loss: tensor(0.3316)\n",
      "19656 Training Loss: tensor(0.3333)\n",
      "19657 Training Loss: tensor(0.3326)\n",
      "19658 Training Loss: tensor(0.3319)\n",
      "19659 Training Loss: tensor(0.3387)\n",
      "19660 Training Loss: tensor(0.3314)\n",
      "19661 Training Loss: tensor(0.3299)\n",
      "19662 Training Loss: tensor(0.3298)\n",
      "19663 Training Loss: tensor(0.3315)\n",
      "19664 Training Loss: tensor(0.3308)\n",
      "19665 Training Loss: tensor(0.3379)\n",
      "19666 Training Loss: tensor(0.3322)\n",
      "19667 Training Loss: tensor(0.3316)\n",
      "19668 Training Loss: tensor(0.3322)\n",
      "19669 Training Loss: tensor(0.3325)\n",
      "19670 Training Loss: tensor(0.3315)\n",
      "19671 Training Loss: tensor(0.3332)\n",
      "19672 Training Loss: tensor(0.3311)\n",
      "19673 Training Loss: tensor(0.3317)\n",
      "19674 Training Loss: tensor(0.3330)\n",
      "19675 Training Loss: tensor(0.3315)\n",
      "19676 Training Loss: tensor(0.3317)\n",
      "19677 Training Loss: tensor(0.3324)\n",
      "19678 Training Loss: tensor(0.3319)\n",
      "19679 Training Loss: tensor(0.3316)\n",
      "19680 Training Loss: tensor(0.3312)\n",
      "19681 Training Loss: tensor(0.3332)\n",
      "19682 Training Loss: tensor(0.3331)\n",
      "19683 Training Loss: tensor(0.3323)\n",
      "19684 Training Loss: tensor(0.3308)\n",
      "19685 Training Loss: tensor(0.3309)\n",
      "19686 Training Loss: tensor(0.3310)\n",
      "19687 Training Loss: tensor(0.3320)\n",
      "19688 Training Loss: tensor(0.3316)\n",
      "19689 Training Loss: tensor(0.3324)\n",
      "19690 Training Loss: tensor(0.3300)\n",
      "19691 Training Loss: tensor(0.3309)\n",
      "19692 Training Loss: tensor(0.3349)\n",
      "19693 Training Loss: tensor(0.3325)\n",
      "19694 Training Loss: tensor(0.3309)\n",
      "19695 Training Loss: tensor(0.3327)\n",
      "19696 Training Loss: tensor(0.3303)\n",
      "19697 Training Loss: tensor(0.3331)\n",
      "19698 Training Loss: tensor(0.3306)\n",
      "19699 Training Loss: tensor(0.3393)\n",
      "19700 Training Loss: tensor(0.3314)\n",
      "19701 Training Loss: tensor(0.3321)\n",
      "19702 Training Loss: tensor(0.3312)\n",
      "19703 Training Loss: tensor(0.3310)\n",
      "19704 Training Loss: tensor(0.3317)\n",
      "19705 Training Loss: tensor(0.3317)\n",
      "19706 Training Loss: tensor(0.3316)\n",
      "19707 Training Loss: tensor(0.3327)\n",
      "19708 Training Loss: tensor(0.3350)\n",
      "19709 Training Loss: tensor(0.3314)\n",
      "19710 Training Loss: tensor(0.3331)\n",
      "19711 Training Loss: tensor(0.3314)\n",
      "19712 Training Loss: tensor(0.3313)\n",
      "19713 Training Loss: tensor(0.3361)\n",
      "19714 Training Loss: tensor(0.3337)\n",
      "19715 Training Loss: tensor(0.3343)\n",
      "19716 Training Loss: tensor(0.3314)\n",
      "19717 Training Loss: tensor(0.3324)\n",
      "19718 Training Loss: tensor(0.3310)\n",
      "19719 Training Loss: tensor(0.3332)\n",
      "19720 Training Loss: tensor(0.3309)\n",
      "19721 Training Loss: tensor(0.3331)\n",
      "19722 Training Loss: tensor(0.3302)\n",
      "19723 Training Loss: tensor(0.3317)\n",
      "19724 Training Loss: tensor(0.3308)\n",
      "19725 Training Loss: tensor(0.3313)\n",
      "19726 Training Loss: tensor(0.3316)\n",
      "19727 Training Loss: tensor(0.3314)\n",
      "19728 Training Loss: tensor(0.3352)\n",
      "19729 Training Loss: tensor(0.3334)\n",
      "19730 Training Loss: tensor(0.3316)\n",
      "19731 Training Loss: tensor(0.3330)\n",
      "19732 Training Loss: tensor(0.3308)\n",
      "19733 Training Loss: tensor(0.3355)\n",
      "19734 Training Loss: tensor(0.3315)\n",
      "19735 Training Loss: tensor(0.3341)\n",
      "19736 Training Loss: tensor(0.3311)\n",
      "19737 Training Loss: tensor(0.3312)\n",
      "19738 Training Loss: tensor(0.3325)\n",
      "19739 Training Loss: tensor(0.3340)\n",
      "19740 Training Loss: tensor(0.3339)\n",
      "19741 Training Loss: tensor(0.3314)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19742 Training Loss: tensor(0.3307)\n",
      "19743 Training Loss: tensor(0.3334)\n",
      "19744 Training Loss: tensor(0.3341)\n",
      "19745 Training Loss: tensor(0.3332)\n",
      "19746 Training Loss: tensor(0.3348)\n",
      "19747 Training Loss: tensor(0.3302)\n",
      "19748 Training Loss: tensor(0.3318)\n",
      "19749 Training Loss: tensor(0.3333)\n",
      "19750 Training Loss: tensor(0.3326)\n",
      "19751 Training Loss: tensor(0.3335)\n",
      "19752 Training Loss: tensor(0.3308)\n",
      "19753 Training Loss: tensor(0.3312)\n",
      "19754 Training Loss: tensor(0.3342)\n",
      "19755 Training Loss: tensor(0.3328)\n",
      "19756 Training Loss: tensor(0.3349)\n",
      "19757 Training Loss: tensor(0.3310)\n",
      "19758 Training Loss: tensor(0.3317)\n",
      "19759 Training Loss: tensor(0.3349)\n",
      "19760 Training Loss: tensor(0.3335)\n",
      "19761 Training Loss: tensor(0.3322)\n",
      "19762 Training Loss: tensor(0.3320)\n",
      "19763 Training Loss: tensor(0.3343)\n",
      "19764 Training Loss: tensor(0.3343)\n",
      "19765 Training Loss: tensor(0.3317)\n",
      "19766 Training Loss: tensor(0.3330)\n",
      "19767 Training Loss: tensor(0.3327)\n",
      "19768 Training Loss: tensor(0.3320)\n",
      "19769 Training Loss: tensor(0.3335)\n",
      "19770 Training Loss: tensor(0.3314)\n",
      "19771 Training Loss: tensor(0.3320)\n",
      "19772 Training Loss: tensor(0.3332)\n",
      "19773 Training Loss: tensor(0.3315)\n",
      "19774 Training Loss: tensor(0.3314)\n",
      "19775 Training Loss: tensor(0.3324)\n",
      "19776 Training Loss: tensor(0.3386)\n",
      "19777 Training Loss: tensor(0.3321)\n",
      "19778 Training Loss: tensor(0.3321)\n",
      "19779 Training Loss: tensor(0.3331)\n",
      "19780 Training Loss: tensor(0.3313)\n",
      "19781 Training Loss: tensor(0.3313)\n",
      "19782 Training Loss: tensor(0.3321)\n",
      "19783 Training Loss: tensor(0.3328)\n",
      "19784 Training Loss: tensor(0.3333)\n",
      "19785 Training Loss: tensor(0.3320)\n",
      "19786 Training Loss: tensor(0.3307)\n",
      "19787 Training Loss: tensor(0.3320)\n",
      "19788 Training Loss: tensor(0.3309)\n",
      "19789 Training Loss: tensor(0.3337)\n",
      "19790 Training Loss: tensor(0.3343)\n",
      "19791 Training Loss: tensor(0.3313)\n",
      "19792 Training Loss: tensor(0.3315)\n",
      "19793 Training Loss: tensor(0.3335)\n",
      "19794 Training Loss: tensor(0.3325)\n",
      "19795 Training Loss: tensor(0.3331)\n",
      "19796 Training Loss: tensor(0.3335)\n",
      "19797 Training Loss: tensor(0.3318)\n",
      "19798 Training Loss: tensor(0.3311)\n",
      "19799 Training Loss: tensor(0.3328)\n",
      "19800 Training Loss: tensor(0.3347)\n",
      "19801 Training Loss: tensor(0.3316)\n",
      "19802 Training Loss: tensor(0.3319)\n",
      "19803 Training Loss: tensor(0.3341)\n",
      "19804 Training Loss: tensor(0.3332)\n",
      "19805 Training Loss: tensor(0.3362)\n",
      "19806 Training Loss: tensor(0.3325)\n",
      "19807 Training Loss: tensor(0.3318)\n",
      "19808 Training Loss: tensor(0.3336)\n",
      "19809 Training Loss: tensor(0.3322)\n",
      "19810 Training Loss: tensor(0.3329)\n",
      "19811 Training Loss: tensor(0.3324)\n",
      "19812 Training Loss: tensor(0.3318)\n",
      "19813 Training Loss: tensor(0.3328)\n",
      "19814 Training Loss: tensor(0.3314)\n",
      "19815 Training Loss: tensor(0.3331)\n",
      "19816 Training Loss: tensor(0.3318)\n",
      "19817 Training Loss: tensor(0.3319)\n",
      "19818 Training Loss: tensor(0.3326)\n",
      "19819 Training Loss: tensor(0.3329)\n",
      "19820 Training Loss: tensor(0.3333)\n",
      "19821 Training Loss: tensor(0.3313)\n",
      "19822 Training Loss: tensor(0.3327)\n",
      "19823 Training Loss: tensor(0.3322)\n",
      "19824 Training Loss: tensor(0.3304)\n",
      "19825 Training Loss: tensor(0.3323)\n",
      "19826 Training Loss: tensor(0.3344)\n",
      "19827 Training Loss: tensor(0.3327)\n",
      "19828 Training Loss: tensor(0.3365)\n",
      "19829 Training Loss: tensor(0.3331)\n",
      "19830 Training Loss: tensor(0.3328)\n",
      "19831 Training Loss: tensor(0.3325)\n",
      "19832 Training Loss: tensor(0.3315)\n",
      "19833 Training Loss: tensor(0.3307)\n",
      "19834 Training Loss: tensor(0.3324)\n",
      "19835 Training Loss: tensor(0.3348)\n",
      "19836 Training Loss: tensor(0.3338)\n",
      "19837 Training Loss: tensor(0.3319)\n",
      "19838 Training Loss: tensor(0.3311)\n",
      "19839 Training Loss: tensor(0.3322)\n",
      "19840 Training Loss: tensor(0.3322)\n",
      "19841 Training Loss: tensor(0.3321)\n",
      "19842 Training Loss: tensor(0.3336)\n",
      "19843 Training Loss: tensor(0.3327)\n",
      "19844 Training Loss: tensor(0.3319)\n",
      "19845 Training Loss: tensor(0.3314)\n",
      "19846 Training Loss: tensor(0.3322)\n",
      "19847 Training Loss: tensor(0.3324)\n",
      "19848 Training Loss: tensor(0.3331)\n",
      "19849 Training Loss: tensor(0.3347)\n",
      "19850 Training Loss: tensor(0.3311)\n",
      "19851 Training Loss: tensor(0.3316)\n",
      "19852 Training Loss: tensor(0.3340)\n",
      "19853 Training Loss: tensor(0.3317)\n",
      "19854 Training Loss: tensor(0.3316)\n",
      "19855 Training Loss: tensor(0.3322)\n",
      "19856 Training Loss: tensor(0.3314)\n",
      "19857 Training Loss: tensor(0.3326)\n",
      "19858 Training Loss: tensor(0.3322)\n",
      "19859 Training Loss: tensor(0.3324)\n",
      "19860 Training Loss: tensor(0.3325)\n",
      "19861 Training Loss: tensor(0.3309)\n",
      "19862 Training Loss: tensor(0.3336)\n",
      "19863 Training Loss: tensor(0.3317)\n",
      "19864 Training Loss: tensor(0.3325)\n",
      "19865 Training Loss: tensor(0.3307)\n",
      "19866 Training Loss: tensor(0.3306)\n",
      "19867 Training Loss: tensor(0.3309)\n",
      "19868 Training Loss: tensor(0.3396)\n",
      "19869 Training Loss: tensor(0.3304)\n",
      "19870 Training Loss: tensor(0.3349)\n",
      "19871 Training Loss: tensor(0.3334)\n",
      "19872 Training Loss: tensor(0.3338)\n",
      "19873 Training Loss: tensor(0.3316)\n",
      "19874 Training Loss: tensor(0.3311)\n",
      "19875 Training Loss: tensor(0.3360)\n",
      "19876 Training Loss: tensor(0.3317)\n",
      "19877 Training Loss: tensor(0.3310)\n",
      "19878 Training Loss: tensor(0.3328)\n",
      "19879 Training Loss: tensor(0.3335)\n",
      "19880 Training Loss: tensor(0.3313)\n",
      "19881 Training Loss: tensor(0.3321)\n",
      "19882 Training Loss: tensor(0.3318)\n",
      "19883 Training Loss: tensor(0.3322)\n",
      "19884 Training Loss: tensor(0.3305)\n",
      "19885 Training Loss: tensor(0.3328)\n",
      "19886 Training Loss: tensor(0.3314)\n",
      "19887 Training Loss: tensor(0.3323)\n",
      "19888 Training Loss: tensor(0.3310)\n",
      "19889 Training Loss: tensor(0.3308)\n",
      "19890 Training Loss: tensor(0.3338)\n",
      "19891 Training Loss: tensor(0.3318)\n",
      "19892 Training Loss: tensor(0.3341)\n",
      "19893 Training Loss: tensor(0.3345)\n",
      "19894 Training Loss: tensor(0.3321)\n",
      "19895 Training Loss: tensor(0.3331)\n",
      "19896 Training Loss: tensor(0.3322)\n",
      "19897 Training Loss: tensor(0.3320)\n",
      "19898 Training Loss: tensor(0.3343)\n",
      "19899 Training Loss: tensor(0.3371)\n",
      "19900 Training Loss: tensor(0.3320)\n",
      "19901 Training Loss: tensor(0.3317)\n",
      "19902 Training Loss: tensor(0.3341)\n",
      "19903 Training Loss: tensor(0.3349)\n",
      "19904 Training Loss: tensor(0.3315)\n",
      "19905 Training Loss: tensor(0.3338)\n",
      "19906 Training Loss: tensor(0.3328)\n",
      "19907 Training Loss: tensor(0.3323)\n",
      "19908 Training Loss: tensor(0.3310)\n",
      "19909 Training Loss: tensor(0.3333)\n",
      "19910 Training Loss: tensor(0.3334)\n",
      "19911 Training Loss: tensor(0.3337)\n",
      "19912 Training Loss: tensor(0.3356)\n",
      "19913 Training Loss: tensor(0.3322)\n",
      "19914 Training Loss: tensor(0.3313)\n",
      "19915 Training Loss: tensor(0.3326)\n",
      "19916 Training Loss: tensor(0.3307)\n",
      "19917 Training Loss: tensor(0.3343)\n",
      "19918 Training Loss: tensor(0.3327)\n",
      "19919 Training Loss: tensor(0.3316)\n",
      "19920 Training Loss: tensor(0.3325)\n",
      "19921 Training Loss: tensor(0.3315)\n",
      "19922 Training Loss: tensor(0.3331)\n",
      "19923 Training Loss: tensor(0.3388)\n",
      "19924 Training Loss: tensor(0.3301)\n",
      "19925 Training Loss: tensor(0.3309)\n",
      "19926 Training Loss: tensor(0.3332)\n",
      "19927 Training Loss: tensor(0.3310)\n",
      "19928 Training Loss: tensor(0.3326)\n",
      "19929 Training Loss: tensor(0.3326)\n",
      "19930 Training Loss: tensor(0.3312)\n",
      "19931 Training Loss: tensor(0.3328)\n",
      "19932 Training Loss: tensor(0.3322)\n",
      "19933 Training Loss: tensor(0.3320)\n",
      "19934 Training Loss: tensor(0.3317)\n",
      "19935 Training Loss: tensor(0.3328)\n",
      "19936 Training Loss: tensor(0.3320)\n",
      "19937 Training Loss: tensor(0.3308)\n",
      "19938 Training Loss: tensor(0.3324)\n",
      "19939 Training Loss: tensor(0.3302)\n",
      "19940 Training Loss: tensor(0.3322)\n",
      "19941 Training Loss: tensor(0.3309)\n",
      "19942 Training Loss: tensor(0.3321)\n",
      "19943 Training Loss: tensor(0.3309)\n",
      "19944 Training Loss: tensor(0.3338)\n",
      "19945 Training Loss: tensor(0.3346)\n",
      "19946 Training Loss: tensor(0.3305)\n",
      "19947 Training Loss: tensor(0.3304)\n",
      "19948 Training Loss: tensor(0.3309)\n",
      "19949 Training Loss: tensor(0.3310)\n",
      "19950 Training Loss: tensor(0.3310)\n",
      "19951 Training Loss: tensor(0.3334)\n",
      "19952 Training Loss: tensor(0.3304)\n",
      "19953 Training Loss: tensor(0.3307)\n",
      "19954 Training Loss: tensor(0.3304)\n",
      "19955 Training Loss: tensor(0.3327)\n",
      "19956 Training Loss: tensor(0.3304)\n",
      "19957 Training Loss: tensor(0.3383)\n",
      "19958 Training Loss: tensor(0.3358)\n",
      "19959 Training Loss: tensor(0.3341)\n",
      "19960 Training Loss: tensor(0.3316)\n",
      "19961 Training Loss: tensor(0.3330)\n",
      "19962 Training Loss: tensor(0.3334)\n",
      "19963 Training Loss: tensor(0.3326)\n",
      "19964 Training Loss: tensor(0.3335)\n",
      "19965 Training Loss: tensor(0.3312)\n",
      "19966 Training Loss: tensor(0.3341)\n",
      "19967 Training Loss: tensor(0.3303)\n",
      "19968 Training Loss: tensor(0.3327)\n",
      "19969 Training Loss: tensor(0.3331)\n",
      "19970 Training Loss: tensor(0.3318)\n",
      "19971 Training Loss: tensor(0.3327)\n",
      "19972 Training Loss: tensor(0.3316)\n",
      "19973 Training Loss: tensor(0.3316)\n",
      "19974 Training Loss: tensor(0.3312)\n",
      "19975 Training Loss: tensor(0.3309)\n",
      "19976 Training Loss: tensor(0.3325)\n",
      "19977 Training Loss: tensor(0.3334)\n",
      "19978 Training Loss: tensor(0.3301)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19979 Training Loss: tensor(0.3314)\n",
      "19980 Training Loss: tensor(0.3336)\n",
      "19981 Training Loss: tensor(0.3322)\n",
      "19982 Training Loss: tensor(0.3311)\n",
      "19983 Training Loss: tensor(0.3330)\n",
      "19984 Training Loss: tensor(0.3327)\n",
      "19985 Training Loss: tensor(0.3302)\n",
      "19986 Training Loss: tensor(0.3322)\n",
      "19987 Training Loss: tensor(0.3314)\n",
      "19988 Training Loss: tensor(0.3328)\n",
      "19989 Training Loss: tensor(0.3325)\n",
      "19990 Training Loss: tensor(0.3343)\n",
      "19991 Training Loss: tensor(0.3317)\n",
      "19992 Training Loss: tensor(0.3309)\n",
      "19993 Training Loss: tensor(0.3310)\n",
      "19994 Training Loss: tensor(0.3330)\n",
      "19995 Training Loss: tensor(0.3343)\n",
      "19996 Training Loss: tensor(0.3302)\n",
      "19997 Training Loss: tensor(0.3332)\n",
      "19998 Training Loss: tensor(0.3310)\n",
      "19999 Training Loss: tensor(0.3309)\n",
      "20000 Training Loss: tensor(0.3309)\n",
      "20001 Training Loss: tensor(0.3316)\n",
      "20002 Training Loss: tensor(0.3323)\n",
      "20003 Training Loss: tensor(0.3335)\n",
      "20004 Training Loss: tensor(0.3321)\n",
      "20005 Training Loss: tensor(0.3323)\n",
      "20006 Training Loss: tensor(0.3330)\n",
      "20007 Training Loss: tensor(0.3323)\n",
      "20008 Training Loss: tensor(0.3338)\n",
      "20009 Training Loss: tensor(0.3329)\n",
      "20010 Training Loss: tensor(0.3303)\n",
      "20011 Training Loss: tensor(0.3335)\n",
      "20012 Training Loss: tensor(0.3330)\n",
      "20013 Training Loss: tensor(0.3309)\n",
      "20014 Training Loss: tensor(0.3321)\n",
      "20015 Training Loss: tensor(0.3321)\n",
      "20016 Training Loss: tensor(0.3315)\n",
      "20017 Training Loss: tensor(0.3323)\n",
      "20018 Training Loss: tensor(0.3311)\n",
      "20019 Training Loss: tensor(0.3331)\n",
      "20020 Training Loss: tensor(0.3306)\n",
      "20021 Training Loss: tensor(0.3325)\n",
      "20022 Training Loss: tensor(0.3309)\n",
      "20023 Training Loss: tensor(0.3370)\n",
      "20024 Training Loss: tensor(0.3308)\n",
      "20025 Training Loss: tensor(0.3313)\n",
      "20026 Training Loss: tensor(0.3305)\n",
      "20027 Training Loss: tensor(0.3314)\n",
      "20028 Training Loss: tensor(0.3304)\n",
      "20029 Training Loss: tensor(0.3349)\n",
      "20030 Training Loss: tensor(0.3320)\n",
      "20031 Training Loss: tensor(0.3326)\n",
      "20032 Training Loss: tensor(0.3311)\n",
      "20033 Training Loss: tensor(0.3304)\n",
      "20034 Training Loss: tensor(0.3312)\n",
      "20035 Training Loss: tensor(0.3308)\n",
      "20036 Training Loss: tensor(0.3312)\n",
      "20037 Training Loss: tensor(0.3307)\n",
      "20038 Training Loss: tensor(0.3303)\n",
      "20039 Training Loss: tensor(0.3306)\n",
      "20040 Training Loss: tensor(0.3351)\n",
      "20041 Training Loss: tensor(0.3318)\n",
      "20042 Training Loss: tensor(0.3297)\n",
      "20043 Training Loss: tensor(0.3305)\n",
      "20044 Training Loss: tensor(0.3308)\n",
      "20045 Training Loss: tensor(0.3306)\n",
      "20046 Training Loss: tensor(0.3311)\n",
      "20047 Training Loss: tensor(0.3313)\n",
      "20048 Training Loss: tensor(0.3308)\n",
      "20049 Training Loss: tensor(0.3305)\n",
      "20050 Training Loss: tensor(0.3313)\n",
      "20051 Training Loss: tensor(0.3306)\n",
      "20052 Training Loss: tensor(0.3300)\n",
      "20053 Training Loss: tensor(0.3309)\n",
      "20054 Training Loss: tensor(0.3307)\n",
      "20055 Training Loss: tensor(0.3381)\n",
      "20056 Training Loss: tensor(0.3309)\n",
      "20057 Training Loss: tensor(0.3308)\n",
      "20058 Training Loss: tensor(0.3366)\n",
      "20059 Training Loss: tensor(0.3302)\n",
      "20060 Training Loss: tensor(0.3300)\n",
      "20061 Training Loss: tensor(0.3328)\n",
      "20062 Training Loss: tensor(0.3351)\n",
      "20063 Training Loss: tensor(0.3322)\n",
      "20064 Training Loss: tensor(0.3313)\n",
      "20065 Training Loss: tensor(0.3319)\n",
      "20066 Training Loss: tensor(0.3306)\n",
      "20067 Training Loss: tensor(0.3320)\n",
      "20068 Training Loss: tensor(0.3331)\n",
      "20069 Training Loss: tensor(0.3326)\n",
      "20070 Training Loss: tensor(0.3313)\n",
      "20071 Training Loss: tensor(0.3308)\n",
      "20072 Training Loss: tensor(0.3317)\n",
      "20073 Training Loss: tensor(0.3301)\n",
      "20074 Training Loss: tensor(0.3320)\n",
      "20075 Training Loss: tensor(0.3311)\n",
      "20076 Training Loss: tensor(0.3319)\n",
      "20077 Training Loss: tensor(0.3346)\n",
      "20078 Training Loss: tensor(0.3340)\n",
      "20079 Training Loss: tensor(0.3298)\n",
      "20080 Training Loss: tensor(0.3318)\n",
      "20081 Training Loss: tensor(0.3311)\n",
      "20082 Training Loss: tensor(0.3309)\n",
      "20083 Training Loss: tensor(0.3320)\n",
      "20084 Training Loss: tensor(0.3311)\n",
      "20085 Training Loss: tensor(0.3307)\n",
      "20086 Training Loss: tensor(0.3309)\n",
      "20087 Training Loss: tensor(0.3313)\n",
      "20088 Training Loss: tensor(0.3374)\n",
      "20089 Training Loss: tensor(0.3328)\n",
      "20090 Training Loss: tensor(0.3341)\n",
      "20091 Training Loss: tensor(0.3325)\n",
      "20092 Training Loss: tensor(0.3313)\n",
      "20093 Training Loss: tensor(0.3311)\n",
      "20094 Training Loss: tensor(0.3314)\n",
      "20095 Training Loss: tensor(0.3332)\n",
      "20096 Training Loss: tensor(0.3319)\n",
      "20097 Training Loss: tensor(0.3320)\n",
      "20098 Training Loss: tensor(0.3311)\n",
      "20099 Training Loss: tensor(0.3300)\n",
      "20100 Training Loss: tensor(0.3326)\n",
      "20101 Training Loss: tensor(0.3305)\n",
      "20102 Training Loss: tensor(0.3342)\n",
      "20103 Training Loss: tensor(0.3298)\n",
      "20104 Training Loss: tensor(0.3298)\n",
      "20105 Training Loss: tensor(0.3329)\n",
      "20106 Training Loss: tensor(0.3321)\n",
      "20107 Training Loss: tensor(0.3351)\n",
      "20108 Training Loss: tensor(0.3304)\n",
      "20109 Training Loss: tensor(0.3330)\n",
      "20110 Training Loss: tensor(0.3305)\n",
      "20111 Training Loss: tensor(0.3299)\n",
      "20112 Training Loss: tensor(0.3316)\n",
      "20113 Training Loss: tensor(0.3311)\n",
      "20114 Training Loss: tensor(0.3300)\n",
      "20115 Training Loss: tensor(0.3311)\n",
      "20116 Training Loss: tensor(0.3328)\n",
      "20117 Training Loss: tensor(0.3310)\n",
      "20118 Training Loss: tensor(0.3303)\n",
      "20119 Training Loss: tensor(0.3294)\n",
      "20120 Training Loss: tensor(0.3312)\n",
      "20121 Training Loss: tensor(0.3321)\n",
      "20122 Training Loss: tensor(0.3330)\n",
      "20123 Training Loss: tensor(0.3388)\n",
      "20124 Training Loss: tensor(0.3309)\n",
      "20125 Training Loss: tensor(0.3309)\n",
      "20126 Training Loss: tensor(0.3325)\n",
      "20127 Training Loss: tensor(0.3305)\n",
      "20128 Training Loss: tensor(0.3317)\n",
      "20129 Training Loss: tensor(0.3302)\n",
      "20130 Training Loss: tensor(0.3332)\n",
      "20131 Training Loss: tensor(0.3313)\n",
      "20132 Training Loss: tensor(0.3328)\n",
      "20133 Training Loss: tensor(0.3318)\n",
      "20134 Training Loss: tensor(0.3350)\n",
      "20135 Training Loss: tensor(0.3319)\n",
      "20136 Training Loss: tensor(0.3317)\n",
      "20137 Training Loss: tensor(0.3319)\n",
      "20138 Training Loss: tensor(0.3319)\n",
      "20139 Training Loss: tensor(0.3311)\n",
      "20140 Training Loss: tensor(0.3359)\n",
      "20141 Training Loss: tensor(0.3312)\n",
      "20142 Training Loss: tensor(0.3356)\n",
      "20143 Training Loss: tensor(0.3309)\n",
      "20144 Training Loss: tensor(0.3314)\n",
      "20145 Training Loss: tensor(0.3310)\n",
      "20146 Training Loss: tensor(0.3325)\n",
      "20147 Training Loss: tensor(0.3348)\n",
      "20148 Training Loss: tensor(0.3325)\n",
      "20149 Training Loss: tensor(0.3338)\n",
      "20150 Training Loss: tensor(0.3319)\n",
      "20151 Training Loss: tensor(0.3310)\n",
      "20152 Training Loss: tensor(0.3309)\n",
      "20153 Training Loss: tensor(0.3308)\n",
      "20154 Training Loss: tensor(0.3309)\n",
      "20155 Training Loss: tensor(0.3306)\n",
      "20156 Training Loss: tensor(0.3313)\n",
      "20157 Training Loss: tensor(0.3323)\n",
      "20158 Training Loss: tensor(0.3307)\n",
      "20159 Training Loss: tensor(0.3327)\n",
      "20160 Training Loss: tensor(0.3343)\n",
      "20161 Training Loss: tensor(0.3310)\n",
      "20162 Training Loss: tensor(0.3328)\n",
      "20163 Training Loss: tensor(0.3312)\n",
      "20164 Training Loss: tensor(0.3326)\n",
      "20165 Training Loss: tensor(0.3336)\n",
      "20166 Training Loss: tensor(0.3303)\n",
      "20167 Training Loss: tensor(0.3312)\n",
      "20168 Training Loss: tensor(0.3323)\n",
      "20169 Training Loss: tensor(0.3331)\n",
      "20170 Training Loss: tensor(0.3334)\n",
      "20171 Training Loss: tensor(0.3307)\n",
      "20172 Training Loss: tensor(0.3327)\n",
      "20173 Training Loss: tensor(0.3335)\n",
      "20174 Training Loss: tensor(0.3330)\n",
      "20175 Training Loss: tensor(0.3310)\n",
      "20176 Training Loss: tensor(0.3355)\n",
      "20177 Training Loss: tensor(0.3323)\n",
      "20178 Training Loss: tensor(0.3309)\n",
      "20179 Training Loss: tensor(0.3325)\n",
      "20180 Training Loss: tensor(0.3355)\n",
      "20181 Training Loss: tensor(0.3303)\n",
      "20182 Training Loss: tensor(0.3320)\n",
      "20183 Training Loss: tensor(0.3320)\n",
      "20184 Training Loss: tensor(0.3335)\n",
      "20185 Training Loss: tensor(0.3331)\n",
      "20186 Training Loss: tensor(0.3313)\n",
      "20187 Training Loss: tensor(0.3307)\n",
      "20188 Training Loss: tensor(0.3305)\n",
      "20189 Training Loss: tensor(0.3326)\n",
      "20190 Training Loss: tensor(0.3308)\n",
      "20191 Training Loss: tensor(0.3311)\n",
      "20192 Training Loss: tensor(0.3322)\n",
      "20193 Training Loss: tensor(0.3315)\n",
      "20194 Training Loss: tensor(0.3309)\n",
      "20195 Training Loss: tensor(0.3314)\n",
      "20196 Training Loss: tensor(0.3320)\n",
      "20197 Training Loss: tensor(0.3332)\n",
      "20198 Training Loss: tensor(0.3310)\n",
      "20199 Training Loss: tensor(0.3333)\n",
      "20200 Training Loss: tensor(0.3323)\n",
      "20201 Training Loss: tensor(0.3331)\n",
      "20202 Training Loss: tensor(0.3320)\n",
      "20203 Training Loss: tensor(0.3311)\n",
      "20204 Training Loss: tensor(0.3327)\n",
      "20205 Training Loss: tensor(0.3317)\n",
      "20206 Training Loss: tensor(0.3324)\n",
      "20207 Training Loss: tensor(0.3327)\n",
      "20208 Training Loss: tensor(0.3315)\n",
      "20209 Training Loss: tensor(0.3327)\n",
      "20210 Training Loss: tensor(0.3314)\n",
      "20211 Training Loss: tensor(0.3307)\n",
      "20212 Training Loss: tensor(0.3307)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20213 Training Loss: tensor(0.3313)\n",
      "20214 Training Loss: tensor(0.3312)\n",
      "20215 Training Loss: tensor(0.3315)\n",
      "20216 Training Loss: tensor(0.3298)\n",
      "20217 Training Loss: tensor(0.3348)\n",
      "20218 Training Loss: tensor(0.3313)\n",
      "20219 Training Loss: tensor(0.3309)\n",
      "20220 Training Loss: tensor(0.3344)\n",
      "20221 Training Loss: tensor(0.3309)\n",
      "20222 Training Loss: tensor(0.3314)\n",
      "20223 Training Loss: tensor(0.3309)\n",
      "20224 Training Loss: tensor(0.3314)\n",
      "20225 Training Loss: tensor(0.3308)\n",
      "20226 Training Loss: tensor(0.3340)\n",
      "20227 Training Loss: tensor(0.3307)\n",
      "20228 Training Loss: tensor(0.3308)\n",
      "20229 Training Loss: tensor(0.3303)\n",
      "20230 Training Loss: tensor(0.3337)\n",
      "20231 Training Loss: tensor(0.3306)\n",
      "20232 Training Loss: tensor(0.3301)\n",
      "20233 Training Loss: tensor(0.3321)\n",
      "20234 Training Loss: tensor(0.3320)\n",
      "20235 Training Loss: tensor(0.3326)\n",
      "20236 Training Loss: tensor(0.3325)\n",
      "20237 Training Loss: tensor(0.3300)\n",
      "20238 Training Loss: tensor(0.3326)\n",
      "20239 Training Loss: tensor(0.3307)\n",
      "20240 Training Loss: tensor(0.3310)\n",
      "20241 Training Loss: tensor(0.3315)\n",
      "20242 Training Loss: tensor(0.3309)\n",
      "20243 Training Loss: tensor(0.3346)\n",
      "20244 Training Loss: tensor(0.3334)\n",
      "20245 Training Loss: tensor(0.3307)\n",
      "20246 Training Loss: tensor(0.3339)\n",
      "20247 Training Loss: tensor(0.3310)\n",
      "20248 Training Loss: tensor(0.3321)\n",
      "20249 Training Loss: tensor(0.3325)\n",
      "20250 Training Loss: tensor(0.3310)\n",
      "20251 Training Loss: tensor(0.3323)\n",
      "20252 Training Loss: tensor(0.3311)\n",
      "20253 Training Loss: tensor(0.3314)\n",
      "20254 Training Loss: tensor(0.3306)\n",
      "20255 Training Loss: tensor(0.3317)\n",
      "20256 Training Loss: tensor(0.3310)\n",
      "20257 Training Loss: tensor(0.3309)\n",
      "20258 Training Loss: tensor(0.3321)\n",
      "20259 Training Loss: tensor(0.3343)\n",
      "20260 Training Loss: tensor(0.3301)\n",
      "20261 Training Loss: tensor(0.3313)\n",
      "20262 Training Loss: tensor(0.3305)\n",
      "20263 Training Loss: tensor(0.3306)\n",
      "20264 Training Loss: tensor(0.3317)\n",
      "20265 Training Loss: tensor(0.3306)\n",
      "20266 Training Loss: tensor(0.3344)\n",
      "20267 Training Loss: tensor(0.3314)\n",
      "20268 Training Loss: tensor(0.3336)\n",
      "20269 Training Loss: tensor(0.3308)\n",
      "20270 Training Loss: tensor(0.3301)\n",
      "20271 Training Loss: tensor(0.3340)\n",
      "20272 Training Loss: tensor(0.3302)\n",
      "20273 Training Loss: tensor(0.3314)\n",
      "20274 Training Loss: tensor(0.3317)\n",
      "20275 Training Loss: tensor(0.3329)\n",
      "20276 Training Loss: tensor(0.3330)\n",
      "20277 Training Loss: tensor(0.3306)\n",
      "20278 Training Loss: tensor(0.3310)\n",
      "20279 Training Loss: tensor(0.3306)\n",
      "20280 Training Loss: tensor(0.3303)\n",
      "20281 Training Loss: tensor(0.3309)\n",
      "20282 Training Loss: tensor(0.3303)\n",
      "20283 Training Loss: tensor(0.3339)\n",
      "20284 Training Loss: tensor(0.3335)\n",
      "20285 Training Loss: tensor(0.3309)\n",
      "20286 Training Loss: tensor(0.3319)\n",
      "20287 Training Loss: tensor(0.3315)\n",
      "20288 Training Loss: tensor(0.3314)\n",
      "20289 Training Loss: tensor(0.3300)\n",
      "20290 Training Loss: tensor(0.3335)\n",
      "20291 Training Loss: tensor(0.3307)\n",
      "20292 Training Loss: tensor(0.3347)\n",
      "20293 Training Loss: tensor(0.3315)\n",
      "20294 Training Loss: tensor(0.3318)\n",
      "20295 Training Loss: tensor(0.3305)\n",
      "20296 Training Loss: tensor(0.3306)\n",
      "20297 Training Loss: tensor(0.3308)\n",
      "20298 Training Loss: tensor(0.3327)\n",
      "20299 Training Loss: tensor(0.3315)\n",
      "20300 Training Loss: tensor(0.3313)\n",
      "20301 Training Loss: tensor(0.3300)\n",
      "20302 Training Loss: tensor(0.3304)\n",
      "20303 Training Loss: tensor(0.3305)\n",
      "20304 Training Loss: tensor(0.3303)\n",
      "20305 Training Loss: tensor(0.3305)\n",
      "20306 Training Loss: tensor(0.3303)\n",
      "20307 Training Loss: tensor(0.3339)\n",
      "20308 Training Loss: tensor(0.3332)\n",
      "20309 Training Loss: tensor(0.3343)\n",
      "20310 Training Loss: tensor(0.3330)\n",
      "20311 Training Loss: tensor(0.3340)\n",
      "20312 Training Loss: tensor(0.3308)\n",
      "20313 Training Loss: tensor(0.3300)\n",
      "20314 Training Loss: tensor(0.3304)\n",
      "20315 Training Loss: tensor(0.3310)\n",
      "20316 Training Loss: tensor(0.3312)\n",
      "20317 Training Loss: tensor(0.3325)\n",
      "20318 Training Loss: tensor(0.3302)\n",
      "20319 Training Loss: tensor(0.3300)\n",
      "20320 Training Loss: tensor(0.3341)\n",
      "20321 Training Loss: tensor(0.3314)\n",
      "20322 Training Loss: tensor(0.3307)\n",
      "20323 Training Loss: tensor(0.3315)\n",
      "20324 Training Loss: tensor(0.3320)\n",
      "20325 Training Loss: tensor(0.3306)\n",
      "20326 Training Loss: tensor(0.3320)\n",
      "20327 Training Loss: tensor(0.3327)\n",
      "20328 Training Loss: tensor(0.3310)\n",
      "20329 Training Loss: tensor(0.3367)\n",
      "20330 Training Loss: tensor(0.3316)\n",
      "20331 Training Loss: tensor(0.3313)\n",
      "20332 Training Loss: tensor(0.3309)\n",
      "20333 Training Loss: tensor(0.3310)\n",
      "20334 Training Loss: tensor(0.3321)\n",
      "20335 Training Loss: tensor(0.3312)\n",
      "20336 Training Loss: tensor(0.3310)\n",
      "20337 Training Loss: tensor(0.3324)\n",
      "20338 Training Loss: tensor(0.3317)\n",
      "20339 Training Loss: tensor(0.3319)\n",
      "20340 Training Loss: tensor(0.3354)\n",
      "20341 Training Loss: tensor(0.3322)\n",
      "20342 Training Loss: tensor(0.3344)\n",
      "20343 Training Loss: tensor(0.3300)\n",
      "20344 Training Loss: tensor(0.3332)\n",
      "20345 Training Loss: tensor(0.3323)\n",
      "20346 Training Loss: tensor(0.3343)\n",
      "20347 Training Loss: tensor(0.3317)\n",
      "20348 Training Loss: tensor(0.3305)\n",
      "20349 Training Loss: tensor(0.3323)\n",
      "20350 Training Loss: tensor(0.3324)\n",
      "20351 Training Loss: tensor(0.3306)\n",
      "20352 Training Loss: tensor(0.3311)\n",
      "20353 Training Loss: tensor(0.3326)\n",
      "20354 Training Loss: tensor(0.3315)\n",
      "20355 Training Loss: tensor(0.3342)\n",
      "20356 Training Loss: tensor(0.3314)\n",
      "20357 Training Loss: tensor(0.3303)\n",
      "20358 Training Loss: tensor(0.3324)\n",
      "20359 Training Loss: tensor(0.3324)\n",
      "20360 Training Loss: tensor(0.3308)\n",
      "20361 Training Loss: tensor(0.3312)\n",
      "20362 Training Loss: tensor(0.3337)\n",
      "20363 Training Loss: tensor(0.3307)\n",
      "20364 Training Loss: tensor(0.3307)\n",
      "20365 Training Loss: tensor(0.3305)\n",
      "20366 Training Loss: tensor(0.3301)\n",
      "20367 Training Loss: tensor(0.3319)\n",
      "20368 Training Loss: tensor(0.3299)\n",
      "20369 Training Loss: tensor(0.3307)\n",
      "20370 Training Loss: tensor(0.3366)\n",
      "20371 Training Loss: tensor(0.3327)\n",
      "20372 Training Loss: tensor(0.3302)\n",
      "20373 Training Loss: tensor(0.3320)\n",
      "20374 Training Loss: tensor(0.3306)\n",
      "20375 Training Loss: tensor(0.3309)\n",
      "20376 Training Loss: tensor(0.3326)\n",
      "20377 Training Loss: tensor(0.3325)\n",
      "20378 Training Loss: tensor(0.3305)\n",
      "20379 Training Loss: tensor(0.3342)\n",
      "20380 Training Loss: tensor(0.3314)\n",
      "20381 Training Loss: tensor(0.3313)\n",
      "20382 Training Loss: tensor(0.3311)\n",
      "20383 Training Loss: tensor(0.3319)\n",
      "20384 Training Loss: tensor(0.3329)\n",
      "20385 Training Loss: tensor(0.3330)\n",
      "20386 Training Loss: tensor(0.3315)\n",
      "20387 Training Loss: tensor(0.3321)\n",
      "20388 Training Loss: tensor(0.3308)\n",
      "20389 Training Loss: tensor(0.3317)\n",
      "20390 Training Loss: tensor(0.3321)\n",
      "20391 Training Loss: tensor(0.3313)\n",
      "20392 Training Loss: tensor(0.3303)\n",
      "20393 Training Loss: tensor(0.3308)\n",
      "20394 Training Loss: tensor(0.3338)\n",
      "20395 Training Loss: tensor(0.3318)\n",
      "20396 Training Loss: tensor(0.3383)\n",
      "20397 Training Loss: tensor(0.3310)\n",
      "20398 Training Loss: tensor(0.3306)\n",
      "20399 Training Loss: tensor(0.3303)\n",
      "20400 Training Loss: tensor(0.3307)\n",
      "20401 Training Loss: tensor(0.3334)\n",
      "20402 Training Loss: tensor(0.3336)\n",
      "20403 Training Loss: tensor(0.3310)\n",
      "20404 Training Loss: tensor(0.3322)\n",
      "20405 Training Loss: tensor(0.3316)\n",
      "20406 Training Loss: tensor(0.3307)\n",
      "20407 Training Loss: tensor(0.3308)\n",
      "20408 Training Loss: tensor(0.3353)\n",
      "20409 Training Loss: tensor(0.3358)\n",
      "20410 Training Loss: tensor(0.3307)\n",
      "20411 Training Loss: tensor(0.3305)\n",
      "20412 Training Loss: tensor(0.3319)\n",
      "20413 Training Loss: tensor(0.3304)\n",
      "20414 Training Loss: tensor(0.3323)\n",
      "20415 Training Loss: tensor(0.3324)\n",
      "20416 Training Loss: tensor(0.3316)\n",
      "20417 Training Loss: tensor(0.3307)\n",
      "20418 Training Loss: tensor(0.3323)\n",
      "20419 Training Loss: tensor(0.3308)\n",
      "20420 Training Loss: tensor(0.3312)\n",
      "20421 Training Loss: tensor(0.3301)\n",
      "20422 Training Loss: tensor(0.3324)\n",
      "20423 Training Loss: tensor(0.3303)\n",
      "20424 Training Loss: tensor(0.3324)\n",
      "20425 Training Loss: tensor(0.3331)\n",
      "20426 Training Loss: tensor(0.3310)\n",
      "20427 Training Loss: tensor(0.3325)\n",
      "20428 Training Loss: tensor(0.3395)\n",
      "20429 Training Loss: tensor(0.3322)\n",
      "20430 Training Loss: tensor(0.3365)\n",
      "20431 Training Loss: tensor(0.3322)\n",
      "20432 Training Loss: tensor(0.3315)\n",
      "20433 Training Loss: tensor(0.3311)\n",
      "20434 Training Loss: tensor(0.3328)\n",
      "20435 Training Loss: tensor(0.3339)\n",
      "20436 Training Loss: tensor(0.3341)\n",
      "20437 Training Loss: tensor(0.3344)\n",
      "20438 Training Loss: tensor(0.3325)\n",
      "20439 Training Loss: tensor(0.3333)\n",
      "20440 Training Loss: tensor(0.3349)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20441 Training Loss: tensor(0.3328)\n",
      "20442 Training Loss: tensor(0.3317)\n",
      "20443 Training Loss: tensor(0.3321)\n",
      "20444 Training Loss: tensor(0.3304)\n",
      "20445 Training Loss: tensor(0.3317)\n",
      "20446 Training Loss: tensor(0.3307)\n",
      "20447 Training Loss: tensor(0.3332)\n",
      "20448 Training Loss: tensor(0.3321)\n",
      "20449 Training Loss: tensor(0.3320)\n",
      "20450 Training Loss: tensor(0.3311)\n",
      "20451 Training Loss: tensor(0.3307)\n",
      "20452 Training Loss: tensor(0.3309)\n",
      "20453 Training Loss: tensor(0.3334)\n",
      "20454 Training Loss: tensor(0.3319)\n",
      "20455 Training Loss: tensor(0.3320)\n",
      "20456 Training Loss: tensor(0.3315)\n",
      "20457 Training Loss: tensor(0.3311)\n",
      "20458 Training Loss: tensor(0.3325)\n",
      "20459 Training Loss: tensor(0.3306)\n",
      "20460 Training Loss: tensor(0.3321)\n",
      "20461 Training Loss: tensor(0.3351)\n",
      "20462 Training Loss: tensor(0.3321)\n",
      "20463 Training Loss: tensor(0.3313)\n",
      "20464 Training Loss: tensor(0.3306)\n",
      "20465 Training Loss: tensor(0.3317)\n",
      "20466 Training Loss: tensor(0.3376)\n",
      "20467 Training Loss: tensor(0.3321)\n",
      "20468 Training Loss: tensor(0.3330)\n",
      "20469 Training Loss: tensor(0.3326)\n",
      "20470 Training Loss: tensor(0.3320)\n",
      "20471 Training Loss: tensor(0.3323)\n",
      "20472 Training Loss: tensor(0.3314)\n",
      "20473 Training Loss: tensor(0.3327)\n",
      "20474 Training Loss: tensor(0.3308)\n",
      "20475 Training Loss: tensor(0.3323)\n",
      "20476 Training Loss: tensor(0.3318)\n",
      "20477 Training Loss: tensor(0.3301)\n",
      "20478 Training Loss: tensor(0.3311)\n",
      "20479 Training Loss: tensor(0.3308)\n",
      "20480 Training Loss: tensor(0.3312)\n",
      "20481 Training Loss: tensor(0.3304)\n",
      "20482 Training Loss: tensor(0.3320)\n",
      "20483 Training Loss: tensor(0.3337)\n",
      "20484 Training Loss: tensor(0.3322)\n",
      "20485 Training Loss: tensor(0.3313)\n",
      "20486 Training Loss: tensor(0.3302)\n",
      "20487 Training Loss: tensor(0.3311)\n",
      "20488 Training Loss: tensor(0.3301)\n",
      "20489 Training Loss: tensor(0.3310)\n",
      "20490 Training Loss: tensor(0.3303)\n",
      "20491 Training Loss: tensor(0.3324)\n",
      "20492 Training Loss: tensor(0.3326)\n",
      "20493 Training Loss: tensor(0.3337)\n",
      "20494 Training Loss: tensor(0.3313)\n",
      "20495 Training Loss: tensor(0.3302)\n",
      "20496 Training Loss: tensor(0.3297)\n",
      "20497 Training Loss: tensor(0.3301)\n",
      "20498 Training Loss: tensor(0.3312)\n",
      "20499 Training Loss: tensor(0.3319)\n",
      "20500 Training Loss: tensor(0.3311)\n",
      "20501 Training Loss: tensor(0.3318)\n",
      "20502 Training Loss: tensor(0.3294)\n",
      "20503 Training Loss: tensor(0.3297)\n",
      "20504 Training Loss: tensor(0.3296)\n",
      "20505 Training Loss: tensor(0.3304)\n",
      "20506 Training Loss: tensor(0.3338)\n",
      "20507 Training Loss: tensor(0.3334)\n",
      "20508 Training Loss: tensor(0.3318)\n",
      "20509 Training Loss: tensor(0.3303)\n",
      "20510 Training Loss: tensor(0.3294)\n",
      "20511 Training Loss: tensor(0.3308)\n",
      "20512 Training Loss: tensor(0.3324)\n",
      "20513 Training Loss: tensor(0.3309)\n",
      "20514 Training Loss: tensor(0.3314)\n",
      "20515 Training Loss: tensor(0.3310)\n",
      "20516 Training Loss: tensor(0.3311)\n",
      "20517 Training Loss: tensor(0.3305)\n",
      "20518 Training Loss: tensor(0.3313)\n",
      "20519 Training Loss: tensor(0.3318)\n",
      "20520 Training Loss: tensor(0.3298)\n",
      "20521 Training Loss: tensor(0.3323)\n",
      "20522 Training Loss: tensor(0.3300)\n",
      "20523 Training Loss: tensor(0.3295)\n",
      "20524 Training Loss: tensor(0.3303)\n",
      "20525 Training Loss: tensor(0.3309)\n",
      "20526 Training Loss: tensor(0.3389)\n",
      "20527 Training Loss: tensor(0.3310)\n",
      "20528 Training Loss: tensor(0.3314)\n",
      "20529 Training Loss: tensor(0.3314)\n",
      "20530 Training Loss: tensor(0.3329)\n",
      "20531 Training Loss: tensor(0.3315)\n",
      "20532 Training Loss: tensor(0.3326)\n",
      "20533 Training Loss: tensor(0.3333)\n",
      "20534 Training Loss: tensor(0.3304)\n",
      "20535 Training Loss: tensor(0.3307)\n",
      "20536 Training Loss: tensor(0.3310)\n",
      "20537 Training Loss: tensor(0.3300)\n",
      "20538 Training Loss: tensor(0.3309)\n",
      "20539 Training Loss: tensor(0.3342)\n",
      "20540 Training Loss: tensor(0.3306)\n",
      "20541 Training Loss: tensor(0.3308)\n",
      "20542 Training Loss: tensor(0.3302)\n",
      "20543 Training Loss: tensor(0.3323)\n",
      "20544 Training Loss: tensor(0.3321)\n",
      "20545 Training Loss: tensor(0.3317)\n",
      "20546 Training Loss: tensor(0.3310)\n",
      "20547 Training Loss: tensor(0.3305)\n",
      "20548 Training Loss: tensor(0.3323)\n",
      "20549 Training Loss: tensor(0.3304)\n",
      "20550 Training Loss: tensor(0.3308)\n",
      "20551 Training Loss: tensor(0.3309)\n",
      "20552 Training Loss: tensor(0.3292)\n",
      "20553 Training Loss: tensor(0.3300)\n",
      "20554 Training Loss: tensor(0.3366)\n",
      "20555 Training Loss: tensor(0.3329)\n",
      "20556 Training Loss: tensor(0.3319)\n",
      "20557 Training Loss: tensor(0.3304)\n",
      "20558 Training Loss: tensor(0.3331)\n",
      "20559 Training Loss: tensor(0.3303)\n",
      "20560 Training Loss: tensor(0.3363)\n",
      "20561 Training Loss: tensor(0.3297)\n",
      "20562 Training Loss: tensor(0.3309)\n",
      "20563 Training Loss: tensor(0.3326)\n",
      "20564 Training Loss: tensor(0.3314)\n",
      "20565 Training Loss: tensor(0.3323)\n",
      "20566 Training Loss: tensor(0.3312)\n",
      "20567 Training Loss: tensor(0.3307)\n",
      "20568 Training Loss: tensor(0.3306)\n",
      "20569 Training Loss: tensor(0.3313)\n",
      "20570 Training Loss: tensor(0.3346)\n",
      "20571 Training Loss: tensor(0.3301)\n",
      "20572 Training Loss: tensor(0.3322)\n",
      "20573 Training Loss: tensor(0.3303)\n",
      "20574 Training Loss: tensor(0.3313)\n",
      "20575 Training Loss: tensor(0.3306)\n",
      "20576 Training Loss: tensor(0.3332)\n",
      "20577 Training Loss: tensor(0.3316)\n",
      "20578 Training Loss: tensor(0.3299)\n",
      "20579 Training Loss: tensor(0.3349)\n",
      "20580 Training Loss: tensor(0.3303)\n",
      "20581 Training Loss: tensor(0.3312)\n",
      "20582 Training Loss: tensor(0.3328)\n",
      "20583 Training Loss: tensor(0.3305)\n",
      "20584 Training Loss: tensor(0.3299)\n",
      "20585 Training Loss: tensor(0.3311)\n",
      "20586 Training Loss: tensor(0.3301)\n",
      "20587 Training Loss: tensor(0.3332)\n",
      "20588 Training Loss: tensor(0.3316)\n",
      "20589 Training Loss: tensor(0.3296)\n",
      "20590 Training Loss: tensor(0.3304)\n",
      "20591 Training Loss: tensor(0.3295)\n",
      "20592 Training Loss: tensor(0.3330)\n",
      "20593 Training Loss: tensor(0.3306)\n",
      "20594 Training Loss: tensor(0.3302)\n",
      "20595 Training Loss: tensor(0.3310)\n",
      "20596 Training Loss: tensor(0.3365)\n",
      "20597 Training Loss: tensor(0.3317)\n",
      "20598 Training Loss: tensor(0.3307)\n",
      "20599 Training Loss: tensor(0.3318)\n",
      "20600 Training Loss: tensor(0.3313)\n",
      "20601 Training Loss: tensor(0.3301)\n",
      "20602 Training Loss: tensor(0.3322)\n",
      "20603 Training Loss: tensor(0.3297)\n",
      "20604 Training Loss: tensor(0.3338)\n",
      "20605 Training Loss: tensor(0.3291)\n",
      "20606 Training Loss: tensor(0.3307)\n",
      "20607 Training Loss: tensor(0.3311)\n",
      "20608 Training Loss: tensor(0.3380)\n",
      "20609 Training Loss: tensor(0.3301)\n",
      "20610 Training Loss: tensor(0.3326)\n",
      "20611 Training Loss: tensor(0.3300)\n",
      "20612 Training Loss: tensor(0.3334)\n",
      "20613 Training Loss: tensor(0.3295)\n",
      "20614 Training Loss: tensor(0.3309)\n",
      "20615 Training Loss: tensor(0.3326)\n",
      "20616 Training Loss: tensor(0.3305)\n",
      "20617 Training Loss: tensor(0.3310)\n",
      "20618 Training Loss: tensor(0.3314)\n",
      "20619 Training Loss: tensor(0.3321)\n",
      "20620 Training Loss: tensor(0.3325)\n",
      "20621 Training Loss: tensor(0.3301)\n",
      "20622 Training Loss: tensor(0.3330)\n",
      "20623 Training Loss: tensor(0.3325)\n",
      "20624 Training Loss: tensor(0.3310)\n",
      "20625 Training Loss: tensor(0.3330)\n",
      "20626 Training Loss: tensor(0.3334)\n",
      "20627 Training Loss: tensor(0.3310)\n",
      "20628 Training Loss: tensor(0.3320)\n",
      "20629 Training Loss: tensor(0.3315)\n",
      "20630 Training Loss: tensor(0.3314)\n",
      "20631 Training Loss: tensor(0.3303)\n",
      "20632 Training Loss: tensor(0.3312)\n",
      "20633 Training Loss: tensor(0.3321)\n",
      "20634 Training Loss: tensor(0.3307)\n",
      "20635 Training Loss: tensor(0.3342)\n",
      "20636 Training Loss: tensor(0.3299)\n",
      "20637 Training Loss: tensor(0.3327)\n",
      "20638 Training Loss: tensor(0.3306)\n",
      "20639 Training Loss: tensor(0.3312)\n",
      "20640 Training Loss: tensor(0.3314)\n",
      "20641 Training Loss: tensor(0.3331)\n",
      "20642 Training Loss: tensor(0.3303)\n",
      "20643 Training Loss: tensor(0.3313)\n",
      "20644 Training Loss: tensor(0.3331)\n",
      "20645 Training Loss: tensor(0.3299)\n",
      "20646 Training Loss: tensor(0.3300)\n",
      "20647 Training Loss: tensor(0.3310)\n",
      "20648 Training Loss: tensor(0.3308)\n",
      "20649 Training Loss: tensor(0.3304)\n",
      "20650 Training Loss: tensor(0.3357)\n",
      "20651 Training Loss: tensor(0.3316)\n",
      "20652 Training Loss: tensor(0.3311)\n",
      "20653 Training Loss: tensor(0.3311)\n",
      "20654 Training Loss: tensor(0.3312)\n",
      "20655 Training Loss: tensor(0.3303)\n",
      "20656 Training Loss: tensor(0.3346)\n",
      "20657 Training Loss: tensor(0.3322)\n",
      "20658 Training Loss: tensor(0.3307)\n",
      "20659 Training Loss: tensor(0.3297)\n",
      "20660 Training Loss: tensor(0.3293)\n",
      "20661 Training Loss: tensor(0.3312)\n",
      "20662 Training Loss: tensor(0.3297)\n",
      "20663 Training Loss: tensor(0.3310)\n",
      "20664 Training Loss: tensor(0.3306)\n",
      "20665 Training Loss: tensor(0.3306)\n",
      "20666 Training Loss: tensor(0.3299)\n",
      "20667 Training Loss: tensor(0.3312)\n",
      "20668 Training Loss: tensor(0.3310)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20669 Training Loss: tensor(0.3343)\n",
      "20670 Training Loss: tensor(0.3305)\n",
      "20671 Training Loss: tensor(0.3312)\n",
      "20672 Training Loss: tensor(0.3342)\n",
      "20673 Training Loss: tensor(0.3309)\n",
      "20674 Training Loss: tensor(0.3343)\n",
      "20675 Training Loss: tensor(0.3300)\n",
      "20676 Training Loss: tensor(0.3333)\n",
      "20677 Training Loss: tensor(0.3321)\n",
      "20678 Training Loss: tensor(0.3316)\n",
      "20679 Training Loss: tensor(0.3307)\n",
      "20680 Training Loss: tensor(0.3308)\n",
      "20681 Training Loss: tensor(0.3311)\n",
      "20682 Training Loss: tensor(0.3302)\n",
      "20683 Training Loss: tensor(0.3311)\n",
      "20684 Training Loss: tensor(0.3308)\n",
      "20685 Training Loss: tensor(0.3293)\n",
      "20686 Training Loss: tensor(0.3304)\n",
      "20687 Training Loss: tensor(0.3294)\n",
      "20688 Training Loss: tensor(0.3308)\n",
      "20689 Training Loss: tensor(0.3317)\n",
      "20690 Training Loss: tensor(0.3319)\n",
      "20691 Training Loss: tensor(0.3301)\n",
      "20692 Training Loss: tensor(0.3303)\n",
      "20693 Training Loss: tensor(0.3307)\n",
      "20694 Training Loss: tensor(0.3326)\n",
      "20695 Training Loss: tensor(0.3310)\n",
      "20696 Training Loss: tensor(0.3348)\n",
      "20697 Training Loss: tensor(0.3294)\n",
      "20698 Training Loss: tensor(0.3305)\n",
      "20699 Training Loss: tensor(0.3302)\n",
      "20700 Training Loss: tensor(0.3332)\n",
      "20701 Training Loss: tensor(0.3307)\n",
      "20702 Training Loss: tensor(0.3298)\n",
      "20703 Training Loss: tensor(0.3332)\n",
      "20704 Training Loss: tensor(0.3311)\n",
      "20705 Training Loss: tensor(0.3313)\n",
      "20706 Training Loss: tensor(0.3305)\n",
      "20707 Training Loss: tensor(0.3333)\n",
      "20708 Training Loss: tensor(0.3335)\n",
      "20709 Training Loss: tensor(0.3330)\n",
      "20710 Training Loss: tensor(0.3309)\n",
      "20711 Training Loss: tensor(0.3291)\n",
      "20712 Training Loss: tensor(0.3310)\n",
      "20713 Training Loss: tensor(0.3300)\n",
      "20714 Training Loss: tensor(0.3310)\n",
      "20715 Training Loss: tensor(0.3302)\n",
      "20716 Training Loss: tensor(0.3305)\n",
      "20717 Training Loss: tensor(0.3301)\n",
      "20718 Training Loss: tensor(0.3318)\n",
      "20719 Training Loss: tensor(0.3300)\n",
      "20720 Training Loss: tensor(0.3299)\n",
      "20721 Training Loss: tensor(0.3327)\n",
      "20722 Training Loss: tensor(0.3303)\n",
      "20723 Training Loss: tensor(0.3328)\n",
      "20724 Training Loss: tensor(0.3306)\n",
      "20725 Training Loss: tensor(0.3320)\n",
      "20726 Training Loss: tensor(0.3313)\n",
      "20727 Training Loss: tensor(0.3305)\n",
      "20728 Training Loss: tensor(0.3318)\n",
      "20729 Training Loss: tensor(0.3306)\n",
      "20730 Training Loss: tensor(0.3297)\n",
      "20731 Training Loss: tensor(0.3319)\n",
      "20732 Training Loss: tensor(0.3310)\n",
      "20733 Training Loss: tensor(0.3299)\n",
      "20734 Training Loss: tensor(0.3353)\n",
      "20735 Training Loss: tensor(0.3302)\n",
      "20736 Training Loss: tensor(0.3367)\n",
      "20737 Training Loss: tensor(0.3309)\n",
      "20738 Training Loss: tensor(0.3308)\n",
      "20739 Training Loss: tensor(0.3302)\n",
      "20740 Training Loss: tensor(0.3304)\n",
      "20741 Training Loss: tensor(0.3305)\n",
      "20742 Training Loss: tensor(0.3309)\n",
      "20743 Training Loss: tensor(0.3318)\n",
      "20744 Training Loss: tensor(0.3297)\n",
      "20745 Training Loss: tensor(0.3304)\n",
      "20746 Training Loss: tensor(0.3303)\n",
      "20747 Training Loss: tensor(0.3309)\n",
      "20748 Training Loss: tensor(0.3298)\n",
      "20749 Training Loss: tensor(0.3311)\n",
      "20750 Training Loss: tensor(0.3304)\n",
      "20751 Training Loss: tensor(0.3345)\n",
      "20752 Training Loss: tensor(0.3313)\n",
      "20753 Training Loss: tensor(0.3307)\n",
      "20754 Training Loss: tensor(0.3380)\n",
      "20755 Training Loss: tensor(0.3330)\n",
      "20756 Training Loss: tensor(0.3313)\n",
      "20757 Training Loss: tensor(0.3333)\n",
      "20758 Training Loss: tensor(0.3305)\n",
      "20759 Training Loss: tensor(0.3309)\n",
      "20760 Training Loss: tensor(0.3331)\n",
      "20761 Training Loss: tensor(0.3319)\n",
      "20762 Training Loss: tensor(0.3311)\n",
      "20763 Training Loss: tensor(0.3331)\n",
      "20764 Training Loss: tensor(0.3314)\n",
      "20765 Training Loss: tensor(0.3308)\n",
      "20766 Training Loss: tensor(0.3345)\n",
      "20767 Training Loss: tensor(0.3320)\n",
      "20768 Training Loss: tensor(0.3320)\n",
      "20769 Training Loss: tensor(0.3308)\n",
      "20770 Training Loss: tensor(0.3313)\n",
      "20771 Training Loss: tensor(0.3337)\n",
      "20772 Training Loss: tensor(0.3308)\n",
      "20773 Training Loss: tensor(0.3298)\n",
      "20774 Training Loss: tensor(0.3334)\n",
      "20775 Training Loss: tensor(0.3315)\n",
      "20776 Training Loss: tensor(0.3311)\n",
      "20777 Training Loss: tensor(0.3312)\n",
      "20778 Training Loss: tensor(0.3344)\n",
      "20779 Training Loss: tensor(0.3319)\n",
      "20780 Training Loss: tensor(0.3298)\n",
      "20781 Training Loss: tensor(0.3306)\n",
      "20782 Training Loss: tensor(0.3304)\n",
      "20783 Training Loss: tensor(0.3309)\n",
      "20784 Training Loss: tensor(0.3313)\n",
      "20785 Training Loss: tensor(0.3310)\n",
      "20786 Training Loss: tensor(0.3302)\n",
      "20787 Training Loss: tensor(0.3316)\n",
      "20788 Training Loss: tensor(0.3309)\n",
      "20789 Training Loss: tensor(0.3340)\n",
      "20790 Training Loss: tensor(0.3308)\n",
      "20791 Training Loss: tensor(0.3315)\n",
      "20792 Training Loss: tensor(0.3305)\n",
      "20793 Training Loss: tensor(0.3315)\n",
      "20794 Training Loss: tensor(0.3321)\n",
      "20795 Training Loss: tensor(0.3347)\n",
      "20796 Training Loss: tensor(0.3321)\n",
      "20797 Training Loss: tensor(0.3303)\n",
      "20798 Training Loss: tensor(0.3303)\n",
      "20799 Training Loss: tensor(0.3330)\n",
      "20800 Training Loss: tensor(0.3306)\n",
      "20801 Training Loss: tensor(0.3311)\n",
      "20802 Training Loss: tensor(0.3314)\n",
      "20803 Training Loss: tensor(0.3320)\n",
      "20804 Training Loss: tensor(0.3304)\n",
      "20805 Training Loss: tensor(0.3321)\n",
      "20806 Training Loss: tensor(0.3305)\n",
      "20807 Training Loss: tensor(0.3307)\n",
      "20808 Training Loss: tensor(0.3322)\n",
      "20809 Training Loss: tensor(0.3303)\n",
      "20810 Training Loss: tensor(0.3347)\n",
      "20811 Training Loss: tensor(0.3302)\n",
      "20812 Training Loss: tensor(0.3303)\n",
      "20813 Training Loss: tensor(0.3307)\n",
      "20814 Training Loss: tensor(0.3325)\n",
      "20815 Training Loss: tensor(0.3354)\n",
      "20816 Training Loss: tensor(0.3341)\n",
      "20817 Training Loss: tensor(0.3312)\n",
      "20818 Training Loss: tensor(0.3310)\n",
      "20819 Training Loss: tensor(0.3306)\n",
      "20820 Training Loss: tensor(0.3316)\n",
      "20821 Training Loss: tensor(0.3309)\n",
      "20822 Training Loss: tensor(0.3318)\n",
      "20823 Training Loss: tensor(0.3311)\n",
      "20824 Training Loss: tensor(0.3301)\n",
      "20825 Training Loss: tensor(0.3309)\n",
      "20826 Training Loss: tensor(0.3323)\n",
      "20827 Training Loss: tensor(0.3298)\n",
      "20828 Training Loss: tensor(0.3329)\n",
      "20829 Training Loss: tensor(0.3314)\n",
      "20830 Training Loss: tensor(0.3307)\n",
      "20831 Training Loss: tensor(0.3308)\n",
      "20832 Training Loss: tensor(0.3303)\n",
      "20833 Training Loss: tensor(0.3297)\n",
      "20834 Training Loss: tensor(0.3312)\n",
      "20835 Training Loss: tensor(0.3309)\n",
      "20836 Training Loss: tensor(0.3303)\n",
      "20837 Training Loss: tensor(0.3311)\n",
      "20838 Training Loss: tensor(0.3308)\n",
      "20839 Training Loss: tensor(0.3351)\n",
      "20840 Training Loss: tensor(0.3326)\n",
      "20841 Training Loss: tensor(0.3312)\n",
      "20842 Training Loss: tensor(0.3312)\n",
      "20843 Training Loss: tensor(0.3309)\n",
      "20844 Training Loss: tensor(0.3312)\n",
      "20845 Training Loss: tensor(0.3319)\n",
      "20846 Training Loss: tensor(0.3309)\n",
      "20847 Training Loss: tensor(0.3304)\n",
      "20848 Training Loss: tensor(0.3338)\n",
      "20849 Training Loss: tensor(0.3291)\n",
      "20850 Training Loss: tensor(0.3319)\n",
      "20851 Training Loss: tensor(0.3298)\n",
      "20852 Training Loss: tensor(0.3305)\n",
      "20853 Training Loss: tensor(0.3305)\n",
      "20854 Training Loss: tensor(0.3300)\n",
      "20855 Training Loss: tensor(0.3306)\n",
      "20856 Training Loss: tensor(0.3298)\n",
      "20857 Training Loss: tensor(0.3326)\n",
      "20858 Training Loss: tensor(0.3302)\n",
      "20859 Training Loss: tensor(0.3334)\n",
      "20860 Training Loss: tensor(0.3310)\n",
      "20861 Training Loss: tensor(0.3305)\n",
      "20862 Training Loss: tensor(0.3308)\n",
      "20863 Training Loss: tensor(0.3319)\n",
      "20864 Training Loss: tensor(0.3325)\n",
      "20865 Training Loss: tensor(0.3297)\n",
      "20866 Training Loss: tensor(0.3300)\n",
      "20867 Training Loss: tensor(0.3316)\n",
      "20868 Training Loss: tensor(0.3305)\n",
      "20869 Training Loss: tensor(0.3316)\n",
      "20870 Training Loss: tensor(0.3298)\n",
      "20871 Training Loss: tensor(0.3336)\n",
      "20872 Training Loss: tensor(0.3311)\n",
      "20873 Training Loss: tensor(0.3300)\n",
      "20874 Training Loss: tensor(0.3303)\n",
      "20875 Training Loss: tensor(0.3291)\n",
      "20876 Training Loss: tensor(0.3305)\n",
      "20877 Training Loss: tensor(0.3319)\n",
      "20878 Training Loss: tensor(0.3311)\n",
      "20879 Training Loss: tensor(0.3291)\n",
      "20880 Training Loss: tensor(0.3331)\n",
      "20881 Training Loss: tensor(0.3310)\n",
      "20882 Training Loss: tensor(0.3299)\n",
      "20883 Training Loss: tensor(0.3302)\n",
      "20884 Training Loss: tensor(0.3348)\n",
      "20885 Training Loss: tensor(0.3311)\n",
      "20886 Training Loss: tensor(0.3310)\n",
      "20887 Training Loss: tensor(0.3308)\n",
      "20888 Training Loss: tensor(0.3316)\n",
      "20889 Training Loss: tensor(0.3325)\n",
      "20890 Training Loss: tensor(0.3333)\n",
      "20891 Training Loss: tensor(0.3344)\n",
      "20892 Training Loss: tensor(0.3318)\n",
      "20893 Training Loss: tensor(0.3368)\n",
      "20894 Training Loss: tensor(0.3324)\n",
      "20895 Training Loss: tensor(0.3302)\n",
      "20896 Training Loss: tensor(0.3336)\n",
      "20897 Training Loss: tensor(0.3303)\n",
      "20898 Training Loss: tensor(0.3362)\n",
      "20899 Training Loss: tensor(0.3309)\n",
      "20900 Training Loss: tensor(0.3329)\n",
      "20901 Training Loss: tensor(0.3316)\n",
      "20902 Training Loss: tensor(0.3311)\n",
      "20903 Training Loss: tensor(0.3330)\n",
      "20904 Training Loss: tensor(0.3311)\n",
      "20905 Training Loss: tensor(0.3314)\n",
      "20906 Training Loss: tensor(0.3296)\n",
      "20907 Training Loss: tensor(0.3308)\n",
      "20908 Training Loss: tensor(0.3294)\n",
      "20909 Training Loss: tensor(0.3308)\n",
      "20910 Training Loss: tensor(0.3303)\n",
      "20911 Training Loss: tensor(0.3359)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20912 Training Loss: tensor(0.3296)\n",
      "20913 Training Loss: tensor(0.3297)\n",
      "20914 Training Loss: tensor(0.3328)\n",
      "20915 Training Loss: tensor(0.3346)\n",
      "20916 Training Loss: tensor(0.3323)\n",
      "20917 Training Loss: tensor(0.3316)\n",
      "20918 Training Loss: tensor(0.3303)\n",
      "20919 Training Loss: tensor(0.3306)\n",
      "20920 Training Loss: tensor(0.3319)\n",
      "20921 Training Loss: tensor(0.3320)\n",
      "20922 Training Loss: tensor(0.3306)\n",
      "20923 Training Loss: tensor(0.3333)\n",
      "20924 Training Loss: tensor(0.3333)\n",
      "20925 Training Loss: tensor(0.3322)\n",
      "20926 Training Loss: tensor(0.3309)\n",
      "20927 Training Loss: tensor(0.3322)\n",
      "20928 Training Loss: tensor(0.3301)\n",
      "20929 Training Loss: tensor(0.3307)\n",
      "20930 Training Loss: tensor(0.3346)\n",
      "20931 Training Loss: tensor(0.3314)\n",
      "20932 Training Loss: tensor(0.3307)\n",
      "20933 Training Loss: tensor(0.3339)\n",
      "20934 Training Loss: tensor(0.3339)\n",
      "20935 Training Loss: tensor(0.3317)\n",
      "20936 Training Loss: tensor(0.3306)\n",
      "20937 Training Loss: tensor(0.3322)\n",
      "20938 Training Loss: tensor(0.3341)\n",
      "20939 Training Loss: tensor(0.3319)\n",
      "20940 Training Loss: tensor(0.3333)\n",
      "20941 Training Loss: tensor(0.3302)\n",
      "20942 Training Loss: tensor(0.3303)\n",
      "20943 Training Loss: tensor(0.3322)\n",
      "20944 Training Loss: tensor(0.3310)\n",
      "20945 Training Loss: tensor(0.3350)\n",
      "20946 Training Loss: tensor(0.3326)\n",
      "20947 Training Loss: tensor(0.3304)\n",
      "20948 Training Loss: tensor(0.3299)\n",
      "20949 Training Loss: tensor(0.3301)\n",
      "20950 Training Loss: tensor(0.3298)\n",
      "20951 Training Loss: tensor(0.3299)\n",
      "20952 Training Loss: tensor(0.3305)\n",
      "20953 Training Loss: tensor(0.3304)\n",
      "20954 Training Loss: tensor(0.3317)\n",
      "20955 Training Loss: tensor(0.3308)\n",
      "20956 Training Loss: tensor(0.3311)\n",
      "20957 Training Loss: tensor(0.3308)\n",
      "20958 Training Loss: tensor(0.3315)\n",
      "20959 Training Loss: tensor(0.3303)\n",
      "20960 Training Loss: tensor(0.3316)\n",
      "20961 Training Loss: tensor(0.3332)\n",
      "20962 Training Loss: tensor(0.3311)\n",
      "20963 Training Loss: tensor(0.3328)\n",
      "20964 Training Loss: tensor(0.3299)\n",
      "20965 Training Loss: tensor(0.3314)\n",
      "20966 Training Loss: tensor(0.3323)\n",
      "20967 Training Loss: tensor(0.3313)\n",
      "20968 Training Loss: tensor(0.3325)\n",
      "20969 Training Loss: tensor(0.3316)\n",
      "20970 Training Loss: tensor(0.3321)\n",
      "20971 Training Loss: tensor(0.3348)\n",
      "20972 Training Loss: tensor(0.3301)\n",
      "20973 Training Loss: tensor(0.3327)\n",
      "20974 Training Loss: tensor(0.3308)\n",
      "20975 Training Loss: tensor(0.3303)\n",
      "20976 Training Loss: tensor(0.3306)\n",
      "20977 Training Loss: tensor(0.3329)\n",
      "20978 Training Loss: tensor(0.3298)\n",
      "20979 Training Loss: tensor(0.3315)\n",
      "20980 Training Loss: tensor(0.3300)\n",
      "20981 Training Loss: tensor(0.3318)\n",
      "20982 Training Loss: tensor(0.3340)\n",
      "20983 Training Loss: tensor(0.3322)\n",
      "20984 Training Loss: tensor(0.3326)\n",
      "20985 Training Loss: tensor(0.3308)\n",
      "20986 Training Loss: tensor(0.3299)\n",
      "20987 Training Loss: tensor(0.3306)\n",
      "20988 Training Loss: tensor(0.3305)\n",
      "20989 Training Loss: tensor(0.3316)\n",
      "20990 Training Loss: tensor(0.3332)\n",
      "20991 Training Loss: tensor(0.3323)\n",
      "20992 Training Loss: tensor(0.3318)\n",
      "20993 Training Loss: tensor(0.3316)\n",
      "20994 Training Loss: tensor(0.3311)\n",
      "20995 Training Loss: tensor(0.3349)\n",
      "20996 Training Loss: tensor(0.3337)\n",
      "20997 Training Loss: tensor(0.3314)\n",
      "20998 Training Loss: tensor(0.3319)\n",
      "20999 Training Loss: tensor(0.3309)\n",
      "21000 Training Loss: tensor(0.3310)\n",
      "21001 Training Loss: tensor(0.3304)\n",
      "21002 Training Loss: tensor(0.3320)\n",
      "21003 Training Loss: tensor(0.3328)\n",
      "21004 Training Loss: tensor(0.3311)\n",
      "21005 Training Loss: tensor(0.3317)\n",
      "21006 Training Loss: tensor(0.3314)\n",
      "21007 Training Loss: tensor(0.3314)\n",
      "21008 Training Loss: tensor(0.3326)\n",
      "21009 Training Loss: tensor(0.3329)\n",
      "21010 Training Loss: tensor(0.3309)\n",
      "21011 Training Loss: tensor(0.3335)\n",
      "21012 Training Loss: tensor(0.3308)\n",
      "21013 Training Loss: tensor(0.3302)\n",
      "21014 Training Loss: tensor(0.3318)\n",
      "21015 Training Loss: tensor(0.3309)\n",
      "21016 Training Loss: tensor(0.3300)\n",
      "21017 Training Loss: tensor(0.3325)\n",
      "21018 Training Loss: tensor(0.3313)\n",
      "21019 Training Loss: tensor(0.3297)\n",
      "21020 Training Loss: tensor(0.3333)\n",
      "21021 Training Loss: tensor(0.3312)\n",
      "21022 Training Loss: tensor(0.3328)\n",
      "21023 Training Loss: tensor(0.3300)\n",
      "21024 Training Loss: tensor(0.3301)\n",
      "21025 Training Loss: tensor(0.3300)\n",
      "21026 Training Loss: tensor(0.3319)\n",
      "21027 Training Loss: tensor(0.3312)\n",
      "21028 Training Loss: tensor(0.3316)\n",
      "21029 Training Loss: tensor(0.3308)\n",
      "21030 Training Loss: tensor(0.3301)\n",
      "21031 Training Loss: tensor(0.3301)\n",
      "21032 Training Loss: tensor(0.3320)\n",
      "21033 Training Loss: tensor(0.3295)\n",
      "21034 Training Loss: tensor(0.3296)\n",
      "21035 Training Loss: tensor(0.3309)\n",
      "21036 Training Loss: tensor(0.3308)\n",
      "21037 Training Loss: tensor(0.3339)\n",
      "21038 Training Loss: tensor(0.3296)\n",
      "21039 Training Loss: tensor(0.3302)\n",
      "21040 Training Loss: tensor(0.3314)\n",
      "21041 Training Loss: tensor(0.3290)\n",
      "21042 Training Loss: tensor(0.3307)\n",
      "21043 Training Loss: tensor(0.3315)\n",
      "21044 Training Loss: tensor(0.3303)\n",
      "21045 Training Loss: tensor(0.3306)\n",
      "21046 Training Loss: tensor(0.3307)\n",
      "21047 Training Loss: tensor(0.3307)\n",
      "21048 Training Loss: tensor(0.3304)\n",
      "21049 Training Loss: tensor(0.3302)\n",
      "21050 Training Loss: tensor(0.3326)\n",
      "21051 Training Loss: tensor(0.3305)\n",
      "21052 Training Loss: tensor(0.3313)\n",
      "21053 Training Loss: tensor(0.3307)\n",
      "21054 Training Loss: tensor(0.3324)\n",
      "21055 Training Loss: tensor(0.3318)\n",
      "21056 Training Loss: tensor(0.3306)\n",
      "21057 Training Loss: tensor(0.3315)\n",
      "21058 Training Loss: tensor(0.3303)\n",
      "21059 Training Loss: tensor(0.3309)\n",
      "21060 Training Loss: tensor(0.3298)\n",
      "21061 Training Loss: tensor(0.3342)\n",
      "21062 Training Loss: tensor(0.3306)\n",
      "21063 Training Loss: tensor(0.3298)\n",
      "21064 Training Loss: tensor(0.3316)\n",
      "21065 Training Loss: tensor(0.3305)\n",
      "21066 Training Loss: tensor(0.3296)\n",
      "21067 Training Loss: tensor(0.3309)\n",
      "21068 Training Loss: tensor(0.3312)\n",
      "21069 Training Loss: tensor(0.3292)\n",
      "21070 Training Loss: tensor(0.3293)\n",
      "21071 Training Loss: tensor(0.3314)\n",
      "21072 Training Loss: tensor(0.3295)\n",
      "21073 Training Loss: tensor(0.3319)\n",
      "21074 Training Loss: tensor(0.3307)\n",
      "21075 Training Loss: tensor(0.3317)\n",
      "21076 Training Loss: tensor(0.3308)\n",
      "21077 Training Loss: tensor(0.3295)\n",
      "21078 Training Loss: tensor(0.3294)\n",
      "21079 Training Loss: tensor(0.3292)\n",
      "21080 Training Loss: tensor(0.3333)\n",
      "21081 Training Loss: tensor(0.3296)\n",
      "21082 Training Loss: tensor(0.3328)\n",
      "21083 Training Loss: tensor(0.3314)\n",
      "21084 Training Loss: tensor(0.3289)\n",
      "21085 Training Loss: tensor(0.3318)\n",
      "21086 Training Loss: tensor(0.3335)\n",
      "21087 Training Loss: tensor(0.3291)\n",
      "21088 Training Loss: tensor(0.3316)\n",
      "21089 Training Loss: tensor(0.3297)\n",
      "21090 Training Loss: tensor(0.3306)\n",
      "21091 Training Loss: tensor(0.3293)\n",
      "21092 Training Loss: tensor(0.3319)\n",
      "21093 Training Loss: tensor(0.3301)\n",
      "21094 Training Loss: tensor(0.3301)\n",
      "21095 Training Loss: tensor(0.3303)\n",
      "21096 Training Loss: tensor(0.3316)\n",
      "21097 Training Loss: tensor(0.3301)\n",
      "21098 Training Loss: tensor(0.3300)\n",
      "21099 Training Loss: tensor(0.3302)\n",
      "21100 Training Loss: tensor(0.3296)\n",
      "21101 Training Loss: tensor(0.3302)\n",
      "21102 Training Loss: tensor(0.3304)\n",
      "21103 Training Loss: tensor(0.3298)\n",
      "21104 Training Loss: tensor(0.3314)\n",
      "21105 Training Loss: tensor(0.3352)\n",
      "21106 Training Loss: tensor(0.3328)\n",
      "21107 Training Loss: tensor(0.3313)\n",
      "21108 Training Loss: tensor(0.3301)\n",
      "21109 Training Loss: tensor(0.3308)\n",
      "21110 Training Loss: tensor(0.3303)\n",
      "21111 Training Loss: tensor(0.3307)\n",
      "21112 Training Loss: tensor(0.3317)\n",
      "21113 Training Loss: tensor(0.3309)\n",
      "21114 Training Loss: tensor(0.3322)\n",
      "21115 Training Loss: tensor(0.3315)\n",
      "21116 Training Loss: tensor(0.3327)\n",
      "21117 Training Loss: tensor(0.3308)\n",
      "21118 Training Loss: tensor(0.3309)\n",
      "21119 Training Loss: tensor(0.3318)\n",
      "21120 Training Loss: tensor(0.3320)\n",
      "21121 Training Loss: tensor(0.3322)\n",
      "21122 Training Loss: tensor(0.3321)\n",
      "21123 Training Loss: tensor(0.3335)\n",
      "21124 Training Loss: tensor(0.3320)\n",
      "21125 Training Loss: tensor(0.3313)\n",
      "21126 Training Loss: tensor(0.3304)\n",
      "21127 Training Loss: tensor(0.3303)\n",
      "21128 Training Loss: tensor(0.3331)\n",
      "21129 Training Loss: tensor(0.3311)\n",
      "21130 Training Loss: tensor(0.3294)\n",
      "21131 Training Loss: tensor(0.3345)\n",
      "21132 Training Loss: tensor(0.3304)\n",
      "21133 Training Loss: tensor(0.3347)\n",
      "21134 Training Loss: tensor(0.3296)\n",
      "21135 Training Loss: tensor(0.3298)\n",
      "21136 Training Loss: tensor(0.3316)\n",
      "21137 Training Loss: tensor(0.3301)\n",
      "21138 Training Loss: tensor(0.3337)\n",
      "21139 Training Loss: tensor(0.3297)\n",
      "21140 Training Loss: tensor(0.3298)\n",
      "21141 Training Loss: tensor(0.3309)\n",
      "21142 Training Loss: tensor(0.3319)\n",
      "21143 Training Loss: tensor(0.3332)\n",
      "21144 Training Loss: tensor(0.3308)\n",
      "21145 Training Loss: tensor(0.3303)\n",
      "21146 Training Loss: tensor(0.3316)\n",
      "21147 Training Loss: tensor(0.3334)\n",
      "21148 Training Loss: tensor(0.3337)\n",
      "21149 Training Loss: tensor(0.3315)\n",
      "21150 Training Loss: tensor(0.3324)\n",
      "21151 Training Loss: tensor(0.3297)\n",
      "21152 Training Loss: tensor(0.3324)\n",
      "21153 Training Loss: tensor(0.3301)\n",
      "21154 Training Loss: tensor(0.3327)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21155 Training Loss: tensor(0.3311)\n",
      "21156 Training Loss: tensor(0.3332)\n",
      "21157 Training Loss: tensor(0.3355)\n",
      "21158 Training Loss: tensor(0.3304)\n",
      "21159 Training Loss: tensor(0.3309)\n",
      "21160 Training Loss: tensor(0.3304)\n",
      "21161 Training Loss: tensor(0.3309)\n",
      "21162 Training Loss: tensor(0.3306)\n",
      "21163 Training Loss: tensor(0.3304)\n",
      "21164 Training Loss: tensor(0.3300)\n",
      "21165 Training Loss: tensor(0.3300)\n",
      "21166 Training Loss: tensor(0.3292)\n",
      "21167 Training Loss: tensor(0.3319)\n",
      "21168 Training Loss: tensor(0.3311)\n",
      "21169 Training Loss: tensor(0.3316)\n",
      "21170 Training Loss: tensor(0.3298)\n",
      "21171 Training Loss: tensor(0.3289)\n",
      "21172 Training Loss: tensor(0.3297)\n",
      "21173 Training Loss: tensor(0.3296)\n",
      "21174 Training Loss: tensor(0.3321)\n",
      "21175 Training Loss: tensor(0.3345)\n",
      "21176 Training Loss: tensor(0.3328)\n",
      "21177 Training Loss: tensor(0.3317)\n",
      "21178 Training Loss: tensor(0.3305)\n",
      "21179 Training Loss: tensor(0.3317)\n",
      "21180 Training Loss: tensor(0.3320)\n",
      "21181 Training Loss: tensor(0.3307)\n",
      "21182 Training Loss: tensor(0.3309)\n",
      "21183 Training Loss: tensor(0.3300)\n",
      "21184 Training Loss: tensor(0.3352)\n",
      "21185 Training Loss: tensor(0.3300)\n",
      "21186 Training Loss: tensor(0.3297)\n",
      "21187 Training Loss: tensor(0.3341)\n",
      "21188 Training Loss: tensor(0.3305)\n",
      "21189 Training Loss: tensor(0.3355)\n",
      "21190 Training Loss: tensor(0.3297)\n",
      "21191 Training Loss: tensor(0.3301)\n",
      "21192 Training Loss: tensor(0.3312)\n",
      "21193 Training Loss: tensor(0.3302)\n",
      "21194 Training Loss: tensor(0.3306)\n",
      "21195 Training Loss: tensor(0.3315)\n",
      "21196 Training Loss: tensor(0.3298)\n",
      "21197 Training Loss: tensor(0.3341)\n",
      "21198 Training Loss: tensor(0.3321)\n",
      "21199 Training Loss: tensor(0.3308)\n",
      "21200 Training Loss: tensor(0.3306)\n",
      "21201 Training Loss: tensor(0.3315)\n",
      "21202 Training Loss: tensor(0.3296)\n",
      "21203 Training Loss: tensor(0.3312)\n",
      "21204 Training Loss: tensor(0.3295)\n",
      "21205 Training Loss: tensor(0.3314)\n",
      "21206 Training Loss: tensor(0.3307)\n",
      "21207 Training Loss: tensor(0.3303)\n",
      "21208 Training Loss: tensor(0.3304)\n",
      "21209 Training Loss: tensor(0.3298)\n",
      "21210 Training Loss: tensor(0.3311)\n",
      "21211 Training Loss: tensor(0.3306)\n",
      "21212 Training Loss: tensor(0.3327)\n",
      "21213 Training Loss: tensor(0.3291)\n",
      "21214 Training Loss: tensor(0.3296)\n",
      "21215 Training Loss: tensor(0.3302)\n",
      "21216 Training Loss: tensor(0.3294)\n",
      "21217 Training Loss: tensor(0.3313)\n",
      "21218 Training Loss: tensor(0.3310)\n",
      "21219 Training Loss: tensor(0.3295)\n",
      "21220 Training Loss: tensor(0.3304)\n",
      "21221 Training Loss: tensor(0.3348)\n",
      "21222 Training Loss: tensor(0.3314)\n",
      "21223 Training Loss: tensor(0.3359)\n",
      "21224 Training Loss: tensor(0.3311)\n",
      "21225 Training Loss: tensor(0.3295)\n",
      "21226 Training Loss: tensor(0.3304)\n",
      "21227 Training Loss: tensor(0.3344)\n",
      "21228 Training Loss: tensor(0.3299)\n",
      "21229 Training Loss: tensor(0.3309)\n",
      "21230 Training Loss: tensor(0.3320)\n",
      "21231 Training Loss: tensor(0.3331)\n",
      "21232 Training Loss: tensor(0.3361)\n",
      "21233 Training Loss: tensor(0.3308)\n",
      "21234 Training Loss: tensor(0.3302)\n",
      "21235 Training Loss: tensor(0.3311)\n",
      "21236 Training Loss: tensor(0.3297)\n",
      "21237 Training Loss: tensor(0.3320)\n",
      "21238 Training Loss: tensor(0.3319)\n",
      "21239 Training Loss: tensor(0.3318)\n",
      "21240 Training Loss: tensor(0.3294)\n",
      "21241 Training Loss: tensor(0.3328)\n",
      "21242 Training Loss: tensor(0.3302)\n",
      "21243 Training Loss: tensor(0.3363)\n",
      "21244 Training Loss: tensor(0.3314)\n",
      "21245 Training Loss: tensor(0.3337)\n",
      "21246 Training Loss: tensor(0.3323)\n",
      "21247 Training Loss: tensor(0.3301)\n",
      "21248 Training Loss: tensor(0.3304)\n",
      "21249 Training Loss: tensor(0.3316)\n",
      "21250 Training Loss: tensor(0.3336)\n",
      "21251 Training Loss: tensor(0.3313)\n",
      "21252 Training Loss: tensor(0.3319)\n",
      "21253 Training Loss: tensor(0.3315)\n",
      "21254 Training Loss: tensor(0.3318)\n",
      "21255 Training Loss: tensor(0.3329)\n",
      "21256 Training Loss: tensor(0.3326)\n",
      "21257 Training Loss: tensor(0.3325)\n",
      "21258 Training Loss: tensor(0.3300)\n",
      "21259 Training Loss: tensor(0.3338)\n",
      "21260 Training Loss: tensor(0.3310)\n",
      "21261 Training Loss: tensor(0.3295)\n",
      "21262 Training Loss: tensor(0.3310)\n",
      "21263 Training Loss: tensor(0.3298)\n",
      "21264 Training Loss: tensor(0.3323)\n",
      "21265 Training Loss: tensor(0.3315)\n",
      "21266 Training Loss: tensor(0.3309)\n",
      "21267 Training Loss: tensor(0.3326)\n",
      "21268 Training Loss: tensor(0.3305)\n",
      "21269 Training Loss: tensor(0.3312)\n",
      "21270 Training Loss: tensor(0.3314)\n",
      "21271 Training Loss: tensor(0.3301)\n",
      "21272 Training Loss: tensor(0.3308)\n",
      "21273 Training Loss: tensor(0.3295)\n",
      "21274 Training Loss: tensor(0.3313)\n",
      "21275 Training Loss: tensor(0.3310)\n",
      "21276 Training Loss: tensor(0.3311)\n",
      "21277 Training Loss: tensor(0.3354)\n",
      "21278 Training Loss: tensor(0.3319)\n",
      "21279 Training Loss: tensor(0.3316)\n",
      "21280 Training Loss: tensor(0.3303)\n",
      "21281 Training Loss: tensor(0.3307)\n",
      "21282 Training Loss: tensor(0.3304)\n",
      "21283 Training Loss: tensor(0.3317)\n",
      "21284 Training Loss: tensor(0.3315)\n",
      "21285 Training Loss: tensor(0.3312)\n",
      "21286 Training Loss: tensor(0.3299)\n",
      "21287 Training Loss: tensor(0.3304)\n",
      "21288 Training Loss: tensor(0.3332)\n",
      "21289 Training Loss: tensor(0.3304)\n",
      "21290 Training Loss: tensor(0.3305)\n",
      "21291 Training Loss: tensor(0.3312)\n",
      "21292 Training Loss: tensor(0.3316)\n",
      "21293 Training Loss: tensor(0.3307)\n",
      "21294 Training Loss: tensor(0.3311)\n",
      "21295 Training Loss: tensor(0.3311)\n",
      "21296 Training Loss: tensor(0.3369)\n",
      "21297 Training Loss: tensor(0.3310)\n",
      "21298 Training Loss: tensor(0.3307)\n",
      "21299 Training Loss: tensor(0.3311)\n",
      "21300 Training Loss: tensor(0.3308)\n",
      "21301 Training Loss: tensor(0.3313)\n",
      "21302 Training Loss: tensor(0.3311)\n",
      "21303 Training Loss: tensor(0.3296)\n",
      "21304 Training Loss: tensor(0.3303)\n",
      "21305 Training Loss: tensor(0.3312)\n",
      "21306 Training Loss: tensor(0.3322)\n",
      "21307 Training Loss: tensor(0.3331)\n",
      "21308 Training Loss: tensor(0.3295)\n",
      "21309 Training Loss: tensor(0.3323)\n",
      "21310 Training Loss: tensor(0.3316)\n",
      "21311 Training Loss: tensor(0.3319)\n",
      "21312 Training Loss: tensor(0.3303)\n",
      "21313 Training Loss: tensor(0.3300)\n",
      "21314 Training Loss: tensor(0.3300)\n",
      "21315 Training Loss: tensor(0.3303)\n",
      "21316 Training Loss: tensor(0.3315)\n",
      "21317 Training Loss: tensor(0.3312)\n",
      "21318 Training Loss: tensor(0.3292)\n",
      "21319 Training Loss: tensor(0.3304)\n",
      "21320 Training Loss: tensor(0.3305)\n",
      "21321 Training Loss: tensor(0.3300)\n",
      "21322 Training Loss: tensor(0.3292)\n",
      "21323 Training Loss: tensor(0.3325)\n",
      "21324 Training Loss: tensor(0.3330)\n",
      "21325 Training Loss: tensor(0.3320)\n",
      "21326 Training Loss: tensor(0.3312)\n",
      "21327 Training Loss: tensor(0.3317)\n",
      "21328 Training Loss: tensor(0.3313)\n",
      "21329 Training Loss: tensor(0.3305)\n",
      "21330 Training Loss: tensor(0.3307)\n",
      "21331 Training Loss: tensor(0.3307)\n",
      "21332 Training Loss: tensor(0.3313)\n",
      "21333 Training Loss: tensor(0.3316)\n",
      "21334 Training Loss: tensor(0.3300)\n",
      "21335 Training Loss: tensor(0.3298)\n",
      "21336 Training Loss: tensor(0.3301)\n",
      "21337 Training Loss: tensor(0.3357)\n",
      "21338 Training Loss: tensor(0.3306)\n",
      "21339 Training Loss: tensor(0.3349)\n",
      "21340 Training Loss: tensor(0.3319)\n",
      "21341 Training Loss: tensor(0.3293)\n",
      "21342 Training Loss: tensor(0.3314)\n",
      "21343 Training Loss: tensor(0.3303)\n",
      "21344 Training Loss: tensor(0.3315)\n",
      "21345 Training Loss: tensor(0.3319)\n",
      "21346 Training Loss: tensor(0.3346)\n",
      "21347 Training Loss: tensor(0.3309)\n",
      "21348 Training Loss: tensor(0.3315)\n",
      "21349 Training Loss: tensor(0.3320)\n",
      "21350 Training Loss: tensor(0.3317)\n",
      "21351 Training Loss: tensor(0.3319)\n",
      "21352 Training Loss: tensor(0.3319)\n",
      "21353 Training Loss: tensor(0.3315)\n",
      "21354 Training Loss: tensor(0.3331)\n",
      "21355 Training Loss: tensor(0.3306)\n",
      "21356 Training Loss: tensor(0.3316)\n",
      "21357 Training Loss: tensor(0.3310)\n",
      "21358 Training Loss: tensor(0.3310)\n",
      "21359 Training Loss: tensor(0.3317)\n",
      "21360 Training Loss: tensor(0.3302)\n",
      "21361 Training Loss: tensor(0.3314)\n",
      "21362 Training Loss: tensor(0.3294)\n",
      "21363 Training Loss: tensor(0.3323)\n",
      "21364 Training Loss: tensor(0.3304)\n",
      "21365 Training Loss: tensor(0.3303)\n",
      "21366 Training Loss: tensor(0.3334)\n",
      "21367 Training Loss: tensor(0.3313)\n",
      "21368 Training Loss: tensor(0.3313)\n",
      "21369 Training Loss: tensor(0.3357)\n",
      "21370 Training Loss: tensor(0.3303)\n",
      "21371 Training Loss: tensor(0.3310)\n",
      "21372 Training Loss: tensor(0.3296)\n",
      "21373 Training Loss: tensor(0.3313)\n",
      "21374 Training Loss: tensor(0.3322)\n",
      "21375 Training Loss: tensor(0.3307)\n",
      "21376 Training Loss: tensor(0.3330)\n",
      "21377 Training Loss: tensor(0.3308)\n",
      "21378 Training Loss: tensor(0.3319)\n",
      "21379 Training Loss: tensor(0.3301)\n",
      "21380 Training Loss: tensor(0.3310)\n",
      "21381 Training Loss: tensor(0.3304)\n",
      "21382 Training Loss: tensor(0.3311)\n",
      "21383 Training Loss: tensor(0.3304)\n",
      "21384 Training Loss: tensor(0.3301)\n",
      "21385 Training Loss: tensor(0.3305)\n",
      "21386 Training Loss: tensor(0.3313)\n",
      "21387 Training Loss: tensor(0.3301)\n",
      "21388 Training Loss: tensor(0.3308)\n",
      "21389 Training Loss: tensor(0.3302)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21390 Training Loss: tensor(0.3298)\n",
      "21391 Training Loss: tensor(0.3333)\n",
      "21392 Training Loss: tensor(0.3327)\n",
      "21393 Training Loss: tensor(0.3312)\n",
      "21394 Training Loss: tensor(0.3331)\n",
      "21395 Training Loss: tensor(0.3299)\n",
      "21396 Training Loss: tensor(0.3307)\n",
      "21397 Training Loss: tensor(0.3301)\n",
      "21398 Training Loss: tensor(0.3290)\n",
      "21399 Training Loss: tensor(0.3297)\n",
      "21400 Training Loss: tensor(0.3306)\n",
      "21401 Training Loss: tensor(0.3297)\n",
      "21402 Training Loss: tensor(0.3302)\n",
      "21403 Training Loss: tensor(0.3291)\n",
      "21404 Training Loss: tensor(0.3321)\n",
      "21405 Training Loss: tensor(0.3308)\n",
      "21406 Training Loss: tensor(0.3302)\n",
      "21407 Training Loss: tensor(0.3311)\n",
      "21408 Training Loss: tensor(0.3325)\n",
      "21409 Training Loss: tensor(0.3335)\n",
      "21410 Training Loss: tensor(0.3302)\n",
      "21411 Training Loss: tensor(0.3292)\n",
      "21412 Training Loss: tensor(0.3310)\n",
      "21413 Training Loss: tensor(0.3344)\n",
      "21414 Training Loss: tensor(0.3303)\n",
      "21415 Training Loss: tensor(0.3303)\n",
      "21416 Training Loss: tensor(0.3301)\n",
      "21417 Training Loss: tensor(0.3315)\n",
      "21418 Training Loss: tensor(0.3297)\n",
      "21419 Training Loss: tensor(0.3296)\n",
      "21420 Training Loss: tensor(0.3314)\n",
      "21421 Training Loss: tensor(0.3302)\n",
      "21422 Training Loss: tensor(0.3313)\n",
      "21423 Training Loss: tensor(0.3346)\n",
      "21424 Training Loss: tensor(0.3309)\n",
      "21425 Training Loss: tensor(0.3327)\n",
      "21426 Training Loss: tensor(0.3349)\n",
      "21427 Training Loss: tensor(0.3303)\n",
      "21428 Training Loss: tensor(0.3319)\n",
      "21429 Training Loss: tensor(0.3299)\n",
      "21430 Training Loss: tensor(0.3305)\n",
      "21431 Training Loss: tensor(0.3329)\n",
      "21432 Training Loss: tensor(0.3291)\n",
      "21433 Training Loss: tensor(0.3302)\n",
      "21434 Training Loss: tensor(0.3351)\n",
      "21435 Training Loss: tensor(0.3371)\n",
      "21436 Training Loss: tensor(0.3298)\n",
      "21437 Training Loss: tensor(0.3300)\n",
      "21438 Training Loss: tensor(0.3316)\n",
      "21439 Training Loss: tensor(0.3324)\n",
      "21440 Training Loss: tensor(0.3310)\n",
      "21441 Training Loss: tensor(0.3317)\n",
      "21442 Training Loss: tensor(0.3308)\n",
      "21443 Training Loss: tensor(0.3308)\n",
      "21444 Training Loss: tensor(0.3305)\n",
      "21445 Training Loss: tensor(0.3304)\n",
      "21446 Training Loss: tensor(0.3307)\n",
      "21447 Training Loss: tensor(0.3309)\n",
      "21448 Training Loss: tensor(0.3290)\n",
      "21449 Training Loss: tensor(0.3307)\n",
      "21450 Training Loss: tensor(0.3353)\n",
      "21451 Training Loss: tensor(0.3312)\n",
      "21452 Training Loss: tensor(0.3304)\n",
      "21453 Training Loss: tensor(0.3309)\n",
      "21454 Training Loss: tensor(0.3314)\n",
      "21455 Training Loss: tensor(0.3319)\n",
      "21456 Training Loss: tensor(0.3303)\n",
      "21457 Training Loss: tensor(0.3303)\n",
      "21458 Training Loss: tensor(0.3307)\n",
      "21459 Training Loss: tensor(0.3304)\n",
      "21460 Training Loss: tensor(0.3301)\n",
      "21461 Training Loss: tensor(0.3305)\n",
      "21462 Training Loss: tensor(0.3339)\n",
      "21463 Training Loss: tensor(0.3321)\n",
      "21464 Training Loss: tensor(0.3301)\n",
      "21465 Training Loss: tensor(0.3317)\n",
      "21466 Training Loss: tensor(0.3320)\n",
      "21467 Training Loss: tensor(0.3308)\n",
      "21468 Training Loss: tensor(0.3304)\n",
      "21469 Training Loss: tensor(0.3304)\n",
      "21470 Training Loss: tensor(0.3311)\n",
      "21471 Training Loss: tensor(0.3299)\n",
      "21472 Training Loss: tensor(0.3311)\n",
      "21473 Training Loss: tensor(0.3299)\n",
      "21474 Training Loss: tensor(0.3313)\n",
      "21475 Training Loss: tensor(0.3310)\n",
      "21476 Training Loss: tensor(0.3300)\n",
      "21477 Training Loss: tensor(0.3334)\n",
      "21478 Training Loss: tensor(0.3301)\n",
      "21479 Training Loss: tensor(0.3314)\n",
      "21480 Training Loss: tensor(0.3334)\n",
      "21481 Training Loss: tensor(0.3307)\n",
      "21482 Training Loss: tensor(0.3310)\n",
      "21483 Training Loss: tensor(0.3309)\n",
      "21484 Training Loss: tensor(0.3294)\n",
      "21485 Training Loss: tensor(0.3299)\n",
      "21486 Training Loss: tensor(0.3306)\n",
      "21487 Training Loss: tensor(0.3302)\n",
      "21488 Training Loss: tensor(0.3306)\n",
      "21489 Training Loss: tensor(0.3332)\n",
      "21490 Training Loss: tensor(0.3350)\n",
      "21491 Training Loss: tensor(0.3309)\n",
      "21492 Training Loss: tensor(0.3307)\n",
      "21493 Training Loss: tensor(0.3310)\n",
      "21494 Training Loss: tensor(0.3350)\n",
      "21495 Training Loss: tensor(0.3319)\n",
      "21496 Training Loss: tensor(0.3305)\n",
      "21497 Training Loss: tensor(0.3314)\n",
      "21498 Training Loss: tensor(0.3325)\n",
      "21499 Training Loss: tensor(0.3318)\n",
      "21500 Training Loss: tensor(0.3306)\n",
      "21501 Training Loss: tensor(0.3311)\n",
      "21502 Training Loss: tensor(0.3321)\n",
      "21503 Training Loss: tensor(0.3301)\n",
      "21504 Training Loss: tensor(0.3318)\n",
      "21505 Training Loss: tensor(0.3309)\n",
      "21506 Training Loss: tensor(0.3302)\n",
      "21507 Training Loss: tensor(0.3320)\n",
      "21508 Training Loss: tensor(0.3316)\n",
      "21509 Training Loss: tensor(0.3313)\n",
      "21510 Training Loss: tensor(0.3300)\n",
      "21511 Training Loss: tensor(0.3307)\n",
      "21512 Training Loss: tensor(0.3300)\n",
      "21513 Training Loss: tensor(0.3303)\n",
      "21514 Training Loss: tensor(0.3308)\n",
      "21515 Training Loss: tensor(0.3311)\n",
      "21516 Training Loss: tensor(0.3289)\n",
      "21517 Training Loss: tensor(0.3332)\n",
      "21518 Training Loss: tensor(0.3332)\n",
      "21519 Training Loss: tensor(0.3307)\n",
      "21520 Training Loss: tensor(0.3310)\n",
      "21521 Training Loss: tensor(0.3299)\n",
      "21522 Training Loss: tensor(0.3302)\n",
      "21523 Training Loss: tensor(0.3299)\n",
      "21524 Training Loss: tensor(0.3295)\n",
      "21525 Training Loss: tensor(0.3302)\n",
      "21526 Training Loss: tensor(0.3322)\n",
      "21527 Training Loss: tensor(0.3300)\n",
      "21528 Training Loss: tensor(0.3328)\n",
      "21529 Training Loss: tensor(0.3292)\n",
      "21530 Training Loss: tensor(0.3296)\n",
      "21531 Training Loss: tensor(0.3295)\n",
      "21532 Training Loss: tensor(0.3306)\n",
      "21533 Training Loss: tensor(0.3317)\n",
      "21534 Training Loss: tensor(0.3308)\n",
      "21535 Training Loss: tensor(0.3293)\n",
      "21536 Training Loss: tensor(0.3308)\n",
      "21537 Training Loss: tensor(0.3303)\n",
      "21538 Training Loss: tensor(0.3303)\n",
      "21539 Training Loss: tensor(0.3330)\n",
      "21540 Training Loss: tensor(0.3297)\n",
      "21541 Training Loss: tensor(0.3297)\n",
      "21542 Training Loss: tensor(0.3292)\n",
      "21543 Training Loss: tensor(0.3304)\n",
      "21544 Training Loss: tensor(0.3341)\n",
      "21545 Training Loss: tensor(0.3313)\n",
      "21546 Training Loss: tensor(0.3305)\n",
      "21547 Training Loss: tensor(0.3296)\n",
      "21548 Training Loss: tensor(0.3302)\n",
      "21549 Training Loss: tensor(0.3284)\n",
      "21550 Training Loss: tensor(0.3297)\n",
      "21551 Training Loss: tensor(0.3299)\n",
      "21552 Training Loss: tensor(0.3295)\n",
      "21553 Training Loss: tensor(0.3367)\n",
      "21554 Training Loss: tensor(0.3289)\n",
      "21555 Training Loss: tensor(0.3306)\n",
      "21556 Training Loss: tensor(0.3328)\n",
      "21557 Training Loss: tensor(0.3306)\n",
      "21558 Training Loss: tensor(0.3305)\n",
      "21559 Training Loss: tensor(0.3307)\n",
      "21560 Training Loss: tensor(0.3290)\n",
      "21561 Training Loss: tensor(0.3324)\n",
      "21562 Training Loss: tensor(0.3316)\n",
      "21563 Training Loss: tensor(0.3295)\n",
      "21564 Training Loss: tensor(0.3299)\n",
      "21565 Training Loss: tensor(0.3292)\n",
      "21566 Training Loss: tensor(0.3311)\n",
      "21567 Training Loss: tensor(0.3292)\n",
      "21568 Training Loss: tensor(0.3299)\n",
      "21569 Training Loss: tensor(0.3296)\n",
      "21570 Training Loss: tensor(0.3292)\n",
      "21571 Training Loss: tensor(0.3297)\n",
      "21572 Training Loss: tensor(0.3299)\n",
      "21573 Training Loss: tensor(0.3303)\n",
      "21574 Training Loss: tensor(0.3300)\n",
      "21575 Training Loss: tensor(0.3328)\n",
      "21576 Training Loss: tensor(0.3305)\n",
      "21577 Training Loss: tensor(0.3293)\n",
      "21578 Training Loss: tensor(0.3287)\n",
      "21579 Training Loss: tensor(0.3299)\n",
      "21580 Training Loss: tensor(0.3299)\n",
      "21581 Training Loss: tensor(0.3299)\n",
      "21582 Training Loss: tensor(0.3341)\n",
      "21583 Training Loss: tensor(0.3293)\n",
      "21584 Training Loss: tensor(0.3306)\n",
      "21585 Training Loss: tensor(0.3349)\n",
      "21586 Training Loss: tensor(0.3319)\n",
      "21587 Training Loss: tensor(0.3298)\n",
      "21588 Training Loss: tensor(0.3309)\n",
      "21589 Training Loss: tensor(0.3310)\n",
      "21590 Training Loss: tensor(0.3302)\n",
      "21591 Training Loss: tensor(0.3296)\n",
      "21592 Training Loss: tensor(0.3302)\n",
      "21593 Training Loss: tensor(0.3292)\n",
      "21594 Training Loss: tensor(0.3308)\n",
      "21595 Training Loss: tensor(0.3330)\n",
      "21596 Training Loss: tensor(0.3317)\n",
      "21597 Training Loss: tensor(0.3310)\n",
      "21598 Training Loss: tensor(0.3307)\n",
      "21599 Training Loss: tensor(0.3304)\n",
      "21600 Training Loss: tensor(0.3301)\n",
      "21601 Training Loss: tensor(0.3298)\n",
      "21602 Training Loss: tensor(0.3330)\n",
      "21603 Training Loss: tensor(0.3312)\n",
      "21604 Training Loss: tensor(0.3300)\n",
      "21605 Training Loss: tensor(0.3313)\n",
      "21606 Training Loss: tensor(0.3300)\n",
      "21607 Training Loss: tensor(0.3292)\n",
      "21608 Training Loss: tensor(0.3302)\n",
      "21609 Training Loss: tensor(0.3297)\n",
      "21610 Training Loss: tensor(0.3313)\n",
      "21611 Training Loss: tensor(0.3290)\n",
      "21612 Training Loss: tensor(0.3298)\n",
      "21613 Training Loss: tensor(0.3298)\n",
      "21614 Training Loss: tensor(0.3365)\n",
      "21615 Training Loss: tensor(0.3357)\n",
      "21616 Training Loss: tensor(0.3297)\n",
      "21617 Training Loss: tensor(0.3309)\n",
      "21618 Training Loss: tensor(0.3295)\n",
      "21619 Training Loss: tensor(0.3298)\n",
      "21620 Training Loss: tensor(0.3314)\n",
      "21621 Training Loss: tensor(0.3302)\n",
      "21622 Training Loss: tensor(0.3297)\n",
      "21623 Training Loss: tensor(0.3319)\n",
      "21624 Training Loss: tensor(0.3316)\n",
      "21625 Training Loss: tensor(0.3311)\n",
      "21626 Training Loss: tensor(0.3311)\n",
      "21627 Training Loss: tensor(0.3340)\n",
      "21628 Training Loss: tensor(0.3299)\n",
      "21629 Training Loss: tensor(0.3304)\n",
      "21630 Training Loss: tensor(0.3308)\n",
      "21631 Training Loss: tensor(0.3316)\n",
      "21632 Training Loss: tensor(0.3314)\n",
      "21633 Training Loss: tensor(0.3312)\n",
      "21634 Training Loss: tensor(0.3340)\n",
      "21635 Training Loss: tensor(0.3304)\n",
      "21636 Training Loss: tensor(0.3317)\n",
      "21637 Training Loss: tensor(0.3302)\n",
      "21638 Training Loss: tensor(0.3322)\n",
      "21639 Training Loss: tensor(0.3308)\n",
      "21640 Training Loss: tensor(0.3301)\n",
      "21641 Training Loss: tensor(0.3296)\n",
      "21642 Training Loss: tensor(0.3304)\n",
      "21643 Training Loss: tensor(0.3316)\n",
      "21644 Training Loss: tensor(0.3304)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21645 Training Loss: tensor(0.3304)\n",
      "21646 Training Loss: tensor(0.3311)\n",
      "21647 Training Loss: tensor(0.3319)\n",
      "21648 Training Loss: tensor(0.3298)\n",
      "21649 Training Loss: tensor(0.3316)\n",
      "21650 Training Loss: tensor(0.3302)\n",
      "21651 Training Loss: tensor(0.3327)\n",
      "21652 Training Loss: tensor(0.3299)\n",
      "21653 Training Loss: tensor(0.3304)\n",
      "21654 Training Loss: tensor(0.3318)\n",
      "21655 Training Loss: tensor(0.3302)\n",
      "21656 Training Loss: tensor(0.3301)\n",
      "21657 Training Loss: tensor(0.3297)\n",
      "21658 Training Loss: tensor(0.3305)\n",
      "21659 Training Loss: tensor(0.3301)\n",
      "21660 Training Loss: tensor(0.3314)\n",
      "21661 Training Loss: tensor(0.3341)\n",
      "21662 Training Loss: tensor(0.3302)\n",
      "21663 Training Loss: tensor(0.3309)\n",
      "21664 Training Loss: tensor(0.3303)\n",
      "21665 Training Loss: tensor(0.3315)\n",
      "21666 Training Loss: tensor(0.3302)\n",
      "21667 Training Loss: tensor(0.3306)\n",
      "21668 Training Loss: tensor(0.3308)\n",
      "21669 Training Loss: tensor(0.3298)\n",
      "21670 Training Loss: tensor(0.3315)\n",
      "21671 Training Loss: tensor(0.3305)\n",
      "21672 Training Loss: tensor(0.3342)\n",
      "21673 Training Loss: tensor(0.3295)\n",
      "21674 Training Loss: tensor(0.3306)\n",
      "21675 Training Loss: tensor(0.3304)\n",
      "21676 Training Loss: tensor(0.3300)\n",
      "21677 Training Loss: tensor(0.3302)\n",
      "21678 Training Loss: tensor(0.3298)\n",
      "21679 Training Loss: tensor(0.3304)\n",
      "21680 Training Loss: tensor(0.3299)\n",
      "21681 Training Loss: tensor(0.3309)\n",
      "21682 Training Loss: tensor(0.3303)\n",
      "21683 Training Loss: tensor(0.3291)\n",
      "21684 Training Loss: tensor(0.3291)\n",
      "21685 Training Loss: tensor(0.3301)\n",
      "21686 Training Loss: tensor(0.3342)\n",
      "21687 Training Loss: tensor(0.3294)\n",
      "21688 Training Loss: tensor(0.3298)\n",
      "21689 Training Loss: tensor(0.3325)\n",
      "21690 Training Loss: tensor(0.3341)\n",
      "21691 Training Loss: tensor(0.3301)\n",
      "21692 Training Loss: tensor(0.3304)\n",
      "21693 Training Loss: tensor(0.3302)\n",
      "21694 Training Loss: tensor(0.3309)\n",
      "21695 Training Loss: tensor(0.3303)\n",
      "21696 Training Loss: tensor(0.3294)\n",
      "21697 Training Loss: tensor(0.3312)\n",
      "21698 Training Loss: tensor(0.3308)\n",
      "21699 Training Loss: tensor(0.3318)\n",
      "21700 Training Loss: tensor(0.3327)\n",
      "21701 Training Loss: tensor(0.3299)\n",
      "21702 Training Loss: tensor(0.3308)\n",
      "21703 Training Loss: tensor(0.3327)\n",
      "21704 Training Loss: tensor(0.3306)\n",
      "21705 Training Loss: tensor(0.3335)\n",
      "21706 Training Loss: tensor(0.3310)\n",
      "21707 Training Loss: tensor(0.3332)\n",
      "21708 Training Loss: tensor(0.3302)\n",
      "21709 Training Loss: tensor(0.3329)\n",
      "21710 Training Loss: tensor(0.3309)\n",
      "21711 Training Loss: tensor(0.3300)\n",
      "21712 Training Loss: tensor(0.3299)\n",
      "21713 Training Loss: tensor(0.3297)\n",
      "21714 Training Loss: tensor(0.3311)\n",
      "21715 Training Loss: tensor(0.3295)\n",
      "21716 Training Loss: tensor(0.3315)\n",
      "21717 Training Loss: tensor(0.3296)\n",
      "21718 Training Loss: tensor(0.3338)\n",
      "21719 Training Loss: tensor(0.3301)\n",
      "21720 Training Loss: tensor(0.3323)\n",
      "21721 Training Loss: tensor(0.3293)\n",
      "21722 Training Loss: tensor(0.3292)\n",
      "21723 Training Loss: tensor(0.3300)\n",
      "21724 Training Loss: tensor(0.3297)\n",
      "21725 Training Loss: tensor(0.3294)\n",
      "21726 Training Loss: tensor(0.3324)\n",
      "21727 Training Loss: tensor(0.3304)\n",
      "21728 Training Loss: tensor(0.3293)\n",
      "21729 Training Loss: tensor(0.3292)\n",
      "21730 Training Loss: tensor(0.3328)\n",
      "21731 Training Loss: tensor(0.3311)\n",
      "21732 Training Loss: tensor(0.3300)\n",
      "21733 Training Loss: tensor(0.3297)\n",
      "21734 Training Loss: tensor(0.3319)\n",
      "21735 Training Loss: tensor(0.3299)\n",
      "21736 Training Loss: tensor(0.3293)\n",
      "21737 Training Loss: tensor(0.3321)\n",
      "21738 Training Loss: tensor(0.3327)\n",
      "21739 Training Loss: tensor(0.3298)\n",
      "21740 Training Loss: tensor(0.3300)\n",
      "21741 Training Loss: tensor(0.3303)\n",
      "21742 Training Loss: tensor(0.3315)\n",
      "21743 Training Loss: tensor(0.3305)\n",
      "21744 Training Loss: tensor(0.3306)\n",
      "21745 Training Loss: tensor(0.3289)\n",
      "21746 Training Loss: tensor(0.3294)\n",
      "21747 Training Loss: tensor(0.3296)\n",
      "21748 Training Loss: tensor(0.3321)\n",
      "21749 Training Loss: tensor(0.3289)\n",
      "21750 Training Loss: tensor(0.3306)\n",
      "21751 Training Loss: tensor(0.3300)\n",
      "21752 Training Loss: tensor(0.3290)\n",
      "21753 Training Loss: tensor(0.3302)\n",
      "21754 Training Loss: tensor(0.3298)\n",
      "21755 Training Loss: tensor(0.3289)\n",
      "21756 Training Loss: tensor(0.3290)\n",
      "21757 Training Loss: tensor(0.3285)\n",
      "21758 Training Loss: tensor(0.3298)\n",
      "21759 Training Loss: tensor(0.3296)\n",
      "21760 Training Loss: tensor(0.3374)\n",
      "21761 Training Loss: tensor(0.3319)\n",
      "21762 Training Loss: tensor(0.3286)\n",
      "21763 Training Loss: tensor(0.3289)\n",
      "21764 Training Loss: tensor(0.3302)\n",
      "21765 Training Loss: tensor(0.3305)\n",
      "21766 Training Loss: tensor(0.3302)\n",
      "21767 Training Loss: tensor(0.3301)\n",
      "21768 Training Loss: tensor(0.3297)\n",
      "21769 Training Loss: tensor(0.3323)\n",
      "21770 Training Loss: tensor(0.3309)\n",
      "21771 Training Loss: tensor(0.3323)\n",
      "21772 Training Loss: tensor(0.3341)\n",
      "21773 Training Loss: tensor(0.3295)\n",
      "21774 Training Loss: tensor(0.3293)\n",
      "21775 Training Loss: tensor(0.3287)\n",
      "21776 Training Loss: tensor(0.3310)\n",
      "21777 Training Loss: tensor(0.3295)\n",
      "21778 Training Loss: tensor(0.3297)\n",
      "21779 Training Loss: tensor(0.3317)\n",
      "21780 Training Loss: tensor(0.3310)\n",
      "21781 Training Loss: tensor(0.3301)\n",
      "21782 Training Loss: tensor(0.3302)\n",
      "21783 Training Loss: tensor(0.3295)\n",
      "21784 Training Loss: tensor(0.3284)\n",
      "21785 Training Loss: tensor(0.3297)\n",
      "21786 Training Loss: tensor(0.3333)\n",
      "21787 Training Loss: tensor(0.3290)\n",
      "21788 Training Loss: tensor(0.3301)\n",
      "21789 Training Loss: tensor(0.3299)\n",
      "21790 Training Loss: tensor(0.3306)\n",
      "21791 Training Loss: tensor(0.3324)\n",
      "21792 Training Loss: tensor(0.3293)\n",
      "21793 Training Loss: tensor(0.3314)\n",
      "21794 Training Loss: tensor(0.3311)\n",
      "21795 Training Loss: tensor(0.3311)\n",
      "21796 Training Loss: tensor(0.3293)\n",
      "21797 Training Loss: tensor(0.3298)\n",
      "21798 Training Loss: tensor(0.3305)\n",
      "21799 Training Loss: tensor(0.3307)\n",
      "21800 Training Loss: tensor(0.3297)\n",
      "21801 Training Loss: tensor(0.3299)\n",
      "21802 Training Loss: tensor(0.3301)\n",
      "21803 Training Loss: tensor(0.3292)\n",
      "21804 Training Loss: tensor(0.3298)\n",
      "21805 Training Loss: tensor(0.3316)\n",
      "21806 Training Loss: tensor(0.3292)\n",
      "21807 Training Loss: tensor(0.3313)\n",
      "21808 Training Loss: tensor(0.3296)\n",
      "21809 Training Loss: tensor(0.3299)\n",
      "21810 Training Loss: tensor(0.3311)\n",
      "21811 Training Loss: tensor(0.3289)\n",
      "21812 Training Loss: tensor(0.3289)\n",
      "21813 Training Loss: tensor(0.3293)\n",
      "21814 Training Loss: tensor(0.3302)\n",
      "21815 Training Loss: tensor(0.3310)\n",
      "21816 Training Loss: tensor(0.3306)\n",
      "21817 Training Loss: tensor(0.3307)\n",
      "21818 Training Loss: tensor(0.3300)\n",
      "21819 Training Loss: tensor(0.3299)\n",
      "21820 Training Loss: tensor(0.3304)\n",
      "21821 Training Loss: tensor(0.3297)\n",
      "21822 Training Loss: tensor(0.3349)\n",
      "21823 Training Loss: tensor(0.3288)\n",
      "21824 Training Loss: tensor(0.3289)\n",
      "21825 Training Loss: tensor(0.3296)\n",
      "21826 Training Loss: tensor(0.3293)\n",
      "21827 Training Loss: tensor(0.3298)\n",
      "21828 Training Loss: tensor(0.3291)\n",
      "21829 Training Loss: tensor(0.3304)\n",
      "21830 Training Loss: tensor(0.3322)\n",
      "21831 Training Loss: tensor(0.3317)\n",
      "21832 Training Loss: tensor(0.3299)\n",
      "21833 Training Loss: tensor(0.3297)\n",
      "21834 Training Loss: tensor(0.3322)\n",
      "21835 Training Loss: tensor(0.3296)\n",
      "21836 Training Loss: tensor(0.3338)\n",
      "21837 Training Loss: tensor(0.3305)\n",
      "21838 Training Loss: tensor(0.3309)\n",
      "21839 Training Loss: tensor(0.3300)\n",
      "21840 Training Loss: tensor(0.3305)\n",
      "21841 Training Loss: tensor(0.3311)\n",
      "21842 Training Loss: tensor(0.3304)\n",
      "21843 Training Loss: tensor(0.3320)\n",
      "21844 Training Loss: tensor(0.3290)\n",
      "21845 Training Loss: tensor(0.3337)\n",
      "21846 Training Loss: tensor(0.3310)\n",
      "21847 Training Loss: tensor(0.3302)\n",
      "21848 Training Loss: tensor(0.3293)\n",
      "21849 Training Loss: tensor(0.3287)\n",
      "21850 Training Loss: tensor(0.3300)\n",
      "21851 Training Loss: tensor(0.3294)\n",
      "21852 Training Loss: tensor(0.3301)\n",
      "21853 Training Loss: tensor(0.3306)\n",
      "21854 Training Loss: tensor(0.3306)\n",
      "21855 Training Loss: tensor(0.3299)\n",
      "21856 Training Loss: tensor(0.3305)\n",
      "21857 Training Loss: tensor(0.3296)\n",
      "21858 Training Loss: tensor(0.3306)\n",
      "21859 Training Loss: tensor(0.3304)\n",
      "21860 Training Loss: tensor(0.3296)\n",
      "21861 Training Loss: tensor(0.3299)\n",
      "21862 Training Loss: tensor(0.3302)\n",
      "21863 Training Loss: tensor(0.3291)\n",
      "21864 Training Loss: tensor(0.3280)\n",
      "21865 Training Loss: tensor(0.3285)\n",
      "21866 Training Loss: tensor(0.3294)\n",
      "21867 Training Loss: tensor(0.3322)\n",
      "21868 Training Loss: tensor(0.3295)\n",
      "21869 Training Loss: tensor(0.3289)\n",
      "21870 Training Loss: tensor(0.3287)\n",
      "21871 Training Loss: tensor(0.3310)\n",
      "21872 Training Loss: tensor(0.3285)\n",
      "21873 Training Loss: tensor(0.3304)\n",
      "21874 Training Loss: tensor(0.3292)\n",
      "21875 Training Loss: tensor(0.3297)\n",
      "21876 Training Loss: tensor(0.3333)\n",
      "21877 Training Loss: tensor(0.3331)\n",
      "21878 Training Loss: tensor(0.3295)\n",
      "21879 Training Loss: tensor(0.3294)\n",
      "21880 Training Loss: tensor(0.3295)\n",
      "21881 Training Loss: tensor(0.3313)\n",
      "21882 Training Loss: tensor(0.3335)\n",
      "21883 Training Loss: tensor(0.3294)\n",
      "21884 Training Loss: tensor(0.3297)\n",
      "21885 Training Loss: tensor(0.3298)\n",
      "21886 Training Loss: tensor(0.3296)\n",
      "21887 Training Loss: tensor(0.3290)\n",
      "21888 Training Loss: tensor(0.3293)\n",
      "21889 Training Loss: tensor(0.3301)\n",
      "21890 Training Loss: tensor(0.3290)\n",
      "21891 Training Loss: tensor(0.3303)\n",
      "21892 Training Loss: tensor(0.3320)\n",
      "21893 Training Loss: tensor(0.3316)\n",
      "21894 Training Loss: tensor(0.3290)\n",
      "21895 Training Loss: tensor(0.3304)\n",
      "21896 Training Loss: tensor(0.3286)\n",
      "21897 Training Loss: tensor(0.3299)\n",
      "21898 Training Loss: tensor(0.3293)\n",
      "21899 Training Loss: tensor(0.3327)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21900 Training Loss: tensor(0.3295)\n",
      "21901 Training Loss: tensor(0.3310)\n",
      "21902 Training Loss: tensor(0.3336)\n",
      "21903 Training Loss: tensor(0.3298)\n",
      "21904 Training Loss: tensor(0.3299)\n",
      "21905 Training Loss: tensor(0.3299)\n",
      "21906 Training Loss: tensor(0.3291)\n",
      "21907 Training Loss: tensor(0.3298)\n",
      "21908 Training Loss: tensor(0.3290)\n",
      "21909 Training Loss: tensor(0.3301)\n",
      "21910 Training Loss: tensor(0.3295)\n",
      "21911 Training Loss: tensor(0.3297)\n",
      "21912 Training Loss: tensor(0.3317)\n",
      "21913 Training Loss: tensor(0.3293)\n",
      "21914 Training Loss: tensor(0.3313)\n",
      "21915 Training Loss: tensor(0.3295)\n",
      "21916 Training Loss: tensor(0.3298)\n",
      "21917 Training Loss: tensor(0.3291)\n",
      "21918 Training Loss: tensor(0.3302)\n",
      "21919 Training Loss: tensor(0.3293)\n",
      "21920 Training Loss: tensor(0.3311)\n",
      "21921 Training Loss: tensor(0.3309)\n",
      "21922 Training Loss: tensor(0.3295)\n",
      "21923 Training Loss: tensor(0.3303)\n",
      "21924 Training Loss: tensor(0.3306)\n",
      "21925 Training Loss: tensor(0.3292)\n",
      "21926 Training Loss: tensor(0.3338)\n",
      "21927 Training Loss: tensor(0.3319)\n",
      "21928 Training Loss: tensor(0.3294)\n",
      "21929 Training Loss: tensor(0.3316)\n",
      "21930 Training Loss: tensor(0.3315)\n",
      "21931 Training Loss: tensor(0.3303)\n",
      "21932 Training Loss: tensor(0.3315)\n",
      "21933 Training Loss: tensor(0.3293)\n",
      "21934 Training Loss: tensor(0.3298)\n",
      "21935 Training Loss: tensor(0.3334)\n",
      "21936 Training Loss: tensor(0.3308)\n",
      "21937 Training Loss: tensor(0.3322)\n",
      "21938 Training Loss: tensor(0.3307)\n",
      "21939 Training Loss: tensor(0.3309)\n",
      "21940 Training Loss: tensor(0.3292)\n",
      "21941 Training Loss: tensor(0.3315)\n",
      "21942 Training Loss: tensor(0.3305)\n",
      "21943 Training Loss: tensor(0.3298)\n",
      "21944 Training Loss: tensor(0.3333)\n",
      "21945 Training Loss: tensor(0.3310)\n",
      "21946 Training Loss: tensor(0.3288)\n",
      "21947 Training Loss: tensor(0.3298)\n",
      "21948 Training Loss: tensor(0.3298)\n",
      "21949 Training Loss: tensor(0.3331)\n",
      "21950 Training Loss: tensor(0.3309)\n",
      "21951 Training Loss: tensor(0.3301)\n",
      "21952 Training Loss: tensor(0.3337)\n",
      "21953 Training Loss: tensor(0.3300)\n",
      "21954 Training Loss: tensor(0.3294)\n",
      "21955 Training Loss: tensor(0.3295)\n",
      "21956 Training Loss: tensor(0.3304)\n",
      "21957 Training Loss: tensor(0.3313)\n",
      "21958 Training Loss: tensor(0.3307)\n",
      "21959 Training Loss: tensor(0.3311)\n",
      "21960 Training Loss: tensor(0.3293)\n",
      "21961 Training Loss: tensor(0.3309)\n",
      "21962 Training Loss: tensor(0.3300)\n",
      "21963 Training Loss: tensor(0.3328)\n",
      "21964 Training Loss: tensor(0.3303)\n",
      "21965 Training Loss: tensor(0.3310)\n",
      "21966 Training Loss: tensor(0.3297)\n",
      "21967 Training Loss: tensor(0.3303)\n",
      "21968 Training Loss: tensor(0.3300)\n",
      "21969 Training Loss: tensor(0.3297)\n",
      "21970 Training Loss: tensor(0.3306)\n",
      "21971 Training Loss: tensor(0.3302)\n",
      "21972 Training Loss: tensor(0.3302)\n",
      "21973 Training Loss: tensor(0.3312)\n",
      "21974 Training Loss: tensor(0.3300)\n",
      "21975 Training Loss: tensor(0.3291)\n",
      "21976 Training Loss: tensor(0.3313)\n",
      "21977 Training Loss: tensor(0.3295)\n",
      "21978 Training Loss: tensor(0.3294)\n",
      "21979 Training Loss: tensor(0.3298)\n",
      "21980 Training Loss: tensor(0.3295)\n",
      "21981 Training Loss: tensor(0.3291)\n",
      "21982 Training Loss: tensor(0.3288)\n",
      "21983 Training Loss: tensor(0.3319)\n",
      "21984 Training Loss: tensor(0.3308)\n",
      "21985 Training Loss: tensor(0.3361)\n",
      "21986 Training Loss: tensor(0.3299)\n",
      "21987 Training Loss: tensor(0.3292)\n",
      "21988 Training Loss: tensor(0.3294)\n",
      "21989 Training Loss: tensor(0.3293)\n",
      "21990 Training Loss: tensor(0.3316)\n",
      "21991 Training Loss: tensor(0.3300)\n",
      "21992 Training Loss: tensor(0.3317)\n",
      "21993 Training Loss: tensor(0.3307)\n",
      "21994 Training Loss: tensor(0.3311)\n",
      "21995 Training Loss: tensor(0.3332)\n",
      "21996 Training Loss: tensor(0.3286)\n",
      "21997 Training Loss: tensor(0.3300)\n",
      "21998 Training Loss: tensor(0.3292)\n",
      "21999 Training Loss: tensor(0.3304)\n",
      "22000 Training Loss: tensor(0.3292)\n",
      "22001 Training Loss: tensor(0.3297)\n",
      "22002 Training Loss: tensor(0.3293)\n",
      "22003 Training Loss: tensor(0.3302)\n",
      "22004 Training Loss: tensor(0.3337)\n",
      "22005 Training Loss: tensor(0.3316)\n",
      "22006 Training Loss: tensor(0.3301)\n",
      "22007 Training Loss: tensor(0.3315)\n",
      "22008 Training Loss: tensor(0.3323)\n",
      "22009 Training Loss: tensor(0.3298)\n",
      "22010 Training Loss: tensor(0.3293)\n",
      "22011 Training Loss: tensor(0.3291)\n",
      "22012 Training Loss: tensor(0.3291)\n",
      "22013 Training Loss: tensor(0.3290)\n",
      "22014 Training Loss: tensor(0.3289)\n",
      "22015 Training Loss: tensor(0.3306)\n",
      "22016 Training Loss: tensor(0.3293)\n",
      "22017 Training Loss: tensor(0.3293)\n",
      "22018 Training Loss: tensor(0.3298)\n",
      "22019 Training Loss: tensor(0.3288)\n",
      "22020 Training Loss: tensor(0.3302)\n",
      "22021 Training Loss: tensor(0.3307)\n",
      "22022 Training Loss: tensor(0.3302)\n",
      "22023 Training Loss: tensor(0.3302)\n",
      "22024 Training Loss: tensor(0.3316)\n",
      "22025 Training Loss: tensor(0.3291)\n",
      "22026 Training Loss: tensor(0.3302)\n",
      "22027 Training Loss: tensor(0.3291)\n",
      "22028 Training Loss: tensor(0.3282)\n",
      "22029 Training Loss: tensor(0.3335)\n",
      "22030 Training Loss: tensor(0.3310)\n",
      "22031 Training Loss: tensor(0.3311)\n",
      "22032 Training Loss: tensor(0.3298)\n",
      "22033 Training Loss: tensor(0.3315)\n",
      "22034 Training Loss: tensor(0.3300)\n",
      "22035 Training Loss: tensor(0.3295)\n",
      "22036 Training Loss: tensor(0.3327)\n",
      "22037 Training Loss: tensor(0.3338)\n",
      "22038 Training Loss: tensor(0.3306)\n",
      "22039 Training Loss: tensor(0.3308)\n",
      "22040 Training Loss: tensor(0.3290)\n",
      "22041 Training Loss: tensor(0.3299)\n",
      "22042 Training Loss: tensor(0.3307)\n",
      "22043 Training Loss: tensor(0.3306)\n",
      "22044 Training Loss: tensor(0.3295)\n",
      "22045 Training Loss: tensor(0.3305)\n",
      "22046 Training Loss: tensor(0.3303)\n",
      "22047 Training Loss: tensor(0.3301)\n",
      "22048 Training Loss: tensor(0.3296)\n",
      "22049 Training Loss: tensor(0.3308)\n",
      "22050 Training Loss: tensor(0.3292)\n",
      "22051 Training Loss: tensor(0.3311)\n",
      "22052 Training Loss: tensor(0.3329)\n",
      "22053 Training Loss: tensor(0.3300)\n",
      "22054 Training Loss: tensor(0.3297)\n",
      "22055 Training Loss: tensor(0.3291)\n",
      "22056 Training Loss: tensor(0.3293)\n",
      "22057 Training Loss: tensor(0.3315)\n",
      "22058 Training Loss: tensor(0.3292)\n",
      "22059 Training Loss: tensor(0.3301)\n",
      "22060 Training Loss: tensor(0.3304)\n",
      "22061 Training Loss: tensor(0.3298)\n",
      "22062 Training Loss: tensor(0.3324)\n",
      "22063 Training Loss: tensor(0.3293)\n",
      "22064 Training Loss: tensor(0.3308)\n",
      "22065 Training Loss: tensor(0.3290)\n",
      "22066 Training Loss: tensor(0.3295)\n",
      "22067 Training Loss: tensor(0.3313)\n",
      "22068 Training Loss: tensor(0.3301)\n",
      "22069 Training Loss: tensor(0.3319)\n",
      "22070 Training Loss: tensor(0.3290)\n",
      "22071 Training Loss: tensor(0.3294)\n",
      "22072 Training Loss: tensor(0.3323)\n",
      "22073 Training Loss: tensor(0.3308)\n",
      "22074 Training Loss: tensor(0.3302)\n",
      "22075 Training Loss: tensor(0.3297)\n",
      "22076 Training Loss: tensor(0.3311)\n",
      "22077 Training Loss: tensor(0.3299)\n",
      "22078 Training Loss: tensor(0.3302)\n",
      "22079 Training Loss: tensor(0.3322)\n",
      "22080 Training Loss: tensor(0.3324)\n",
      "22081 Training Loss: tensor(0.3289)\n",
      "22082 Training Loss: tensor(0.3291)\n",
      "22083 Training Loss: tensor(0.3314)\n",
      "22084 Training Loss: tensor(0.3293)\n",
      "22085 Training Loss: tensor(0.3310)\n",
      "22086 Training Loss: tensor(0.3317)\n",
      "22087 Training Loss: tensor(0.3296)\n",
      "22088 Training Loss: tensor(0.3314)\n",
      "22089 Training Loss: tensor(0.3299)\n",
      "22090 Training Loss: tensor(0.3291)\n",
      "22091 Training Loss: tensor(0.3302)\n",
      "22092 Training Loss: tensor(0.3332)\n",
      "22093 Training Loss: tensor(0.3302)\n",
      "22094 Training Loss: tensor(0.3292)\n",
      "22095 Training Loss: tensor(0.3306)\n",
      "22096 Training Loss: tensor(0.3283)\n",
      "22097 Training Loss: tensor(0.3333)\n",
      "22098 Training Loss: tensor(0.3297)\n",
      "22099 Training Loss: tensor(0.3292)\n",
      "22100 Training Loss: tensor(0.3287)\n",
      "22101 Training Loss: tensor(0.3289)\n",
      "22102 Training Loss: tensor(0.3357)\n",
      "22103 Training Loss: tensor(0.3307)\n",
      "22104 Training Loss: tensor(0.3295)\n",
      "22105 Training Loss: tensor(0.3291)\n",
      "22106 Training Loss: tensor(0.3301)\n",
      "22107 Training Loss: tensor(0.3308)\n",
      "22108 Training Loss: tensor(0.3336)\n",
      "22109 Training Loss: tensor(0.3291)\n",
      "22110 Training Loss: tensor(0.3312)\n",
      "22111 Training Loss: tensor(0.3306)\n",
      "22112 Training Loss: tensor(0.3301)\n",
      "22113 Training Loss: tensor(0.3333)\n",
      "22114 Training Loss: tensor(0.3346)\n",
      "22115 Training Loss: tensor(0.3302)\n",
      "22116 Training Loss: tensor(0.3302)\n",
      "22117 Training Loss: tensor(0.3297)\n",
      "22118 Training Loss: tensor(0.3311)\n",
      "22119 Training Loss: tensor(0.3316)\n",
      "22120 Training Loss: tensor(0.3296)\n",
      "22121 Training Loss: tensor(0.3300)\n",
      "22122 Training Loss: tensor(0.3292)\n",
      "22123 Training Loss: tensor(0.3296)\n",
      "22124 Training Loss: tensor(0.3297)\n",
      "22125 Training Loss: tensor(0.3296)\n",
      "22126 Training Loss: tensor(0.3324)\n",
      "22127 Training Loss: tensor(0.3322)\n",
      "22128 Training Loss: tensor(0.3307)\n",
      "22129 Training Loss: tensor(0.3305)\n",
      "22130 Training Loss: tensor(0.3294)\n",
      "22131 Training Loss: tensor(0.3315)\n",
      "22132 Training Loss: tensor(0.3289)\n",
      "22133 Training Loss: tensor(0.3309)\n",
      "22134 Training Loss: tensor(0.3299)\n",
      "22135 Training Loss: tensor(0.3302)\n",
      "22136 Training Loss: tensor(0.3316)\n",
      "22137 Training Loss: tensor(0.3298)\n",
      "22138 Training Loss: tensor(0.3311)\n",
      "22139 Training Loss: tensor(0.3296)\n",
      "22140 Training Loss: tensor(0.3309)\n",
      "22141 Training Loss: tensor(0.3302)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22142 Training Loss: tensor(0.3301)\n",
      "22143 Training Loss: tensor(0.3314)\n",
      "22144 Training Loss: tensor(0.3287)\n",
      "22145 Training Loss: tensor(0.3314)\n",
      "22146 Training Loss: tensor(0.3320)\n",
      "22147 Training Loss: tensor(0.3299)\n",
      "22148 Training Loss: tensor(0.3292)\n",
      "22149 Training Loss: tensor(0.3297)\n",
      "22150 Training Loss: tensor(0.3301)\n",
      "22151 Training Loss: tensor(0.3290)\n",
      "22152 Training Loss: tensor(0.3291)\n",
      "22153 Training Loss: tensor(0.3289)\n",
      "22154 Training Loss: tensor(0.3302)\n",
      "22155 Training Loss: tensor(0.3286)\n",
      "22156 Training Loss: tensor(0.3325)\n",
      "22157 Training Loss: tensor(0.3295)\n",
      "22158 Training Loss: tensor(0.3292)\n",
      "22159 Training Loss: tensor(0.3289)\n",
      "22160 Training Loss: tensor(0.3308)\n",
      "22161 Training Loss: tensor(0.3284)\n",
      "22162 Training Loss: tensor(0.3294)\n",
      "22163 Training Loss: tensor(0.3282)\n",
      "22164 Training Loss: tensor(0.3287)\n",
      "22165 Training Loss: tensor(0.3298)\n",
      "22166 Training Loss: tensor(0.3350)\n",
      "22167 Training Loss: tensor(0.3294)\n",
      "22168 Training Loss: tensor(0.3281)\n",
      "22169 Training Loss: tensor(0.3289)\n",
      "22170 Training Loss: tensor(0.3290)\n",
      "22171 Training Loss: tensor(0.3291)\n",
      "22172 Training Loss: tensor(0.3319)\n",
      "22173 Training Loss: tensor(0.3321)\n",
      "22174 Training Loss: tensor(0.3315)\n",
      "22175 Training Loss: tensor(0.3310)\n",
      "22176 Training Loss: tensor(0.3295)\n",
      "22177 Training Loss: tensor(0.3297)\n",
      "22178 Training Loss: tensor(0.3303)\n",
      "22179 Training Loss: tensor(0.3314)\n",
      "22180 Training Loss: tensor(0.3307)\n",
      "22181 Training Loss: tensor(0.3290)\n",
      "22182 Training Loss: tensor(0.3286)\n",
      "22183 Training Loss: tensor(0.3309)\n",
      "22184 Training Loss: tensor(0.3304)\n",
      "22185 Training Loss: tensor(0.3303)\n",
      "22186 Training Loss: tensor(0.3321)\n",
      "22187 Training Loss: tensor(0.3324)\n",
      "22188 Training Loss: tensor(0.3290)\n",
      "22189 Training Loss: tensor(0.3291)\n",
      "22190 Training Loss: tensor(0.3292)\n",
      "22191 Training Loss: tensor(0.3298)\n",
      "22192 Training Loss: tensor(0.3320)\n",
      "22193 Training Loss: tensor(0.3313)\n",
      "22194 Training Loss: tensor(0.3313)\n",
      "22195 Training Loss: tensor(0.3312)\n",
      "22196 Training Loss: tensor(0.3298)\n",
      "22197 Training Loss: tensor(0.3300)\n",
      "22198 Training Loss: tensor(0.3313)\n",
      "22199 Training Loss: tensor(0.3302)\n",
      "22200 Training Loss: tensor(0.3304)\n",
      "22201 Training Loss: tensor(0.3301)\n",
      "22202 Training Loss: tensor(0.3299)\n",
      "22203 Training Loss: tensor(0.3293)\n",
      "22204 Training Loss: tensor(0.3300)\n",
      "22205 Training Loss: tensor(0.3293)\n",
      "22206 Training Loss: tensor(0.3305)\n",
      "22207 Training Loss: tensor(0.3324)\n",
      "22208 Training Loss: tensor(0.3318)\n",
      "22209 Training Loss: tensor(0.3288)\n",
      "22210 Training Loss: tensor(0.3300)\n",
      "22211 Training Loss: tensor(0.3293)\n",
      "22212 Training Loss: tensor(0.3340)\n",
      "22213 Training Loss: tensor(0.3320)\n",
      "22214 Training Loss: tensor(0.3311)\n",
      "22215 Training Loss: tensor(0.3305)\n",
      "22216 Training Loss: tensor(0.3292)\n",
      "22217 Training Loss: tensor(0.3320)\n",
      "22218 Training Loss: tensor(0.3290)\n",
      "22219 Training Loss: tensor(0.3308)\n",
      "22220 Training Loss: tensor(0.3311)\n",
      "22221 Training Loss: tensor(0.3304)\n",
      "22222 Training Loss: tensor(0.3296)\n",
      "22223 Training Loss: tensor(0.3287)\n",
      "22224 Training Loss: tensor(0.3296)\n",
      "22225 Training Loss: tensor(0.3288)\n",
      "22226 Training Loss: tensor(0.3296)\n",
      "22227 Training Loss: tensor(0.3310)\n",
      "22228 Training Loss: tensor(0.3295)\n",
      "22229 Training Loss: tensor(0.3303)\n",
      "22230 Training Loss: tensor(0.3321)\n",
      "22231 Training Loss: tensor(0.3309)\n",
      "22232 Training Loss: tensor(0.3331)\n",
      "22233 Training Loss: tensor(0.3313)\n",
      "22234 Training Loss: tensor(0.3319)\n",
      "22235 Training Loss: tensor(0.3295)\n",
      "22236 Training Loss: tensor(0.3309)\n",
      "22237 Training Loss: tensor(0.3315)\n",
      "22238 Training Loss: tensor(0.3319)\n",
      "22239 Training Loss: tensor(0.3288)\n",
      "22240 Training Loss: tensor(0.3298)\n",
      "22241 Training Loss: tensor(0.3298)\n",
      "22242 Training Loss: tensor(0.3295)\n",
      "22243 Training Loss: tensor(0.3294)\n",
      "22244 Training Loss: tensor(0.3295)\n",
      "22245 Training Loss: tensor(0.3290)\n",
      "22246 Training Loss: tensor(0.3297)\n",
      "22247 Training Loss: tensor(0.3300)\n",
      "22248 Training Loss: tensor(0.3288)\n",
      "22249 Training Loss: tensor(0.3279)\n",
      "22250 Training Loss: tensor(0.3306)\n",
      "22251 Training Loss: tensor(0.3302)\n",
      "22252 Training Loss: tensor(0.3316)\n",
      "22253 Training Loss: tensor(0.3310)\n",
      "22254 Training Loss: tensor(0.3294)\n",
      "22255 Training Loss: tensor(0.3296)\n",
      "22256 Training Loss: tensor(0.3338)\n",
      "22257 Training Loss: tensor(0.3299)\n",
      "22258 Training Loss: tensor(0.3294)\n",
      "22259 Training Loss: tensor(0.3305)\n",
      "22260 Training Loss: tensor(0.3354)\n",
      "22261 Training Loss: tensor(0.3296)\n",
      "22262 Training Loss: tensor(0.3293)\n",
      "22263 Training Loss: tensor(0.3304)\n",
      "22264 Training Loss: tensor(0.3321)\n",
      "22265 Training Loss: tensor(0.3293)\n",
      "22266 Training Loss: tensor(0.3309)\n",
      "22267 Training Loss: tensor(0.3297)\n",
      "22268 Training Loss: tensor(0.3322)\n",
      "22269 Training Loss: tensor(0.3293)\n",
      "22270 Training Loss: tensor(0.3312)\n",
      "22271 Training Loss: tensor(0.3288)\n",
      "22272 Training Loss: tensor(0.3315)\n",
      "22273 Training Loss: tensor(0.3294)\n",
      "22274 Training Loss: tensor(0.3300)\n",
      "22275 Training Loss: tensor(0.3288)\n",
      "22276 Training Loss: tensor(0.3301)\n",
      "22277 Training Loss: tensor(0.3288)\n",
      "22278 Training Loss: tensor(0.3293)\n",
      "22279 Training Loss: tensor(0.3292)\n",
      "22280 Training Loss: tensor(0.3296)\n",
      "22281 Training Loss: tensor(0.3307)\n",
      "22282 Training Loss: tensor(0.3320)\n",
      "22283 Training Loss: tensor(0.3326)\n",
      "22284 Training Loss: tensor(0.3322)\n",
      "22285 Training Loss: tensor(0.3296)\n",
      "22286 Training Loss: tensor(0.3299)\n",
      "22287 Training Loss: tensor(0.3294)\n",
      "22288 Training Loss: tensor(0.3309)\n",
      "22289 Training Loss: tensor(0.3326)\n",
      "22290 Training Loss: tensor(0.3327)\n",
      "22291 Training Loss: tensor(0.3292)\n",
      "22292 Training Loss: tensor(0.3308)\n",
      "22293 Training Loss: tensor(0.3307)\n",
      "22294 Training Loss: tensor(0.3306)\n",
      "22295 Training Loss: tensor(0.3308)\n",
      "22296 Training Loss: tensor(0.3315)\n",
      "22297 Training Loss: tensor(0.3306)\n",
      "22298 Training Loss: tensor(0.3287)\n",
      "22299 Training Loss: tensor(0.3311)\n",
      "22300 Training Loss: tensor(0.3320)\n",
      "22301 Training Loss: tensor(0.3316)\n",
      "22302 Training Loss: tensor(0.3314)\n",
      "22303 Training Loss: tensor(0.3305)\n",
      "22304 Training Loss: tensor(0.3320)\n",
      "22305 Training Loss: tensor(0.3316)\n",
      "22306 Training Loss: tensor(0.3310)\n",
      "22307 Training Loss: tensor(0.3293)\n",
      "22308 Training Loss: tensor(0.3306)\n",
      "22309 Training Loss: tensor(0.3302)\n",
      "22310 Training Loss: tensor(0.3298)\n",
      "22311 Training Loss: tensor(0.3299)\n",
      "22312 Training Loss: tensor(0.3299)\n",
      "22313 Training Loss: tensor(0.3295)\n",
      "22314 Training Loss: tensor(0.3317)\n",
      "22315 Training Loss: tensor(0.3293)\n",
      "22316 Training Loss: tensor(0.3297)\n",
      "22317 Training Loss: tensor(0.3293)\n",
      "22318 Training Loss: tensor(0.3297)\n",
      "22319 Training Loss: tensor(0.3295)\n",
      "22320 Training Loss: tensor(0.3294)\n",
      "22321 Training Loss: tensor(0.3289)\n",
      "22322 Training Loss: tensor(0.3299)\n",
      "22323 Training Loss: tensor(0.3290)\n",
      "22324 Training Loss: tensor(0.3302)\n",
      "22325 Training Loss: tensor(0.3298)\n",
      "22326 Training Loss: tensor(0.3291)\n",
      "22327 Training Loss: tensor(0.3312)\n",
      "22328 Training Loss: tensor(0.3293)\n",
      "22329 Training Loss: tensor(0.3356)\n",
      "22330 Training Loss: tensor(0.3299)\n",
      "22331 Training Loss: tensor(0.3294)\n",
      "22332 Training Loss: tensor(0.3290)\n",
      "22333 Training Loss: tensor(0.3285)\n",
      "22334 Training Loss: tensor(0.3285)\n",
      "22335 Training Loss: tensor(0.3293)\n",
      "22336 Training Loss: tensor(0.3322)\n",
      "22337 Training Loss: tensor(0.3290)\n",
      "22338 Training Loss: tensor(0.3291)\n",
      "22339 Training Loss: tensor(0.3290)\n",
      "22340 Training Loss: tensor(0.3293)\n",
      "22341 Training Loss: tensor(0.3292)\n",
      "22342 Training Loss: tensor(0.3297)\n",
      "22343 Training Loss: tensor(0.3300)\n",
      "22344 Training Loss: tensor(0.3309)\n",
      "22345 Training Loss: tensor(0.3310)\n",
      "22346 Training Loss: tensor(0.3285)\n",
      "22347 Training Loss: tensor(0.3283)\n",
      "22348 Training Loss: tensor(0.3312)\n",
      "22349 Training Loss: tensor(0.3294)\n",
      "22350 Training Loss: tensor(0.3292)\n",
      "22351 Training Loss: tensor(0.3292)\n",
      "22352 Training Loss: tensor(0.3311)\n",
      "22353 Training Loss: tensor(0.3305)\n",
      "22354 Training Loss: tensor(0.3285)\n",
      "22355 Training Loss: tensor(0.3300)\n",
      "22356 Training Loss: tensor(0.3296)\n",
      "22357 Training Loss: tensor(0.3304)\n",
      "22358 Training Loss: tensor(0.3297)\n",
      "22359 Training Loss: tensor(0.3297)\n",
      "22360 Training Loss: tensor(0.3319)\n",
      "22361 Training Loss: tensor(0.3288)\n",
      "22362 Training Loss: tensor(0.3307)\n",
      "22363 Training Loss: tensor(0.3321)\n",
      "22364 Training Loss: tensor(0.3332)\n",
      "22365 Training Loss: tensor(0.3301)\n",
      "22366 Training Loss: tensor(0.3297)\n",
      "22367 Training Loss: tensor(0.3330)\n",
      "22368 Training Loss: tensor(0.3321)\n",
      "22369 Training Loss: tensor(0.3304)\n",
      "22370 Training Loss: tensor(0.3311)\n",
      "22371 Training Loss: tensor(0.3303)\n",
      "22372 Training Loss: tensor(0.3300)\n",
      "22373 Training Loss: tensor(0.3295)\n",
      "22374 Training Loss: tensor(0.3309)\n",
      "22375 Training Loss: tensor(0.3293)\n",
      "22376 Training Loss: tensor(0.3317)\n",
      "22377 Training Loss: tensor(0.3299)\n",
      "22378 Training Loss: tensor(0.3288)\n",
      "22379 Training Loss: tensor(0.3323)\n",
      "22380 Training Loss: tensor(0.3313)\n",
      "22381 Training Loss: tensor(0.3301)\n",
      "22382 Training Loss: tensor(0.3301)\n",
      "22383 Training Loss: tensor(0.3288)\n",
      "22384 Training Loss: tensor(0.3299)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22385 Training Loss: tensor(0.3299)\n",
      "22386 Training Loss: tensor(0.3337)\n",
      "22387 Training Loss: tensor(0.3308)\n",
      "22388 Training Loss: tensor(0.3293)\n",
      "22389 Training Loss: tensor(0.3301)\n",
      "22390 Training Loss: tensor(0.3286)\n",
      "22391 Training Loss: tensor(0.3308)\n",
      "22392 Training Loss: tensor(0.3289)\n",
      "22393 Training Loss: tensor(0.3319)\n",
      "22394 Training Loss: tensor(0.3290)\n",
      "22395 Training Loss: tensor(0.3326)\n",
      "22396 Training Loss: tensor(0.3308)\n",
      "22397 Training Loss: tensor(0.3290)\n",
      "22398 Training Loss: tensor(0.3296)\n",
      "22399 Training Loss: tensor(0.3296)\n",
      "22400 Training Loss: tensor(0.3301)\n",
      "22401 Training Loss: tensor(0.3316)\n",
      "22402 Training Loss: tensor(0.3320)\n",
      "22403 Training Loss: tensor(0.3299)\n",
      "22404 Training Loss: tensor(0.3296)\n",
      "22405 Training Loss: tensor(0.3294)\n",
      "22406 Training Loss: tensor(0.3290)\n",
      "22407 Training Loss: tensor(0.3290)\n",
      "22408 Training Loss: tensor(0.3300)\n",
      "22409 Training Loss: tensor(0.3299)\n",
      "22410 Training Loss: tensor(0.3297)\n",
      "22411 Training Loss: tensor(0.3290)\n",
      "22412 Training Loss: tensor(0.3302)\n",
      "22413 Training Loss: tensor(0.3299)\n",
      "22414 Training Loss: tensor(0.3316)\n",
      "22415 Training Loss: tensor(0.3313)\n",
      "22416 Training Loss: tensor(0.3285)\n",
      "22417 Training Loss: tensor(0.3311)\n",
      "22418 Training Loss: tensor(0.3293)\n",
      "22419 Training Loss: tensor(0.3311)\n",
      "22420 Training Loss: tensor(0.3294)\n",
      "22421 Training Loss: tensor(0.3313)\n",
      "22422 Training Loss: tensor(0.3305)\n",
      "22423 Training Loss: tensor(0.3299)\n",
      "22424 Training Loss: tensor(0.3295)\n",
      "22425 Training Loss: tensor(0.3297)\n",
      "22426 Training Loss: tensor(0.3310)\n",
      "22427 Training Loss: tensor(0.3297)\n",
      "22428 Training Loss: tensor(0.3302)\n",
      "22429 Training Loss: tensor(0.3295)\n",
      "22430 Training Loss: tensor(0.3287)\n",
      "22431 Training Loss: tensor(0.3307)\n",
      "22432 Training Loss: tensor(0.3291)\n",
      "22433 Training Loss: tensor(0.3303)\n",
      "22434 Training Loss: tensor(0.3285)\n",
      "22435 Training Loss: tensor(0.3287)\n",
      "22436 Training Loss: tensor(0.3289)\n",
      "22437 Training Loss: tensor(0.3283)\n",
      "22438 Training Loss: tensor(0.3313)\n",
      "22439 Training Loss: tensor(0.3348)\n",
      "22440 Training Loss: tensor(0.3293)\n",
      "22441 Training Loss: tensor(0.3294)\n",
      "22442 Training Loss: tensor(0.3297)\n",
      "22443 Training Loss: tensor(0.3291)\n",
      "22444 Training Loss: tensor(0.3320)\n",
      "22445 Training Loss: tensor(0.3289)\n",
      "22446 Training Loss: tensor(0.3308)\n",
      "22447 Training Loss: tensor(0.3305)\n",
      "22448 Training Loss: tensor(0.3309)\n",
      "22449 Training Loss: tensor(0.3304)\n",
      "22450 Training Loss: tensor(0.3296)\n",
      "22451 Training Loss: tensor(0.3296)\n",
      "22452 Training Loss: tensor(0.3311)\n",
      "22453 Training Loss: tensor(0.3289)\n",
      "22454 Training Loss: tensor(0.3293)\n",
      "22455 Training Loss: tensor(0.3297)\n",
      "22456 Training Loss: tensor(0.3285)\n",
      "22457 Training Loss: tensor(0.3307)\n",
      "22458 Training Loss: tensor(0.3297)\n",
      "22459 Training Loss: tensor(0.3373)\n",
      "22460 Training Loss: tensor(0.3295)\n",
      "22461 Training Loss: tensor(0.3309)\n",
      "22462 Training Loss: tensor(0.3293)\n",
      "22463 Training Loss: tensor(0.3308)\n",
      "22464 Training Loss: tensor(0.3309)\n",
      "22465 Training Loss: tensor(0.3301)\n",
      "22466 Training Loss: tensor(0.3317)\n",
      "22467 Training Loss: tensor(0.3307)\n",
      "22468 Training Loss: tensor(0.3294)\n",
      "22469 Training Loss: tensor(0.3293)\n",
      "22470 Training Loss: tensor(0.3302)\n",
      "22471 Training Loss: tensor(0.3341)\n",
      "22472 Training Loss: tensor(0.3314)\n",
      "22473 Training Loss: tensor(0.3291)\n",
      "22474 Training Loss: tensor(0.3306)\n",
      "22475 Training Loss: tensor(0.3295)\n",
      "22476 Training Loss: tensor(0.3297)\n",
      "22477 Training Loss: tensor(0.3298)\n",
      "22478 Training Loss: tensor(0.3295)\n",
      "22479 Training Loss: tensor(0.3295)\n",
      "22480 Training Loss: tensor(0.3321)\n",
      "22481 Training Loss: tensor(0.3316)\n",
      "22482 Training Loss: tensor(0.3300)\n",
      "22483 Training Loss: tensor(0.3296)\n",
      "22484 Training Loss: tensor(0.3286)\n",
      "22485 Training Loss: tensor(0.3299)\n",
      "22486 Training Loss: tensor(0.3314)\n",
      "22487 Training Loss: tensor(0.3291)\n",
      "22488 Training Loss: tensor(0.3298)\n",
      "22489 Training Loss: tensor(0.3298)\n",
      "22490 Training Loss: tensor(0.3303)\n",
      "22491 Training Loss: tensor(0.3302)\n",
      "22492 Training Loss: tensor(0.3293)\n",
      "22493 Training Loss: tensor(0.3293)\n",
      "22494 Training Loss: tensor(0.3290)\n",
      "22495 Training Loss: tensor(0.3292)\n",
      "22496 Training Loss: tensor(0.3313)\n",
      "22497 Training Loss: tensor(0.3327)\n",
      "22498 Training Loss: tensor(0.3301)\n",
      "22499 Training Loss: tensor(0.3348)\n",
      "22500 Training Loss: tensor(0.3285)\n",
      "22501 Training Loss: tensor(0.3298)\n",
      "22502 Training Loss: tensor(0.3300)\n",
      "22503 Training Loss: tensor(0.3292)\n",
      "22504 Training Loss: tensor(0.3291)\n",
      "22505 Training Loss: tensor(0.3319)\n",
      "22506 Training Loss: tensor(0.3294)\n",
      "22507 Training Loss: tensor(0.3301)\n",
      "22508 Training Loss: tensor(0.3294)\n",
      "22509 Training Loss: tensor(0.3301)\n",
      "22510 Training Loss: tensor(0.3309)\n",
      "22511 Training Loss: tensor(0.3323)\n",
      "22512 Training Loss: tensor(0.3302)\n",
      "22513 Training Loss: tensor(0.3288)\n",
      "22514 Training Loss: tensor(0.3288)\n",
      "22515 Training Loss: tensor(0.3296)\n",
      "22516 Training Loss: tensor(0.3314)\n",
      "22517 Training Loss: tensor(0.3310)\n",
      "22518 Training Loss: tensor(0.3289)\n",
      "22519 Training Loss: tensor(0.3295)\n",
      "22520 Training Loss: tensor(0.3308)\n",
      "22521 Training Loss: tensor(0.3294)\n",
      "22522 Training Loss: tensor(0.3294)\n",
      "22523 Training Loss: tensor(0.3295)\n",
      "22524 Training Loss: tensor(0.3285)\n",
      "22525 Training Loss: tensor(0.3307)\n",
      "22526 Training Loss: tensor(0.3297)\n",
      "22527 Training Loss: tensor(0.3291)\n",
      "22528 Training Loss: tensor(0.3302)\n",
      "22529 Training Loss: tensor(0.3320)\n",
      "22530 Training Loss: tensor(0.3293)\n",
      "22531 Training Loss: tensor(0.3295)\n",
      "22532 Training Loss: tensor(0.3297)\n",
      "22533 Training Loss: tensor(0.3285)\n",
      "22534 Training Loss: tensor(0.3291)\n",
      "22535 Training Loss: tensor(0.3328)\n",
      "22536 Training Loss: tensor(0.3327)\n",
      "22537 Training Loss: tensor(0.3291)\n",
      "22538 Training Loss: tensor(0.3309)\n",
      "22539 Training Loss: tensor(0.3293)\n",
      "22540 Training Loss: tensor(0.3324)\n",
      "22541 Training Loss: tensor(0.3299)\n",
      "22542 Training Loss: tensor(0.3323)\n",
      "22543 Training Loss: tensor(0.3283)\n",
      "22544 Training Loss: tensor(0.3284)\n",
      "22545 Training Loss: tensor(0.3296)\n",
      "22546 Training Loss: tensor(0.3296)\n",
      "22547 Training Loss: tensor(0.3294)\n",
      "22548 Training Loss: tensor(0.3306)\n",
      "22549 Training Loss: tensor(0.3305)\n",
      "22550 Training Loss: tensor(0.3290)\n",
      "22551 Training Loss: tensor(0.3293)\n",
      "22552 Training Loss: tensor(0.3298)\n",
      "22553 Training Loss: tensor(0.3320)\n",
      "22554 Training Loss: tensor(0.3312)\n",
      "22555 Training Loss: tensor(0.3301)\n",
      "22556 Training Loss: tensor(0.3297)\n",
      "22557 Training Loss: tensor(0.3297)\n",
      "22558 Training Loss: tensor(0.3292)\n",
      "22559 Training Loss: tensor(0.3299)\n",
      "22560 Training Loss: tensor(0.3291)\n",
      "22561 Training Loss: tensor(0.3290)\n",
      "22562 Training Loss: tensor(0.3306)\n",
      "22563 Training Loss: tensor(0.3293)\n",
      "22564 Training Loss: tensor(0.3300)\n",
      "22565 Training Loss: tensor(0.3326)\n",
      "22566 Training Loss: tensor(0.3301)\n",
      "22567 Training Loss: tensor(0.3301)\n",
      "22568 Training Loss: tensor(0.3285)\n",
      "22569 Training Loss: tensor(0.3319)\n",
      "22570 Training Loss: tensor(0.3320)\n",
      "22571 Training Loss: tensor(0.3303)\n",
      "22572 Training Loss: tensor(0.3296)\n",
      "22573 Training Loss: tensor(0.3301)\n",
      "22574 Training Loss: tensor(0.3303)\n",
      "22575 Training Loss: tensor(0.3287)\n",
      "22576 Training Loss: tensor(0.3304)\n",
      "22577 Training Loss: tensor(0.3318)\n",
      "22578 Training Loss: tensor(0.3293)\n",
      "22579 Training Loss: tensor(0.3300)\n",
      "22580 Training Loss: tensor(0.3292)\n",
      "22581 Training Loss: tensor(0.3287)\n",
      "22582 Training Loss: tensor(0.3295)\n",
      "22583 Training Loss: tensor(0.3299)\n",
      "22584 Training Loss: tensor(0.3304)\n",
      "22585 Training Loss: tensor(0.3299)\n",
      "22586 Training Loss: tensor(0.3301)\n",
      "22587 Training Loss: tensor(0.3284)\n",
      "22588 Training Loss: tensor(0.3279)\n",
      "22589 Training Loss: tensor(0.3336)\n",
      "22590 Training Loss: tensor(0.3351)\n",
      "22591 Training Loss: tensor(0.3289)\n",
      "22592 Training Loss: tensor(0.3299)\n",
      "22593 Training Loss: tensor(0.3312)\n",
      "22594 Training Loss: tensor(0.3298)\n",
      "22595 Training Loss: tensor(0.3301)\n",
      "22596 Training Loss: tensor(0.3299)\n",
      "22597 Training Loss: tensor(0.3290)\n",
      "22598 Training Loss: tensor(0.3301)\n",
      "22599 Training Loss: tensor(0.3288)\n",
      "22600 Training Loss: tensor(0.3299)\n",
      "22601 Training Loss: tensor(0.3295)\n",
      "22602 Training Loss: tensor(0.3288)\n",
      "22603 Training Loss: tensor(0.3292)\n",
      "22604 Training Loss: tensor(0.3330)\n",
      "22605 Training Loss: tensor(0.3308)\n",
      "22606 Training Loss: tensor(0.3302)\n",
      "22607 Training Loss: tensor(0.3292)\n",
      "22608 Training Loss: tensor(0.3304)\n",
      "22609 Training Loss: tensor(0.3330)\n",
      "22610 Training Loss: tensor(0.3290)\n",
      "22611 Training Loss: tensor(0.3294)\n",
      "22612 Training Loss: tensor(0.3290)\n",
      "22613 Training Loss: tensor(0.3290)\n",
      "22614 Training Loss: tensor(0.3291)\n",
      "22615 Training Loss: tensor(0.3293)\n",
      "22616 Training Loss: tensor(0.3312)\n",
      "22617 Training Loss: tensor(0.3311)\n",
      "22618 Training Loss: tensor(0.3292)\n",
      "22619 Training Loss: tensor(0.3294)\n",
      "22620 Training Loss: tensor(0.3301)\n",
      "22621 Training Loss: tensor(0.3292)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22622 Training Loss: tensor(0.3314)\n",
      "22623 Training Loss: tensor(0.3292)\n",
      "22624 Training Loss: tensor(0.3297)\n",
      "22625 Training Loss: tensor(0.3288)\n",
      "22626 Training Loss: tensor(0.3289)\n",
      "22627 Training Loss: tensor(0.3292)\n",
      "22628 Training Loss: tensor(0.3292)\n",
      "22629 Training Loss: tensor(0.3322)\n",
      "22630 Training Loss: tensor(0.3293)\n",
      "22631 Training Loss: tensor(0.3281)\n",
      "22632 Training Loss: tensor(0.3321)\n",
      "22633 Training Loss: tensor(0.3294)\n",
      "22634 Training Loss: tensor(0.3287)\n",
      "22635 Training Loss: tensor(0.3286)\n",
      "22636 Training Loss: tensor(0.3298)\n",
      "22637 Training Loss: tensor(0.3296)\n",
      "22638 Training Loss: tensor(0.3330)\n",
      "22639 Training Loss: tensor(0.3304)\n",
      "22640 Training Loss: tensor(0.3317)\n",
      "22641 Training Loss: tensor(0.3295)\n",
      "22642 Training Loss: tensor(0.3307)\n",
      "22643 Training Loss: tensor(0.3340)\n",
      "22644 Training Loss: tensor(0.3305)\n",
      "22645 Training Loss: tensor(0.3298)\n",
      "22646 Training Loss: tensor(0.3323)\n",
      "22647 Training Loss: tensor(0.3294)\n",
      "22648 Training Loss: tensor(0.3300)\n",
      "22649 Training Loss: tensor(0.3321)\n",
      "22650 Training Loss: tensor(0.3320)\n",
      "22651 Training Loss: tensor(0.3298)\n",
      "22652 Training Loss: tensor(0.3297)\n",
      "22653 Training Loss: tensor(0.3293)\n",
      "22654 Training Loss: tensor(0.3319)\n",
      "22655 Training Loss: tensor(0.3308)\n",
      "22656 Training Loss: tensor(0.3307)\n",
      "22657 Training Loss: tensor(0.3298)\n",
      "22658 Training Loss: tensor(0.3302)\n",
      "22659 Training Loss: tensor(0.3296)\n",
      "22660 Training Loss: tensor(0.3292)\n",
      "22661 Training Loss: tensor(0.3335)\n",
      "22662 Training Loss: tensor(0.3294)\n",
      "22663 Training Loss: tensor(0.3290)\n",
      "22664 Training Loss: tensor(0.3327)\n",
      "22665 Training Loss: tensor(0.3319)\n",
      "22666 Training Loss: tensor(0.3327)\n",
      "22667 Training Loss: tensor(0.3300)\n",
      "22668 Training Loss: tensor(0.3310)\n",
      "22669 Training Loss: tensor(0.3291)\n",
      "22670 Training Loss: tensor(0.3297)\n",
      "22671 Training Loss: tensor(0.3299)\n",
      "22672 Training Loss: tensor(0.3303)\n",
      "22673 Training Loss: tensor(0.3298)\n",
      "22674 Training Loss: tensor(0.3295)\n",
      "22675 Training Loss: tensor(0.3304)\n",
      "22676 Training Loss: tensor(0.3295)\n",
      "22677 Training Loss: tensor(0.3288)\n",
      "22678 Training Loss: tensor(0.3296)\n",
      "22679 Training Loss: tensor(0.3358)\n",
      "22680 Training Loss: tensor(0.3301)\n",
      "22681 Training Loss: tensor(0.3291)\n",
      "22682 Training Loss: tensor(0.3305)\n",
      "22683 Training Loss: tensor(0.3299)\n",
      "22684 Training Loss: tensor(0.3294)\n",
      "22685 Training Loss: tensor(0.3297)\n",
      "22686 Training Loss: tensor(0.3296)\n",
      "22687 Training Loss: tensor(0.3312)\n",
      "22688 Training Loss: tensor(0.3295)\n",
      "22689 Training Loss: tensor(0.3286)\n",
      "22690 Training Loss: tensor(0.3291)\n",
      "22691 Training Loss: tensor(0.3304)\n",
      "22692 Training Loss: tensor(0.3287)\n",
      "22693 Training Loss: tensor(0.3306)\n",
      "22694 Training Loss: tensor(0.3285)\n",
      "22695 Training Loss: tensor(0.3336)\n",
      "22696 Training Loss: tensor(0.3305)\n",
      "22697 Training Loss: tensor(0.3305)\n",
      "22698 Training Loss: tensor(0.3302)\n",
      "22699 Training Loss: tensor(0.3294)\n",
      "22700 Training Loss: tensor(0.3294)\n",
      "22701 Training Loss: tensor(0.3291)\n",
      "22702 Training Loss: tensor(0.3307)\n",
      "22703 Training Loss: tensor(0.3289)\n",
      "22704 Training Loss: tensor(0.3299)\n",
      "22705 Training Loss: tensor(0.3286)\n",
      "22706 Training Loss: tensor(0.3292)\n",
      "22707 Training Loss: tensor(0.3301)\n",
      "22708 Training Loss: tensor(0.3294)\n",
      "22709 Training Loss: tensor(0.3304)\n",
      "22710 Training Loss: tensor(0.3299)\n",
      "22711 Training Loss: tensor(0.3319)\n",
      "22712 Training Loss: tensor(0.3293)\n",
      "22713 Training Loss: tensor(0.3291)\n",
      "22714 Training Loss: tensor(0.3348)\n",
      "22715 Training Loss: tensor(0.3319)\n",
      "22716 Training Loss: tensor(0.3301)\n",
      "22717 Training Loss: tensor(0.3304)\n",
      "22718 Training Loss: tensor(0.3304)\n",
      "22719 Training Loss: tensor(0.3316)\n",
      "22720 Training Loss: tensor(0.3294)\n",
      "22721 Training Loss: tensor(0.3310)\n",
      "22722 Training Loss: tensor(0.3303)\n",
      "22723 Training Loss: tensor(0.3300)\n",
      "22724 Training Loss: tensor(0.3292)\n",
      "22725 Training Loss: tensor(0.3298)\n",
      "22726 Training Loss: tensor(0.3306)\n",
      "22727 Training Loss: tensor(0.3292)\n",
      "22728 Training Loss: tensor(0.3296)\n",
      "22729 Training Loss: tensor(0.3297)\n",
      "22730 Training Loss: tensor(0.3303)\n",
      "22731 Training Loss: tensor(0.3299)\n",
      "22732 Training Loss: tensor(0.3286)\n",
      "22733 Training Loss: tensor(0.3298)\n",
      "22734 Training Loss: tensor(0.3300)\n",
      "22735 Training Loss: tensor(0.3310)\n",
      "22736 Training Loss: tensor(0.3286)\n",
      "22737 Training Loss: tensor(0.3341)\n",
      "22738 Training Loss: tensor(0.3300)\n",
      "22739 Training Loss: tensor(0.3301)\n",
      "22740 Training Loss: tensor(0.3329)\n",
      "22741 Training Loss: tensor(0.3293)\n",
      "22742 Training Loss: tensor(0.3305)\n",
      "22743 Training Loss: tensor(0.3302)\n",
      "22744 Training Loss: tensor(0.3303)\n",
      "22745 Training Loss: tensor(0.3298)\n",
      "22746 Training Loss: tensor(0.3304)\n",
      "22747 Training Loss: tensor(0.3297)\n",
      "22748 Training Loss: tensor(0.3303)\n",
      "22749 Training Loss: tensor(0.3329)\n",
      "22750 Training Loss: tensor(0.3292)\n",
      "22751 Training Loss: tensor(0.3305)\n",
      "22752 Training Loss: tensor(0.3300)\n",
      "22753 Training Loss: tensor(0.3301)\n",
      "22754 Training Loss: tensor(0.3303)\n",
      "22755 Training Loss: tensor(0.3297)\n",
      "22756 Training Loss: tensor(0.3289)\n",
      "22757 Training Loss: tensor(0.3316)\n",
      "22758 Training Loss: tensor(0.3315)\n",
      "22759 Training Loss: tensor(0.3293)\n",
      "22760 Training Loss: tensor(0.3326)\n",
      "22761 Training Loss: tensor(0.3300)\n",
      "22762 Training Loss: tensor(0.3302)\n",
      "22763 Training Loss: tensor(0.3302)\n",
      "22764 Training Loss: tensor(0.3296)\n",
      "22765 Training Loss: tensor(0.3309)\n",
      "22766 Training Loss: tensor(0.3300)\n",
      "22767 Training Loss: tensor(0.3298)\n",
      "22768 Training Loss: tensor(0.3292)\n",
      "22769 Training Loss: tensor(0.3290)\n",
      "22770 Training Loss: tensor(0.3292)\n",
      "22771 Training Loss: tensor(0.3292)\n",
      "22772 Training Loss: tensor(0.3292)\n",
      "22773 Training Loss: tensor(0.3288)\n",
      "22774 Training Loss: tensor(0.3299)\n",
      "22775 Training Loss: tensor(0.3304)\n",
      "22776 Training Loss: tensor(0.3330)\n",
      "22777 Training Loss: tensor(0.3294)\n",
      "22778 Training Loss: tensor(0.3335)\n",
      "22779 Training Loss: tensor(0.3294)\n",
      "22780 Training Loss: tensor(0.3310)\n",
      "22781 Training Loss: tensor(0.3320)\n",
      "22782 Training Loss: tensor(0.3292)\n",
      "22783 Training Loss: tensor(0.3310)\n",
      "22784 Training Loss: tensor(0.3307)\n",
      "22785 Training Loss: tensor(0.3298)\n",
      "22786 Training Loss: tensor(0.3300)\n",
      "22787 Training Loss: tensor(0.3302)\n",
      "22788 Training Loss: tensor(0.3297)\n",
      "22789 Training Loss: tensor(0.3287)\n",
      "22790 Training Loss: tensor(0.3297)\n",
      "22791 Training Loss: tensor(0.3287)\n",
      "22792 Training Loss: tensor(0.3342)\n",
      "22793 Training Loss: tensor(0.3293)\n",
      "22794 Training Loss: tensor(0.3295)\n",
      "22795 Training Loss: tensor(0.3291)\n",
      "22796 Training Loss: tensor(0.3296)\n",
      "22797 Training Loss: tensor(0.3312)\n",
      "22798 Training Loss: tensor(0.3304)\n",
      "22799 Training Loss: tensor(0.3303)\n",
      "22800 Training Loss: tensor(0.3280)\n",
      "22801 Training Loss: tensor(0.3288)\n",
      "22802 Training Loss: tensor(0.3292)\n",
      "22803 Training Loss: tensor(0.3298)\n",
      "22804 Training Loss: tensor(0.3286)\n",
      "22805 Training Loss: tensor(0.3296)\n",
      "22806 Training Loss: tensor(0.3295)\n",
      "22807 Training Loss: tensor(0.3281)\n",
      "22808 Training Loss: tensor(0.3291)\n",
      "22809 Training Loss: tensor(0.3284)\n",
      "22810 Training Loss: tensor(0.3292)\n",
      "22811 Training Loss: tensor(0.3302)\n",
      "22812 Training Loss: tensor(0.3286)\n",
      "22813 Training Loss: tensor(0.3305)\n",
      "22814 Training Loss: tensor(0.3295)\n",
      "22815 Training Loss: tensor(0.3283)\n",
      "22816 Training Loss: tensor(0.3308)\n",
      "22817 Training Loss: tensor(0.3299)\n",
      "22818 Training Loss: tensor(0.3287)\n",
      "22819 Training Loss: tensor(0.3282)\n",
      "22820 Training Loss: tensor(0.3295)\n",
      "22821 Training Loss: tensor(0.3291)\n",
      "22822 Training Loss: tensor(0.3288)\n",
      "22823 Training Loss: tensor(0.3315)\n",
      "22824 Training Loss: tensor(0.3292)\n",
      "22825 Training Loss: tensor(0.3316)\n",
      "22826 Training Loss: tensor(0.3294)\n",
      "22827 Training Loss: tensor(0.3290)\n",
      "22828 Training Loss: tensor(0.3285)\n",
      "22829 Training Loss: tensor(0.3293)\n",
      "22830 Training Loss: tensor(0.3287)\n",
      "22831 Training Loss: tensor(0.3288)\n",
      "22832 Training Loss: tensor(0.3342)\n",
      "22833 Training Loss: tensor(0.3287)\n",
      "22834 Training Loss: tensor(0.3314)\n",
      "22835 Training Loss: tensor(0.3298)\n",
      "22836 Training Loss: tensor(0.3287)\n",
      "22837 Training Loss: tensor(0.3315)\n",
      "22838 Training Loss: tensor(0.3295)\n",
      "22839 Training Loss: tensor(0.3300)\n",
      "22840 Training Loss: tensor(0.3298)\n",
      "22841 Training Loss: tensor(0.3300)\n",
      "22842 Training Loss: tensor(0.3294)\n",
      "22843 Training Loss: tensor(0.3304)\n",
      "22844 Training Loss: tensor(0.3288)\n",
      "22845 Training Loss: tensor(0.3287)\n",
      "22846 Training Loss: tensor(0.3293)\n",
      "22847 Training Loss: tensor(0.3287)\n",
      "22848 Training Loss: tensor(0.3287)\n",
      "22849 Training Loss: tensor(0.3298)\n",
      "22850 Training Loss: tensor(0.3299)\n",
      "22851 Training Loss: tensor(0.3297)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22852 Training Loss: tensor(0.3280)\n",
      "22853 Training Loss: tensor(0.3388)\n",
      "22854 Training Loss: tensor(0.3316)\n",
      "22855 Training Loss: tensor(0.3321)\n",
      "22856 Training Loss: tensor(0.3299)\n",
      "22857 Training Loss: tensor(0.3293)\n",
      "22858 Training Loss: tensor(0.3323)\n",
      "22859 Training Loss: tensor(0.3294)\n",
      "22860 Training Loss: tensor(0.3298)\n",
      "22861 Training Loss: tensor(0.3335)\n",
      "22862 Training Loss: tensor(0.3302)\n",
      "22863 Training Loss: tensor(0.3297)\n",
      "22864 Training Loss: tensor(0.3302)\n",
      "22865 Training Loss: tensor(0.3321)\n",
      "22866 Training Loss: tensor(0.3299)\n",
      "22867 Training Loss: tensor(0.3295)\n",
      "22868 Training Loss: tensor(0.3293)\n",
      "22869 Training Loss: tensor(0.3311)\n",
      "22870 Training Loss: tensor(0.3296)\n",
      "22871 Training Loss: tensor(0.3289)\n",
      "22872 Training Loss: tensor(0.3309)\n",
      "22873 Training Loss: tensor(0.3301)\n",
      "22874 Training Loss: tensor(0.3302)\n",
      "22875 Training Loss: tensor(0.3303)\n",
      "22876 Training Loss: tensor(0.3293)\n",
      "22877 Training Loss: tensor(0.3314)\n",
      "22878 Training Loss: tensor(0.3305)\n",
      "22879 Training Loss: tensor(0.3296)\n",
      "22880 Training Loss: tensor(0.3298)\n",
      "22881 Training Loss: tensor(0.3295)\n",
      "22882 Training Loss: tensor(0.3298)\n",
      "22883 Training Loss: tensor(0.3325)\n",
      "22884 Training Loss: tensor(0.3294)\n",
      "22885 Training Loss: tensor(0.3298)\n",
      "22886 Training Loss: tensor(0.3295)\n",
      "22887 Training Loss: tensor(0.3292)\n",
      "22888 Training Loss: tensor(0.3295)\n",
      "22889 Training Loss: tensor(0.3304)\n",
      "22890 Training Loss: tensor(0.3303)\n",
      "22891 Training Loss: tensor(0.3287)\n",
      "22892 Training Loss: tensor(0.3304)\n",
      "22893 Training Loss: tensor(0.3313)\n",
      "22894 Training Loss: tensor(0.3288)\n",
      "22895 Training Loss: tensor(0.3303)\n",
      "22896 Training Loss: tensor(0.3325)\n",
      "22897 Training Loss: tensor(0.3302)\n",
      "22898 Training Loss: tensor(0.3294)\n",
      "22899 Training Loss: tensor(0.3289)\n",
      "22900 Training Loss: tensor(0.3295)\n",
      "22901 Training Loss: tensor(0.3290)\n",
      "22902 Training Loss: tensor(0.3284)\n",
      "22903 Training Loss: tensor(0.3289)\n",
      "22904 Training Loss: tensor(0.3279)\n",
      "22905 Training Loss: tensor(0.3286)\n",
      "22906 Training Loss: tensor(0.3300)\n",
      "22907 Training Loss: tensor(0.3312)\n",
      "22908 Training Loss: tensor(0.3286)\n",
      "22909 Training Loss: tensor(0.3285)\n",
      "22910 Training Loss: tensor(0.3327)\n",
      "22911 Training Loss: tensor(0.3319)\n",
      "22912 Training Loss: tensor(0.3292)\n",
      "22913 Training Loss: tensor(0.3289)\n",
      "22914 Training Loss: tensor(0.3300)\n",
      "22915 Training Loss: tensor(0.3292)\n",
      "22916 Training Loss: tensor(0.3295)\n",
      "22917 Training Loss: tensor(0.3301)\n",
      "22918 Training Loss: tensor(0.3299)\n",
      "22919 Training Loss: tensor(0.3285)\n",
      "22920 Training Loss: tensor(0.3295)\n",
      "22921 Training Loss: tensor(0.3293)\n",
      "22922 Training Loss: tensor(0.3287)\n",
      "22923 Training Loss: tensor(0.3325)\n",
      "22924 Training Loss: tensor(0.3347)\n",
      "22925 Training Loss: tensor(0.3286)\n",
      "22926 Training Loss: tensor(0.3296)\n",
      "22927 Training Loss: tensor(0.3289)\n",
      "22928 Training Loss: tensor(0.3285)\n",
      "22929 Training Loss: tensor(0.3307)\n",
      "22930 Training Loss: tensor(0.3284)\n",
      "22931 Training Loss: tensor(0.3301)\n",
      "22932 Training Loss: tensor(0.3323)\n",
      "22933 Training Loss: tensor(0.3290)\n",
      "22934 Training Loss: tensor(0.3301)\n",
      "22935 Training Loss: tensor(0.3287)\n",
      "22936 Training Loss: tensor(0.3313)\n",
      "22937 Training Loss: tensor(0.3290)\n",
      "22938 Training Loss: tensor(0.3314)\n",
      "22939 Training Loss: tensor(0.3304)\n",
      "22940 Training Loss: tensor(0.3290)\n",
      "22941 Training Loss: tensor(0.3306)\n",
      "22942 Training Loss: tensor(0.3295)\n",
      "22943 Training Loss: tensor(0.3292)\n",
      "22944 Training Loss: tensor(0.3306)\n",
      "22945 Training Loss: tensor(0.3303)\n",
      "22946 Training Loss: tensor(0.3291)\n",
      "22947 Training Loss: tensor(0.3294)\n",
      "22948 Training Loss: tensor(0.3293)\n",
      "22949 Training Loss: tensor(0.3307)\n",
      "22950 Training Loss: tensor(0.3290)\n",
      "22951 Training Loss: tensor(0.3298)\n",
      "22952 Training Loss: tensor(0.3295)\n",
      "22953 Training Loss: tensor(0.3298)\n",
      "22954 Training Loss: tensor(0.3286)\n",
      "22955 Training Loss: tensor(0.3288)\n",
      "22956 Training Loss: tensor(0.3282)\n",
      "22957 Training Loss: tensor(0.3291)\n",
      "22958 Training Loss: tensor(0.3309)\n",
      "22959 Training Loss: tensor(0.3285)\n",
      "22960 Training Loss: tensor(0.3293)\n",
      "22961 Training Loss: tensor(0.3287)\n",
      "22962 Training Loss: tensor(0.3280)\n",
      "22963 Training Loss: tensor(0.3294)\n",
      "22964 Training Loss: tensor(0.3312)\n",
      "22965 Training Loss: tensor(0.3309)\n",
      "22966 Training Loss: tensor(0.3292)\n",
      "22967 Training Loss: tensor(0.3295)\n",
      "22968 Training Loss: tensor(0.3290)\n",
      "22969 Training Loss: tensor(0.3296)\n",
      "22970 Training Loss: tensor(0.3295)\n",
      "22971 Training Loss: tensor(0.3288)\n",
      "22972 Training Loss: tensor(0.3299)\n",
      "22973 Training Loss: tensor(0.3328)\n",
      "22974 Training Loss: tensor(0.3286)\n",
      "22975 Training Loss: tensor(0.3288)\n",
      "22976 Training Loss: tensor(0.3298)\n",
      "22977 Training Loss: tensor(0.3291)\n",
      "22978 Training Loss: tensor(0.3288)\n",
      "22979 Training Loss: tensor(0.3289)\n",
      "22980 Training Loss: tensor(0.3297)\n",
      "22981 Training Loss: tensor(0.3286)\n",
      "22982 Training Loss: tensor(0.3284)\n",
      "22983 Training Loss: tensor(0.3291)\n",
      "22984 Training Loss: tensor(0.3289)\n",
      "22985 Training Loss: tensor(0.3291)\n",
      "22986 Training Loss: tensor(0.3281)\n",
      "22987 Training Loss: tensor(0.3293)\n",
      "22988 Training Loss: tensor(0.3298)\n",
      "22989 Training Loss: tensor(0.3292)\n",
      "22990 Training Loss: tensor(0.3292)\n",
      "22991 Training Loss: tensor(0.3315)\n",
      "22992 Training Loss: tensor(0.3285)\n",
      "22993 Training Loss: tensor(0.3321)\n",
      "22994 Training Loss: tensor(0.3320)\n",
      "22995 Training Loss: tensor(0.3296)\n",
      "22996 Training Loss: tensor(0.3330)\n",
      "22997 Training Loss: tensor(0.3313)\n",
      "22998 Training Loss: tensor(0.3314)\n",
      "22999 Training Loss: tensor(0.3295)\n",
      "23000 Training Loss: tensor(0.3309)\n",
      "23001 Training Loss: tensor(0.3309)\n",
      "23002 Training Loss: tensor(0.3301)\n",
      "23003 Training Loss: tensor(0.3303)\n",
      "23004 Training Loss: tensor(0.3289)\n",
      "23005 Training Loss: tensor(0.3299)\n",
      "23006 Training Loss: tensor(0.3306)\n",
      "23007 Training Loss: tensor(0.3309)\n",
      "23008 Training Loss: tensor(0.3303)\n",
      "23009 Training Loss: tensor(0.3294)\n",
      "23010 Training Loss: tensor(0.3288)\n",
      "23011 Training Loss: tensor(0.3289)\n",
      "23012 Training Loss: tensor(0.3294)\n",
      "23013 Training Loss: tensor(0.3298)\n",
      "23014 Training Loss: tensor(0.3296)\n",
      "23015 Training Loss: tensor(0.3304)\n",
      "23016 Training Loss: tensor(0.3303)\n",
      "23017 Training Loss: tensor(0.3300)\n",
      "23018 Training Loss: tensor(0.3285)\n",
      "23019 Training Loss: tensor(0.3294)\n",
      "23020 Training Loss: tensor(0.3285)\n",
      "23021 Training Loss: tensor(0.3307)\n",
      "23022 Training Loss: tensor(0.3286)\n",
      "23023 Training Loss: tensor(0.3294)\n",
      "23024 Training Loss: tensor(0.3303)\n",
      "23025 Training Loss: tensor(0.3289)\n",
      "23026 Training Loss: tensor(0.3286)\n",
      "23027 Training Loss: tensor(0.3289)\n",
      "23028 Training Loss: tensor(0.3285)\n",
      "23029 Training Loss: tensor(0.3287)\n",
      "23030 Training Loss: tensor(0.3296)\n",
      "23031 Training Loss: tensor(0.3297)\n",
      "23032 Training Loss: tensor(0.3293)\n",
      "23033 Training Loss: tensor(0.3301)\n",
      "23034 Training Loss: tensor(0.3292)\n",
      "23035 Training Loss: tensor(0.3292)\n",
      "23036 Training Loss: tensor(0.3294)\n",
      "23037 Training Loss: tensor(0.3285)\n",
      "23038 Training Loss: tensor(0.3286)\n",
      "23039 Training Loss: tensor(0.3286)\n",
      "23040 Training Loss: tensor(0.3298)\n",
      "23041 Training Loss: tensor(0.3286)\n",
      "23042 Training Loss: tensor(0.3285)\n",
      "23043 Training Loss: tensor(0.3283)\n",
      "23044 Training Loss: tensor(0.3291)\n",
      "23045 Training Loss: tensor(0.3319)\n",
      "23046 Training Loss: tensor(0.3310)\n",
      "23047 Training Loss: tensor(0.3283)\n",
      "23048 Training Loss: tensor(0.3372)\n",
      "23049 Training Loss: tensor(0.3292)\n",
      "23050 Training Loss: tensor(0.3286)\n",
      "23051 Training Loss: tensor(0.3280)\n",
      "23052 Training Loss: tensor(0.3296)\n",
      "23053 Training Loss: tensor(0.3294)\n",
      "23054 Training Loss: tensor(0.3301)\n",
      "23055 Training Loss: tensor(0.3284)\n",
      "23056 Training Loss: tensor(0.3301)\n",
      "23057 Training Loss: tensor(0.3293)\n",
      "23058 Training Loss: tensor(0.3291)\n",
      "23059 Training Loss: tensor(0.3290)\n",
      "23060 Training Loss: tensor(0.3321)\n",
      "23061 Training Loss: tensor(0.3318)\n",
      "23062 Training Loss: tensor(0.3333)\n",
      "23063 Training Loss: tensor(0.3293)\n",
      "23064 Training Loss: tensor(0.3297)\n",
      "23065 Training Loss: tensor(0.3296)\n",
      "23066 Training Loss: tensor(0.3283)\n",
      "23067 Training Loss: tensor(0.3304)\n",
      "23068 Training Loss: tensor(0.3329)\n",
      "23069 Training Loss: tensor(0.3316)\n",
      "23070 Training Loss: tensor(0.3292)\n",
      "23071 Training Loss: tensor(0.3298)\n",
      "23072 Training Loss: tensor(0.3289)\n",
      "23073 Training Loss: tensor(0.3292)\n",
      "23074 Training Loss: tensor(0.3294)\n",
      "23075 Training Loss: tensor(0.3309)\n",
      "23076 Training Loss: tensor(0.3296)\n",
      "23077 Training Loss: tensor(0.3296)\n",
      "23078 Training Loss: tensor(0.3322)\n",
      "23079 Training Loss: tensor(0.3293)\n",
      "23080 Training Loss: tensor(0.3293)\n",
      "23081 Training Loss: tensor(0.3289)\n",
      "23082 Training Loss: tensor(0.3297)\n",
      "23083 Training Loss: tensor(0.3314)\n",
      "23084 Training Loss: tensor(0.3304)\n",
      "23085 Training Loss: tensor(0.3300)\n",
      "23086 Training Loss: tensor(0.3293)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23087 Training Loss: tensor(0.3301)\n",
      "23088 Training Loss: tensor(0.3306)\n",
      "23089 Training Loss: tensor(0.3291)\n",
      "23090 Training Loss: tensor(0.3290)\n",
      "23091 Training Loss: tensor(0.3290)\n",
      "23092 Training Loss: tensor(0.3296)\n",
      "23093 Training Loss: tensor(0.3284)\n",
      "23094 Training Loss: tensor(0.3287)\n",
      "23095 Training Loss: tensor(0.3334)\n",
      "23096 Training Loss: tensor(0.3289)\n",
      "23097 Training Loss: tensor(0.3301)\n",
      "23098 Training Loss: tensor(0.3289)\n",
      "23099 Training Loss: tensor(0.3292)\n",
      "23100 Training Loss: tensor(0.3303)\n",
      "23101 Training Loss: tensor(0.3282)\n",
      "23102 Training Loss: tensor(0.3298)\n",
      "23103 Training Loss: tensor(0.3295)\n",
      "23104 Training Loss: tensor(0.3287)\n",
      "23105 Training Loss: tensor(0.3298)\n",
      "23106 Training Loss: tensor(0.3291)\n",
      "23107 Training Loss: tensor(0.3306)\n",
      "23108 Training Loss: tensor(0.3286)\n",
      "23109 Training Loss: tensor(0.3294)\n",
      "23110 Training Loss: tensor(0.3283)\n",
      "23111 Training Loss: tensor(0.3314)\n",
      "23112 Training Loss: tensor(0.3296)\n",
      "23113 Training Loss: tensor(0.3290)\n",
      "23114 Training Loss: tensor(0.3304)\n",
      "23115 Training Loss: tensor(0.3297)\n",
      "23116 Training Loss: tensor(0.3285)\n",
      "23117 Training Loss: tensor(0.3283)\n",
      "23118 Training Loss: tensor(0.3293)\n",
      "23119 Training Loss: tensor(0.3302)\n",
      "23120 Training Loss: tensor(0.3293)\n",
      "23121 Training Loss: tensor(0.3283)\n",
      "23122 Training Loss: tensor(0.3296)\n",
      "23123 Training Loss: tensor(0.3300)\n",
      "23124 Training Loss: tensor(0.3307)\n",
      "23125 Training Loss: tensor(0.3298)\n",
      "23126 Training Loss: tensor(0.3286)\n",
      "23127 Training Loss: tensor(0.3292)\n",
      "23128 Training Loss: tensor(0.3286)\n",
      "23129 Training Loss: tensor(0.3293)\n",
      "23130 Training Loss: tensor(0.3286)\n",
      "23131 Training Loss: tensor(0.3287)\n",
      "23132 Training Loss: tensor(0.3285)\n",
      "23133 Training Loss: tensor(0.3284)\n",
      "23134 Training Loss: tensor(0.3284)\n",
      "23135 Training Loss: tensor(0.3284)\n",
      "23136 Training Loss: tensor(0.3280)\n",
      "23137 Training Loss: tensor(0.3301)\n",
      "23138 Training Loss: tensor(0.3302)\n",
      "23139 Training Loss: tensor(0.3352)\n",
      "23140 Training Loss: tensor(0.3294)\n",
      "23141 Training Loss: tensor(0.3284)\n",
      "23142 Training Loss: tensor(0.3291)\n",
      "23143 Training Loss: tensor(0.3310)\n",
      "23144 Training Loss: tensor(0.3299)\n",
      "23145 Training Loss: tensor(0.3288)\n",
      "23146 Training Loss: tensor(0.3301)\n",
      "23147 Training Loss: tensor(0.3299)\n",
      "23148 Training Loss: tensor(0.3320)\n",
      "23149 Training Loss: tensor(0.3294)\n",
      "23150 Training Loss: tensor(0.3291)\n",
      "23151 Training Loss: tensor(0.3291)\n",
      "23152 Training Loss: tensor(0.3297)\n",
      "23153 Training Loss: tensor(0.3296)\n",
      "23154 Training Loss: tensor(0.3290)\n",
      "23155 Training Loss: tensor(0.3287)\n",
      "23156 Training Loss: tensor(0.3285)\n",
      "23157 Training Loss: tensor(0.3291)\n",
      "23158 Training Loss: tensor(0.3294)\n",
      "23159 Training Loss: tensor(0.3286)\n",
      "23160 Training Loss: tensor(0.3299)\n",
      "23161 Training Loss: tensor(0.3340)\n",
      "23162 Training Loss: tensor(0.3301)\n",
      "23163 Training Loss: tensor(0.3284)\n",
      "23164 Training Loss: tensor(0.3305)\n",
      "23165 Training Loss: tensor(0.3294)\n",
      "23166 Training Loss: tensor(0.3332)\n",
      "23167 Training Loss: tensor(0.3305)\n",
      "23168 Training Loss: tensor(0.3297)\n",
      "23169 Training Loss: tensor(0.3287)\n",
      "23170 Training Loss: tensor(0.3299)\n",
      "23171 Training Loss: tensor(0.3287)\n",
      "23172 Training Loss: tensor(0.3306)\n",
      "23173 Training Loss: tensor(0.3290)\n",
      "23174 Training Loss: tensor(0.3313)\n",
      "23175 Training Loss: tensor(0.3305)\n",
      "23176 Training Loss: tensor(0.3322)\n",
      "23177 Training Loss: tensor(0.3291)\n",
      "23178 Training Loss: tensor(0.3302)\n",
      "23179 Training Loss: tensor(0.3298)\n",
      "23180 Training Loss: tensor(0.3302)\n",
      "23181 Training Loss: tensor(0.3316)\n",
      "23182 Training Loss: tensor(0.3289)\n",
      "23183 Training Loss: tensor(0.3291)\n",
      "23184 Training Loss: tensor(0.3282)\n",
      "23185 Training Loss: tensor(0.3295)\n",
      "23186 Training Loss: tensor(0.3293)\n",
      "23187 Training Loss: tensor(0.3292)\n",
      "23188 Training Loss: tensor(0.3290)\n",
      "23189 Training Loss: tensor(0.3308)\n",
      "23190 Training Loss: tensor(0.3292)\n",
      "23191 Training Loss: tensor(0.3290)\n",
      "23192 Training Loss: tensor(0.3299)\n",
      "23193 Training Loss: tensor(0.3295)\n",
      "23194 Training Loss: tensor(0.3286)\n",
      "23195 Training Loss: tensor(0.3343)\n",
      "23196 Training Loss: tensor(0.3285)\n",
      "23197 Training Loss: tensor(0.3285)\n",
      "23198 Training Loss: tensor(0.3280)\n",
      "23199 Training Loss: tensor(0.3285)\n",
      "23200 Training Loss: tensor(0.3298)\n",
      "23201 Training Loss: tensor(0.3288)\n",
      "23202 Training Loss: tensor(0.3293)\n",
      "23203 Training Loss: tensor(0.3301)\n",
      "23204 Training Loss: tensor(0.3304)\n",
      "23205 Training Loss: tensor(0.3305)\n",
      "23206 Training Loss: tensor(0.3309)\n",
      "23207 Training Loss: tensor(0.3294)\n",
      "23208 Training Loss: tensor(0.3295)\n",
      "23209 Training Loss: tensor(0.3302)\n",
      "23210 Training Loss: tensor(0.3280)\n",
      "23211 Training Loss: tensor(0.3292)\n",
      "23212 Training Loss: tensor(0.3307)\n",
      "23213 Training Loss: tensor(0.3315)\n",
      "23214 Training Loss: tensor(0.3287)\n",
      "23215 Training Loss: tensor(0.3294)\n",
      "23216 Training Loss: tensor(0.3295)\n",
      "23217 Training Loss: tensor(0.3296)\n",
      "23218 Training Loss: tensor(0.3302)\n",
      "23219 Training Loss: tensor(0.3293)\n",
      "23220 Training Loss: tensor(0.3296)\n",
      "23221 Training Loss: tensor(0.3284)\n",
      "23222 Training Loss: tensor(0.3290)\n",
      "23223 Training Loss: tensor(0.3301)\n",
      "23224 Training Loss: tensor(0.3286)\n",
      "23225 Training Loss: tensor(0.3332)\n",
      "23226 Training Loss: tensor(0.3320)\n",
      "23227 Training Loss: tensor(0.3339)\n",
      "23228 Training Loss: tensor(0.3308)\n",
      "23229 Training Loss: tensor(0.3298)\n",
      "23230 Training Loss: tensor(0.3324)\n",
      "23231 Training Loss: tensor(0.3304)\n",
      "23232 Training Loss: tensor(0.3307)\n",
      "23233 Training Loss: tensor(0.3301)\n",
      "23234 Training Loss: tensor(0.3295)\n",
      "23235 Training Loss: tensor(0.3322)\n",
      "23236 Training Loss: tensor(0.3295)\n",
      "23237 Training Loss: tensor(0.3294)\n",
      "23238 Training Loss: tensor(0.3298)\n",
      "23239 Training Loss: tensor(0.3289)\n",
      "23240 Training Loss: tensor(0.3284)\n",
      "23241 Training Loss: tensor(0.3286)\n",
      "23242 Training Loss: tensor(0.3300)\n",
      "23243 Training Loss: tensor(0.3293)\n",
      "23244 Training Loss: tensor(0.3309)\n",
      "23245 Training Loss: tensor(0.3305)\n",
      "23246 Training Loss: tensor(0.3298)\n",
      "23247 Training Loss: tensor(0.3290)\n",
      "23248 Training Loss: tensor(0.3313)\n",
      "23249 Training Loss: tensor(0.3295)\n",
      "23250 Training Loss: tensor(0.3292)\n",
      "23251 Training Loss: tensor(0.3296)\n",
      "23252 Training Loss: tensor(0.3290)\n",
      "23253 Training Loss: tensor(0.3288)\n",
      "23254 Training Loss: tensor(0.3287)\n",
      "23255 Training Loss: tensor(0.3283)\n",
      "23256 Training Loss: tensor(0.3293)\n",
      "23257 Training Loss: tensor(0.3286)\n",
      "23258 Training Loss: tensor(0.3284)\n",
      "23259 Training Loss: tensor(0.3310)\n",
      "23260 Training Loss: tensor(0.3317)\n",
      "23261 Training Loss: tensor(0.3299)\n",
      "23262 Training Loss: tensor(0.3291)\n",
      "23263 Training Loss: tensor(0.3297)\n",
      "23264 Training Loss: tensor(0.3317)\n",
      "23265 Training Loss: tensor(0.3289)\n",
      "23266 Training Loss: tensor(0.3294)\n",
      "23267 Training Loss: tensor(0.3298)\n",
      "23268 Training Loss: tensor(0.3297)\n",
      "23269 Training Loss: tensor(0.3285)\n",
      "23270 Training Loss: tensor(0.3283)\n",
      "23271 Training Loss: tensor(0.3312)\n",
      "23272 Training Loss: tensor(0.3286)\n",
      "23273 Training Loss: tensor(0.3338)\n",
      "23274 Training Loss: tensor(0.3298)\n",
      "23275 Training Loss: tensor(0.3330)\n",
      "23276 Training Loss: tensor(0.3295)\n",
      "23277 Training Loss: tensor(0.3294)\n",
      "23278 Training Loss: tensor(0.3289)\n",
      "23279 Training Loss: tensor(0.3292)\n",
      "23280 Training Loss: tensor(0.3300)\n",
      "23281 Training Loss: tensor(0.3299)\n",
      "23282 Training Loss: tensor(0.3296)\n",
      "23283 Training Loss: tensor(0.3291)\n",
      "23284 Training Loss: tensor(0.3287)\n",
      "23285 Training Loss: tensor(0.3299)\n",
      "23286 Training Loss: tensor(0.3313)\n",
      "23287 Training Loss: tensor(0.3286)\n",
      "23288 Training Loss: tensor(0.3305)\n",
      "23289 Training Loss: tensor(0.3297)\n",
      "23290 Training Loss: tensor(0.3310)\n",
      "23291 Training Loss: tensor(0.3281)\n",
      "23292 Training Loss: tensor(0.3287)\n",
      "23293 Training Loss: tensor(0.3295)\n",
      "23294 Training Loss: tensor(0.3285)\n",
      "23295 Training Loss: tensor(0.3314)\n",
      "23296 Training Loss: tensor(0.3292)\n",
      "23297 Training Loss: tensor(0.3286)\n",
      "23298 Training Loss: tensor(0.3290)\n",
      "23299 Training Loss: tensor(0.3293)\n",
      "23300 Training Loss: tensor(0.3295)\n",
      "23301 Training Loss: tensor(0.3285)\n",
      "23302 Training Loss: tensor(0.3295)\n",
      "23303 Training Loss: tensor(0.3297)\n",
      "23304 Training Loss: tensor(0.3283)\n",
      "23305 Training Loss: tensor(0.3295)\n",
      "23306 Training Loss: tensor(0.3298)\n",
      "23307 Training Loss: tensor(0.3286)\n",
      "23308 Training Loss: tensor(0.3286)\n",
      "23309 Training Loss: tensor(0.3305)\n",
      "23310 Training Loss: tensor(0.3287)\n",
      "23311 Training Loss: tensor(0.3290)\n",
      "23312 Training Loss: tensor(0.3289)\n",
      "23313 Training Loss: tensor(0.3285)\n",
      "23314 Training Loss: tensor(0.3317)\n",
      "23315 Training Loss: tensor(0.3283)\n",
      "23316 Training Loss: tensor(0.3279)\n",
      "23317 Training Loss: tensor(0.3295)\n",
      "23318 Training Loss: tensor(0.3286)\n",
      "23319 Training Loss: tensor(0.3296)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23320 Training Loss: tensor(0.3284)\n",
      "23321 Training Loss: tensor(0.3282)\n",
      "23322 Training Loss: tensor(0.3290)\n",
      "23323 Training Loss: tensor(0.3289)\n",
      "23324 Training Loss: tensor(0.3293)\n",
      "23325 Training Loss: tensor(0.3296)\n",
      "23326 Training Loss: tensor(0.3295)\n",
      "23327 Training Loss: tensor(0.3296)\n",
      "23328 Training Loss: tensor(0.3295)\n",
      "23329 Training Loss: tensor(0.3291)\n",
      "23330 Training Loss: tensor(0.3288)\n",
      "23331 Training Loss: tensor(0.3281)\n",
      "23332 Training Loss: tensor(0.3296)\n",
      "23333 Training Loss: tensor(0.3295)\n",
      "23334 Training Loss: tensor(0.3289)\n",
      "23335 Training Loss: tensor(0.3303)\n",
      "23336 Training Loss: tensor(0.3319)\n",
      "23337 Training Loss: tensor(0.3282)\n",
      "23338 Training Loss: tensor(0.3299)\n",
      "23339 Training Loss: tensor(0.3287)\n",
      "23340 Training Loss: tensor(0.3286)\n",
      "23341 Training Loss: tensor(0.3311)\n",
      "23342 Training Loss: tensor(0.3326)\n",
      "23343 Training Loss: tensor(0.3293)\n",
      "23344 Training Loss: tensor(0.3280)\n",
      "23345 Training Loss: tensor(0.3278)\n",
      "23346 Training Loss: tensor(0.3292)\n",
      "23347 Training Loss: tensor(0.3293)\n",
      "23348 Training Loss: tensor(0.3297)\n",
      "23349 Training Loss: tensor(0.3305)\n",
      "23350 Training Loss: tensor(0.3278)\n",
      "23351 Training Loss: tensor(0.3286)\n",
      "23352 Training Loss: tensor(0.3298)\n",
      "23353 Training Loss: tensor(0.3280)\n",
      "23354 Training Loss: tensor(0.3302)\n",
      "23355 Training Loss: tensor(0.3315)\n",
      "23356 Training Loss: tensor(0.3286)\n",
      "23357 Training Loss: tensor(0.3293)\n",
      "23358 Training Loss: tensor(0.3283)\n",
      "23359 Training Loss: tensor(0.3285)\n",
      "23360 Training Loss: tensor(0.3285)\n",
      "23361 Training Loss: tensor(0.3287)\n",
      "23362 Training Loss: tensor(0.3287)\n",
      "23363 Training Loss: tensor(0.3322)\n",
      "23364 Training Loss: tensor(0.3284)\n",
      "23365 Training Loss: tensor(0.3294)\n",
      "23366 Training Loss: tensor(0.3286)\n",
      "23367 Training Loss: tensor(0.3285)\n",
      "23368 Training Loss: tensor(0.3302)\n",
      "23369 Training Loss: tensor(0.3293)\n",
      "23370 Training Loss: tensor(0.3292)\n",
      "23371 Training Loss: tensor(0.3318)\n",
      "23372 Training Loss: tensor(0.3290)\n",
      "23373 Training Loss: tensor(0.3297)\n",
      "23374 Training Loss: tensor(0.3285)\n",
      "23375 Training Loss: tensor(0.3293)\n",
      "23376 Training Loss: tensor(0.3300)\n",
      "23377 Training Loss: tensor(0.3292)\n",
      "23378 Training Loss: tensor(0.3280)\n",
      "23379 Training Loss: tensor(0.3313)\n",
      "23380 Training Loss: tensor(0.3308)\n",
      "23381 Training Loss: tensor(0.3291)\n",
      "23382 Training Loss: tensor(0.3281)\n",
      "23383 Training Loss: tensor(0.3305)\n",
      "23384 Training Loss: tensor(0.3294)\n",
      "23385 Training Loss: tensor(0.3306)\n",
      "23386 Training Loss: tensor(0.3290)\n",
      "23387 Training Loss: tensor(0.3314)\n",
      "23388 Training Loss: tensor(0.3285)\n",
      "23389 Training Loss: tensor(0.3326)\n",
      "23390 Training Loss: tensor(0.3298)\n",
      "23391 Training Loss: tensor(0.3304)\n",
      "23392 Training Loss: tensor(0.3297)\n",
      "23393 Training Loss: tensor(0.3286)\n",
      "23394 Training Loss: tensor(0.3290)\n",
      "23395 Training Loss: tensor(0.3301)\n",
      "23396 Training Loss: tensor(0.3286)\n",
      "23397 Training Loss: tensor(0.3297)\n",
      "23398 Training Loss: tensor(0.3326)\n",
      "23399 Training Loss: tensor(0.3294)\n",
      "23400 Training Loss: tensor(0.3325)\n",
      "23401 Training Loss: tensor(0.3286)\n",
      "23402 Training Loss: tensor(0.3283)\n",
      "23403 Training Loss: tensor(0.3322)\n",
      "23404 Training Loss: tensor(0.3314)\n",
      "23405 Training Loss: tensor(0.3306)\n",
      "23406 Training Loss: tensor(0.3308)\n",
      "23407 Training Loss: tensor(0.3303)\n",
      "23408 Training Loss: tensor(0.3299)\n",
      "23409 Training Loss: tensor(0.3314)\n",
      "23410 Training Loss: tensor(0.3307)\n",
      "23411 Training Loss: tensor(0.3290)\n",
      "23412 Training Loss: tensor(0.3303)\n",
      "23413 Training Loss: tensor(0.3307)\n",
      "23414 Training Loss: tensor(0.3288)\n",
      "23415 Training Loss: tensor(0.3300)\n",
      "23416 Training Loss: tensor(0.3311)\n",
      "23417 Training Loss: tensor(0.3288)\n",
      "23418 Training Loss: tensor(0.3296)\n",
      "23419 Training Loss: tensor(0.3312)\n",
      "23420 Training Loss: tensor(0.3285)\n",
      "23421 Training Loss: tensor(0.3289)\n",
      "23422 Training Loss: tensor(0.3292)\n",
      "23423 Training Loss: tensor(0.3309)\n",
      "23424 Training Loss: tensor(0.3311)\n",
      "23425 Training Loss: tensor(0.3301)\n",
      "23426 Training Loss: tensor(0.3316)\n",
      "23427 Training Loss: tensor(0.3325)\n",
      "23428 Training Loss: tensor(0.3290)\n",
      "23429 Training Loss: tensor(0.3292)\n",
      "23430 Training Loss: tensor(0.3296)\n",
      "23431 Training Loss: tensor(0.3291)\n",
      "23432 Training Loss: tensor(0.3308)\n",
      "23433 Training Loss: tensor(0.3297)\n",
      "23434 Training Loss: tensor(0.3298)\n",
      "23435 Training Loss: tensor(0.3296)\n",
      "23436 Training Loss: tensor(0.3289)\n",
      "23437 Training Loss: tensor(0.3294)\n",
      "23438 Training Loss: tensor(0.3301)\n",
      "23439 Training Loss: tensor(0.3305)\n",
      "23440 Training Loss: tensor(0.3296)\n",
      "23441 Training Loss: tensor(0.3289)\n",
      "23442 Training Loss: tensor(0.3284)\n",
      "23443 Training Loss: tensor(0.3283)\n",
      "23444 Training Loss: tensor(0.3288)\n",
      "23445 Training Loss: tensor(0.3290)\n",
      "23446 Training Loss: tensor(0.3288)\n",
      "23447 Training Loss: tensor(0.3283)\n",
      "23448 Training Loss: tensor(0.3305)\n",
      "23449 Training Loss: tensor(0.3302)\n",
      "23450 Training Loss: tensor(0.3312)\n",
      "23451 Training Loss: tensor(0.3284)\n",
      "23452 Training Loss: tensor(0.3291)\n",
      "23453 Training Loss: tensor(0.3317)\n",
      "23454 Training Loss: tensor(0.3293)\n",
      "23455 Training Loss: tensor(0.3286)\n",
      "23456 Training Loss: tensor(0.3285)\n",
      "23457 Training Loss: tensor(0.3297)\n",
      "23458 Training Loss: tensor(0.3290)\n",
      "23459 Training Loss: tensor(0.3298)\n",
      "23460 Training Loss: tensor(0.3307)\n",
      "23461 Training Loss: tensor(0.3279)\n",
      "23462 Training Loss: tensor(0.3295)\n",
      "23463 Training Loss: tensor(0.3289)\n",
      "23464 Training Loss: tensor(0.3284)\n",
      "23465 Training Loss: tensor(0.3288)\n",
      "23466 Training Loss: tensor(0.3282)\n",
      "23467 Training Loss: tensor(0.3291)\n",
      "23468 Training Loss: tensor(0.3281)\n",
      "23469 Training Loss: tensor(0.3284)\n",
      "23470 Training Loss: tensor(0.3299)\n",
      "23471 Training Loss: tensor(0.3291)\n",
      "23472 Training Loss: tensor(0.3319)\n",
      "23473 Training Loss: tensor(0.3279)\n",
      "23474 Training Loss: tensor(0.3292)\n",
      "23475 Training Loss: tensor(0.3284)\n",
      "23476 Training Loss: tensor(0.3318)\n",
      "23477 Training Loss: tensor(0.3302)\n",
      "23478 Training Loss: tensor(0.3289)\n",
      "23479 Training Loss: tensor(0.3306)\n",
      "23480 Training Loss: tensor(0.3286)\n",
      "23481 Training Loss: tensor(0.3308)\n",
      "23482 Training Loss: tensor(0.3291)\n",
      "23483 Training Loss: tensor(0.3313)\n",
      "23484 Training Loss: tensor(0.3293)\n",
      "23485 Training Loss: tensor(0.3296)\n",
      "23486 Training Loss: tensor(0.3286)\n",
      "23487 Training Loss: tensor(0.3305)\n",
      "23488 Training Loss: tensor(0.3298)\n",
      "23489 Training Loss: tensor(0.3316)\n",
      "23490 Training Loss: tensor(0.3285)\n",
      "23491 Training Loss: tensor(0.3282)\n",
      "23492 Training Loss: tensor(0.3287)\n",
      "23493 Training Loss: tensor(0.3278)\n",
      "23494 Training Loss: tensor(0.3282)\n",
      "23495 Training Loss: tensor(0.3289)\n",
      "23496 Training Loss: tensor(0.3309)\n",
      "23497 Training Loss: tensor(0.3284)\n",
      "23498 Training Loss: tensor(0.3286)\n",
      "23499 Training Loss: tensor(0.3286)\n",
      "23500 Training Loss: tensor(0.3286)\n",
      "23501 Training Loss: tensor(0.3302)\n",
      "23502 Training Loss: tensor(0.3301)\n",
      "23503 Training Loss: tensor(0.3296)\n",
      "23504 Training Loss: tensor(0.3288)\n",
      "23505 Training Loss: tensor(0.3310)\n",
      "23506 Training Loss: tensor(0.3294)\n",
      "23507 Training Loss: tensor(0.3288)\n",
      "23508 Training Loss: tensor(0.3285)\n",
      "23509 Training Loss: tensor(0.3302)\n",
      "23510 Training Loss: tensor(0.3292)\n",
      "23511 Training Loss: tensor(0.3294)\n",
      "23512 Training Loss: tensor(0.3282)\n",
      "23513 Training Loss: tensor(0.3298)\n",
      "23514 Training Loss: tensor(0.3338)\n",
      "23515 Training Loss: tensor(0.3304)\n",
      "23516 Training Loss: tensor(0.3298)\n",
      "23517 Training Loss: tensor(0.3308)\n",
      "23518 Training Loss: tensor(0.3305)\n",
      "23519 Training Loss: tensor(0.3290)\n",
      "23520 Training Loss: tensor(0.3300)\n",
      "23521 Training Loss: tensor(0.3295)\n",
      "23522 Training Loss: tensor(0.3287)\n",
      "23523 Training Loss: tensor(0.3291)\n",
      "23524 Training Loss: tensor(0.3294)\n",
      "23525 Training Loss: tensor(0.3285)\n",
      "23526 Training Loss: tensor(0.3288)\n",
      "23527 Training Loss: tensor(0.3295)\n",
      "23528 Training Loss: tensor(0.3299)\n",
      "23529 Training Loss: tensor(0.3284)\n",
      "23530 Training Loss: tensor(0.3297)\n",
      "23531 Training Loss: tensor(0.3282)\n",
      "23532 Training Loss: tensor(0.3320)\n",
      "23533 Training Loss: tensor(0.3296)\n",
      "23534 Training Loss: tensor(0.3291)\n",
      "23535 Training Loss: tensor(0.3298)\n",
      "23536 Training Loss: tensor(0.3292)\n",
      "23537 Training Loss: tensor(0.3287)\n",
      "23538 Training Loss: tensor(0.3292)\n",
      "23539 Training Loss: tensor(0.3285)\n",
      "23540 Training Loss: tensor(0.3297)\n",
      "23541 Training Loss: tensor(0.3320)\n",
      "23542 Training Loss: tensor(0.3276)\n",
      "23543 Training Loss: tensor(0.3293)\n",
      "23544 Training Loss: tensor(0.3306)\n",
      "23545 Training Loss: tensor(0.3300)\n",
      "23546 Training Loss: tensor(0.3280)\n",
      "23547 Training Loss: tensor(0.3292)\n",
      "23548 Training Loss: tensor(0.3303)\n",
      "23549 Training Loss: tensor(0.3310)\n",
      "23550 Training Loss: tensor(0.3289)\n",
      "23551 Training Loss: tensor(0.3288)\n",
      "23552 Training Loss: tensor(0.3287)\n",
      "23553 Training Loss: tensor(0.3284)\n",
      "23554 Training Loss: tensor(0.3295)\n",
      "23555 Training Loss: tensor(0.3281)\n",
      "23556 Training Loss: tensor(0.3337)\n",
      "23557 Training Loss: tensor(0.3283)\n",
      "23558 Training Loss: tensor(0.3281)\n",
      "23559 Training Loss: tensor(0.3285)\n",
      "23560 Training Loss: tensor(0.3285)\n",
      "23561 Training Loss: tensor(0.3280)\n",
      "23562 Training Loss: tensor(0.3297)\n",
      "23563 Training Loss: tensor(0.3298)\n",
      "23564 Training Loss: tensor(0.3302)\n",
      "23565 Training Loss: tensor(0.3292)\n",
      "23566 Training Loss: tensor(0.3301)\n",
      "23567 Training Loss: tensor(0.3304)\n",
      "23568 Training Loss: tensor(0.3288)\n",
      "23569 Training Loss: tensor(0.3287)\n",
      "23570 Training Loss: tensor(0.3317)\n",
      "23571 Training Loss: tensor(0.3279)\n",
      "23572 Training Loss: tensor(0.3293)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23573 Training Loss: tensor(0.3283)\n",
      "23574 Training Loss: tensor(0.3288)\n",
      "23575 Training Loss: tensor(0.3288)\n",
      "23576 Training Loss: tensor(0.3284)\n",
      "23577 Training Loss: tensor(0.3296)\n",
      "23578 Training Loss: tensor(0.3324)\n",
      "23579 Training Loss: tensor(0.3287)\n",
      "23580 Training Loss: tensor(0.3309)\n",
      "23581 Training Loss: tensor(0.3275)\n",
      "23582 Training Loss: tensor(0.3297)\n",
      "23583 Training Loss: tensor(0.3295)\n",
      "23584 Training Loss: tensor(0.3288)\n",
      "23585 Training Loss: tensor(0.3290)\n",
      "23586 Training Loss: tensor(0.3294)\n",
      "23587 Training Loss: tensor(0.3296)\n",
      "23588 Training Loss: tensor(0.3279)\n",
      "23589 Training Loss: tensor(0.3291)\n",
      "23590 Training Loss: tensor(0.3288)\n",
      "23591 Training Loss: tensor(0.3314)\n",
      "23592 Training Loss: tensor(0.3280)\n",
      "23593 Training Loss: tensor(0.3280)\n",
      "23594 Training Loss: tensor(0.3284)\n",
      "23595 Training Loss: tensor(0.3295)\n",
      "23596 Training Loss: tensor(0.3303)\n",
      "23597 Training Loss: tensor(0.3313)\n",
      "23598 Training Loss: tensor(0.3287)\n",
      "23599 Training Loss: tensor(0.3288)\n",
      "23600 Training Loss: tensor(0.3284)\n",
      "23601 Training Loss: tensor(0.3291)\n",
      "23602 Training Loss: tensor(0.3302)\n",
      "23603 Training Loss: tensor(0.3291)\n",
      "23604 Training Loss: tensor(0.3295)\n",
      "23605 Training Loss: tensor(0.3295)\n",
      "23606 Training Loss: tensor(0.3287)\n",
      "23607 Training Loss: tensor(0.3290)\n",
      "23608 Training Loss: tensor(0.3286)\n",
      "23609 Training Loss: tensor(0.3277)\n",
      "23610 Training Loss: tensor(0.3285)\n",
      "23611 Training Loss: tensor(0.3287)\n",
      "23612 Training Loss: tensor(0.3292)\n",
      "23613 Training Loss: tensor(0.3299)\n",
      "23614 Training Loss: tensor(0.3296)\n",
      "23615 Training Loss: tensor(0.3288)\n",
      "23616 Training Loss: tensor(0.3298)\n",
      "23617 Training Loss: tensor(0.3285)\n",
      "23618 Training Loss: tensor(0.3290)\n",
      "23619 Training Loss: tensor(0.3283)\n",
      "23620 Training Loss: tensor(0.3286)\n",
      "23621 Training Loss: tensor(0.3291)\n",
      "23622 Training Loss: tensor(0.3282)\n",
      "23623 Training Loss: tensor(0.3286)\n",
      "23624 Training Loss: tensor(0.3293)\n",
      "23625 Training Loss: tensor(0.3289)\n",
      "23626 Training Loss: tensor(0.3282)\n",
      "23627 Training Loss: tensor(0.3283)\n",
      "23628 Training Loss: tensor(0.3286)\n",
      "23629 Training Loss: tensor(0.3283)\n",
      "23630 Training Loss: tensor(0.3285)\n",
      "23631 Training Loss: tensor(0.3282)\n",
      "23632 Training Loss: tensor(0.3286)\n",
      "23633 Training Loss: tensor(0.3290)\n",
      "23634 Training Loss: tensor(0.3305)\n",
      "23635 Training Loss: tensor(0.3284)\n",
      "23636 Training Loss: tensor(0.3301)\n",
      "23637 Training Loss: tensor(0.3283)\n",
      "23638 Training Loss: tensor(0.3294)\n",
      "23639 Training Loss: tensor(0.3319)\n",
      "23640 Training Loss: tensor(0.3287)\n",
      "23641 Training Loss: tensor(0.3323)\n",
      "23642 Training Loss: tensor(0.3293)\n",
      "23643 Training Loss: tensor(0.3310)\n",
      "23644 Training Loss: tensor(0.3289)\n",
      "23645 Training Loss: tensor(0.3279)\n",
      "23646 Training Loss: tensor(0.3291)\n",
      "23647 Training Loss: tensor(0.3298)\n",
      "23648 Training Loss: tensor(0.3307)\n",
      "23649 Training Loss: tensor(0.3301)\n",
      "23650 Training Loss: tensor(0.3290)\n",
      "23651 Training Loss: tensor(0.3287)\n",
      "23652 Training Loss: tensor(0.3282)\n",
      "23653 Training Loss: tensor(0.3287)\n",
      "23654 Training Loss: tensor(0.3294)\n",
      "23655 Training Loss: tensor(0.3282)\n",
      "23656 Training Loss: tensor(0.3292)\n",
      "23657 Training Loss: tensor(0.3306)\n",
      "23658 Training Loss: tensor(0.3279)\n",
      "23659 Training Loss: tensor(0.3280)\n",
      "23660 Training Loss: tensor(0.3297)\n",
      "23661 Training Loss: tensor(0.3298)\n",
      "23662 Training Loss: tensor(0.3292)\n",
      "23663 Training Loss: tensor(0.3295)\n",
      "23664 Training Loss: tensor(0.3282)\n",
      "23665 Training Loss: tensor(0.3303)\n",
      "23666 Training Loss: tensor(0.3285)\n",
      "23667 Training Loss: tensor(0.3285)\n",
      "23668 Training Loss: tensor(0.3286)\n",
      "23669 Training Loss: tensor(0.3280)\n",
      "23670 Training Loss: tensor(0.3304)\n",
      "23671 Training Loss: tensor(0.3288)\n",
      "23672 Training Loss: tensor(0.3300)\n",
      "23673 Training Loss: tensor(0.3288)\n",
      "23674 Training Loss: tensor(0.3290)\n",
      "23675 Training Loss: tensor(0.3286)\n",
      "23676 Training Loss: tensor(0.3313)\n",
      "23677 Training Loss: tensor(0.3310)\n",
      "23678 Training Loss: tensor(0.3279)\n",
      "23679 Training Loss: tensor(0.3293)\n",
      "23680 Training Loss: tensor(0.3288)\n",
      "23681 Training Loss: tensor(0.3285)\n",
      "23682 Training Loss: tensor(0.3285)\n",
      "23683 Training Loss: tensor(0.3305)\n",
      "23684 Training Loss: tensor(0.3291)\n",
      "23685 Training Loss: tensor(0.3283)\n",
      "23686 Training Loss: tensor(0.3291)\n",
      "23687 Training Loss: tensor(0.3296)\n",
      "23688 Training Loss: tensor(0.3318)\n",
      "23689 Training Loss: tensor(0.3304)\n",
      "23690 Training Loss: tensor(0.3294)\n",
      "23691 Training Loss: tensor(0.3316)\n",
      "23692 Training Loss: tensor(0.3291)\n",
      "23693 Training Loss: tensor(0.3287)\n",
      "23694 Training Loss: tensor(0.3303)\n",
      "23695 Training Loss: tensor(0.3292)\n",
      "23696 Training Loss: tensor(0.3294)\n",
      "23697 Training Loss: tensor(0.3298)\n",
      "23698 Training Loss: tensor(0.3281)\n",
      "23699 Training Loss: tensor(0.3283)\n",
      "23700 Training Loss: tensor(0.3292)\n",
      "23701 Training Loss: tensor(0.3285)\n",
      "23702 Training Loss: tensor(0.3280)\n",
      "23703 Training Loss: tensor(0.3291)\n",
      "23704 Training Loss: tensor(0.3282)\n",
      "23705 Training Loss: tensor(0.3297)\n",
      "23706 Training Loss: tensor(0.3312)\n",
      "23707 Training Loss: tensor(0.3286)\n",
      "23708 Training Loss: tensor(0.3281)\n",
      "23709 Training Loss: tensor(0.3298)\n",
      "23710 Training Loss: tensor(0.3289)\n",
      "23711 Training Loss: tensor(0.3318)\n",
      "23712 Training Loss: tensor(0.3294)\n",
      "23713 Training Loss: tensor(0.3287)\n",
      "23714 Training Loss: tensor(0.3279)\n",
      "23715 Training Loss: tensor(0.3289)\n",
      "23716 Training Loss: tensor(0.3284)\n",
      "23717 Training Loss: tensor(0.3282)\n",
      "23718 Training Loss: tensor(0.3307)\n",
      "23719 Training Loss: tensor(0.3279)\n",
      "23720 Training Loss: tensor(0.3299)\n",
      "23721 Training Loss: tensor(0.3294)\n",
      "23722 Training Loss: tensor(0.3294)\n",
      "23723 Training Loss: tensor(0.3281)\n",
      "23724 Training Loss: tensor(0.3284)\n",
      "23725 Training Loss: tensor(0.3285)\n",
      "23726 Training Loss: tensor(0.3306)\n",
      "23727 Training Loss: tensor(0.3305)\n",
      "23728 Training Loss: tensor(0.3281)\n",
      "23729 Training Loss: tensor(0.3280)\n",
      "23730 Training Loss: tensor(0.3279)\n",
      "23731 Training Loss: tensor(0.3281)\n",
      "23732 Training Loss: tensor(0.3307)\n",
      "23733 Training Loss: tensor(0.3284)\n",
      "23734 Training Loss: tensor(0.3292)\n",
      "23735 Training Loss: tensor(0.3294)\n",
      "23736 Training Loss: tensor(0.3287)\n",
      "23737 Training Loss: tensor(0.3294)\n",
      "23738 Training Loss: tensor(0.3281)\n",
      "23739 Training Loss: tensor(0.3330)\n",
      "23740 Training Loss: tensor(0.3277)\n",
      "23741 Training Loss: tensor(0.3285)\n",
      "23742 Training Loss: tensor(0.3313)\n",
      "23743 Training Loss: tensor(0.3292)\n",
      "23744 Training Loss: tensor(0.3300)\n",
      "23745 Training Loss: tensor(0.3298)\n",
      "23746 Training Loss: tensor(0.3294)\n",
      "23747 Training Loss: tensor(0.3305)\n",
      "23748 Training Loss: tensor(0.3282)\n",
      "23749 Training Loss: tensor(0.3299)\n",
      "23750 Training Loss: tensor(0.3291)\n",
      "23751 Training Loss: tensor(0.3290)\n",
      "23752 Training Loss: tensor(0.3280)\n",
      "23753 Training Loss: tensor(0.3278)\n",
      "23754 Training Loss: tensor(0.3287)\n",
      "23755 Training Loss: tensor(0.3305)\n",
      "23756 Training Loss: tensor(0.3289)\n",
      "23757 Training Loss: tensor(0.3285)\n",
      "23758 Training Loss: tensor(0.3296)\n",
      "23759 Training Loss: tensor(0.3284)\n",
      "23760 Training Loss: tensor(0.3284)\n",
      "23761 Training Loss: tensor(0.3293)\n",
      "23762 Training Loss: tensor(0.3302)\n",
      "23763 Training Loss: tensor(0.3292)\n",
      "23764 Training Loss: tensor(0.3284)\n",
      "23765 Training Loss: tensor(0.3294)\n",
      "23766 Training Loss: tensor(0.3296)\n",
      "23767 Training Loss: tensor(0.3296)\n",
      "23768 Training Loss: tensor(0.3296)\n",
      "23769 Training Loss: tensor(0.3288)\n",
      "23770 Training Loss: tensor(0.3286)\n",
      "23771 Training Loss: tensor(0.3310)\n",
      "23772 Training Loss: tensor(0.3279)\n",
      "23773 Training Loss: tensor(0.3293)\n",
      "23774 Training Loss: tensor(0.3321)\n",
      "23775 Training Loss: tensor(0.3313)\n",
      "23776 Training Loss: tensor(0.3330)\n",
      "23777 Training Loss: tensor(0.3285)\n",
      "23778 Training Loss: tensor(0.3293)\n",
      "23779 Training Loss: tensor(0.3291)\n",
      "23780 Training Loss: tensor(0.3294)\n",
      "23781 Training Loss: tensor(0.3287)\n",
      "23782 Training Loss: tensor(0.3294)\n",
      "23783 Training Loss: tensor(0.3287)\n",
      "23784 Training Loss: tensor(0.3287)\n",
      "23785 Training Loss: tensor(0.3283)\n",
      "23786 Training Loss: tensor(0.3282)\n",
      "23787 Training Loss: tensor(0.3296)\n",
      "23788 Training Loss: tensor(0.3287)\n",
      "23789 Training Loss: tensor(0.3280)\n",
      "23790 Training Loss: tensor(0.3283)\n",
      "23791 Training Loss: tensor(0.3285)\n",
      "23792 Training Loss: tensor(0.3283)\n",
      "23793 Training Loss: tensor(0.3284)\n",
      "23794 Training Loss: tensor(0.3306)\n",
      "23795 Training Loss: tensor(0.3281)\n",
      "23796 Training Loss: tensor(0.3331)\n",
      "23797 Training Loss: tensor(0.3280)\n",
      "23798 Training Loss: tensor(0.3281)\n",
      "23799 Training Loss: tensor(0.3309)\n",
      "23800 Training Loss: tensor(0.3296)\n",
      "23801 Training Loss: tensor(0.3296)\n",
      "23802 Training Loss: tensor(0.3292)\n",
      "23803 Training Loss: tensor(0.3301)\n",
      "23804 Training Loss: tensor(0.3298)\n",
      "23805 Training Loss: tensor(0.3302)\n",
      "23806 Training Loss: tensor(0.3291)\n",
      "23807 Training Loss: tensor(0.3284)\n",
      "23808 Training Loss: tensor(0.3289)\n",
      "23809 Training Loss: tensor(0.3285)\n",
      "23810 Training Loss: tensor(0.3307)\n",
      "23811 Training Loss: tensor(0.3284)\n",
      "23812 Training Loss: tensor(0.3286)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23813 Training Loss: tensor(0.3284)\n",
      "23814 Training Loss: tensor(0.3290)\n",
      "23815 Training Loss: tensor(0.3295)\n",
      "23816 Training Loss: tensor(0.3282)\n",
      "23817 Training Loss: tensor(0.3311)\n",
      "23818 Training Loss: tensor(0.3295)\n",
      "23819 Training Loss: tensor(0.3292)\n",
      "23820 Training Loss: tensor(0.3289)\n",
      "23821 Training Loss: tensor(0.3312)\n",
      "23822 Training Loss: tensor(0.3304)\n",
      "23823 Training Loss: tensor(0.3294)\n",
      "23824 Training Loss: tensor(0.3280)\n",
      "23825 Training Loss: tensor(0.3288)\n",
      "23826 Training Loss: tensor(0.3288)\n",
      "23827 Training Loss: tensor(0.3297)\n",
      "23828 Training Loss: tensor(0.3286)\n",
      "23829 Training Loss: tensor(0.3288)\n",
      "23830 Training Loss: tensor(0.3289)\n",
      "23831 Training Loss: tensor(0.3306)\n",
      "23832 Training Loss: tensor(0.3304)\n",
      "23833 Training Loss: tensor(0.3295)\n",
      "23834 Training Loss: tensor(0.3287)\n",
      "23835 Training Loss: tensor(0.3307)\n",
      "23836 Training Loss: tensor(0.3286)\n",
      "23837 Training Loss: tensor(0.3289)\n",
      "23838 Training Loss: tensor(0.3294)\n",
      "23839 Training Loss: tensor(0.3301)\n",
      "23840 Training Loss: tensor(0.3286)\n",
      "23841 Training Loss: tensor(0.3287)\n",
      "23842 Training Loss: tensor(0.3288)\n",
      "23843 Training Loss: tensor(0.3288)\n",
      "23844 Training Loss: tensor(0.3282)\n",
      "23845 Training Loss: tensor(0.3280)\n",
      "23846 Training Loss: tensor(0.3310)\n",
      "23847 Training Loss: tensor(0.3295)\n",
      "23848 Training Loss: tensor(0.3289)\n",
      "23849 Training Loss: tensor(0.3283)\n",
      "23850 Training Loss: tensor(0.3309)\n",
      "23851 Training Loss: tensor(0.3289)\n",
      "23852 Training Loss: tensor(0.3278)\n",
      "23853 Training Loss: tensor(0.3292)\n",
      "23854 Training Loss: tensor(0.3301)\n",
      "23855 Training Loss: tensor(0.3280)\n",
      "23856 Training Loss: tensor(0.3282)\n",
      "23857 Training Loss: tensor(0.3316)\n",
      "23858 Training Loss: tensor(0.3297)\n",
      "23859 Training Loss: tensor(0.3290)\n",
      "23860 Training Loss: tensor(0.3318)\n",
      "23861 Training Loss: tensor(0.3294)\n",
      "23862 Training Loss: tensor(0.3285)\n",
      "23863 Training Loss: tensor(0.3308)\n",
      "23864 Training Loss: tensor(0.3313)\n",
      "23865 Training Loss: tensor(0.3287)\n",
      "23866 Training Loss: tensor(0.3304)\n",
      "23867 Training Loss: tensor(0.3294)\n",
      "23868 Training Loss: tensor(0.3288)\n",
      "23869 Training Loss: tensor(0.3307)\n",
      "23870 Training Loss: tensor(0.3287)\n",
      "23871 Training Loss: tensor(0.3277)\n",
      "23872 Training Loss: tensor(0.3280)\n",
      "23873 Training Loss: tensor(0.3277)\n",
      "23874 Training Loss: tensor(0.3281)\n",
      "23875 Training Loss: tensor(0.3298)\n",
      "23876 Training Loss: tensor(0.3289)\n",
      "23877 Training Loss: tensor(0.3291)\n",
      "23878 Training Loss: tensor(0.3325)\n",
      "23879 Training Loss: tensor(0.3284)\n",
      "23880 Training Loss: tensor(0.3284)\n",
      "23881 Training Loss: tensor(0.3286)\n",
      "23882 Training Loss: tensor(0.3280)\n",
      "23883 Training Loss: tensor(0.3281)\n",
      "23884 Training Loss: tensor(0.3302)\n",
      "23885 Training Loss: tensor(0.3292)\n",
      "23886 Training Loss: tensor(0.3313)\n",
      "23887 Training Loss: tensor(0.3284)\n",
      "23888 Training Loss: tensor(0.3288)\n",
      "23889 Training Loss: tensor(0.3286)\n",
      "23890 Training Loss: tensor(0.3285)\n",
      "23891 Training Loss: tensor(0.3303)\n",
      "23892 Training Loss: tensor(0.3285)\n",
      "23893 Training Loss: tensor(0.3299)\n",
      "23894 Training Loss: tensor(0.3285)\n",
      "23895 Training Loss: tensor(0.3296)\n",
      "23896 Training Loss: tensor(0.3288)\n",
      "23897 Training Loss: tensor(0.3277)\n",
      "23898 Training Loss: tensor(0.3315)\n",
      "23899 Training Loss: tensor(0.3280)\n",
      "23900 Training Loss: tensor(0.3297)\n",
      "23901 Training Loss: tensor(0.3304)\n",
      "23902 Training Loss: tensor(0.3289)\n",
      "23903 Training Loss: tensor(0.3286)\n",
      "23904 Training Loss: tensor(0.3302)\n",
      "23905 Training Loss: tensor(0.3304)\n",
      "23906 Training Loss: tensor(0.3277)\n",
      "23907 Training Loss: tensor(0.3312)\n",
      "23908 Training Loss: tensor(0.3306)\n",
      "23909 Training Loss: tensor(0.3284)\n",
      "23910 Training Loss: tensor(0.3284)\n",
      "23911 Training Loss: tensor(0.3286)\n",
      "23912 Training Loss: tensor(0.3285)\n",
      "23913 Training Loss: tensor(0.3294)\n",
      "23914 Training Loss: tensor(0.3283)\n",
      "23915 Training Loss: tensor(0.3304)\n",
      "23916 Training Loss: tensor(0.3286)\n",
      "23917 Training Loss: tensor(0.3281)\n",
      "23918 Training Loss: tensor(0.3294)\n",
      "23919 Training Loss: tensor(0.3288)\n",
      "23920 Training Loss: tensor(0.3304)\n",
      "23921 Training Loss: tensor(0.3287)\n",
      "23922 Training Loss: tensor(0.3288)\n",
      "23923 Training Loss: tensor(0.3287)\n",
      "23924 Training Loss: tensor(0.3284)\n",
      "23925 Training Loss: tensor(0.3291)\n",
      "23926 Training Loss: tensor(0.3289)\n",
      "23927 Training Loss: tensor(0.3291)\n",
      "23928 Training Loss: tensor(0.3306)\n",
      "23929 Training Loss: tensor(0.3283)\n",
      "23930 Training Loss: tensor(0.3285)\n",
      "23931 Training Loss: tensor(0.3277)\n",
      "23932 Training Loss: tensor(0.3290)\n",
      "23933 Training Loss: tensor(0.3289)\n",
      "23934 Training Loss: tensor(0.3281)\n",
      "23935 Training Loss: tensor(0.3299)\n",
      "23936 Training Loss: tensor(0.3281)\n",
      "23937 Training Loss: tensor(0.3283)\n",
      "23938 Training Loss: tensor(0.3285)\n",
      "23939 Training Loss: tensor(0.3282)\n",
      "23940 Training Loss: tensor(0.3284)\n",
      "23941 Training Loss: tensor(0.3278)\n",
      "23942 Training Loss: tensor(0.3289)\n",
      "23943 Training Loss: tensor(0.3333)\n",
      "23944 Training Loss: tensor(0.3278)\n",
      "23945 Training Loss: tensor(0.3284)\n",
      "23946 Training Loss: tensor(0.3283)\n",
      "23947 Training Loss: tensor(0.3280)\n",
      "23948 Training Loss: tensor(0.3319)\n",
      "23949 Training Loss: tensor(0.3298)\n",
      "23950 Training Loss: tensor(0.3282)\n",
      "23951 Training Loss: tensor(0.3275)\n",
      "23952 Training Loss: tensor(0.3286)\n",
      "23953 Training Loss: tensor(0.3283)\n",
      "23954 Training Loss: tensor(0.3282)\n",
      "23955 Training Loss: tensor(0.3281)\n",
      "23956 Training Loss: tensor(0.3306)\n",
      "23957 Training Loss: tensor(0.3280)\n",
      "23958 Training Loss: tensor(0.3317)\n",
      "23959 Training Loss: tensor(0.3289)\n",
      "23960 Training Loss: tensor(0.3283)\n",
      "23961 Training Loss: tensor(0.3320)\n",
      "23962 Training Loss: tensor(0.3285)\n",
      "23963 Training Loss: tensor(0.3309)\n",
      "23964 Training Loss: tensor(0.3283)\n",
      "23965 Training Loss: tensor(0.3300)\n",
      "23966 Training Loss: tensor(0.3281)\n",
      "23967 Training Loss: tensor(0.3287)\n",
      "23968 Training Loss: tensor(0.3284)\n",
      "23969 Training Loss: tensor(0.3318)\n",
      "23970 Training Loss: tensor(0.3304)\n",
      "23971 Training Loss: tensor(0.3294)\n",
      "23972 Training Loss: tensor(0.3285)\n",
      "23973 Training Loss: tensor(0.3295)\n",
      "23974 Training Loss: tensor(0.3296)\n",
      "23975 Training Loss: tensor(0.3301)\n",
      "23976 Training Loss: tensor(0.3282)\n",
      "23977 Training Loss: tensor(0.3288)\n",
      "23978 Training Loss: tensor(0.3286)\n",
      "23979 Training Loss: tensor(0.3281)\n",
      "23980 Training Loss: tensor(0.3280)\n",
      "23981 Training Loss: tensor(0.3288)\n",
      "23982 Training Loss: tensor(0.3302)\n",
      "23983 Training Loss: tensor(0.3291)\n",
      "23984 Training Loss: tensor(0.3282)\n",
      "23985 Training Loss: tensor(0.3291)\n",
      "23986 Training Loss: tensor(0.3286)\n",
      "23987 Training Loss: tensor(0.3282)\n",
      "23988 Training Loss: tensor(0.3279)\n",
      "23989 Training Loss: tensor(0.3308)\n",
      "23990 Training Loss: tensor(0.3284)\n",
      "23991 Training Loss: tensor(0.3293)\n",
      "23992 Training Loss: tensor(0.3275)\n",
      "23993 Training Loss: tensor(0.3287)\n",
      "23994 Training Loss: tensor(0.3319)\n",
      "23995 Training Loss: tensor(0.3291)\n",
      "23996 Training Loss: tensor(0.3280)\n",
      "23997 Training Loss: tensor(0.3298)\n",
      "23998 Training Loss: tensor(0.3279)\n",
      "23999 Training Loss: tensor(0.3286)\n",
      "24000 Training Loss: tensor(0.3283)\n",
      "24001 Training Loss: tensor(0.3279)\n",
      "24002 Training Loss: tensor(0.3309)\n",
      "24003 Training Loss: tensor(0.3290)\n",
      "24004 Training Loss: tensor(0.3300)\n",
      "24005 Training Loss: tensor(0.3286)\n",
      "24006 Training Loss: tensor(0.3287)\n",
      "24007 Training Loss: tensor(0.3344)\n",
      "24008 Training Loss: tensor(0.3295)\n",
      "24009 Training Loss: tensor(0.3289)\n",
      "24010 Training Loss: tensor(0.3295)\n",
      "24011 Training Loss: tensor(0.3291)\n",
      "24012 Training Loss: tensor(0.3294)\n",
      "24013 Training Loss: tensor(0.3317)\n",
      "24014 Training Loss: tensor(0.3283)\n",
      "24015 Training Loss: tensor(0.3289)\n",
      "24016 Training Loss: tensor(0.3285)\n",
      "24017 Training Loss: tensor(0.3286)\n",
      "24018 Training Loss: tensor(0.3285)\n",
      "24019 Training Loss: tensor(0.3288)\n",
      "24020 Training Loss: tensor(0.3298)\n",
      "24021 Training Loss: tensor(0.3283)\n",
      "24022 Training Loss: tensor(0.3276)\n",
      "24023 Training Loss: tensor(0.3284)\n",
      "24024 Training Loss: tensor(0.3283)\n",
      "24025 Training Loss: tensor(0.3281)\n",
      "24026 Training Loss: tensor(0.3299)\n",
      "24027 Training Loss: tensor(0.3280)\n",
      "24028 Training Loss: tensor(0.3276)\n",
      "24029 Training Loss: tensor(0.3333)\n",
      "24030 Training Loss: tensor(0.3281)\n",
      "24031 Training Loss: tensor(0.3280)\n",
      "24032 Training Loss: tensor(0.3285)\n",
      "24033 Training Loss: tensor(0.3280)\n",
      "24034 Training Loss: tensor(0.3283)\n",
      "24035 Training Loss: tensor(0.3282)\n",
      "24036 Training Loss: tensor(0.3280)\n",
      "24037 Training Loss: tensor(0.3302)\n",
      "24038 Training Loss: tensor(0.3287)\n",
      "24039 Training Loss: tensor(0.3278)\n",
      "24040 Training Loss: tensor(0.3280)\n",
      "24041 Training Loss: tensor(0.3281)\n",
      "24042 Training Loss: tensor(0.3286)\n",
      "24043 Training Loss: tensor(0.3290)\n",
      "24044 Training Loss: tensor(0.3287)\n",
      "24045 Training Loss: tensor(0.3273)\n",
      "24046 Training Loss: tensor(0.3282)\n",
      "24047 Training Loss: tensor(0.3279)\n",
      "24048 Training Loss: tensor(0.3284)\n",
      "24049 Training Loss: tensor(0.3286)\n",
      "24050 Training Loss: tensor(0.3311)\n",
      "24051 Training Loss: tensor(0.3293)\n",
      "24052 Training Loss: tensor(0.3287)\n",
      "24053 Training Loss: tensor(0.3282)\n",
      "24054 Training Loss: tensor(0.3306)\n",
      "24055 Training Loss: tensor(0.3275)\n",
      "24056 Training Loss: tensor(0.3295)\n",
      "24057 Training Loss: tensor(0.3286)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24058 Training Loss: tensor(0.3296)\n",
      "24059 Training Loss: tensor(0.3284)\n",
      "24060 Training Loss: tensor(0.3279)\n",
      "24061 Training Loss: tensor(0.3284)\n",
      "24062 Training Loss: tensor(0.3281)\n",
      "24063 Training Loss: tensor(0.3280)\n",
      "24064 Training Loss: tensor(0.3291)\n",
      "24065 Training Loss: tensor(0.3288)\n",
      "24066 Training Loss: tensor(0.3278)\n",
      "24067 Training Loss: tensor(0.3328)\n",
      "24068 Training Loss: tensor(0.3294)\n",
      "24069 Training Loss: tensor(0.3278)\n",
      "24070 Training Loss: tensor(0.3275)\n",
      "24071 Training Loss: tensor(0.3285)\n",
      "24072 Training Loss: tensor(0.3287)\n",
      "24073 Training Loss: tensor(0.3282)\n",
      "24074 Training Loss: tensor(0.3294)\n",
      "24075 Training Loss: tensor(0.3286)\n",
      "24076 Training Loss: tensor(0.3279)\n",
      "24077 Training Loss: tensor(0.3289)\n",
      "24078 Training Loss: tensor(0.3313)\n",
      "24079 Training Loss: tensor(0.3293)\n",
      "24080 Training Loss: tensor(0.3279)\n",
      "24081 Training Loss: tensor(0.3313)\n",
      "24082 Training Loss: tensor(0.3294)\n",
      "24083 Training Loss: tensor(0.3305)\n",
      "24084 Training Loss: tensor(0.3285)\n",
      "24085 Training Loss: tensor(0.3303)\n",
      "24086 Training Loss: tensor(0.3290)\n",
      "24087 Training Loss: tensor(0.3289)\n",
      "24088 Training Loss: tensor(0.3312)\n",
      "24089 Training Loss: tensor(0.3291)\n",
      "24090 Training Loss: tensor(0.3289)\n",
      "24091 Training Loss: tensor(0.3287)\n",
      "24092 Training Loss: tensor(0.3288)\n",
      "24093 Training Loss: tensor(0.3290)\n",
      "24094 Training Loss: tensor(0.3286)\n",
      "24095 Training Loss: tensor(0.3345)\n",
      "24096 Training Loss: tensor(0.3302)\n",
      "24097 Training Loss: tensor(0.3279)\n",
      "24098 Training Loss: tensor(0.3282)\n",
      "24099 Training Loss: tensor(0.3315)\n",
      "24100 Training Loss: tensor(0.3291)\n",
      "24101 Training Loss: tensor(0.3285)\n",
      "24102 Training Loss: tensor(0.3289)\n",
      "24103 Training Loss: tensor(0.3281)\n",
      "24104 Training Loss: tensor(0.3308)\n",
      "24105 Training Loss: tensor(0.3292)\n",
      "24106 Training Loss: tensor(0.3307)\n",
      "24107 Training Loss: tensor(0.3286)\n",
      "24108 Training Loss: tensor(0.3290)\n",
      "24109 Training Loss: tensor(0.3290)\n",
      "24110 Training Loss: tensor(0.3286)\n",
      "24111 Training Loss: tensor(0.3292)\n",
      "24112 Training Loss: tensor(0.3284)\n",
      "24113 Training Loss: tensor(0.3286)\n",
      "24114 Training Loss: tensor(0.3315)\n",
      "24115 Training Loss: tensor(0.3286)\n",
      "24116 Training Loss: tensor(0.3282)\n",
      "24117 Training Loss: tensor(0.3286)\n",
      "24118 Training Loss: tensor(0.3302)\n",
      "24119 Training Loss: tensor(0.3286)\n",
      "24120 Training Loss: tensor(0.3282)\n",
      "24121 Training Loss: tensor(0.3281)\n",
      "24122 Training Loss: tensor(0.3283)\n",
      "24123 Training Loss: tensor(0.3285)\n",
      "24124 Training Loss: tensor(0.3285)\n",
      "24125 Training Loss: tensor(0.3307)\n",
      "24126 Training Loss: tensor(0.3287)\n",
      "24127 Training Loss: tensor(0.3305)\n",
      "24128 Training Loss: tensor(0.3288)\n",
      "24129 Training Loss: tensor(0.3297)\n",
      "24130 Training Loss: tensor(0.3288)\n",
      "24131 Training Loss: tensor(0.3291)\n",
      "24132 Training Loss: tensor(0.3289)\n",
      "24133 Training Loss: tensor(0.3304)\n",
      "24134 Training Loss: tensor(0.3307)\n",
      "24135 Training Loss: tensor(0.3319)\n",
      "24136 Training Loss: tensor(0.3301)\n",
      "24137 Training Loss: tensor(0.3288)\n",
      "24138 Training Loss: tensor(0.3282)\n",
      "24139 Training Loss: tensor(0.3281)\n",
      "24140 Training Loss: tensor(0.3281)\n",
      "24141 Training Loss: tensor(0.3284)\n",
      "24142 Training Loss: tensor(0.3319)\n",
      "24143 Training Loss: tensor(0.3279)\n",
      "24144 Training Loss: tensor(0.3275)\n",
      "24145 Training Loss: tensor(0.3303)\n",
      "24146 Training Loss: tensor(0.3289)\n",
      "24147 Training Loss: tensor(0.3300)\n",
      "24148 Training Loss: tensor(0.3279)\n",
      "24149 Training Loss: tensor(0.3291)\n",
      "24150 Training Loss: tensor(0.3295)\n",
      "24151 Training Loss: tensor(0.3284)\n",
      "24152 Training Loss: tensor(0.3285)\n",
      "24153 Training Loss: tensor(0.3285)\n",
      "24154 Training Loss: tensor(0.3285)\n",
      "24155 Training Loss: tensor(0.3284)\n",
      "24156 Training Loss: tensor(0.3278)\n",
      "24157 Training Loss: tensor(0.3282)\n",
      "24158 Training Loss: tensor(0.3283)\n",
      "24159 Training Loss: tensor(0.3288)\n",
      "24160 Training Loss: tensor(0.3281)\n",
      "24161 Training Loss: tensor(0.3282)\n",
      "24162 Training Loss: tensor(0.3281)\n",
      "24163 Training Loss: tensor(0.3283)\n",
      "24164 Training Loss: tensor(0.3276)\n",
      "24165 Training Loss: tensor(0.3315)\n",
      "24166 Training Loss: tensor(0.3281)\n",
      "24167 Training Loss: tensor(0.3280)\n",
      "24168 Training Loss: tensor(0.3277)\n",
      "24169 Training Loss: tensor(0.3289)\n",
      "24170 Training Loss: tensor(0.3277)\n",
      "24171 Training Loss: tensor(0.3287)\n",
      "24172 Training Loss: tensor(0.3281)\n",
      "24173 Training Loss: tensor(0.3279)\n",
      "24174 Training Loss: tensor(0.3287)\n",
      "24175 Training Loss: tensor(0.3322)\n",
      "24176 Training Loss: tensor(0.3281)\n",
      "24177 Training Loss: tensor(0.3282)\n",
      "24178 Training Loss: tensor(0.3346)\n",
      "24179 Training Loss: tensor(0.3308)\n",
      "24180 Training Loss: tensor(0.3279)\n",
      "24181 Training Loss: tensor(0.3302)\n",
      "24182 Training Loss: tensor(0.3298)\n",
      "24183 Training Loss: tensor(0.3294)\n",
      "24184 Training Loss: tensor(0.3293)\n",
      "24185 Training Loss: tensor(0.3325)\n",
      "24186 Training Loss: tensor(0.3293)\n",
      "24187 Training Loss: tensor(0.3288)\n",
      "24188 Training Loss: tensor(0.3302)\n",
      "24189 Training Loss: tensor(0.3286)\n",
      "24190 Training Loss: tensor(0.3292)\n",
      "24191 Training Loss: tensor(0.3313)\n",
      "24192 Training Loss: tensor(0.3295)\n",
      "24193 Training Loss: tensor(0.3280)\n",
      "24194 Training Loss: tensor(0.3294)\n",
      "24195 Training Loss: tensor(0.3281)\n",
      "24196 Training Loss: tensor(0.3287)\n",
      "24197 Training Loss: tensor(0.3315)\n",
      "24198 Training Loss: tensor(0.3296)\n",
      "24199 Training Loss: tensor(0.3289)\n",
      "24200 Training Loss: tensor(0.3289)\n",
      "24201 Training Loss: tensor(0.3300)\n",
      "24202 Training Loss: tensor(0.3288)\n",
      "24203 Training Loss: tensor(0.3306)\n",
      "24204 Training Loss: tensor(0.3287)\n",
      "24205 Training Loss: tensor(0.3305)\n",
      "24206 Training Loss: tensor(0.3291)\n",
      "24207 Training Loss: tensor(0.3299)\n",
      "24208 Training Loss: tensor(0.3311)\n",
      "24209 Training Loss: tensor(0.3282)\n",
      "24210 Training Loss: tensor(0.3287)\n",
      "24211 Training Loss: tensor(0.3295)\n",
      "24212 Training Loss: tensor(0.3282)\n",
      "24213 Training Loss: tensor(0.3283)\n",
      "24214 Training Loss: tensor(0.3292)\n",
      "24215 Training Loss: tensor(0.3302)\n",
      "24216 Training Loss: tensor(0.3279)\n",
      "24217 Training Loss: tensor(0.3284)\n",
      "24218 Training Loss: tensor(0.3281)\n",
      "24219 Training Loss: tensor(0.3281)\n",
      "24220 Training Loss: tensor(0.3284)\n",
      "24221 Training Loss: tensor(0.3280)\n",
      "24222 Training Loss: tensor(0.3292)\n",
      "24223 Training Loss: tensor(0.3285)\n",
      "24224 Training Loss: tensor(0.3284)\n",
      "24225 Training Loss: tensor(0.3294)\n",
      "24226 Training Loss: tensor(0.3302)\n",
      "24227 Training Loss: tensor(0.3272)\n",
      "24228 Training Loss: tensor(0.3297)\n",
      "24229 Training Loss: tensor(0.3290)\n",
      "24230 Training Loss: tensor(0.3303)\n",
      "24231 Training Loss: tensor(0.3280)\n",
      "24232 Training Loss: tensor(0.3284)\n",
      "24233 Training Loss: tensor(0.3295)\n",
      "24234 Training Loss: tensor(0.3281)\n",
      "24235 Training Loss: tensor(0.3286)\n",
      "24236 Training Loss: tensor(0.3292)\n",
      "24237 Training Loss: tensor(0.3287)\n",
      "24238 Training Loss: tensor(0.3285)\n",
      "24239 Training Loss: tensor(0.3297)\n",
      "24240 Training Loss: tensor(0.3278)\n",
      "24241 Training Loss: tensor(0.3312)\n",
      "24242 Training Loss: tensor(0.3281)\n",
      "24243 Training Loss: tensor(0.3283)\n",
      "24244 Training Loss: tensor(0.3280)\n",
      "24245 Training Loss: tensor(0.3303)\n",
      "24246 Training Loss: tensor(0.3279)\n",
      "24247 Training Loss: tensor(0.3278)\n",
      "24248 Training Loss: tensor(0.3299)\n",
      "24249 Training Loss: tensor(0.3291)\n",
      "24250 Training Loss: tensor(0.3279)\n",
      "24251 Training Loss: tensor(0.3289)\n",
      "24252 Training Loss: tensor(0.3299)\n",
      "24253 Training Loss: tensor(0.3282)\n",
      "24254 Training Loss: tensor(0.3284)\n",
      "24255 Training Loss: tensor(0.3294)\n",
      "24256 Training Loss: tensor(0.3287)\n",
      "24257 Training Loss: tensor(0.3288)\n",
      "24258 Training Loss: tensor(0.3299)\n",
      "24259 Training Loss: tensor(0.3302)\n",
      "24260 Training Loss: tensor(0.3279)\n",
      "24261 Training Loss: tensor(0.3301)\n",
      "24262 Training Loss: tensor(0.3284)\n",
      "24263 Training Loss: tensor(0.3305)\n",
      "24264 Training Loss: tensor(0.3280)\n",
      "24265 Training Loss: tensor(0.3291)\n",
      "24266 Training Loss: tensor(0.3290)\n",
      "24267 Training Loss: tensor(0.3281)\n",
      "24268 Training Loss: tensor(0.3286)\n",
      "24269 Training Loss: tensor(0.3293)\n",
      "24270 Training Loss: tensor(0.3304)\n",
      "24271 Training Loss: tensor(0.3290)\n",
      "24272 Training Loss: tensor(0.3297)\n",
      "24273 Training Loss: tensor(0.3281)\n",
      "24274 Training Loss: tensor(0.3283)\n",
      "24275 Training Loss: tensor(0.3279)\n",
      "24276 Training Loss: tensor(0.3283)\n",
      "24277 Training Loss: tensor(0.3297)\n",
      "24278 Training Loss: tensor(0.3280)\n",
      "24279 Training Loss: tensor(0.3299)\n",
      "24280 Training Loss: tensor(0.3296)\n",
      "24281 Training Loss: tensor(0.3297)\n",
      "24282 Training Loss: tensor(0.3307)\n",
      "24283 Training Loss: tensor(0.3281)\n",
      "24284 Training Loss: tensor(0.3278)\n",
      "24285 Training Loss: tensor(0.3289)\n",
      "24286 Training Loss: tensor(0.3300)\n",
      "24287 Training Loss: tensor(0.3277)\n",
      "24288 Training Loss: tensor(0.3289)\n",
      "24289 Training Loss: tensor(0.3286)\n",
      "24290 Training Loss: tensor(0.3292)\n",
      "24291 Training Loss: tensor(0.3288)\n",
      "24292 Training Loss: tensor(0.3277)\n",
      "24293 Training Loss: tensor(0.3288)\n",
      "24294 Training Loss: tensor(0.3302)\n",
      "24295 Training Loss: tensor(0.3288)\n",
      "24296 Training Loss: tensor(0.3282)\n",
      "24297 Training Loss: tensor(0.3294)\n",
      "24298 Training Loss: tensor(0.3283)\n",
      "24299 Training Loss: tensor(0.3280)\n",
      "24300 Training Loss: tensor(0.3294)\n",
      "24301 Training Loss: tensor(0.3281)\n",
      "24302 Training Loss: tensor(0.3288)\n",
      "24303 Training Loss: tensor(0.3277)\n",
      "24304 Training Loss: tensor(0.3277)\n",
      "24305 Training Loss: tensor(0.3283)\n",
      "24306 Training Loss: tensor(0.3277)\n",
      "24307 Training Loss: tensor(0.3283)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24308 Training Loss: tensor(0.3287)\n",
      "24309 Training Loss: tensor(0.3275)\n",
      "24310 Training Loss: tensor(0.3277)\n",
      "24311 Training Loss: tensor(0.3309)\n",
      "24312 Training Loss: tensor(0.3279)\n",
      "24313 Training Loss: tensor(0.3280)\n",
      "24314 Training Loss: tensor(0.3296)\n",
      "24315 Training Loss: tensor(0.3277)\n",
      "24316 Training Loss: tensor(0.3291)\n",
      "24317 Training Loss: tensor(0.3281)\n",
      "24318 Training Loss: tensor(0.3285)\n",
      "24319 Training Loss: tensor(0.3285)\n",
      "24320 Training Loss: tensor(0.3293)\n",
      "24321 Training Loss: tensor(0.3282)\n",
      "24322 Training Loss: tensor(0.3292)\n",
      "24323 Training Loss: tensor(0.3290)\n",
      "24324 Training Loss: tensor(0.3290)\n",
      "24325 Training Loss: tensor(0.3282)\n",
      "24326 Training Loss: tensor(0.3278)\n",
      "24327 Training Loss: tensor(0.3310)\n",
      "24328 Training Loss: tensor(0.3322)\n",
      "24329 Training Loss: tensor(0.3283)\n",
      "24330 Training Loss: tensor(0.3279)\n",
      "24331 Training Loss: tensor(0.3297)\n",
      "24332 Training Loss: tensor(0.3278)\n",
      "24333 Training Loss: tensor(0.3280)\n",
      "24334 Training Loss: tensor(0.3284)\n",
      "24335 Training Loss: tensor(0.3281)\n",
      "24336 Training Loss: tensor(0.3281)\n",
      "24337 Training Loss: tensor(0.3291)\n",
      "24338 Training Loss: tensor(0.3280)\n",
      "24339 Training Loss: tensor(0.3285)\n",
      "24340 Training Loss: tensor(0.3293)\n",
      "24341 Training Loss: tensor(0.3277)\n",
      "24342 Training Loss: tensor(0.3286)\n",
      "24343 Training Loss: tensor(0.3276)\n",
      "24344 Training Loss: tensor(0.3302)\n",
      "24345 Training Loss: tensor(0.3280)\n",
      "24346 Training Loss: tensor(0.3286)\n",
      "24347 Training Loss: tensor(0.3279)\n",
      "24348 Training Loss: tensor(0.3300)\n",
      "24349 Training Loss: tensor(0.3288)\n",
      "24350 Training Loss: tensor(0.3280)\n",
      "24351 Training Loss: tensor(0.3285)\n",
      "24352 Training Loss: tensor(0.3281)\n",
      "24353 Training Loss: tensor(0.3277)\n",
      "24354 Training Loss: tensor(0.3279)\n",
      "24355 Training Loss: tensor(0.3307)\n",
      "24356 Training Loss: tensor(0.3276)\n",
      "24357 Training Loss: tensor(0.3273)\n",
      "24358 Training Loss: tensor(0.3299)\n",
      "24359 Training Loss: tensor(0.3285)\n",
      "24360 Training Loss: tensor(0.3289)\n",
      "24361 Training Loss: tensor(0.3280)\n",
      "24362 Training Loss: tensor(0.3292)\n",
      "24363 Training Loss: tensor(0.3279)\n",
      "24364 Training Loss: tensor(0.3286)\n",
      "24365 Training Loss: tensor(0.3282)\n",
      "24366 Training Loss: tensor(0.3288)\n",
      "24367 Training Loss: tensor(0.3285)\n",
      "24368 Training Loss: tensor(0.3295)\n",
      "24369 Training Loss: tensor(0.3282)\n",
      "24370 Training Loss: tensor(0.3300)\n",
      "24371 Training Loss: tensor(0.3285)\n",
      "24372 Training Loss: tensor(0.3301)\n",
      "24373 Training Loss: tensor(0.3281)\n",
      "24374 Training Loss: tensor(0.3294)\n",
      "24375 Training Loss: tensor(0.3278)\n",
      "24376 Training Loss: tensor(0.3285)\n",
      "24377 Training Loss: tensor(0.3286)\n",
      "24378 Training Loss: tensor(0.3309)\n",
      "24379 Training Loss: tensor(0.3278)\n",
      "24380 Training Loss: tensor(0.3283)\n",
      "24381 Training Loss: tensor(0.3328)\n",
      "24382 Training Loss: tensor(0.3300)\n",
      "24383 Training Loss: tensor(0.3287)\n",
      "24384 Training Loss: tensor(0.3285)\n",
      "24385 Training Loss: tensor(0.3280)\n",
      "24386 Training Loss: tensor(0.3349)\n",
      "24387 Training Loss: tensor(0.3283)\n",
      "24388 Training Loss: tensor(0.3282)\n",
      "24389 Training Loss: tensor(0.3292)\n",
      "24390 Training Loss: tensor(0.3288)\n",
      "24391 Training Loss: tensor(0.3283)\n",
      "24392 Training Loss: tensor(0.3309)\n",
      "24393 Training Loss: tensor(0.3282)\n",
      "24394 Training Loss: tensor(0.3282)\n",
      "24395 Training Loss: tensor(0.3310)\n",
      "24396 Training Loss: tensor(0.3289)\n",
      "24397 Training Loss: tensor(0.3288)\n",
      "24398 Training Loss: tensor(0.3274)\n",
      "24399 Training Loss: tensor(0.3298)\n",
      "24400 Training Loss: tensor(0.3295)\n",
      "24401 Training Loss: tensor(0.3305)\n",
      "24402 Training Loss: tensor(0.3290)\n",
      "24403 Training Loss: tensor(0.3299)\n",
      "24404 Training Loss: tensor(0.3292)\n",
      "24405 Training Loss: tensor(0.3283)\n",
      "24406 Training Loss: tensor(0.3276)\n",
      "24407 Training Loss: tensor(0.3289)\n",
      "24408 Training Loss: tensor(0.3279)\n",
      "24409 Training Loss: tensor(0.3283)\n",
      "24410 Training Loss: tensor(0.3285)\n",
      "24411 Training Loss: tensor(0.3284)\n",
      "24412 Training Loss: tensor(0.3281)\n",
      "24413 Training Loss: tensor(0.3291)\n",
      "24414 Training Loss: tensor(0.3282)\n",
      "24415 Training Loss: tensor(0.3291)\n",
      "24416 Training Loss: tensor(0.3280)\n",
      "24417 Training Loss: tensor(0.3280)\n",
      "24418 Training Loss: tensor(0.3288)\n",
      "24419 Training Loss: tensor(0.3280)\n",
      "24420 Training Loss: tensor(0.3295)\n",
      "24421 Training Loss: tensor(0.3288)\n",
      "24422 Training Loss: tensor(0.3289)\n",
      "24423 Training Loss: tensor(0.3301)\n",
      "24424 Training Loss: tensor(0.3295)\n",
      "24425 Training Loss: tensor(0.3281)\n",
      "24426 Training Loss: tensor(0.3298)\n",
      "24427 Training Loss: tensor(0.3288)\n",
      "24428 Training Loss: tensor(0.3294)\n",
      "24429 Training Loss: tensor(0.3315)\n",
      "24430 Training Loss: tensor(0.3284)\n",
      "24431 Training Loss: tensor(0.3294)\n",
      "24432 Training Loss: tensor(0.3292)\n",
      "24433 Training Loss: tensor(0.3293)\n",
      "24434 Training Loss: tensor(0.3348)\n",
      "24435 Training Loss: tensor(0.3297)\n",
      "24436 Training Loss: tensor(0.3298)\n",
      "24437 Training Loss: tensor(0.3305)\n",
      "24438 Training Loss: tensor(0.3297)\n",
      "24439 Training Loss: tensor(0.3299)\n",
      "24440 Training Loss: tensor(0.3319)\n",
      "24441 Training Loss: tensor(0.3297)\n",
      "24442 Training Loss: tensor(0.3287)\n",
      "24443 Training Loss: tensor(0.3283)\n",
      "24444 Training Loss: tensor(0.3285)\n",
      "24445 Training Loss: tensor(0.3283)\n",
      "24446 Training Loss: tensor(0.3293)\n",
      "24447 Training Loss: tensor(0.3299)\n",
      "24448 Training Loss: tensor(0.3322)\n",
      "24449 Training Loss: tensor(0.3323)\n",
      "24450 Training Loss: tensor(0.3278)\n",
      "24451 Training Loss: tensor(0.3286)\n",
      "24452 Training Loss: tensor(0.3301)\n",
      "24453 Training Loss: tensor(0.3295)\n",
      "24454 Training Loss: tensor(0.3302)\n",
      "24455 Training Loss: tensor(0.3306)\n",
      "24456 Training Loss: tensor(0.3294)\n",
      "24457 Training Loss: tensor(0.3294)\n",
      "24458 Training Loss: tensor(0.3284)\n",
      "24459 Training Loss: tensor(0.3299)\n",
      "24460 Training Loss: tensor(0.3293)\n",
      "24461 Training Loss: tensor(0.3299)\n",
      "24462 Training Loss: tensor(0.3297)\n",
      "24463 Training Loss: tensor(0.3285)\n",
      "24464 Training Loss: tensor(0.3294)\n",
      "24465 Training Loss: tensor(0.3290)\n",
      "24466 Training Loss: tensor(0.3291)\n",
      "24467 Training Loss: tensor(0.3289)\n",
      "24468 Training Loss: tensor(0.3307)\n",
      "24469 Training Loss: tensor(0.3302)\n",
      "24470 Training Loss: tensor(0.3300)\n",
      "24471 Training Loss: tensor(0.3301)\n",
      "24472 Training Loss: tensor(0.3285)\n",
      "24473 Training Loss: tensor(0.3288)\n",
      "24474 Training Loss: tensor(0.3281)\n",
      "24475 Training Loss: tensor(0.3291)\n",
      "24476 Training Loss: tensor(0.3278)\n",
      "24477 Training Loss: tensor(0.3293)\n",
      "24478 Training Loss: tensor(0.3295)\n",
      "24479 Training Loss: tensor(0.3289)\n",
      "24480 Training Loss: tensor(0.3282)\n",
      "24481 Training Loss: tensor(0.3289)\n",
      "24482 Training Loss: tensor(0.3282)\n",
      "24483 Training Loss: tensor(0.3276)\n",
      "24484 Training Loss: tensor(0.3285)\n",
      "24485 Training Loss: tensor(0.3290)\n",
      "24486 Training Loss: tensor(0.3278)\n",
      "24487 Training Loss: tensor(0.3280)\n",
      "24488 Training Loss: tensor(0.3277)\n",
      "24489 Training Loss: tensor(0.3277)\n",
      "24490 Training Loss: tensor(0.3297)\n",
      "24491 Training Loss: tensor(0.3301)\n",
      "24492 Training Loss: tensor(0.3281)\n",
      "24493 Training Loss: tensor(0.3281)\n",
      "24494 Training Loss: tensor(0.3272)\n",
      "24495 Training Loss: tensor(0.3282)\n",
      "24496 Training Loss: tensor(0.3277)\n",
      "24497 Training Loss: tensor(0.3279)\n",
      "24498 Training Loss: tensor(0.3286)\n",
      "24499 Training Loss: tensor(0.3279)\n",
      "24500 Training Loss: tensor(0.3288)\n",
      "24501 Training Loss: tensor(0.3288)\n",
      "24502 Training Loss: tensor(0.3283)\n",
      "24503 Training Loss: tensor(0.3299)\n",
      "24504 Training Loss: tensor(0.3288)\n",
      "24505 Training Loss: tensor(0.3285)\n",
      "24506 Training Loss: tensor(0.3299)\n",
      "24507 Training Loss: tensor(0.3275)\n",
      "24508 Training Loss: tensor(0.3282)\n",
      "24509 Training Loss: tensor(0.3280)\n",
      "24510 Training Loss: tensor(0.3324)\n",
      "24511 Training Loss: tensor(0.3274)\n",
      "24512 Training Loss: tensor(0.3291)\n",
      "24513 Training Loss: tensor(0.3286)\n",
      "24514 Training Loss: tensor(0.3281)\n",
      "24515 Training Loss: tensor(0.3289)\n",
      "24516 Training Loss: tensor(0.3285)\n",
      "24517 Training Loss: tensor(0.3276)\n",
      "24518 Training Loss: tensor(0.3281)\n",
      "24519 Training Loss: tensor(0.3286)\n",
      "24520 Training Loss: tensor(0.3286)\n",
      "24521 Training Loss: tensor(0.3281)\n",
      "24522 Training Loss: tensor(0.3277)\n",
      "24523 Training Loss: tensor(0.3290)\n",
      "24524 Training Loss: tensor(0.3275)\n",
      "24525 Training Loss: tensor(0.3275)\n",
      "24526 Training Loss: tensor(0.3274)\n",
      "24527 Training Loss: tensor(0.3278)\n",
      "24528 Training Loss: tensor(0.3281)\n",
      "24529 Training Loss: tensor(0.3283)\n",
      "24530 Training Loss: tensor(0.3274)\n",
      "24531 Training Loss: tensor(0.3278)\n",
      "24532 Training Loss: tensor(0.3279)\n",
      "24533 Training Loss: tensor(0.3286)\n",
      "24534 Training Loss: tensor(0.3306)\n",
      "24535 Training Loss: tensor(0.3271)\n",
      "24536 Training Loss: tensor(0.3295)\n",
      "24537 Training Loss: tensor(0.3284)\n",
      "24538 Training Loss: tensor(0.3278)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24539 Training Loss: tensor(0.3300)\n",
      "24540 Training Loss: tensor(0.3277)\n",
      "24541 Training Loss: tensor(0.3278)\n",
      "24542 Training Loss: tensor(0.3306)\n",
      "24543 Training Loss: tensor(0.3280)\n",
      "24544 Training Loss: tensor(0.3286)\n",
      "24545 Training Loss: tensor(0.3280)\n",
      "24546 Training Loss: tensor(0.3313)\n",
      "24547 Training Loss: tensor(0.3292)\n",
      "24548 Training Loss: tensor(0.3280)\n",
      "24549 Training Loss: tensor(0.3292)\n",
      "24550 Training Loss: tensor(0.3284)\n",
      "24551 Training Loss: tensor(0.3295)\n",
      "24552 Training Loss: tensor(0.3278)\n",
      "24553 Training Loss: tensor(0.3281)\n",
      "24554 Training Loss: tensor(0.3300)\n",
      "24555 Training Loss: tensor(0.3278)\n",
      "24556 Training Loss: tensor(0.3285)\n",
      "24557 Training Loss: tensor(0.3313)\n",
      "24558 Training Loss: tensor(0.3289)\n",
      "24559 Training Loss: tensor(0.3282)\n",
      "24560 Training Loss: tensor(0.3284)\n",
      "24561 Training Loss: tensor(0.3289)\n",
      "24562 Training Loss: tensor(0.3285)\n",
      "24563 Training Loss: tensor(0.3281)\n",
      "24564 Training Loss: tensor(0.3297)\n",
      "24565 Training Loss: tensor(0.3287)\n",
      "24566 Training Loss: tensor(0.3285)\n",
      "24567 Training Loss: tensor(0.3289)\n",
      "24568 Training Loss: tensor(0.3291)\n",
      "24569 Training Loss: tensor(0.3284)\n",
      "24570 Training Loss: tensor(0.3309)\n",
      "24571 Training Loss: tensor(0.3281)\n",
      "24572 Training Loss: tensor(0.3340)\n",
      "24573 Training Loss: tensor(0.3280)\n",
      "24574 Training Loss: tensor(0.3294)\n",
      "24575 Training Loss: tensor(0.3301)\n",
      "24576 Training Loss: tensor(0.3302)\n",
      "24577 Training Loss: tensor(0.3285)\n",
      "24578 Training Loss: tensor(0.3278)\n",
      "24579 Training Loss: tensor(0.3288)\n",
      "24580 Training Loss: tensor(0.3305)\n",
      "24581 Training Loss: tensor(0.3298)\n",
      "24582 Training Loss: tensor(0.3290)\n",
      "24583 Training Loss: tensor(0.3303)\n",
      "24584 Training Loss: tensor(0.3287)\n",
      "24585 Training Loss: tensor(0.3279)\n",
      "24586 Training Loss: tensor(0.3286)\n",
      "24587 Training Loss: tensor(0.3297)\n",
      "24588 Training Loss: tensor(0.3280)\n",
      "24589 Training Loss: tensor(0.3276)\n",
      "24590 Training Loss: tensor(0.3282)\n",
      "24591 Training Loss: tensor(0.3279)\n",
      "24592 Training Loss: tensor(0.3305)\n",
      "24593 Training Loss: tensor(0.3281)\n",
      "24594 Training Loss: tensor(0.3275)\n",
      "24595 Training Loss: tensor(0.3284)\n",
      "24596 Training Loss: tensor(0.3281)\n",
      "24597 Training Loss: tensor(0.3296)\n",
      "24598 Training Loss: tensor(0.3295)\n",
      "24599 Training Loss: tensor(0.3279)\n",
      "24600 Training Loss: tensor(0.3295)\n",
      "24601 Training Loss: tensor(0.3286)\n",
      "24602 Training Loss: tensor(0.3282)\n",
      "24603 Training Loss: tensor(0.3287)\n",
      "24604 Training Loss: tensor(0.3278)\n",
      "24605 Training Loss: tensor(0.3286)\n",
      "24606 Training Loss: tensor(0.3285)\n",
      "24607 Training Loss: tensor(0.3283)\n",
      "24608 Training Loss: tensor(0.3273)\n",
      "24609 Training Loss: tensor(0.3309)\n",
      "24610 Training Loss: tensor(0.3275)\n",
      "24611 Training Loss: tensor(0.3283)\n",
      "24612 Training Loss: tensor(0.3283)\n",
      "24613 Training Loss: tensor(0.3274)\n",
      "24614 Training Loss: tensor(0.3282)\n",
      "24615 Training Loss: tensor(0.3274)\n",
      "24616 Training Loss: tensor(0.3274)\n",
      "24617 Training Loss: tensor(0.3279)\n",
      "24618 Training Loss: tensor(0.3328)\n",
      "24619 Training Loss: tensor(0.3327)\n",
      "24620 Training Loss: tensor(0.3276)\n",
      "24621 Training Loss: tensor(0.3283)\n",
      "24622 Training Loss: tensor(0.3281)\n",
      "24623 Training Loss: tensor(0.3282)\n",
      "24624 Training Loss: tensor(0.3274)\n",
      "24625 Training Loss: tensor(0.3301)\n",
      "24626 Training Loss: tensor(0.3274)\n",
      "24627 Training Loss: tensor(0.3294)\n",
      "24628 Training Loss: tensor(0.3292)\n",
      "24629 Training Loss: tensor(0.3276)\n",
      "24630 Training Loss: tensor(0.3285)\n",
      "24631 Training Loss: tensor(0.3288)\n",
      "24632 Training Loss: tensor(0.3284)\n",
      "24633 Training Loss: tensor(0.3289)\n",
      "24634 Training Loss: tensor(0.3305)\n",
      "24635 Training Loss: tensor(0.3311)\n",
      "24636 Training Loss: tensor(0.3286)\n",
      "24637 Training Loss: tensor(0.3281)\n",
      "24638 Training Loss: tensor(0.3304)\n",
      "24639 Training Loss: tensor(0.3284)\n",
      "24640 Training Loss: tensor(0.3282)\n",
      "24641 Training Loss: tensor(0.3281)\n",
      "24642 Training Loss: tensor(0.3287)\n",
      "24643 Training Loss: tensor(0.3303)\n",
      "24644 Training Loss: tensor(0.3280)\n",
      "24645 Training Loss: tensor(0.3295)\n",
      "24646 Training Loss: tensor(0.3331)\n",
      "24647 Training Loss: tensor(0.3285)\n",
      "24648 Training Loss: tensor(0.3284)\n",
      "24649 Training Loss: tensor(0.3291)\n",
      "24650 Training Loss: tensor(0.3295)\n",
      "24651 Training Loss: tensor(0.3286)\n",
      "24652 Training Loss: tensor(0.3283)\n",
      "24653 Training Loss: tensor(0.3289)\n",
      "24654 Training Loss: tensor(0.3281)\n",
      "24655 Training Loss: tensor(0.3285)\n",
      "24656 Training Loss: tensor(0.3285)\n",
      "24657 Training Loss: tensor(0.3289)\n",
      "24658 Training Loss: tensor(0.3286)\n",
      "24659 Training Loss: tensor(0.3282)\n",
      "24660 Training Loss: tensor(0.3280)\n",
      "24661 Training Loss: tensor(0.3307)\n",
      "24662 Training Loss: tensor(0.3280)\n",
      "24663 Training Loss: tensor(0.3287)\n",
      "24664 Training Loss: tensor(0.3279)\n",
      "24665 Training Loss: tensor(0.3283)\n",
      "24666 Training Loss: tensor(0.3276)\n",
      "24667 Training Loss: tensor(0.3289)\n",
      "24668 Training Loss: tensor(0.3292)\n",
      "24669 Training Loss: tensor(0.3285)\n",
      "24670 Training Loss: tensor(0.3289)\n",
      "24671 Training Loss: tensor(0.3319)\n",
      "24672 Training Loss: tensor(0.3277)\n",
      "24673 Training Loss: tensor(0.3292)\n",
      "24674 Training Loss: tensor(0.3287)\n",
      "24675 Training Loss: tensor(0.3285)\n",
      "24676 Training Loss: tensor(0.3281)\n",
      "24677 Training Loss: tensor(0.3281)\n",
      "24678 Training Loss: tensor(0.3286)\n",
      "24679 Training Loss: tensor(0.3293)\n",
      "24680 Training Loss: tensor(0.3274)\n",
      "24681 Training Loss: tensor(0.3299)\n",
      "24682 Training Loss: tensor(0.3307)\n",
      "24683 Training Loss: tensor(0.3297)\n",
      "24684 Training Loss: tensor(0.3280)\n",
      "24685 Training Loss: tensor(0.3284)\n",
      "24686 Training Loss: tensor(0.3277)\n",
      "24687 Training Loss: tensor(0.3279)\n",
      "24688 Training Loss: tensor(0.3286)\n",
      "24689 Training Loss: tensor(0.3300)\n",
      "24690 Training Loss: tensor(0.3277)\n",
      "24691 Training Loss: tensor(0.3298)\n",
      "24692 Training Loss: tensor(0.3286)\n",
      "24693 Training Loss: tensor(0.3274)\n",
      "24694 Training Loss: tensor(0.3285)\n",
      "24695 Training Loss: tensor(0.3280)\n",
      "24696 Training Loss: tensor(0.3277)\n",
      "24697 Training Loss: tensor(0.3283)\n",
      "24698 Training Loss: tensor(0.3278)\n",
      "24699 Training Loss: tensor(0.3275)\n",
      "24700 Training Loss: tensor(0.3278)\n",
      "24701 Training Loss: tensor(0.3291)\n",
      "24702 Training Loss: tensor(0.3319)\n",
      "24703 Training Loss: tensor(0.3295)\n",
      "24704 Training Loss: tensor(0.3308)\n",
      "24705 Training Loss: tensor(0.3283)\n",
      "24706 Training Loss: tensor(0.3288)\n",
      "24707 Training Loss: tensor(0.3278)\n",
      "24708 Training Loss: tensor(0.3287)\n",
      "24709 Training Loss: tensor(0.3282)\n",
      "24710 Training Loss: tensor(0.3280)\n",
      "24711 Training Loss: tensor(0.3280)\n",
      "24712 Training Loss: tensor(0.3284)\n",
      "24713 Training Loss: tensor(0.3279)\n",
      "24714 Training Loss: tensor(0.3295)\n",
      "24715 Training Loss: tensor(0.3299)\n",
      "24716 Training Loss: tensor(0.3278)\n",
      "24717 Training Loss: tensor(0.3281)\n",
      "24718 Training Loss: tensor(0.3279)\n",
      "24719 Training Loss: tensor(0.3278)\n",
      "24720 Training Loss: tensor(0.3280)\n",
      "24721 Training Loss: tensor(0.3284)\n",
      "24722 Training Loss: tensor(0.3280)\n",
      "24723 Training Loss: tensor(0.3287)\n",
      "24724 Training Loss: tensor(0.3286)\n",
      "24725 Training Loss: tensor(0.3274)\n",
      "24726 Training Loss: tensor(0.3288)\n",
      "24727 Training Loss: tensor(0.3282)\n",
      "24728 Training Loss: tensor(0.3277)\n",
      "24729 Training Loss: tensor(0.3338)\n",
      "24730 Training Loss: tensor(0.3283)\n",
      "24731 Training Loss: tensor(0.3282)\n",
      "24732 Training Loss: tensor(0.3284)\n",
      "24733 Training Loss: tensor(0.3282)\n",
      "24734 Training Loss: tensor(0.3283)\n",
      "24735 Training Loss: tensor(0.3275)\n",
      "24736 Training Loss: tensor(0.3293)\n",
      "24737 Training Loss: tensor(0.3273)\n",
      "24738 Training Loss: tensor(0.3285)\n",
      "24739 Training Loss: tensor(0.3284)\n",
      "24740 Training Loss: tensor(0.3277)\n",
      "24741 Training Loss: tensor(0.3295)\n",
      "24742 Training Loss: tensor(0.3276)\n",
      "24743 Training Loss: tensor(0.3284)\n",
      "24744 Training Loss: tensor(0.3284)\n",
      "24745 Training Loss: tensor(0.3294)\n",
      "24746 Training Loss: tensor(0.3301)\n",
      "24747 Training Loss: tensor(0.3276)\n",
      "24748 Training Loss: tensor(0.3297)\n",
      "24749 Training Loss: tensor(0.3292)\n",
      "24750 Training Loss: tensor(0.3278)\n",
      "24751 Training Loss: tensor(0.3314)\n",
      "24752 Training Loss: tensor(0.3304)\n",
      "24753 Training Loss: tensor(0.3284)\n",
      "24754 Training Loss: tensor(0.3293)\n",
      "24755 Training Loss: tensor(0.3303)\n",
      "24756 Training Loss: tensor(0.3276)\n",
      "24757 Training Loss: tensor(0.3280)\n",
      "24758 Training Loss: tensor(0.3281)\n",
      "24759 Training Loss: tensor(0.3320)\n",
      "24760 Training Loss: tensor(0.3276)\n",
      "24761 Training Loss: tensor(0.3283)\n",
      "24762 Training Loss: tensor(0.3277)\n",
      "24763 Training Loss: tensor(0.3287)\n",
      "24764 Training Loss: tensor(0.3286)\n",
      "24765 Training Loss: tensor(0.3288)\n",
      "24766 Training Loss: tensor(0.3289)\n",
      "24767 Training Loss: tensor(0.3282)\n",
      "24768 Training Loss: tensor(0.3276)\n",
      "24769 Training Loss: tensor(0.3286)\n",
      "24770 Training Loss: tensor(0.3283)\n",
      "24771 Training Loss: tensor(0.3280)\n",
      "24772 Training Loss: tensor(0.3290)\n",
      "24773 Training Loss: tensor(0.3284)\n",
      "24774 Training Loss: tensor(0.3286)\n",
      "24775 Training Loss: tensor(0.3280)\n",
      "24776 Training Loss: tensor(0.3287)\n",
      "24777 Training Loss: tensor(0.3287)\n",
      "24778 Training Loss: tensor(0.3283)\n",
      "24779 Training Loss: tensor(0.3278)\n",
      "24780 Training Loss: tensor(0.3289)\n",
      "24781 Training Loss: tensor(0.3282)\n",
      "24782 Training Loss: tensor(0.3297)\n",
      "24783 Training Loss: tensor(0.3280)\n",
      "24784 Training Loss: tensor(0.3276)\n",
      "24785 Training Loss: tensor(0.3276)\n",
      "24786 Training Loss: tensor(0.3281)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24787 Training Loss: tensor(0.3286)\n",
      "24788 Training Loss: tensor(0.3309)\n",
      "24789 Training Loss: tensor(0.3281)\n",
      "24790 Training Loss: tensor(0.3279)\n",
      "24791 Training Loss: tensor(0.3278)\n",
      "24792 Training Loss: tensor(0.3278)\n",
      "24793 Training Loss: tensor(0.3282)\n",
      "24794 Training Loss: tensor(0.3279)\n",
      "24795 Training Loss: tensor(0.3282)\n",
      "24796 Training Loss: tensor(0.3283)\n",
      "24797 Training Loss: tensor(0.3277)\n",
      "24798 Training Loss: tensor(0.3281)\n",
      "24799 Training Loss: tensor(0.3279)\n",
      "24800 Training Loss: tensor(0.3274)\n",
      "24801 Training Loss: tensor(0.3279)\n",
      "24802 Training Loss: tensor(0.3308)\n",
      "24803 Training Loss: tensor(0.3285)\n",
      "24804 Training Loss: tensor(0.3315)\n",
      "24805 Training Loss: tensor(0.3269)\n",
      "24806 Training Loss: tensor(0.3288)\n",
      "24807 Training Loss: tensor(0.3276)\n",
      "24808 Training Loss: tensor(0.3277)\n",
      "24809 Training Loss: tensor(0.3277)\n",
      "24810 Training Loss: tensor(0.3280)\n",
      "24811 Training Loss: tensor(0.3277)\n",
      "24812 Training Loss: tensor(0.3280)\n",
      "24813 Training Loss: tensor(0.3287)\n",
      "24814 Training Loss: tensor(0.3283)\n",
      "24815 Training Loss: tensor(0.3290)\n",
      "24816 Training Loss: tensor(0.3281)\n",
      "24817 Training Loss: tensor(0.3289)\n",
      "24818 Training Loss: tensor(0.3271)\n",
      "24819 Training Loss: tensor(0.3296)\n",
      "24820 Training Loss: tensor(0.3279)\n",
      "24821 Training Loss: tensor(0.3298)\n",
      "24822 Training Loss: tensor(0.3283)\n",
      "24823 Training Loss: tensor(0.3309)\n",
      "24824 Training Loss: tensor(0.3286)\n",
      "24825 Training Loss: tensor(0.3278)\n",
      "24826 Training Loss: tensor(0.3299)\n",
      "24827 Training Loss: tensor(0.3274)\n",
      "24828 Training Loss: tensor(0.3276)\n",
      "24829 Training Loss: tensor(0.3273)\n",
      "24830 Training Loss: tensor(0.3280)\n",
      "24831 Training Loss: tensor(0.3277)\n",
      "24832 Training Loss: tensor(0.3281)\n",
      "24833 Training Loss: tensor(0.3276)\n",
      "24834 Training Loss: tensor(0.3281)\n",
      "24835 Training Loss: tensor(0.3291)\n",
      "24836 Training Loss: tensor(0.3305)\n",
      "24837 Training Loss: tensor(0.3285)\n",
      "24838 Training Loss: tensor(0.3296)\n",
      "24839 Training Loss: tensor(0.3278)\n",
      "24840 Training Loss: tensor(0.3292)\n",
      "24841 Training Loss: tensor(0.3285)\n",
      "24842 Training Loss: tensor(0.3287)\n",
      "24843 Training Loss: tensor(0.3288)\n",
      "24844 Training Loss: tensor(0.3288)\n",
      "24845 Training Loss: tensor(0.3279)\n",
      "24846 Training Loss: tensor(0.3297)\n",
      "24847 Training Loss: tensor(0.3288)\n",
      "24848 Training Loss: tensor(0.3278)\n",
      "24849 Training Loss: tensor(0.3295)\n",
      "24850 Training Loss: tensor(0.3290)\n",
      "24851 Training Loss: tensor(0.3281)\n",
      "24852 Training Loss: tensor(0.3293)\n",
      "24853 Training Loss: tensor(0.3286)\n",
      "24854 Training Loss: tensor(0.3283)\n",
      "24855 Training Loss: tensor(0.3280)\n",
      "24856 Training Loss: tensor(0.3309)\n",
      "24857 Training Loss: tensor(0.3293)\n",
      "24858 Training Loss: tensor(0.3301)\n",
      "24859 Training Loss: tensor(0.3276)\n",
      "24860 Training Loss: tensor(0.3292)\n",
      "24861 Training Loss: tensor(0.3287)\n",
      "24862 Training Loss: tensor(0.3287)\n",
      "24863 Training Loss: tensor(0.3278)\n",
      "24864 Training Loss: tensor(0.3296)\n",
      "24865 Training Loss: tensor(0.3276)\n",
      "24866 Training Loss: tensor(0.3291)\n",
      "24867 Training Loss: tensor(0.3286)\n",
      "24868 Training Loss: tensor(0.3281)\n",
      "24869 Training Loss: tensor(0.3289)\n",
      "24870 Training Loss: tensor(0.3273)\n",
      "24871 Training Loss: tensor(0.3277)\n",
      "24872 Training Loss: tensor(0.3272)\n",
      "24873 Training Loss: tensor(0.3289)\n",
      "24874 Training Loss: tensor(0.3276)\n",
      "24875 Training Loss: tensor(0.3274)\n",
      "24876 Training Loss: tensor(0.3293)\n",
      "24877 Training Loss: tensor(0.3278)\n",
      "24878 Training Loss: tensor(0.3277)\n",
      "24879 Training Loss: tensor(0.3281)\n",
      "24880 Training Loss: tensor(0.3280)\n",
      "24881 Training Loss: tensor(0.3278)\n",
      "24882 Training Loss: tensor(0.3284)\n",
      "24883 Training Loss: tensor(0.3309)\n",
      "24884 Training Loss: tensor(0.3276)\n",
      "24885 Training Loss: tensor(0.3279)\n",
      "24886 Training Loss: tensor(0.3276)\n",
      "24887 Training Loss: tensor(0.3280)\n",
      "24888 Training Loss: tensor(0.3277)\n",
      "24889 Training Loss: tensor(0.3276)\n",
      "24890 Training Loss: tensor(0.3272)\n",
      "24891 Training Loss: tensor(0.3275)\n",
      "24892 Training Loss: tensor(0.3298)\n",
      "24893 Training Loss: tensor(0.3276)\n",
      "24894 Training Loss: tensor(0.3306)\n",
      "24895 Training Loss: tensor(0.3287)\n",
      "24896 Training Loss: tensor(0.3280)\n",
      "24897 Training Loss: tensor(0.3319)\n",
      "24898 Training Loss: tensor(0.3297)\n",
      "24899 Training Loss: tensor(0.3286)\n",
      "24900 Training Loss: tensor(0.3276)\n",
      "24901 Training Loss: tensor(0.3277)\n",
      "24902 Training Loss: tensor(0.3279)\n",
      "24903 Training Loss: tensor(0.3293)\n",
      "24904 Training Loss: tensor(0.3297)\n",
      "24905 Training Loss: tensor(0.3281)\n",
      "24906 Training Loss: tensor(0.3300)\n",
      "24907 Training Loss: tensor(0.3282)\n",
      "24908 Training Loss: tensor(0.3298)\n",
      "24909 Training Loss: tensor(0.3280)\n",
      "24910 Training Loss: tensor(0.3284)\n",
      "24911 Training Loss: tensor(0.3283)\n",
      "24912 Training Loss: tensor(0.3292)\n",
      "24913 Training Loss: tensor(0.3287)\n",
      "24914 Training Loss: tensor(0.3276)\n",
      "24915 Training Loss: tensor(0.3284)\n",
      "24916 Training Loss: tensor(0.3285)\n",
      "24917 Training Loss: tensor(0.3283)\n",
      "24918 Training Loss: tensor(0.3304)\n",
      "24919 Training Loss: tensor(0.3297)\n",
      "24920 Training Loss: tensor(0.3271)\n",
      "24921 Training Loss: tensor(0.3281)\n",
      "24922 Training Loss: tensor(0.3276)\n",
      "24923 Training Loss: tensor(0.3283)\n",
      "24924 Training Loss: tensor(0.3297)\n",
      "24925 Training Loss: tensor(0.3289)\n",
      "24926 Training Loss: tensor(0.3279)\n",
      "24927 Training Loss: tensor(0.3277)\n",
      "24928 Training Loss: tensor(0.3273)\n",
      "24929 Training Loss: tensor(0.3278)\n",
      "24930 Training Loss: tensor(0.3289)\n",
      "24931 Training Loss: tensor(0.3275)\n",
      "24932 Training Loss: tensor(0.3287)\n",
      "24933 Training Loss: tensor(0.3289)\n",
      "24934 Training Loss: tensor(0.3289)\n",
      "24935 Training Loss: tensor(0.3279)\n",
      "24936 Training Loss: tensor(0.3276)\n",
      "24937 Training Loss: tensor(0.3277)\n",
      "24938 Training Loss: tensor(0.3285)\n",
      "24939 Training Loss: tensor(0.3275)\n",
      "24940 Training Loss: tensor(0.3294)\n",
      "24941 Training Loss: tensor(0.3278)\n",
      "24942 Training Loss: tensor(0.3276)\n",
      "24943 Training Loss: tensor(0.3288)\n",
      "24944 Training Loss: tensor(0.3272)\n",
      "24945 Training Loss: tensor(0.3279)\n",
      "24946 Training Loss: tensor(0.3278)\n",
      "24947 Training Loss: tensor(0.3287)\n",
      "24948 Training Loss: tensor(0.3277)\n",
      "24949 Training Loss: tensor(0.3279)\n",
      "24950 Training Loss: tensor(0.3283)\n",
      "24951 Training Loss: tensor(0.3278)\n",
      "24952 Training Loss: tensor(0.3298)\n",
      "24953 Training Loss: tensor(0.3309)\n",
      "24954 Training Loss: tensor(0.3271)\n",
      "24955 Training Loss: tensor(0.3295)\n",
      "24956 Training Loss: tensor(0.3275)\n",
      "24957 Training Loss: tensor(0.3287)\n",
      "24958 Training Loss: tensor(0.3274)\n",
      "24959 Training Loss: tensor(0.3291)\n",
      "24960 Training Loss: tensor(0.3274)\n",
      "24961 Training Loss: tensor(0.3278)\n",
      "24962 Training Loss: tensor(0.3283)\n",
      "24963 Training Loss: tensor(0.3278)\n",
      "24964 Training Loss: tensor(0.3299)\n",
      "24965 Training Loss: tensor(0.3289)\n",
      "24966 Training Loss: tensor(0.3277)\n",
      "24967 Training Loss: tensor(0.3275)\n",
      "24968 Training Loss: tensor(0.3272)\n",
      "24969 Training Loss: tensor(0.3282)\n",
      "24970 Training Loss: tensor(0.3287)\n",
      "24971 Training Loss: tensor(0.3274)\n",
      "24972 Training Loss: tensor(0.3288)\n",
      "24973 Training Loss: tensor(0.3273)\n",
      "24974 Training Loss: tensor(0.3280)\n",
      "24975 Training Loss: tensor(0.3275)\n",
      "24976 Training Loss: tensor(0.3281)\n",
      "24977 Training Loss: tensor(0.3276)\n",
      "24978 Training Loss: tensor(0.3280)\n",
      "24979 Training Loss: tensor(0.3277)\n",
      "24980 Training Loss: tensor(0.3276)\n",
      "24981 Training Loss: tensor(0.3271)\n",
      "24982 Training Loss: tensor(0.3280)\n",
      "24983 Training Loss: tensor(0.3274)\n",
      "24984 Training Loss: tensor(0.3277)\n",
      "24985 Training Loss: tensor(0.3279)\n",
      "24986 Training Loss: tensor(0.3271)\n",
      "24987 Training Loss: tensor(0.3272)\n",
      "24988 Training Loss: tensor(0.3283)\n",
      "24989 Training Loss: tensor(0.3277)\n",
      "24990 Training Loss: tensor(0.3274)\n",
      "24991 Training Loss: tensor(0.3269)\n",
      "24992 Training Loss: tensor(0.3282)\n",
      "24993 Training Loss: tensor(0.3288)\n",
      "24994 Training Loss: tensor(0.3280)\n",
      "24995 Training Loss: tensor(0.3277)\n",
      "24996 Training Loss: tensor(0.3291)\n",
      "24997 Training Loss: tensor(0.3283)\n",
      "24998 Training Loss: tensor(0.3301)\n",
      "24999 Training Loss: tensor(0.3280)\n",
      "25000 Training Loss: tensor(0.3273)\n",
      "25001 Training Loss: tensor(0.3281)\n",
      "25002 Training Loss: tensor(0.3309)\n",
      "25003 Training Loss: tensor(0.3278)\n",
      "25004 Training Loss: tensor(0.3274)\n",
      "25005 Training Loss: tensor(0.3294)\n",
      "25006 Training Loss: tensor(0.3302)\n",
      "25007 Training Loss: tensor(0.3295)\n",
      "25008 Training Loss: tensor(0.3287)\n",
      "25009 Training Loss: tensor(0.3276)\n",
      "25010 Training Loss: tensor(0.3280)\n",
      "25011 Training Loss: tensor(0.3296)\n",
      "25012 Training Loss: tensor(0.3284)\n",
      "25013 Training Loss: tensor(0.3281)\n",
      "25014 Training Loss: tensor(0.3288)\n",
      "25015 Training Loss: tensor(0.3279)\n",
      "25016 Training Loss: tensor(0.3280)\n",
      "25017 Training Loss: tensor(0.3272)\n",
      "25018 Training Loss: tensor(0.3283)\n",
      "25019 Training Loss: tensor(0.3279)\n",
      "25020 Training Loss: tensor(0.3273)\n",
      "25021 Training Loss: tensor(0.3277)\n",
      "25022 Training Loss: tensor(0.3282)\n",
      "25023 Training Loss: tensor(0.3279)\n",
      "25024 Training Loss: tensor(0.3286)\n",
      "25025 Training Loss: tensor(0.3295)\n",
      "25026 Training Loss: tensor(0.3271)\n",
      "25027 Training Loss: tensor(0.3278)\n",
      "25028 Training Loss: tensor(0.3273)\n",
      "25029 Training Loss: tensor(0.3282)\n",
      "25030 Training Loss: tensor(0.3290)\n",
      "25031 Training Loss: tensor(0.3293)\n",
      "25032 Training Loss: tensor(0.3278)\n",
      "25033 Training Loss: tensor(0.3283)\n",
      "25034 Training Loss: tensor(0.3281)\n",
      "25035 Training Loss: tensor(0.3273)\n",
      "25036 Training Loss: tensor(0.3275)\n",
      "25037 Training Loss: tensor(0.3275)\n",
      "25038 Training Loss: tensor(0.3273)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25039 Training Loss: tensor(0.3278)\n",
      "25040 Training Loss: tensor(0.3280)\n",
      "25041 Training Loss: tensor(0.3289)\n",
      "25042 Training Loss: tensor(0.3276)\n",
      "25043 Training Loss: tensor(0.3291)\n",
      "25044 Training Loss: tensor(0.3275)\n",
      "25045 Training Loss: tensor(0.3282)\n",
      "25046 Training Loss: tensor(0.3283)\n",
      "25047 Training Loss: tensor(0.3283)\n",
      "25048 Training Loss: tensor(0.3273)\n",
      "25049 Training Loss: tensor(0.3295)\n",
      "25050 Training Loss: tensor(0.3275)\n",
      "25051 Training Loss: tensor(0.3273)\n",
      "25052 Training Loss: tensor(0.3310)\n",
      "25053 Training Loss: tensor(0.3277)\n",
      "25054 Training Loss: tensor(0.3281)\n",
      "25055 Training Loss: tensor(0.3300)\n",
      "25056 Training Loss: tensor(0.3282)\n",
      "25057 Training Loss: tensor(0.3274)\n",
      "25058 Training Loss: tensor(0.3277)\n",
      "25059 Training Loss: tensor(0.3276)\n",
      "25060 Training Loss: tensor(0.3295)\n",
      "25061 Training Loss: tensor(0.3272)\n",
      "25062 Training Loss: tensor(0.3299)\n",
      "25063 Training Loss: tensor(0.3300)\n",
      "25064 Training Loss: tensor(0.3309)\n",
      "25065 Training Loss: tensor(0.3283)\n",
      "25066 Training Loss: tensor(0.3288)\n",
      "25067 Training Loss: tensor(0.3276)\n",
      "25068 Training Loss: tensor(0.3279)\n",
      "25069 Training Loss: tensor(0.3282)\n",
      "25070 Training Loss: tensor(0.3276)\n",
      "25071 Training Loss: tensor(0.3276)\n",
      "25072 Training Loss: tensor(0.3272)\n",
      "25073 Training Loss: tensor(0.3296)\n",
      "25074 Training Loss: tensor(0.3326)\n",
      "25075 Training Loss: tensor(0.3279)\n",
      "25076 Training Loss: tensor(0.3282)\n",
      "25077 Training Loss: tensor(0.3276)\n",
      "25078 Training Loss: tensor(0.3280)\n",
      "25079 Training Loss: tensor(0.3281)\n",
      "25080 Training Loss: tensor(0.3281)\n",
      "25081 Training Loss: tensor(0.3277)\n",
      "25082 Training Loss: tensor(0.3294)\n",
      "25083 Training Loss: tensor(0.3290)\n",
      "25084 Training Loss: tensor(0.3286)\n",
      "25085 Training Loss: tensor(0.3284)\n",
      "25086 Training Loss: tensor(0.3278)\n",
      "25087 Training Loss: tensor(0.3289)\n",
      "25088 Training Loss: tensor(0.3282)\n",
      "25089 Training Loss: tensor(0.3311)\n",
      "25090 Training Loss: tensor(0.3316)\n",
      "25091 Training Loss: tensor(0.3279)\n",
      "25092 Training Loss: tensor(0.3277)\n",
      "25093 Training Loss: tensor(0.3281)\n",
      "25094 Training Loss: tensor(0.3280)\n",
      "25095 Training Loss: tensor(0.3281)\n",
      "25096 Training Loss: tensor(0.3275)\n",
      "25097 Training Loss: tensor(0.3285)\n",
      "25098 Training Loss: tensor(0.3282)\n",
      "25099 Training Loss: tensor(0.3277)\n",
      "25100 Training Loss: tensor(0.3299)\n",
      "25101 Training Loss: tensor(0.3283)\n",
      "25102 Training Loss: tensor(0.3278)\n",
      "25103 Training Loss: tensor(0.3281)\n",
      "25104 Training Loss: tensor(0.3280)\n",
      "25105 Training Loss: tensor(0.3279)\n",
      "25106 Training Loss: tensor(0.3274)\n",
      "25107 Training Loss: tensor(0.3287)\n",
      "25108 Training Loss: tensor(0.3274)\n",
      "25109 Training Loss: tensor(0.3315)\n",
      "25110 Training Loss: tensor(0.3278)\n",
      "25111 Training Loss: tensor(0.3277)\n",
      "25112 Training Loss: tensor(0.3297)\n",
      "25113 Training Loss: tensor(0.3276)\n",
      "25114 Training Loss: tensor(0.3303)\n",
      "25115 Training Loss: tensor(0.3279)\n",
      "25116 Training Loss: tensor(0.3281)\n",
      "25117 Training Loss: tensor(0.3282)\n",
      "25118 Training Loss: tensor(0.3275)\n",
      "25119 Training Loss: tensor(0.3283)\n",
      "25120 Training Loss: tensor(0.3278)\n",
      "25121 Training Loss: tensor(0.3287)\n",
      "25122 Training Loss: tensor(0.3279)\n",
      "25123 Training Loss: tensor(0.3275)\n",
      "25124 Training Loss: tensor(0.3281)\n",
      "25125 Training Loss: tensor(0.3276)\n",
      "25126 Training Loss: tensor(0.3284)\n",
      "25127 Training Loss: tensor(0.3278)\n",
      "25128 Training Loss: tensor(0.3278)\n",
      "25129 Training Loss: tensor(0.3285)\n",
      "25130 Training Loss: tensor(0.3273)\n",
      "25131 Training Loss: tensor(0.3280)\n",
      "25132 Training Loss: tensor(0.3274)\n",
      "25133 Training Loss: tensor(0.3274)\n",
      "25134 Training Loss: tensor(0.3280)\n",
      "25135 Training Loss: tensor(0.3346)\n",
      "25136 Training Loss: tensor(0.3277)\n",
      "25137 Training Loss: tensor(0.3287)\n",
      "25138 Training Loss: tensor(0.3288)\n",
      "25139 Training Loss: tensor(0.3277)\n",
      "25140 Training Loss: tensor(0.3277)\n",
      "25141 Training Loss: tensor(0.3307)\n",
      "25142 Training Loss: tensor(0.3299)\n",
      "25143 Training Loss: tensor(0.3294)\n",
      "25144 Training Loss: tensor(0.3283)\n",
      "25145 Training Loss: tensor(0.3275)\n",
      "25146 Training Loss: tensor(0.3283)\n",
      "25147 Training Loss: tensor(0.3275)\n",
      "25148 Training Loss: tensor(0.3279)\n",
      "25149 Training Loss: tensor(0.3278)\n",
      "25150 Training Loss: tensor(0.3273)\n",
      "25151 Training Loss: tensor(0.3277)\n",
      "25152 Training Loss: tensor(0.3274)\n",
      "25153 Training Loss: tensor(0.3277)\n",
      "25154 Training Loss: tensor(0.3277)\n",
      "25155 Training Loss: tensor(0.3299)\n",
      "25156 Training Loss: tensor(0.3276)\n",
      "25157 Training Loss: tensor(0.3298)\n",
      "25158 Training Loss: tensor(0.3279)\n",
      "25159 Training Loss: tensor(0.3315)\n",
      "25160 Training Loss: tensor(0.3274)\n",
      "25161 Training Loss: tensor(0.3277)\n",
      "25162 Training Loss: tensor(0.3296)\n",
      "25163 Training Loss: tensor(0.3274)\n",
      "25164 Training Loss: tensor(0.3283)\n",
      "25165 Training Loss: tensor(0.3296)\n",
      "25166 Training Loss: tensor(0.3285)\n",
      "25167 Training Loss: tensor(0.3286)\n",
      "25168 Training Loss: tensor(0.3277)\n",
      "25169 Training Loss: tensor(0.3280)\n",
      "25170 Training Loss: tensor(0.3279)\n",
      "25171 Training Loss: tensor(0.3281)\n",
      "25172 Training Loss: tensor(0.3270)\n",
      "25173 Training Loss: tensor(0.3275)\n",
      "25174 Training Loss: tensor(0.3279)\n",
      "25175 Training Loss: tensor(0.3280)\n",
      "25176 Training Loss: tensor(0.3273)\n",
      "25177 Training Loss: tensor(0.3273)\n",
      "25178 Training Loss: tensor(0.3274)\n",
      "25179 Training Loss: tensor(0.3281)\n",
      "25180 Training Loss: tensor(0.3288)\n",
      "25181 Training Loss: tensor(0.3319)\n",
      "25182 Training Loss: tensor(0.3306)\n",
      "25183 Training Loss: tensor(0.3274)\n",
      "25184 Training Loss: tensor(0.3286)\n",
      "25185 Training Loss: tensor(0.3275)\n",
      "25186 Training Loss: tensor(0.3277)\n",
      "25187 Training Loss: tensor(0.3289)\n",
      "25188 Training Loss: tensor(0.3284)\n",
      "25189 Training Loss: tensor(0.3283)\n",
      "25190 Training Loss: tensor(0.3283)\n",
      "25191 Training Loss: tensor(0.3287)\n",
      "25192 Training Loss: tensor(0.3314)\n",
      "25193 Training Loss: tensor(0.3278)\n",
      "25194 Training Loss: tensor(0.3271)\n",
      "25195 Training Loss: tensor(0.3273)\n",
      "25196 Training Loss: tensor(0.3288)\n",
      "25197 Training Loss: tensor(0.3285)\n",
      "25198 Training Loss: tensor(0.3279)\n",
      "25199 Training Loss: tensor(0.3276)\n",
      "25200 Training Loss: tensor(0.3274)\n",
      "25201 Training Loss: tensor(0.3273)\n",
      "25202 Training Loss: tensor(0.3288)\n",
      "25203 Training Loss: tensor(0.3278)\n",
      "25204 Training Loss: tensor(0.3271)\n",
      "25205 Training Loss: tensor(0.3285)\n",
      "25206 Training Loss: tensor(0.3285)\n",
      "25207 Training Loss: tensor(0.3279)\n",
      "25208 Training Loss: tensor(0.3279)\n",
      "25209 Training Loss: tensor(0.3271)\n",
      "25210 Training Loss: tensor(0.3273)\n",
      "25211 Training Loss: tensor(0.3277)\n",
      "25212 Training Loss: tensor(0.3345)\n",
      "25213 Training Loss: tensor(0.3287)\n",
      "25214 Training Loss: tensor(0.3290)\n",
      "25215 Training Loss: tensor(0.3294)\n",
      "25216 Training Loss: tensor(0.3281)\n",
      "25217 Training Loss: tensor(0.3287)\n",
      "25218 Training Loss: tensor(0.3294)\n",
      "25219 Training Loss: tensor(0.3281)\n",
      "25220 Training Loss: tensor(0.3283)\n",
      "25221 Training Loss: tensor(0.3285)\n",
      "25222 Training Loss: tensor(0.3276)\n",
      "25223 Training Loss: tensor(0.3301)\n",
      "25224 Training Loss: tensor(0.3277)\n",
      "25225 Training Loss: tensor(0.3276)\n",
      "25226 Training Loss: tensor(0.3282)\n",
      "25227 Training Loss: tensor(0.3305)\n",
      "25228 Training Loss: tensor(0.3276)\n",
      "25229 Training Loss: tensor(0.3275)\n",
      "25230 Training Loss: tensor(0.3281)\n",
      "25231 Training Loss: tensor(0.3272)\n",
      "25232 Training Loss: tensor(0.3269)\n",
      "25233 Training Loss: tensor(0.3271)\n",
      "25234 Training Loss: tensor(0.3282)\n",
      "25235 Training Loss: tensor(0.3286)\n",
      "25236 Training Loss: tensor(0.3275)\n",
      "25237 Training Loss: tensor(0.3279)\n",
      "25238 Training Loss: tensor(0.3288)\n",
      "25239 Training Loss: tensor(0.3281)\n",
      "25240 Training Loss: tensor(0.3279)\n",
      "25241 Training Loss: tensor(0.3270)\n",
      "25242 Training Loss: tensor(0.3275)\n",
      "25243 Training Loss: tensor(0.3277)\n",
      "25244 Training Loss: tensor(0.3273)\n",
      "25245 Training Loss: tensor(0.3290)\n",
      "25246 Training Loss: tensor(0.3280)\n",
      "25247 Training Loss: tensor(0.3292)\n",
      "25248 Training Loss: tensor(0.3276)\n",
      "25249 Training Loss: tensor(0.3272)\n",
      "25250 Training Loss: tensor(0.3284)\n",
      "25251 Training Loss: tensor(0.3283)\n",
      "25252 Training Loss: tensor(0.3293)\n",
      "25253 Training Loss: tensor(0.3303)\n",
      "25254 Training Loss: tensor(0.3308)\n",
      "25255 Training Loss: tensor(0.3276)\n",
      "25256 Training Loss: tensor(0.3275)\n",
      "25257 Training Loss: tensor(0.3285)\n",
      "25258 Training Loss: tensor(0.3293)\n",
      "25259 Training Loss: tensor(0.3288)\n",
      "25260 Training Loss: tensor(0.3280)\n",
      "25261 Training Loss: tensor(0.3282)\n",
      "25262 Training Loss: tensor(0.3270)\n",
      "25263 Training Loss: tensor(0.3311)\n",
      "25264 Training Loss: tensor(0.3282)\n",
      "25265 Training Loss: tensor(0.3282)\n",
      "25266 Training Loss: tensor(0.3285)\n",
      "25267 Training Loss: tensor(0.3291)\n",
      "25268 Training Loss: tensor(0.3357)\n",
      "25269 Training Loss: tensor(0.3289)\n",
      "25270 Training Loss: tensor(0.3277)\n",
      "25271 Training Loss: tensor(0.3296)\n",
      "25272 Training Loss: tensor(0.3306)\n",
      "25273 Training Loss: tensor(0.3322)\n",
      "25274 Training Loss: tensor(0.3297)\n",
      "25275 Training Loss: tensor(0.3286)\n",
      "25276 Training Loss: tensor(0.3290)\n",
      "25277 Training Loss: tensor(0.3286)\n",
      "25278 Training Loss: tensor(0.3283)\n",
      "25279 Training Loss: tensor(0.3284)\n",
      "25280 Training Loss: tensor(0.3285)\n",
      "25281 Training Loss: tensor(0.3280)\n",
      "25282 Training Loss: tensor(0.3289)\n",
      "25283 Training Loss: tensor(0.3286)\n",
      "25284 Training Loss: tensor(0.3277)\n",
      "25285 Training Loss: tensor(0.3291)\n",
      "25286 Training Loss: tensor(0.3279)\n",
      "25287 Training Loss: tensor(0.3284)\n",
      "25288 Training Loss: tensor(0.3279)\n",
      "25289 Training Loss: tensor(0.3284)\n",
      "25290 Training Loss: tensor(0.3277)\n",
      "25291 Training Loss: tensor(0.3302)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25292 Training Loss: tensor(0.3279)\n",
      "25293 Training Loss: tensor(0.3275)\n",
      "25294 Training Loss: tensor(0.3279)\n",
      "25295 Training Loss: tensor(0.3306)\n",
      "25296 Training Loss: tensor(0.3281)\n",
      "25297 Training Loss: tensor(0.3283)\n",
      "25298 Training Loss: tensor(0.3300)\n",
      "25299 Training Loss: tensor(0.3313)\n",
      "25300 Training Loss: tensor(0.3283)\n",
      "25301 Training Loss: tensor(0.3281)\n",
      "25302 Training Loss: tensor(0.3326)\n",
      "25303 Training Loss: tensor(0.3293)\n",
      "25304 Training Loss: tensor(0.3280)\n",
      "25305 Training Loss: tensor(0.3283)\n",
      "25306 Training Loss: tensor(0.3291)\n",
      "25307 Training Loss: tensor(0.3293)\n",
      "25308 Training Loss: tensor(0.3293)\n",
      "25309 Training Loss: tensor(0.3284)\n",
      "25310 Training Loss: tensor(0.3278)\n",
      "25311 Training Loss: tensor(0.3288)\n",
      "25312 Training Loss: tensor(0.3274)\n",
      "25313 Training Loss: tensor(0.3278)\n",
      "25314 Training Loss: tensor(0.3275)\n",
      "25315 Training Loss: tensor(0.3280)\n",
      "25316 Training Loss: tensor(0.3272)\n",
      "25317 Training Loss: tensor(0.3277)\n",
      "25318 Training Loss: tensor(0.3283)\n",
      "25319 Training Loss: tensor(0.3276)\n",
      "25320 Training Loss: tensor(0.3276)\n",
      "25321 Training Loss: tensor(0.3289)\n",
      "25322 Training Loss: tensor(0.3286)\n",
      "25323 Training Loss: tensor(0.3276)\n",
      "25324 Training Loss: tensor(0.3283)\n",
      "25325 Training Loss: tensor(0.3347)\n",
      "25326 Training Loss: tensor(0.3276)\n",
      "25327 Training Loss: tensor(0.3276)\n",
      "25328 Training Loss: tensor(0.3275)\n",
      "25329 Training Loss: tensor(0.3282)\n",
      "25330 Training Loss: tensor(0.3300)\n",
      "25331 Training Loss: tensor(0.3275)\n",
      "25332 Training Loss: tensor(0.3279)\n",
      "25333 Training Loss: tensor(0.3311)\n",
      "25334 Training Loss: tensor(0.3279)\n",
      "25335 Training Loss: tensor(0.3279)\n",
      "25336 Training Loss: tensor(0.3276)\n",
      "25337 Training Loss: tensor(0.3277)\n",
      "25338 Training Loss: tensor(0.3280)\n",
      "25339 Training Loss: tensor(0.3278)\n",
      "25340 Training Loss: tensor(0.3273)\n",
      "25341 Training Loss: tensor(0.3273)\n",
      "25342 Training Loss: tensor(0.3274)\n",
      "25343 Training Loss: tensor(0.3334)\n",
      "25344 Training Loss: tensor(0.3273)\n",
      "25345 Training Loss: tensor(0.3275)\n",
      "25346 Training Loss: tensor(0.3275)\n",
      "25347 Training Loss: tensor(0.3291)\n",
      "25348 Training Loss: tensor(0.3272)\n",
      "25349 Training Loss: tensor(0.3271)\n",
      "25350 Training Loss: tensor(0.3297)\n",
      "25351 Training Loss: tensor(0.3280)\n",
      "25352 Training Loss: tensor(0.3274)\n",
      "25353 Training Loss: tensor(0.3292)\n",
      "25354 Training Loss: tensor(0.3286)\n",
      "25355 Training Loss: tensor(0.3282)\n",
      "25356 Training Loss: tensor(0.3283)\n",
      "25357 Training Loss: tensor(0.3281)\n",
      "25358 Training Loss: tensor(0.3279)\n",
      "25359 Training Loss: tensor(0.3319)\n",
      "25360 Training Loss: tensor(0.3276)\n",
      "25361 Training Loss: tensor(0.3281)\n",
      "25362 Training Loss: tensor(0.3280)\n",
      "25363 Training Loss: tensor(0.3289)\n",
      "25364 Training Loss: tensor(0.3283)\n",
      "25365 Training Loss: tensor(0.3281)\n",
      "25366 Training Loss: tensor(0.3276)\n",
      "25367 Training Loss: tensor(0.3275)\n",
      "25368 Training Loss: tensor(0.3272)\n",
      "25369 Training Loss: tensor(0.3281)\n",
      "25370 Training Loss: tensor(0.3273)\n",
      "25371 Training Loss: tensor(0.3273)\n",
      "25372 Training Loss: tensor(0.3276)\n",
      "25373 Training Loss: tensor(0.3283)\n",
      "25374 Training Loss: tensor(0.3288)\n",
      "25375 Training Loss: tensor(0.3268)\n",
      "25376 Training Loss: tensor(0.3282)\n",
      "25377 Training Loss: tensor(0.3272)\n",
      "25378 Training Loss: tensor(0.3281)\n",
      "25379 Training Loss: tensor(0.3300)\n",
      "25380 Training Loss: tensor(0.3284)\n",
      "25381 Training Loss: tensor(0.3275)\n",
      "25382 Training Loss: tensor(0.3290)\n",
      "25383 Training Loss: tensor(0.3278)\n",
      "25384 Training Loss: tensor(0.3278)\n",
      "25385 Training Loss: tensor(0.3279)\n",
      "25386 Training Loss: tensor(0.3288)\n",
      "25387 Training Loss: tensor(0.3282)\n",
      "25388 Training Loss: tensor(0.3270)\n",
      "25389 Training Loss: tensor(0.3291)\n",
      "25390 Training Loss: tensor(0.3269)\n",
      "25391 Training Loss: tensor(0.3281)\n",
      "25392 Training Loss: tensor(0.3283)\n",
      "25393 Training Loss: tensor(0.3275)\n",
      "25394 Training Loss: tensor(0.3273)\n",
      "25395 Training Loss: tensor(0.3278)\n",
      "25396 Training Loss: tensor(0.3287)\n",
      "25397 Training Loss: tensor(0.3271)\n",
      "25398 Training Loss: tensor(0.3300)\n",
      "25399 Training Loss: tensor(0.3271)\n",
      "25400 Training Loss: tensor(0.3278)\n",
      "25401 Training Loss: tensor(0.3282)\n",
      "25402 Training Loss: tensor(0.3269)\n",
      "25403 Training Loss: tensor(0.3281)\n",
      "25404 Training Loss: tensor(0.3277)\n",
      "25405 Training Loss: tensor(0.3279)\n",
      "25406 Training Loss: tensor(0.3274)\n",
      "25407 Training Loss: tensor(0.3303)\n",
      "25408 Training Loss: tensor(0.3274)\n",
      "25409 Training Loss: tensor(0.3292)\n",
      "25410 Training Loss: tensor(0.3275)\n",
      "25411 Training Loss: tensor(0.3278)\n",
      "25412 Training Loss: tensor(0.3288)\n",
      "25413 Training Loss: tensor(0.3275)\n",
      "25414 Training Loss: tensor(0.3285)\n",
      "25415 Training Loss: tensor(0.3275)\n",
      "25416 Training Loss: tensor(0.3282)\n",
      "25417 Training Loss: tensor(0.3305)\n",
      "25418 Training Loss: tensor(0.3272)\n",
      "25419 Training Loss: tensor(0.3276)\n",
      "25420 Training Loss: tensor(0.3282)\n",
      "25421 Training Loss: tensor(0.3317)\n",
      "25422 Training Loss: tensor(0.3285)\n",
      "25423 Training Loss: tensor(0.3314)\n",
      "25424 Training Loss: tensor(0.3279)\n",
      "25425 Training Loss: tensor(0.3281)\n",
      "25426 Training Loss: tensor(0.3287)\n",
      "25427 Training Loss: tensor(0.3289)\n",
      "25428 Training Loss: tensor(0.3282)\n",
      "25429 Training Loss: tensor(0.3285)\n",
      "25430 Training Loss: tensor(0.3289)\n",
      "25431 Training Loss: tensor(0.3283)\n",
      "25432 Training Loss: tensor(0.3274)\n",
      "25433 Training Loss: tensor(0.3274)\n",
      "25434 Training Loss: tensor(0.3274)\n",
      "25435 Training Loss: tensor(0.3277)\n",
      "25436 Training Loss: tensor(0.3278)\n",
      "25437 Training Loss: tensor(0.3270)\n",
      "25438 Training Loss: tensor(0.3276)\n",
      "25439 Training Loss: tensor(0.3287)\n",
      "25440 Training Loss: tensor(0.3285)\n",
      "25441 Training Loss: tensor(0.3278)\n",
      "25442 Training Loss: tensor(0.3271)\n",
      "25443 Training Loss: tensor(0.3283)\n",
      "25444 Training Loss: tensor(0.3273)\n",
      "25445 Training Loss: tensor(0.3272)\n",
      "25446 Training Loss: tensor(0.3278)\n",
      "25447 Training Loss: tensor(0.3271)\n",
      "25448 Training Loss: tensor(0.3280)\n",
      "25449 Training Loss: tensor(0.3278)\n",
      "25450 Training Loss: tensor(0.3284)\n",
      "25451 Training Loss: tensor(0.3284)\n",
      "25452 Training Loss: tensor(0.3276)\n",
      "25453 Training Loss: tensor(0.3271)\n",
      "25454 Training Loss: tensor(0.3282)\n",
      "25455 Training Loss: tensor(0.3282)\n",
      "25456 Training Loss: tensor(0.3273)\n",
      "25457 Training Loss: tensor(0.3289)\n",
      "25458 Training Loss: tensor(0.3296)\n",
      "25459 Training Loss: tensor(0.3314)\n",
      "25460 Training Loss: tensor(0.3296)\n",
      "25461 Training Loss: tensor(0.3275)\n",
      "25462 Training Loss: tensor(0.3288)\n",
      "25463 Training Loss: tensor(0.3280)\n",
      "25464 Training Loss: tensor(0.3279)\n",
      "25465 Training Loss: tensor(0.3277)\n",
      "25466 Training Loss: tensor(0.3330)\n",
      "25467 Training Loss: tensor(0.3278)\n",
      "25468 Training Loss: tensor(0.3278)\n",
      "25469 Training Loss: tensor(0.3282)\n",
      "25470 Training Loss: tensor(0.3276)\n",
      "25471 Training Loss: tensor(0.3279)\n",
      "25472 Training Loss: tensor(0.3281)\n",
      "25473 Training Loss: tensor(0.3278)\n",
      "25474 Training Loss: tensor(0.3290)\n",
      "25475 Training Loss: tensor(0.3281)\n",
      "25476 Training Loss: tensor(0.3281)\n",
      "25477 Training Loss: tensor(0.3276)\n",
      "25478 Training Loss: tensor(0.3275)\n",
      "25479 Training Loss: tensor(0.3276)\n",
      "25480 Training Loss: tensor(0.3304)\n",
      "25481 Training Loss: tensor(0.3290)\n",
      "25482 Training Loss: tensor(0.3305)\n",
      "25483 Training Loss: tensor(0.3282)\n",
      "25484 Training Loss: tensor(0.3281)\n",
      "25485 Training Loss: tensor(0.3281)\n",
      "25486 Training Loss: tensor(0.3286)\n",
      "25487 Training Loss: tensor(0.3279)\n",
      "25488 Training Loss: tensor(0.3279)\n",
      "25489 Training Loss: tensor(0.3273)\n",
      "25490 Training Loss: tensor(0.3271)\n",
      "25491 Training Loss: tensor(0.3272)\n",
      "25492 Training Loss: tensor(0.3291)\n",
      "25493 Training Loss: tensor(0.3286)\n",
      "25494 Training Loss: tensor(0.3269)\n",
      "25495 Training Loss: tensor(0.3269)\n",
      "25496 Training Loss: tensor(0.3270)\n",
      "25497 Training Loss: tensor(0.3292)\n",
      "25498 Training Loss: tensor(0.3269)\n",
      "25499 Training Loss: tensor(0.3272)\n",
      "25500 Training Loss: tensor(0.3285)\n",
      "25501 Training Loss: tensor(0.3290)\n",
      "25502 Training Loss: tensor(0.3276)\n",
      "25503 Training Loss: tensor(0.3282)\n",
      "25504 Training Loss: tensor(0.3280)\n",
      "25505 Training Loss: tensor(0.3273)\n",
      "25506 Training Loss: tensor(0.3306)\n",
      "25507 Training Loss: tensor(0.3272)\n",
      "25508 Training Loss: tensor(0.3272)\n",
      "25509 Training Loss: tensor(0.3274)\n",
      "25510 Training Loss: tensor(0.3271)\n",
      "25511 Training Loss: tensor(0.3275)\n",
      "25512 Training Loss: tensor(0.3276)\n",
      "25513 Training Loss: tensor(0.3273)\n",
      "25514 Training Loss: tensor(0.3282)\n",
      "25515 Training Loss: tensor(0.3277)\n",
      "25516 Training Loss: tensor(0.3279)\n",
      "25517 Training Loss: tensor(0.3276)\n",
      "25518 Training Loss: tensor(0.3297)\n",
      "25519 Training Loss: tensor(0.3287)\n",
      "25520 Training Loss: tensor(0.3280)\n",
      "25521 Training Loss: tensor(0.3288)\n",
      "25522 Training Loss: tensor(0.3292)\n",
      "25523 Training Loss: tensor(0.3275)\n",
      "25524 Training Loss: tensor(0.3286)\n",
      "25525 Training Loss: tensor(0.3299)\n",
      "25526 Training Loss: tensor(0.3288)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25527 Training Loss: tensor(0.3279)\n",
      "25528 Training Loss: tensor(0.3273)\n",
      "25529 Training Loss: tensor(0.3282)\n",
      "25530 Training Loss: tensor(0.3274)\n",
      "25531 Training Loss: tensor(0.3275)\n",
      "25532 Training Loss: tensor(0.3284)\n",
      "25533 Training Loss: tensor(0.3274)\n",
      "25534 Training Loss: tensor(0.3271)\n",
      "25535 Training Loss: tensor(0.3276)\n",
      "25536 Training Loss: tensor(0.3277)\n",
      "25537 Training Loss: tensor(0.3294)\n",
      "25538 Training Loss: tensor(0.3305)\n",
      "25539 Training Loss: tensor(0.3287)\n",
      "25540 Training Loss: tensor(0.3294)\n",
      "25541 Training Loss: tensor(0.3284)\n",
      "25542 Training Loss: tensor(0.3291)\n",
      "25543 Training Loss: tensor(0.3281)\n",
      "25544 Training Loss: tensor(0.3289)\n",
      "25545 Training Loss: tensor(0.3281)\n",
      "25546 Training Loss: tensor(0.3279)\n",
      "25547 Training Loss: tensor(0.3282)\n",
      "25548 Training Loss: tensor(0.3274)\n",
      "25549 Training Loss: tensor(0.3285)\n",
      "25550 Training Loss: tensor(0.3276)\n",
      "25551 Training Loss: tensor(0.3281)\n",
      "25552 Training Loss: tensor(0.3324)\n",
      "25553 Training Loss: tensor(0.3288)\n",
      "25554 Training Loss: tensor(0.3281)\n",
      "25555 Training Loss: tensor(0.3282)\n",
      "25556 Training Loss: tensor(0.3331)\n",
      "25557 Training Loss: tensor(0.3280)\n",
      "25558 Training Loss: tensor(0.3285)\n",
      "25559 Training Loss: tensor(0.3296)\n",
      "25560 Training Loss: tensor(0.3279)\n",
      "25561 Training Loss: tensor(0.3283)\n",
      "25562 Training Loss: tensor(0.3286)\n",
      "25563 Training Loss: tensor(0.3276)\n",
      "25564 Training Loss: tensor(0.3286)\n",
      "25565 Training Loss: tensor(0.3290)\n",
      "25566 Training Loss: tensor(0.3283)\n",
      "25567 Training Loss: tensor(0.3281)\n",
      "25568 Training Loss: tensor(0.3276)\n",
      "25569 Training Loss: tensor(0.3291)\n",
      "25570 Training Loss: tensor(0.3284)\n",
      "25571 Training Loss: tensor(0.3280)\n",
      "25572 Training Loss: tensor(0.3273)\n",
      "25573 Training Loss: tensor(0.3287)\n",
      "25574 Training Loss: tensor(0.3274)\n",
      "25575 Training Loss: tensor(0.3272)\n",
      "25576 Training Loss: tensor(0.3274)\n",
      "25577 Training Loss: tensor(0.3271)\n",
      "25578 Training Loss: tensor(0.3272)\n",
      "25579 Training Loss: tensor(0.3277)\n",
      "25580 Training Loss: tensor(0.3287)\n",
      "25581 Training Loss: tensor(0.3306)\n",
      "25582 Training Loss: tensor(0.3276)\n",
      "25583 Training Loss: tensor(0.3271)\n",
      "25584 Training Loss: tensor(0.3338)\n",
      "25585 Training Loss: tensor(0.3276)\n",
      "25586 Training Loss: tensor(0.3277)\n",
      "25587 Training Loss: tensor(0.3276)\n",
      "25588 Training Loss: tensor(0.3287)\n",
      "25589 Training Loss: tensor(0.3285)\n",
      "25590 Training Loss: tensor(0.3280)\n",
      "25591 Training Loss: tensor(0.3275)\n",
      "25592 Training Loss: tensor(0.3278)\n",
      "25593 Training Loss: tensor(0.3277)\n",
      "25594 Training Loss: tensor(0.3282)\n",
      "25595 Training Loss: tensor(0.3277)\n",
      "25596 Training Loss: tensor(0.3280)\n",
      "25597 Training Loss: tensor(0.3276)\n",
      "25598 Training Loss: tensor(0.3274)\n",
      "25599 Training Loss: tensor(0.3271)\n",
      "25600 Training Loss: tensor(0.3276)\n",
      "25601 Training Loss: tensor(0.3281)\n",
      "25602 Training Loss: tensor(0.3292)\n",
      "25603 Training Loss: tensor(0.3274)\n",
      "25604 Training Loss: tensor(0.3270)\n",
      "25605 Training Loss: tensor(0.3272)\n",
      "25606 Training Loss: tensor(0.3288)\n",
      "25607 Training Loss: tensor(0.3272)\n",
      "25608 Training Loss: tensor(0.3289)\n",
      "25609 Training Loss: tensor(0.3271)\n",
      "25610 Training Loss: tensor(0.3305)\n",
      "25611 Training Loss: tensor(0.3270)\n",
      "25612 Training Loss: tensor(0.3267)\n",
      "25613 Training Loss: tensor(0.3272)\n",
      "25614 Training Loss: tensor(0.3294)\n",
      "25615 Training Loss: tensor(0.3274)\n",
      "25616 Training Loss: tensor(0.3274)\n",
      "25617 Training Loss: tensor(0.3273)\n",
      "25618 Training Loss: tensor(0.3279)\n",
      "25619 Training Loss: tensor(0.3273)\n",
      "25620 Training Loss: tensor(0.3275)\n",
      "25621 Training Loss: tensor(0.3282)\n",
      "25622 Training Loss: tensor(0.3287)\n",
      "25623 Training Loss: tensor(0.3271)\n",
      "25624 Training Loss: tensor(0.3277)\n",
      "25625 Training Loss: tensor(0.3285)\n",
      "25626 Training Loss: tensor(0.3269)\n",
      "25627 Training Loss: tensor(0.3293)\n",
      "25628 Training Loss: tensor(0.3274)\n",
      "25629 Training Loss: tensor(0.3279)\n",
      "25630 Training Loss: tensor(0.3282)\n",
      "25631 Training Loss: tensor(0.3277)\n",
      "25632 Training Loss: tensor(0.3272)\n",
      "25633 Training Loss: tensor(0.3292)\n",
      "25634 Training Loss: tensor(0.3272)\n",
      "25635 Training Loss: tensor(0.3291)\n",
      "25636 Training Loss: tensor(0.3280)\n",
      "25637 Training Loss: tensor(0.3268)\n",
      "25638 Training Loss: tensor(0.3295)\n",
      "25639 Training Loss: tensor(0.3278)\n",
      "25640 Training Loss: tensor(0.3297)\n",
      "25641 Training Loss: tensor(0.3277)\n",
      "25642 Training Loss: tensor(0.3281)\n",
      "25643 Training Loss: tensor(0.3272)\n",
      "25644 Training Loss: tensor(0.3277)\n",
      "25645 Training Loss: tensor(0.3281)\n",
      "25646 Training Loss: tensor(0.3286)\n",
      "25647 Training Loss: tensor(0.3271)\n",
      "25648 Training Loss: tensor(0.3270)\n",
      "25649 Training Loss: tensor(0.3272)\n",
      "25650 Training Loss: tensor(0.3293)\n",
      "25651 Training Loss: tensor(0.3287)\n",
      "25652 Training Loss: tensor(0.3310)\n",
      "25653 Training Loss: tensor(0.3271)\n",
      "25654 Training Loss: tensor(0.3271)\n",
      "25655 Training Loss: tensor(0.3274)\n",
      "25656 Training Loss: tensor(0.3274)\n",
      "25657 Training Loss: tensor(0.3273)\n",
      "25658 Training Loss: tensor(0.3273)\n",
      "25659 Training Loss: tensor(0.3278)\n",
      "25660 Training Loss: tensor(0.3270)\n",
      "25661 Training Loss: tensor(0.3282)\n",
      "25662 Training Loss: tensor(0.3308)\n",
      "25663 Training Loss: tensor(0.3289)\n",
      "25664 Training Loss: tensor(0.3276)\n",
      "25665 Training Loss: tensor(0.3291)\n",
      "25666 Training Loss: tensor(0.3282)\n",
      "25667 Training Loss: tensor(0.3276)\n",
      "25668 Training Loss: tensor(0.3272)\n",
      "25669 Training Loss: tensor(0.3275)\n",
      "25670 Training Loss: tensor(0.3276)\n",
      "25671 Training Loss: tensor(0.3291)\n",
      "25672 Training Loss: tensor(0.3271)\n",
      "25673 Training Loss: tensor(0.3310)\n",
      "25674 Training Loss: tensor(0.3274)\n",
      "25675 Training Loss: tensor(0.3286)\n",
      "25676 Training Loss: tensor(0.3276)\n",
      "25677 Training Loss: tensor(0.3277)\n",
      "25678 Training Loss: tensor(0.3289)\n",
      "25679 Training Loss: tensor(0.3282)\n",
      "25680 Training Loss: tensor(0.3271)\n",
      "25681 Training Loss: tensor(0.3276)\n",
      "25682 Training Loss: tensor(0.3294)\n",
      "25683 Training Loss: tensor(0.3273)\n",
      "25684 Training Loss: tensor(0.3275)\n",
      "25685 Training Loss: tensor(0.3278)\n",
      "25686 Training Loss: tensor(0.3270)\n",
      "25687 Training Loss: tensor(0.3273)\n",
      "25688 Training Loss: tensor(0.3287)\n",
      "25689 Training Loss: tensor(0.3283)\n",
      "25690 Training Loss: tensor(0.3273)\n",
      "25691 Training Loss: tensor(0.3268)\n",
      "25692 Training Loss: tensor(0.3281)\n",
      "25693 Training Loss: tensor(0.3283)\n",
      "25694 Training Loss: tensor(0.3282)\n",
      "25695 Training Loss: tensor(0.3285)\n",
      "25696 Training Loss: tensor(0.3275)\n",
      "25697 Training Loss: tensor(0.3269)\n",
      "25698 Training Loss: tensor(0.3278)\n",
      "25699 Training Loss: tensor(0.3301)\n",
      "25700 Training Loss: tensor(0.3271)\n",
      "25701 Training Loss: tensor(0.3278)\n",
      "25702 Training Loss: tensor(0.3271)\n",
      "25703 Training Loss: tensor(0.3276)\n",
      "25704 Training Loss: tensor(0.3285)\n",
      "25705 Training Loss: tensor(0.3269)\n",
      "25706 Training Loss: tensor(0.3284)\n",
      "25707 Training Loss: tensor(0.3313)\n",
      "25708 Training Loss: tensor(0.3274)\n",
      "25709 Training Loss: tensor(0.3287)\n",
      "25710 Training Loss: tensor(0.3278)\n",
      "25711 Training Loss: tensor(0.3283)\n",
      "25712 Training Loss: tensor(0.3287)\n",
      "25713 Training Loss: tensor(0.3277)\n",
      "25714 Training Loss: tensor(0.3282)\n",
      "25715 Training Loss: tensor(0.3279)\n",
      "25716 Training Loss: tensor(0.3274)\n",
      "25717 Training Loss: tensor(0.3275)\n",
      "25718 Training Loss: tensor(0.3283)\n",
      "25719 Training Loss: tensor(0.3279)\n",
      "25720 Training Loss: tensor(0.3271)\n",
      "25721 Training Loss: tensor(0.3276)\n",
      "25722 Training Loss: tensor(0.3288)\n",
      "25723 Training Loss: tensor(0.3278)\n",
      "25724 Training Loss: tensor(0.3274)\n",
      "25725 Training Loss: tensor(0.3281)\n",
      "25726 Training Loss: tensor(0.3269)\n",
      "25727 Training Loss: tensor(0.3271)\n",
      "25728 Training Loss: tensor(0.3298)\n",
      "25729 Training Loss: tensor(0.3273)\n",
      "25730 Training Loss: tensor(0.3291)\n",
      "25731 Training Loss: tensor(0.3282)\n",
      "25732 Training Loss: tensor(0.3275)\n",
      "25733 Training Loss: tensor(0.3276)\n",
      "25734 Training Loss: tensor(0.3269)\n",
      "25735 Training Loss: tensor(0.3280)\n",
      "25736 Training Loss: tensor(0.3280)\n",
      "25737 Training Loss: tensor(0.3274)\n",
      "25738 Training Loss: tensor(0.3268)\n",
      "25739 Training Loss: tensor(0.3292)\n",
      "25740 Training Loss: tensor(0.3274)\n",
      "25741 Training Loss: tensor(0.3271)\n",
      "25742 Training Loss: tensor(0.3277)\n",
      "25743 Training Loss: tensor(0.3290)\n",
      "25744 Training Loss: tensor(0.3272)\n",
      "25745 Training Loss: tensor(0.3272)\n",
      "25746 Training Loss: tensor(0.3272)\n",
      "25747 Training Loss: tensor(0.3270)\n",
      "25748 Training Loss: tensor(0.3285)\n",
      "25749 Training Loss: tensor(0.3273)\n",
      "25750 Training Loss: tensor(0.3270)\n",
      "25751 Training Loss: tensor(0.3271)\n",
      "25752 Training Loss: tensor(0.3269)\n",
      "25753 Training Loss: tensor(0.3291)\n",
      "25754 Training Loss: tensor(0.3267)\n",
      "25755 Training Loss: tensor(0.3276)\n",
      "25756 Training Loss: tensor(0.3272)\n",
      "25757 Training Loss: tensor(0.3269)\n",
      "25758 Training Loss: tensor(0.3296)\n",
      "25759 Training Loss: tensor(0.3283)\n",
      "25760 Training Loss: tensor(0.3275)\n",
      "25761 Training Loss: tensor(0.3289)\n",
      "25762 Training Loss: tensor(0.3272)\n",
      "25763 Training Loss: tensor(0.3286)\n",
      "25764 Training Loss: tensor(0.3273)\n",
      "25765 Training Loss: tensor(0.3270)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25766 Training Loss: tensor(0.3273)\n",
      "25767 Training Loss: tensor(0.3273)\n",
      "25768 Training Loss: tensor(0.3273)\n",
      "25769 Training Loss: tensor(0.3277)\n",
      "25770 Training Loss: tensor(0.3275)\n",
      "25771 Training Loss: tensor(0.3283)\n",
      "25772 Training Loss: tensor(0.3270)\n",
      "25773 Training Loss: tensor(0.3271)\n",
      "25774 Training Loss: tensor(0.3271)\n",
      "25775 Training Loss: tensor(0.3277)\n",
      "25776 Training Loss: tensor(0.3269)\n",
      "25777 Training Loss: tensor(0.3270)\n",
      "25778 Training Loss: tensor(0.3282)\n",
      "25779 Training Loss: tensor(0.3290)\n",
      "25780 Training Loss: tensor(0.3276)\n",
      "25781 Training Loss: tensor(0.3269)\n",
      "25782 Training Loss: tensor(0.3282)\n",
      "25783 Training Loss: tensor(0.3273)\n",
      "25784 Training Loss: tensor(0.3287)\n",
      "25785 Training Loss: tensor(0.3277)\n",
      "25786 Training Loss: tensor(0.3277)\n",
      "25787 Training Loss: tensor(0.3274)\n",
      "25788 Training Loss: tensor(0.3278)\n",
      "25789 Training Loss: tensor(0.3290)\n",
      "25790 Training Loss: tensor(0.3278)\n",
      "25791 Training Loss: tensor(0.3274)\n",
      "25792 Training Loss: tensor(0.3274)\n",
      "25793 Training Loss: tensor(0.3296)\n",
      "25794 Training Loss: tensor(0.3273)\n",
      "25795 Training Loss: tensor(0.3274)\n",
      "25796 Training Loss: tensor(0.3277)\n",
      "25797 Training Loss: tensor(0.3277)\n",
      "25798 Training Loss: tensor(0.3270)\n",
      "25799 Training Loss: tensor(0.3288)\n",
      "25800 Training Loss: tensor(0.3281)\n",
      "25801 Training Loss: tensor(0.3273)\n",
      "25802 Training Loss: tensor(0.3286)\n",
      "25803 Training Loss: tensor(0.3295)\n",
      "25804 Training Loss: tensor(0.3277)\n",
      "25805 Training Loss: tensor(0.3278)\n",
      "25806 Training Loss: tensor(0.3270)\n",
      "25807 Training Loss: tensor(0.3281)\n",
      "25808 Training Loss: tensor(0.3275)\n",
      "25809 Training Loss: tensor(0.3280)\n",
      "25810 Training Loss: tensor(0.3276)\n",
      "25811 Training Loss: tensor(0.3277)\n",
      "25812 Training Loss: tensor(0.3280)\n",
      "25813 Training Loss: tensor(0.3278)\n",
      "25814 Training Loss: tensor(0.3266)\n",
      "25815 Training Loss: tensor(0.3266)\n",
      "25816 Training Loss: tensor(0.3273)\n",
      "25817 Training Loss: tensor(0.3267)\n",
      "25818 Training Loss: tensor(0.3276)\n",
      "25819 Training Loss: tensor(0.3267)\n",
      "25820 Training Loss: tensor(0.3274)\n",
      "25821 Training Loss: tensor(0.3275)\n",
      "25822 Training Loss: tensor(0.3269)\n",
      "25823 Training Loss: tensor(0.3271)\n",
      "25824 Training Loss: tensor(0.3300)\n",
      "25825 Training Loss: tensor(0.3273)\n",
      "25826 Training Loss: tensor(0.3268)\n",
      "25827 Training Loss: tensor(0.3272)\n",
      "25828 Training Loss: tensor(0.3287)\n",
      "25829 Training Loss: tensor(0.3281)\n",
      "25830 Training Loss: tensor(0.3280)\n",
      "25831 Training Loss: tensor(0.3277)\n",
      "25832 Training Loss: tensor(0.3270)\n",
      "25833 Training Loss: tensor(0.3271)\n",
      "25834 Training Loss: tensor(0.3277)\n",
      "25835 Training Loss: tensor(0.3296)\n",
      "25836 Training Loss: tensor(0.3279)\n",
      "25837 Training Loss: tensor(0.3275)\n",
      "25838 Training Loss: tensor(0.3271)\n",
      "25839 Training Loss: tensor(0.3282)\n",
      "25840 Training Loss: tensor(0.3270)\n",
      "25841 Training Loss: tensor(0.3283)\n",
      "25842 Training Loss: tensor(0.3278)\n",
      "25843 Training Loss: tensor(0.3270)\n",
      "25844 Training Loss: tensor(0.3271)\n",
      "25845 Training Loss: tensor(0.3272)\n",
      "25846 Training Loss: tensor(0.3276)\n",
      "25847 Training Loss: tensor(0.3274)\n",
      "25848 Training Loss: tensor(0.3281)\n",
      "25849 Training Loss: tensor(0.3276)\n",
      "25850 Training Loss: tensor(0.3278)\n",
      "25851 Training Loss: tensor(0.3267)\n",
      "25852 Training Loss: tensor(0.3294)\n",
      "25853 Training Loss: tensor(0.3279)\n",
      "25854 Training Loss: tensor(0.3284)\n",
      "25855 Training Loss: tensor(0.3275)\n",
      "25856 Training Loss: tensor(0.3276)\n",
      "25857 Training Loss: tensor(0.3274)\n",
      "25858 Training Loss: tensor(0.3281)\n",
      "25859 Training Loss: tensor(0.3270)\n",
      "25860 Training Loss: tensor(0.3273)\n",
      "25861 Training Loss: tensor(0.3270)\n",
      "25862 Training Loss: tensor(0.3272)\n",
      "25863 Training Loss: tensor(0.3270)\n",
      "25864 Training Loss: tensor(0.3271)\n",
      "25865 Training Loss: tensor(0.3269)\n",
      "25866 Training Loss: tensor(0.3273)\n",
      "25867 Training Loss: tensor(0.3276)\n",
      "25868 Training Loss: tensor(0.3280)\n",
      "25869 Training Loss: tensor(0.3288)\n",
      "25870 Training Loss: tensor(0.3290)\n",
      "25871 Training Loss: tensor(0.3271)\n",
      "25872 Training Loss: tensor(0.3275)\n",
      "25873 Training Loss: tensor(0.3276)\n",
      "25874 Training Loss: tensor(0.3275)\n",
      "25875 Training Loss: tensor(0.3270)\n",
      "25876 Training Loss: tensor(0.3281)\n",
      "25877 Training Loss: tensor(0.3269)\n",
      "25878 Training Loss: tensor(0.3270)\n",
      "25879 Training Loss: tensor(0.3297)\n",
      "25880 Training Loss: tensor(0.3276)\n",
      "25881 Training Loss: tensor(0.3296)\n",
      "25882 Training Loss: tensor(0.3274)\n",
      "25883 Training Loss: tensor(0.3274)\n",
      "25884 Training Loss: tensor(0.3269)\n",
      "25885 Training Loss: tensor(0.3280)\n",
      "25886 Training Loss: tensor(0.3273)\n",
      "25887 Training Loss: tensor(0.3285)\n",
      "25888 Training Loss: tensor(0.3269)\n",
      "25889 Training Loss: tensor(0.3281)\n",
      "25890 Training Loss: tensor(0.3284)\n",
      "25891 Training Loss: tensor(0.3270)\n",
      "25892 Training Loss: tensor(0.3273)\n",
      "25893 Training Loss: tensor(0.3271)\n",
      "25894 Training Loss: tensor(0.3277)\n",
      "25895 Training Loss: tensor(0.3276)\n",
      "25896 Training Loss: tensor(0.3289)\n",
      "25897 Training Loss: tensor(0.3293)\n",
      "25898 Training Loss: tensor(0.3282)\n",
      "25899 Training Loss: tensor(0.3270)\n",
      "25900 Training Loss: tensor(0.3289)\n",
      "25901 Training Loss: tensor(0.3278)\n",
      "25902 Training Loss: tensor(0.3270)\n",
      "25903 Training Loss: tensor(0.3270)\n",
      "25904 Training Loss: tensor(0.3277)\n",
      "25905 Training Loss: tensor(0.3272)\n",
      "25906 Training Loss: tensor(0.3285)\n",
      "25907 Training Loss: tensor(0.3274)\n",
      "25908 Training Loss: tensor(0.3275)\n",
      "25909 Training Loss: tensor(0.3296)\n",
      "25910 Training Loss: tensor(0.3271)\n",
      "25911 Training Loss: tensor(0.3281)\n",
      "25912 Training Loss: tensor(0.3276)\n",
      "25913 Training Loss: tensor(0.3274)\n",
      "25914 Training Loss: tensor(0.3272)\n",
      "25915 Training Loss: tensor(0.3275)\n",
      "25916 Training Loss: tensor(0.3271)\n",
      "25917 Training Loss: tensor(0.3273)\n",
      "25918 Training Loss: tensor(0.3288)\n",
      "25919 Training Loss: tensor(0.3288)\n",
      "25920 Training Loss: tensor(0.3270)\n",
      "25921 Training Loss: tensor(0.3281)\n",
      "25922 Training Loss: tensor(0.3279)\n",
      "25923 Training Loss: tensor(0.3276)\n",
      "25924 Training Loss: tensor(0.3270)\n",
      "25925 Training Loss: tensor(0.3277)\n",
      "25926 Training Loss: tensor(0.3278)\n",
      "25927 Training Loss: tensor(0.3274)\n",
      "25928 Training Loss: tensor(0.3279)\n",
      "25929 Training Loss: tensor(0.3272)\n",
      "25930 Training Loss: tensor(0.3278)\n",
      "25931 Training Loss: tensor(0.3276)\n",
      "25932 Training Loss: tensor(0.3279)\n",
      "25933 Training Loss: tensor(0.3272)\n",
      "25934 Training Loss: tensor(0.3267)\n",
      "25935 Training Loss: tensor(0.3268)\n",
      "25936 Training Loss: tensor(0.3276)\n",
      "25937 Training Loss: tensor(0.3278)\n",
      "25938 Training Loss: tensor(0.3272)\n",
      "25939 Training Loss: tensor(0.3276)\n",
      "25940 Training Loss: tensor(0.3268)\n",
      "25941 Training Loss: tensor(0.3320)\n",
      "25942 Training Loss: tensor(0.3268)\n",
      "25943 Training Loss: tensor(0.3268)\n",
      "25944 Training Loss: tensor(0.3301)\n",
      "25945 Training Loss: tensor(0.3282)\n",
      "25946 Training Loss: tensor(0.3281)\n",
      "25947 Training Loss: tensor(0.3279)\n",
      "25948 Training Loss: tensor(0.3285)\n",
      "25949 Training Loss: tensor(0.3273)\n",
      "25950 Training Loss: tensor(0.3270)\n",
      "25951 Training Loss: tensor(0.3272)\n",
      "25952 Training Loss: tensor(0.3269)\n",
      "25953 Training Loss: tensor(0.3310)\n",
      "25954 Training Loss: tensor(0.3275)\n",
      "25955 Training Loss: tensor(0.3294)\n",
      "25956 Training Loss: tensor(0.3279)\n",
      "25957 Training Loss: tensor(0.3283)\n",
      "25958 Training Loss: tensor(0.3277)\n",
      "25959 Training Loss: tensor(0.3286)\n",
      "25960 Training Loss: tensor(0.3270)\n",
      "25961 Training Loss: tensor(0.3282)\n",
      "25962 Training Loss: tensor(0.3270)\n",
      "25963 Training Loss: tensor(0.3270)\n",
      "25964 Training Loss: tensor(0.3270)\n",
      "25965 Training Loss: tensor(0.3298)\n",
      "25966 Training Loss: tensor(0.3271)\n",
      "25967 Training Loss: tensor(0.3292)\n",
      "25968 Training Loss: tensor(0.3283)\n",
      "25969 Training Loss: tensor(0.3276)\n",
      "25970 Training Loss: tensor(0.3278)\n",
      "25971 Training Loss: tensor(0.3290)\n",
      "25972 Training Loss: tensor(0.3272)\n",
      "25973 Training Loss: tensor(0.3286)\n",
      "25974 Training Loss: tensor(0.3270)\n",
      "25975 Training Loss: tensor(0.3283)\n",
      "25976 Training Loss: tensor(0.3272)\n",
      "25977 Training Loss: tensor(0.3281)\n",
      "25978 Training Loss: tensor(0.3276)\n",
      "25979 Training Loss: tensor(0.3270)\n",
      "25980 Training Loss: tensor(0.3274)\n",
      "25981 Training Loss: tensor(0.3272)\n",
      "25982 Training Loss: tensor(0.3274)\n",
      "25983 Training Loss: tensor(0.3272)\n",
      "25984 Training Loss: tensor(0.3273)\n",
      "25985 Training Loss: tensor(0.3286)\n",
      "25986 Training Loss: tensor(0.3282)\n",
      "25987 Training Loss: tensor(0.3275)\n",
      "25988 Training Loss: tensor(0.3278)\n",
      "25989 Training Loss: tensor(0.3269)\n",
      "25990 Training Loss: tensor(0.3269)\n",
      "25991 Training Loss: tensor(0.3270)\n",
      "25992 Training Loss: tensor(0.3326)\n",
      "25993 Training Loss: tensor(0.3290)\n",
      "25994 Training Loss: tensor(0.3278)\n",
      "25995 Training Loss: tensor(0.3276)\n",
      "25996 Training Loss: tensor(0.3280)\n",
      "25997 Training Loss: tensor(0.3274)\n",
      "25998 Training Loss: tensor(0.3299)\n",
      "25999 Training Loss: tensor(0.3288)\n",
      "26000 Training Loss: tensor(0.3276)\n",
      "26001 Training Loss: tensor(0.3269)\n",
      "26002 Training Loss: tensor(0.3282)\n",
      "26003 Training Loss: tensor(0.3274)\n",
      "26004 Training Loss: tensor(0.3269)\n",
      "26005 Training Loss: tensor(0.3269)\n",
      "26006 Training Loss: tensor(0.3274)\n",
      "26007 Training Loss: tensor(0.3282)\n",
      "26008 Training Loss: tensor(0.3286)\n",
      "26009 Training Loss: tensor(0.3282)\n",
      "26010 Training Loss: tensor(0.3290)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26011 Training Loss: tensor(0.3282)\n",
      "26012 Training Loss: tensor(0.3274)\n",
      "26013 Training Loss: tensor(0.3280)\n",
      "26014 Training Loss: tensor(0.3276)\n",
      "26015 Training Loss: tensor(0.3276)\n",
      "26016 Training Loss: tensor(0.3291)\n",
      "26017 Training Loss: tensor(0.3274)\n",
      "26018 Training Loss: tensor(0.3283)\n",
      "26019 Training Loss: tensor(0.3276)\n",
      "26020 Training Loss: tensor(0.3289)\n",
      "26021 Training Loss: tensor(0.3272)\n",
      "26022 Training Loss: tensor(0.3272)\n",
      "26023 Training Loss: tensor(0.3296)\n",
      "26024 Training Loss: tensor(0.3283)\n",
      "26025 Training Loss: tensor(0.3281)\n",
      "26026 Training Loss: tensor(0.3303)\n",
      "26027 Training Loss: tensor(0.3278)\n",
      "26028 Training Loss: tensor(0.3269)\n",
      "26029 Training Loss: tensor(0.3281)\n",
      "26030 Training Loss: tensor(0.3282)\n",
      "26031 Training Loss: tensor(0.3270)\n",
      "26032 Training Loss: tensor(0.3275)\n",
      "26033 Training Loss: tensor(0.3285)\n",
      "26034 Training Loss: tensor(0.3270)\n",
      "26035 Training Loss: tensor(0.3292)\n",
      "26036 Training Loss: tensor(0.3275)\n",
      "26037 Training Loss: tensor(0.3278)\n",
      "26038 Training Loss: tensor(0.3272)\n",
      "26039 Training Loss: tensor(0.3277)\n",
      "26040 Training Loss: tensor(0.3274)\n",
      "26041 Training Loss: tensor(0.3270)\n",
      "26042 Training Loss: tensor(0.3269)\n",
      "26043 Training Loss: tensor(0.3270)\n",
      "26044 Training Loss: tensor(0.3268)\n",
      "26045 Training Loss: tensor(0.3284)\n",
      "26046 Training Loss: tensor(0.3279)\n",
      "26047 Training Loss: tensor(0.3267)\n",
      "26048 Training Loss: tensor(0.3268)\n",
      "26049 Training Loss: tensor(0.3267)\n",
      "26050 Training Loss: tensor(0.3277)\n",
      "26051 Training Loss: tensor(0.3281)\n",
      "26052 Training Loss: tensor(0.3275)\n",
      "26053 Training Loss: tensor(0.3270)\n",
      "26054 Training Loss: tensor(0.3271)\n",
      "26055 Training Loss: tensor(0.3270)\n",
      "26056 Training Loss: tensor(0.3286)\n",
      "26057 Training Loss: tensor(0.3275)\n",
      "26058 Training Loss: tensor(0.3276)\n",
      "26059 Training Loss: tensor(0.3272)\n",
      "26060 Training Loss: tensor(0.3277)\n",
      "26061 Training Loss: tensor(0.3275)\n",
      "26062 Training Loss: tensor(0.3281)\n",
      "26063 Training Loss: tensor(0.3269)\n",
      "26064 Training Loss: tensor(0.3268)\n",
      "26065 Training Loss: tensor(0.3284)\n",
      "26066 Training Loss: tensor(0.3270)\n",
      "26067 Training Loss: tensor(0.3269)\n",
      "26068 Training Loss: tensor(0.3270)\n",
      "26069 Training Loss: tensor(0.3269)\n",
      "26070 Training Loss: tensor(0.3295)\n",
      "26071 Training Loss: tensor(0.3287)\n",
      "26072 Training Loss: tensor(0.3272)\n",
      "26073 Training Loss: tensor(0.3288)\n",
      "26074 Training Loss: tensor(0.3266)\n",
      "26075 Training Loss: tensor(0.3289)\n",
      "26076 Training Loss: tensor(0.3278)\n",
      "26077 Training Loss: tensor(0.3275)\n",
      "26078 Training Loss: tensor(0.3274)\n",
      "26079 Training Loss: tensor(0.3271)\n",
      "26080 Training Loss: tensor(0.3277)\n",
      "26081 Training Loss: tensor(0.3274)\n",
      "26082 Training Loss: tensor(0.3270)\n",
      "26083 Training Loss: tensor(0.3289)\n",
      "26084 Training Loss: tensor(0.3283)\n",
      "26085 Training Loss: tensor(0.3265)\n",
      "26086 Training Loss: tensor(0.3270)\n",
      "26087 Training Loss: tensor(0.3275)\n",
      "26088 Training Loss: tensor(0.3289)\n",
      "26089 Training Loss: tensor(0.3267)\n",
      "26090 Training Loss: tensor(0.3270)\n",
      "26091 Training Loss: tensor(0.3290)\n",
      "26092 Training Loss: tensor(0.3291)\n",
      "26093 Training Loss: tensor(0.3274)\n",
      "26094 Training Loss: tensor(0.3272)\n",
      "26095 Training Loss: tensor(0.3301)\n",
      "26096 Training Loss: tensor(0.3275)\n",
      "26097 Training Loss: tensor(0.3277)\n",
      "26098 Training Loss: tensor(0.3285)\n",
      "26099 Training Loss: tensor(0.3281)\n",
      "26100 Training Loss: tensor(0.3274)\n",
      "26101 Training Loss: tensor(0.3280)\n",
      "26102 Training Loss: tensor(0.3284)\n",
      "26103 Training Loss: tensor(0.3277)\n",
      "26104 Training Loss: tensor(0.3281)\n",
      "26105 Training Loss: tensor(0.3272)\n",
      "26106 Training Loss: tensor(0.3269)\n",
      "26107 Training Loss: tensor(0.3283)\n",
      "26108 Training Loss: tensor(0.3277)\n",
      "26109 Training Loss: tensor(0.3272)\n",
      "26110 Training Loss: tensor(0.3279)\n",
      "26111 Training Loss: tensor(0.3275)\n",
      "26112 Training Loss: tensor(0.3274)\n",
      "26113 Training Loss: tensor(0.3272)\n",
      "26114 Training Loss: tensor(0.3271)\n",
      "26115 Training Loss: tensor(0.3280)\n",
      "26116 Training Loss: tensor(0.3271)\n",
      "26117 Training Loss: tensor(0.3287)\n",
      "26118 Training Loss: tensor(0.3283)\n",
      "26119 Training Loss: tensor(0.3265)\n",
      "26120 Training Loss: tensor(0.3267)\n",
      "26121 Training Loss: tensor(0.3265)\n",
      "26122 Training Loss: tensor(0.3270)\n",
      "26123 Training Loss: tensor(0.3282)\n",
      "26124 Training Loss: tensor(0.3268)\n",
      "26125 Training Loss: tensor(0.3288)\n",
      "26126 Training Loss: tensor(0.3268)\n",
      "26127 Training Loss: tensor(0.3265)\n",
      "26128 Training Loss: tensor(0.3265)\n",
      "26129 Training Loss: tensor(0.3295)\n",
      "26130 Training Loss: tensor(0.3268)\n",
      "26131 Training Loss: tensor(0.3275)\n",
      "26132 Training Loss: tensor(0.3271)\n",
      "26133 Training Loss: tensor(0.3272)\n",
      "26134 Training Loss: tensor(0.3270)\n",
      "26135 Training Loss: tensor(0.3272)\n",
      "26136 Training Loss: tensor(0.3269)\n",
      "26137 Training Loss: tensor(0.3292)\n",
      "26138 Training Loss: tensor(0.3266)\n",
      "26139 Training Loss: tensor(0.3270)\n",
      "26140 Training Loss: tensor(0.3278)\n",
      "26141 Training Loss: tensor(0.3271)\n",
      "26142 Training Loss: tensor(0.3269)\n",
      "26143 Training Loss: tensor(0.3297)\n",
      "26144 Training Loss: tensor(0.3274)\n",
      "26145 Training Loss: tensor(0.3275)\n",
      "26146 Training Loss: tensor(0.3279)\n",
      "26147 Training Loss: tensor(0.3269)\n",
      "26148 Training Loss: tensor(0.3269)\n",
      "26149 Training Loss: tensor(0.3271)\n",
      "26150 Training Loss: tensor(0.3271)\n",
      "26151 Training Loss: tensor(0.3271)\n",
      "26152 Training Loss: tensor(0.3267)\n",
      "26153 Training Loss: tensor(0.3271)\n",
      "26154 Training Loss: tensor(0.3268)\n",
      "26155 Training Loss: tensor(0.3276)\n",
      "26156 Training Loss: tensor(0.3269)\n",
      "26157 Training Loss: tensor(0.3269)\n",
      "26158 Training Loss: tensor(0.3275)\n",
      "26159 Training Loss: tensor(0.3271)\n",
      "26160 Training Loss: tensor(0.3286)\n",
      "26161 Training Loss: tensor(0.3281)\n",
      "26162 Training Loss: tensor(0.3271)\n",
      "26163 Training Loss: tensor(0.3271)\n",
      "26164 Training Loss: tensor(0.3270)\n",
      "26165 Training Loss: tensor(0.3275)\n",
      "26166 Training Loss: tensor(0.3270)\n",
      "26167 Training Loss: tensor(0.3278)\n",
      "26168 Training Loss: tensor(0.3270)\n",
      "26169 Training Loss: tensor(0.3261)\n",
      "26170 Training Loss: tensor(0.3270)\n",
      "26171 Training Loss: tensor(0.3277)\n",
      "26172 Training Loss: tensor(0.3272)\n",
      "26173 Training Loss: tensor(0.3264)\n",
      "26174 Training Loss: tensor(0.3267)\n",
      "26175 Training Loss: tensor(0.3266)\n",
      "26176 Training Loss: tensor(0.3299)\n",
      "26177 Training Loss: tensor(0.3278)\n",
      "26178 Training Loss: tensor(0.3280)\n",
      "26179 Training Loss: tensor(0.3276)\n",
      "26180 Training Loss: tensor(0.3312)\n",
      "26181 Training Loss: tensor(0.3270)\n",
      "26182 Training Loss: tensor(0.3268)\n",
      "26183 Training Loss: tensor(0.3274)\n",
      "26184 Training Loss: tensor(0.3295)\n",
      "26185 Training Loss: tensor(0.3268)\n",
      "26186 Training Loss: tensor(0.3266)\n",
      "26187 Training Loss: tensor(0.3272)\n",
      "26188 Training Loss: tensor(0.3270)\n",
      "26189 Training Loss: tensor(0.3269)\n",
      "26190 Training Loss: tensor(0.3273)\n",
      "26191 Training Loss: tensor(0.3274)\n",
      "26192 Training Loss: tensor(0.3275)\n",
      "26193 Training Loss: tensor(0.3298)\n",
      "26194 Training Loss: tensor(0.3268)\n",
      "26195 Training Loss: tensor(0.3267)\n",
      "26196 Training Loss: tensor(0.3265)\n",
      "26197 Training Loss: tensor(0.3286)\n",
      "26198 Training Loss: tensor(0.3286)\n",
      "26199 Training Loss: tensor(0.3272)\n",
      "26200 Training Loss: tensor(0.3273)\n",
      "26201 Training Loss: tensor(0.3273)\n",
      "26202 Training Loss: tensor(0.3273)\n",
      "26203 Training Loss: tensor(0.3267)\n",
      "26204 Training Loss: tensor(0.3274)\n",
      "26205 Training Loss: tensor(0.3267)\n",
      "26206 Training Loss: tensor(0.3283)\n",
      "26207 Training Loss: tensor(0.3279)\n",
      "26208 Training Loss: tensor(0.3314)\n",
      "26209 Training Loss: tensor(0.3269)\n",
      "26210 Training Loss: tensor(0.3272)\n",
      "26211 Training Loss: tensor(0.3272)\n",
      "26212 Training Loss: tensor(0.3268)\n",
      "26213 Training Loss: tensor(0.3290)\n",
      "26214 Training Loss: tensor(0.3271)\n",
      "26215 Training Loss: tensor(0.3287)\n",
      "26216 Training Loss: tensor(0.3293)\n",
      "26217 Training Loss: tensor(0.3281)\n",
      "26218 Training Loss: tensor(0.3296)\n",
      "26219 Training Loss: tensor(0.3300)\n",
      "26220 Training Loss: tensor(0.3282)\n",
      "26221 Training Loss: tensor(0.3274)\n",
      "26222 Training Loss: tensor(0.3278)\n",
      "26223 Training Loss: tensor(0.3276)\n",
      "26224 Training Loss: tensor(0.3276)\n",
      "26225 Training Loss: tensor(0.3270)\n",
      "26226 Training Loss: tensor(0.3282)\n",
      "26227 Training Loss: tensor(0.3271)\n",
      "26228 Training Loss: tensor(0.3288)\n",
      "26229 Training Loss: tensor(0.3271)\n",
      "26230 Training Loss: tensor(0.3282)\n",
      "26231 Training Loss: tensor(0.3275)\n",
      "26232 Training Loss: tensor(0.3271)\n",
      "26233 Training Loss: tensor(0.3270)\n",
      "26234 Training Loss: tensor(0.3276)\n",
      "26235 Training Loss: tensor(0.3267)\n",
      "26236 Training Loss: tensor(0.3276)\n",
      "26237 Training Loss: tensor(0.3266)\n",
      "26238 Training Loss: tensor(0.3268)\n",
      "26239 Training Loss: tensor(0.3279)\n",
      "26240 Training Loss: tensor(0.3278)\n",
      "26241 Training Loss: tensor(0.3274)\n",
      "26242 Training Loss: tensor(0.3268)\n",
      "26243 Training Loss: tensor(0.3274)\n",
      "26244 Training Loss: tensor(0.3275)\n",
      "26245 Training Loss: tensor(0.3274)\n",
      "26246 Training Loss: tensor(0.3268)\n",
      "26247 Training Loss: tensor(0.3278)\n",
      "26248 Training Loss: tensor(0.3279)\n",
      "26249 Training Loss: tensor(0.3293)\n",
      "26250 Training Loss: tensor(0.3311)\n",
      "26251 Training Loss: tensor(0.3268)\n",
      "26252 Training Loss: tensor(0.3280)\n",
      "26253 Training Loss: tensor(0.3277)\n",
      "26254 Training Loss: tensor(0.3277)\n",
      "26255 Training Loss: tensor(0.3269)\n",
      "26256 Training Loss: tensor(0.3272)\n",
      "26257 Training Loss: tensor(0.3275)\n",
      "26258 Training Loss: tensor(0.3280)\n",
      "26259 Training Loss: tensor(0.3275)\n",
      "26260 Training Loss: tensor(0.3271)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26261 Training Loss: tensor(0.3270)\n",
      "26262 Training Loss: tensor(0.3274)\n",
      "26263 Training Loss: tensor(0.3278)\n",
      "26264 Training Loss: tensor(0.3274)\n",
      "26265 Training Loss: tensor(0.3269)\n",
      "26266 Training Loss: tensor(0.3268)\n",
      "26267 Training Loss: tensor(0.3291)\n",
      "26268 Training Loss: tensor(0.3264)\n",
      "26269 Training Loss: tensor(0.3278)\n",
      "26270 Training Loss: tensor(0.3266)\n",
      "26271 Training Loss: tensor(0.3269)\n",
      "26272 Training Loss: tensor(0.3271)\n",
      "26273 Training Loss: tensor(0.3263)\n",
      "26274 Training Loss: tensor(0.3270)\n",
      "26275 Training Loss: tensor(0.3277)\n",
      "26276 Training Loss: tensor(0.3274)\n",
      "26277 Training Loss: tensor(0.3281)\n",
      "26278 Training Loss: tensor(0.3269)\n",
      "26279 Training Loss: tensor(0.3278)\n",
      "26280 Training Loss: tensor(0.3271)\n",
      "26281 Training Loss: tensor(0.3319)\n",
      "26282 Training Loss: tensor(0.3288)\n",
      "26283 Training Loss: tensor(0.3273)\n",
      "26284 Training Loss: tensor(0.3274)\n",
      "26285 Training Loss: tensor(0.3284)\n",
      "26286 Training Loss: tensor(0.3272)\n",
      "26287 Training Loss: tensor(0.3275)\n",
      "26288 Training Loss: tensor(0.3278)\n",
      "26289 Training Loss: tensor(0.3278)\n",
      "26290 Training Loss: tensor(0.3292)\n",
      "26291 Training Loss: tensor(0.3276)\n",
      "26292 Training Loss: tensor(0.3280)\n",
      "26293 Training Loss: tensor(0.3294)\n",
      "26294 Training Loss: tensor(0.3273)\n",
      "26295 Training Loss: tensor(0.3278)\n",
      "26296 Training Loss: tensor(0.3274)\n",
      "26297 Training Loss: tensor(0.3271)\n",
      "26298 Training Loss: tensor(0.3271)\n",
      "26299 Training Loss: tensor(0.3273)\n",
      "26300 Training Loss: tensor(0.3286)\n",
      "26301 Training Loss: tensor(0.3268)\n",
      "26302 Training Loss: tensor(0.3270)\n",
      "26303 Training Loss: tensor(0.3270)\n",
      "26304 Training Loss: tensor(0.3289)\n",
      "26305 Training Loss: tensor(0.3270)\n",
      "26306 Training Loss: tensor(0.3271)\n",
      "26307 Training Loss: tensor(0.3266)\n",
      "26308 Training Loss: tensor(0.3285)\n",
      "26309 Training Loss: tensor(0.3281)\n",
      "26310 Training Loss: tensor(0.3270)\n",
      "26311 Training Loss: tensor(0.3269)\n",
      "26312 Training Loss: tensor(0.3268)\n",
      "26313 Training Loss: tensor(0.3265)\n",
      "26314 Training Loss: tensor(0.3281)\n",
      "26315 Training Loss: tensor(0.3295)\n",
      "26316 Training Loss: tensor(0.3278)\n",
      "26317 Training Loss: tensor(0.3269)\n",
      "26318 Training Loss: tensor(0.3268)\n",
      "26319 Training Loss: tensor(0.3279)\n",
      "26320 Training Loss: tensor(0.3284)\n",
      "26321 Training Loss: tensor(0.3289)\n",
      "26322 Training Loss: tensor(0.3273)\n",
      "26323 Training Loss: tensor(0.3272)\n",
      "26324 Training Loss: tensor(0.3274)\n",
      "26325 Training Loss: tensor(0.3269)\n",
      "26326 Training Loss: tensor(0.3274)\n",
      "26327 Training Loss: tensor(0.3269)\n",
      "26328 Training Loss: tensor(0.3274)\n",
      "26329 Training Loss: tensor(0.3278)\n",
      "26330 Training Loss: tensor(0.3271)\n",
      "26331 Training Loss: tensor(0.3280)\n",
      "26332 Training Loss: tensor(0.3276)\n",
      "26333 Training Loss: tensor(0.3270)\n",
      "26334 Training Loss: tensor(0.3270)\n",
      "26335 Training Loss: tensor(0.3266)\n",
      "26336 Training Loss: tensor(0.3268)\n",
      "26337 Training Loss: tensor(0.3276)\n",
      "26338 Training Loss: tensor(0.3267)\n",
      "26339 Training Loss: tensor(0.3291)\n",
      "26340 Training Loss: tensor(0.3269)\n",
      "26341 Training Loss: tensor(0.3268)\n",
      "26342 Training Loss: tensor(0.3270)\n",
      "26343 Training Loss: tensor(0.3282)\n",
      "26344 Training Loss: tensor(0.3269)\n",
      "26345 Training Loss: tensor(0.3275)\n",
      "26346 Training Loss: tensor(0.3330)\n",
      "26347 Training Loss: tensor(0.3267)\n",
      "26348 Training Loss: tensor(0.3284)\n",
      "26349 Training Loss: tensor(0.3275)\n",
      "26350 Training Loss: tensor(0.3273)\n",
      "26351 Training Loss: tensor(0.3274)\n",
      "26352 Training Loss: tensor(0.3275)\n",
      "26353 Training Loss: tensor(0.3269)\n",
      "26354 Training Loss: tensor(0.3286)\n",
      "26355 Training Loss: tensor(0.3274)\n",
      "26356 Training Loss: tensor(0.3274)\n",
      "26357 Training Loss: tensor(0.3271)\n",
      "26358 Training Loss: tensor(0.3283)\n",
      "26359 Training Loss: tensor(0.3273)\n",
      "26360 Training Loss: tensor(0.3294)\n",
      "26361 Training Loss: tensor(0.3270)\n",
      "26362 Training Loss: tensor(0.3284)\n",
      "26363 Training Loss: tensor(0.3267)\n",
      "26364 Training Loss: tensor(0.3279)\n",
      "26365 Training Loss: tensor(0.3284)\n",
      "26366 Training Loss: tensor(0.3270)\n",
      "26367 Training Loss: tensor(0.3272)\n",
      "26368 Training Loss: tensor(0.3269)\n",
      "26369 Training Loss: tensor(0.3281)\n",
      "26370 Training Loss: tensor(0.3268)\n",
      "26371 Training Loss: tensor(0.3268)\n",
      "26372 Training Loss: tensor(0.3268)\n",
      "26373 Training Loss: tensor(0.3294)\n",
      "26374 Training Loss: tensor(0.3292)\n",
      "26375 Training Loss: tensor(0.3268)\n",
      "26376 Training Loss: tensor(0.3282)\n",
      "26377 Training Loss: tensor(0.3266)\n",
      "26378 Training Loss: tensor(0.3273)\n",
      "26379 Training Loss: tensor(0.3274)\n",
      "26380 Training Loss: tensor(0.3282)\n",
      "26381 Training Loss: tensor(0.3274)\n",
      "26382 Training Loss: tensor(0.3280)\n",
      "26383 Training Loss: tensor(0.3274)\n",
      "26384 Training Loss: tensor(0.3300)\n",
      "26385 Training Loss: tensor(0.3274)\n",
      "26386 Training Loss: tensor(0.3277)\n",
      "26387 Training Loss: tensor(0.3273)\n",
      "26388 Training Loss: tensor(0.3280)\n",
      "26389 Training Loss: tensor(0.3273)\n",
      "26390 Training Loss: tensor(0.3276)\n",
      "26391 Training Loss: tensor(0.3268)\n",
      "26392 Training Loss: tensor(0.3271)\n",
      "26393 Training Loss: tensor(0.3270)\n",
      "26394 Training Loss: tensor(0.3269)\n",
      "26395 Training Loss: tensor(0.3269)\n",
      "26396 Training Loss: tensor(0.3285)\n",
      "26397 Training Loss: tensor(0.3268)\n",
      "26398 Training Loss: tensor(0.3293)\n",
      "26399 Training Loss: tensor(0.3272)\n",
      "26400 Training Loss: tensor(0.3306)\n",
      "26401 Training Loss: tensor(0.3281)\n",
      "26402 Training Loss: tensor(0.3267)\n",
      "26403 Training Loss: tensor(0.3268)\n",
      "26404 Training Loss: tensor(0.3273)\n",
      "26405 Training Loss: tensor(0.3265)\n",
      "26406 Training Loss: tensor(0.3273)\n",
      "26407 Training Loss: tensor(0.3270)\n",
      "26408 Training Loss: tensor(0.3278)\n",
      "26409 Training Loss: tensor(0.3281)\n",
      "26410 Training Loss: tensor(0.3273)\n",
      "26411 Training Loss: tensor(0.3293)\n",
      "26412 Training Loss: tensor(0.3278)\n",
      "26413 Training Loss: tensor(0.3271)\n",
      "26414 Training Loss: tensor(0.3273)\n",
      "26415 Training Loss: tensor(0.3276)\n",
      "26416 Training Loss: tensor(0.3267)\n",
      "26417 Training Loss: tensor(0.3278)\n",
      "26418 Training Loss: tensor(0.3282)\n",
      "26419 Training Loss: tensor(0.3270)\n",
      "26420 Training Loss: tensor(0.3280)\n",
      "26421 Training Loss: tensor(0.3274)\n",
      "26422 Training Loss: tensor(0.3270)\n",
      "26423 Training Loss: tensor(0.3270)\n",
      "26424 Training Loss: tensor(0.3273)\n",
      "26425 Training Loss: tensor(0.3270)\n",
      "26426 Training Loss: tensor(0.3269)\n",
      "26427 Training Loss: tensor(0.3274)\n",
      "26428 Training Loss: tensor(0.3266)\n",
      "26429 Training Loss: tensor(0.3267)\n",
      "26430 Training Loss: tensor(0.3280)\n",
      "26431 Training Loss: tensor(0.3287)\n",
      "26432 Training Loss: tensor(0.3276)\n",
      "26433 Training Loss: tensor(0.3265)\n",
      "26434 Training Loss: tensor(0.3289)\n",
      "26435 Training Loss: tensor(0.3263)\n",
      "26436 Training Loss: tensor(0.3271)\n",
      "26437 Training Loss: tensor(0.3287)\n",
      "26438 Training Loss: tensor(0.3307)\n",
      "26439 Training Loss: tensor(0.3275)\n",
      "26440 Training Loss: tensor(0.3275)\n",
      "26441 Training Loss: tensor(0.3272)\n",
      "26442 Training Loss: tensor(0.3271)\n",
      "26443 Training Loss: tensor(0.3281)\n",
      "26444 Training Loss: tensor(0.3276)\n",
      "26445 Training Loss: tensor(0.3285)\n",
      "26446 Training Loss: tensor(0.3279)\n",
      "26447 Training Loss: tensor(0.3276)\n",
      "26448 Training Loss: tensor(0.3278)\n",
      "26449 Training Loss: tensor(0.3278)\n",
      "26450 Training Loss: tensor(0.3276)\n",
      "26451 Training Loss: tensor(0.3286)\n",
      "26452 Training Loss: tensor(0.3268)\n",
      "26453 Training Loss: tensor(0.3280)\n",
      "26454 Training Loss: tensor(0.3266)\n",
      "26455 Training Loss: tensor(0.3271)\n",
      "26456 Training Loss: tensor(0.3289)\n",
      "26457 Training Loss: tensor(0.3271)\n",
      "26458 Training Loss: tensor(0.3277)\n",
      "26459 Training Loss: tensor(0.3268)\n",
      "26460 Training Loss: tensor(0.3268)\n",
      "26461 Training Loss: tensor(0.3269)\n",
      "26462 Training Loss: tensor(0.3269)\n",
      "26463 Training Loss: tensor(0.3274)\n",
      "26464 Training Loss: tensor(0.3264)\n",
      "26465 Training Loss: tensor(0.3277)\n",
      "26466 Training Loss: tensor(0.3273)\n",
      "26467 Training Loss: tensor(0.3275)\n",
      "26468 Training Loss: tensor(0.3280)\n",
      "26469 Training Loss: tensor(0.3309)\n",
      "26470 Training Loss: tensor(0.3270)\n",
      "26471 Training Loss: tensor(0.3266)\n",
      "26472 Training Loss: tensor(0.3281)\n",
      "26473 Training Loss: tensor(0.3269)\n",
      "26474 Training Loss: tensor(0.3272)\n",
      "26475 Training Loss: tensor(0.3277)\n",
      "26476 Training Loss: tensor(0.3276)\n",
      "26477 Training Loss: tensor(0.3271)\n",
      "26478 Training Loss: tensor(0.3279)\n",
      "26479 Training Loss: tensor(0.3268)\n",
      "26480 Training Loss: tensor(0.3270)\n",
      "26481 Training Loss: tensor(0.3270)\n",
      "26482 Training Loss: tensor(0.3268)\n",
      "26483 Training Loss: tensor(0.3272)\n",
      "26484 Training Loss: tensor(0.3269)\n",
      "26485 Training Loss: tensor(0.3267)\n",
      "26486 Training Loss: tensor(0.3267)\n",
      "26487 Training Loss: tensor(0.3273)\n",
      "26488 Training Loss: tensor(0.3270)\n",
      "26489 Training Loss: tensor(0.3270)\n",
      "26490 Training Loss: tensor(0.3265)\n",
      "26491 Training Loss: tensor(0.3269)\n",
      "26492 Training Loss: tensor(0.3292)\n",
      "26493 Training Loss: tensor(0.3270)\n",
      "26494 Training Loss: tensor(0.3283)\n",
      "26495 Training Loss: tensor(0.3270)\n",
      "26496 Training Loss: tensor(0.3273)\n",
      "26497 Training Loss: tensor(0.3296)\n",
      "26498 Training Loss: tensor(0.3297)\n",
      "26499 Training Loss: tensor(0.3283)\n",
      "26500 Training Loss: tensor(0.3277)\n",
      "26501 Training Loss: tensor(0.3283)\n",
      "26502 Training Loss: tensor(0.3273)\n",
      "26503 Training Loss: tensor(0.3278)\n",
      "26504 Training Loss: tensor(0.3274)\n",
      "26505 Training Loss: tensor(0.3282)\n",
      "26506 Training Loss: tensor(0.3284)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26507 Training Loss: tensor(0.3270)\n",
      "26508 Training Loss: tensor(0.3284)\n",
      "26509 Training Loss: tensor(0.3270)\n",
      "26510 Training Loss: tensor(0.3280)\n",
      "26511 Training Loss: tensor(0.3277)\n",
      "26512 Training Loss: tensor(0.3278)\n",
      "26513 Training Loss: tensor(0.3270)\n",
      "26514 Training Loss: tensor(0.3269)\n",
      "26515 Training Loss: tensor(0.3272)\n",
      "26516 Training Loss: tensor(0.3270)\n",
      "26517 Training Loss: tensor(0.3269)\n",
      "26518 Training Loss: tensor(0.3282)\n",
      "26519 Training Loss: tensor(0.3268)\n",
      "26520 Training Loss: tensor(0.3268)\n",
      "26521 Training Loss: tensor(0.3272)\n",
      "26522 Training Loss: tensor(0.3272)\n",
      "26523 Training Loss: tensor(0.3296)\n",
      "26524 Training Loss: tensor(0.3275)\n",
      "26525 Training Loss: tensor(0.3269)\n",
      "26526 Training Loss: tensor(0.3286)\n",
      "26527 Training Loss: tensor(0.3271)\n",
      "26528 Training Loss: tensor(0.3279)\n",
      "26529 Training Loss: tensor(0.3270)\n",
      "26530 Training Loss: tensor(0.3268)\n",
      "26531 Training Loss: tensor(0.3270)\n",
      "26532 Training Loss: tensor(0.3276)\n",
      "26533 Training Loss: tensor(0.3263)\n",
      "26534 Training Loss: tensor(0.3279)\n",
      "26535 Training Loss: tensor(0.3281)\n",
      "26536 Training Loss: tensor(0.3267)\n",
      "26537 Training Loss: tensor(0.3264)\n",
      "26538 Training Loss: tensor(0.3268)\n",
      "26539 Training Loss: tensor(0.3267)\n",
      "26540 Training Loss: tensor(0.3272)\n",
      "26541 Training Loss: tensor(0.3263)\n",
      "26542 Training Loss: tensor(0.3267)\n",
      "26543 Training Loss: tensor(0.3281)\n",
      "26544 Training Loss: tensor(0.3282)\n",
      "26545 Training Loss: tensor(0.3269)\n",
      "26546 Training Loss: tensor(0.3266)\n",
      "26547 Training Loss: tensor(0.3269)\n",
      "26548 Training Loss: tensor(0.3265)\n",
      "26549 Training Loss: tensor(0.3274)\n",
      "26550 Training Loss: tensor(0.3265)\n",
      "26551 Training Loss: tensor(0.3264)\n",
      "26552 Training Loss: tensor(0.3266)\n",
      "26553 Training Loss: tensor(0.3268)\n",
      "26554 Training Loss: tensor(0.3288)\n",
      "26555 Training Loss: tensor(0.3268)\n",
      "26556 Training Loss: tensor(0.3267)\n",
      "26557 Training Loss: tensor(0.3264)\n",
      "26558 Training Loss: tensor(0.3275)\n",
      "26559 Training Loss: tensor(0.3299)\n",
      "26560 Training Loss: tensor(0.3268)\n",
      "26561 Training Loss: tensor(0.3265)\n",
      "26562 Training Loss: tensor(0.3270)\n",
      "26563 Training Loss: tensor(0.3273)\n",
      "26564 Training Loss: tensor(0.3276)\n",
      "26565 Training Loss: tensor(0.3270)\n",
      "26566 Training Loss: tensor(0.3269)\n",
      "26567 Training Loss: tensor(0.3270)\n",
      "26568 Training Loss: tensor(0.3288)\n",
      "26569 Training Loss: tensor(0.3278)\n",
      "26570 Training Loss: tensor(0.3263)\n",
      "26571 Training Loss: tensor(0.3265)\n",
      "26572 Training Loss: tensor(0.3270)\n",
      "26573 Training Loss: tensor(0.3277)\n",
      "26574 Training Loss: tensor(0.3268)\n",
      "26575 Training Loss: tensor(0.3288)\n",
      "26576 Training Loss: tensor(0.3274)\n",
      "26577 Training Loss: tensor(0.3296)\n",
      "26578 Training Loss: tensor(0.3267)\n",
      "26579 Training Loss: tensor(0.3292)\n",
      "26580 Training Loss: tensor(0.3272)\n",
      "26581 Training Loss: tensor(0.3271)\n",
      "26582 Training Loss: tensor(0.3276)\n",
      "26583 Training Loss: tensor(0.3273)\n",
      "26584 Training Loss: tensor(0.3276)\n",
      "26585 Training Loss: tensor(0.3268)\n",
      "26586 Training Loss: tensor(0.3272)\n",
      "26587 Training Loss: tensor(0.3272)\n",
      "26588 Training Loss: tensor(0.3267)\n",
      "26589 Training Loss: tensor(0.3272)\n",
      "26590 Training Loss: tensor(0.3295)\n",
      "26591 Training Loss: tensor(0.3283)\n",
      "26592 Training Loss: tensor(0.3297)\n",
      "26593 Training Loss: tensor(0.3271)\n",
      "26594 Training Loss: tensor(0.3273)\n",
      "26595 Training Loss: tensor(0.3286)\n",
      "26596 Training Loss: tensor(0.3309)\n",
      "26597 Training Loss: tensor(0.3291)\n",
      "26598 Training Loss: tensor(0.3273)\n",
      "26599 Training Loss: tensor(0.3280)\n",
      "26600 Training Loss: tensor(0.3276)\n",
      "26601 Training Loss: tensor(0.3292)\n",
      "26602 Training Loss: tensor(0.3295)\n",
      "26603 Training Loss: tensor(0.3276)\n",
      "26604 Training Loss: tensor(0.3295)\n",
      "26605 Training Loss: tensor(0.3277)\n",
      "26606 Training Loss: tensor(0.3273)\n",
      "26607 Training Loss: tensor(0.3277)\n",
      "26608 Training Loss: tensor(0.3287)\n",
      "26609 Training Loss: tensor(0.3281)\n",
      "26610 Training Loss: tensor(0.3283)\n",
      "26611 Training Loss: tensor(0.3295)\n",
      "26612 Training Loss: tensor(0.3280)\n",
      "26613 Training Loss: tensor(0.3274)\n",
      "26614 Training Loss: tensor(0.3279)\n",
      "26615 Training Loss: tensor(0.3274)\n",
      "26616 Training Loss: tensor(0.3280)\n",
      "26617 Training Loss: tensor(0.3271)\n",
      "26618 Training Loss: tensor(0.3267)\n",
      "26619 Training Loss: tensor(0.3269)\n",
      "26620 Training Loss: tensor(0.3266)\n",
      "26621 Training Loss: tensor(0.3285)\n",
      "26622 Training Loss: tensor(0.3269)\n",
      "26623 Training Loss: tensor(0.3278)\n",
      "26624 Training Loss: tensor(0.3278)\n",
      "26625 Training Loss: tensor(0.3269)\n",
      "26626 Training Loss: tensor(0.3270)\n",
      "26627 Training Loss: tensor(0.3273)\n",
      "26628 Training Loss: tensor(0.3270)\n",
      "26629 Training Loss: tensor(0.3269)\n",
      "26630 Training Loss: tensor(0.3269)\n",
      "26631 Training Loss: tensor(0.3265)\n",
      "26632 Training Loss: tensor(0.3269)\n",
      "26633 Training Loss: tensor(0.3273)\n",
      "26634 Training Loss: tensor(0.3266)\n",
      "26635 Training Loss: tensor(0.3267)\n",
      "26636 Training Loss: tensor(0.3273)\n",
      "26637 Training Loss: tensor(0.3264)\n",
      "26638 Training Loss: tensor(0.3302)\n",
      "26639 Training Loss: tensor(0.3287)\n",
      "26640 Training Loss: tensor(0.3291)\n",
      "26641 Training Loss: tensor(0.3274)\n",
      "26642 Training Loss: tensor(0.3265)\n",
      "26643 Training Loss: tensor(0.3265)\n",
      "26644 Training Loss: tensor(0.3276)\n",
      "26645 Training Loss: tensor(0.3273)\n",
      "26646 Training Loss: tensor(0.3272)\n",
      "26647 Training Loss: tensor(0.3273)\n",
      "26648 Training Loss: tensor(0.3280)\n",
      "26649 Training Loss: tensor(0.3271)\n",
      "26650 Training Loss: tensor(0.3272)\n",
      "26651 Training Loss: tensor(0.3273)\n",
      "26652 Training Loss: tensor(0.3276)\n",
      "26653 Training Loss: tensor(0.3269)\n",
      "26654 Training Loss: tensor(0.3272)\n",
      "26655 Training Loss: tensor(0.3269)\n",
      "26656 Training Loss: tensor(0.3266)\n",
      "26657 Training Loss: tensor(0.3290)\n",
      "26658 Training Loss: tensor(0.3274)\n",
      "26659 Training Loss: tensor(0.3277)\n",
      "26660 Training Loss: tensor(0.3268)\n",
      "26661 Training Loss: tensor(0.3300)\n",
      "26662 Training Loss: tensor(0.3270)\n",
      "26663 Training Loss: tensor(0.3266)\n",
      "26664 Training Loss: tensor(0.3288)\n",
      "26665 Training Loss: tensor(0.3264)\n",
      "26666 Training Loss: tensor(0.3286)\n",
      "26667 Training Loss: tensor(0.3278)\n",
      "26668 Training Loss: tensor(0.3266)\n",
      "26669 Training Loss: tensor(0.3273)\n",
      "26670 Training Loss: tensor(0.3277)\n",
      "26671 Training Loss: tensor(0.3274)\n",
      "26672 Training Loss: tensor(0.3272)\n",
      "26673 Training Loss: tensor(0.3273)\n",
      "26674 Training Loss: tensor(0.3279)\n",
      "26675 Training Loss: tensor(0.3269)\n",
      "26676 Training Loss: tensor(0.3273)\n",
      "26677 Training Loss: tensor(0.3268)\n",
      "26678 Training Loss: tensor(0.3269)\n",
      "26679 Training Loss: tensor(0.3268)\n",
      "26680 Training Loss: tensor(0.3268)\n",
      "26681 Training Loss: tensor(0.3270)\n",
      "26682 Training Loss: tensor(0.3269)\n",
      "26683 Training Loss: tensor(0.3266)\n",
      "26684 Training Loss: tensor(0.3266)\n",
      "26685 Training Loss: tensor(0.3270)\n",
      "26686 Training Loss: tensor(0.3266)\n",
      "26687 Training Loss: tensor(0.3275)\n",
      "26688 Training Loss: tensor(0.3264)\n",
      "26689 Training Loss: tensor(0.3278)\n",
      "26690 Training Loss: tensor(0.3264)\n",
      "26691 Training Loss: tensor(0.3269)\n",
      "26692 Training Loss: tensor(0.3268)\n",
      "26693 Training Loss: tensor(0.3279)\n",
      "26694 Training Loss: tensor(0.3265)\n",
      "26695 Training Loss: tensor(0.3277)\n",
      "26696 Training Loss: tensor(0.3266)\n",
      "26697 Training Loss: tensor(0.3265)\n",
      "26698 Training Loss: tensor(0.3267)\n",
      "26699 Training Loss: tensor(0.3275)\n",
      "26700 Training Loss: tensor(0.3270)\n",
      "26701 Training Loss: tensor(0.3267)\n",
      "26702 Training Loss: tensor(0.3279)\n",
      "26703 Training Loss: tensor(0.3292)\n",
      "26704 Training Loss: tensor(0.3289)\n",
      "26705 Training Loss: tensor(0.3270)\n",
      "26706 Training Loss: tensor(0.3272)\n",
      "26707 Training Loss: tensor(0.3273)\n",
      "26708 Training Loss: tensor(0.3266)\n",
      "26709 Training Loss: tensor(0.3268)\n",
      "26710 Training Loss: tensor(0.3268)\n",
      "26711 Training Loss: tensor(0.3269)\n",
      "26712 Training Loss: tensor(0.3268)\n",
      "26713 Training Loss: tensor(0.3304)\n",
      "26714 Training Loss: tensor(0.3269)\n",
      "26715 Training Loss: tensor(0.3269)\n",
      "26716 Training Loss: tensor(0.3281)\n",
      "26717 Training Loss: tensor(0.3267)\n",
      "26718 Training Loss: tensor(0.3266)\n",
      "26719 Training Loss: tensor(0.3266)\n",
      "26720 Training Loss: tensor(0.3270)\n",
      "26721 Training Loss: tensor(0.3266)\n",
      "26722 Training Loss: tensor(0.3270)\n",
      "26723 Training Loss: tensor(0.3266)\n",
      "26724 Training Loss: tensor(0.3280)\n",
      "26725 Training Loss: tensor(0.3267)\n",
      "26726 Training Loss: tensor(0.3265)\n",
      "26727 Training Loss: tensor(0.3277)\n",
      "26728 Training Loss: tensor(0.3270)\n",
      "26729 Training Loss: tensor(0.3266)\n",
      "26730 Training Loss: tensor(0.3269)\n",
      "26731 Training Loss: tensor(0.3306)\n",
      "26732 Training Loss: tensor(0.3274)\n",
      "26733 Training Loss: tensor(0.3272)\n",
      "26734 Training Loss: tensor(0.3283)\n",
      "26735 Training Loss: tensor(0.3268)\n",
      "26736 Training Loss: tensor(0.3272)\n",
      "26737 Training Loss: tensor(0.3266)\n",
      "26738 Training Loss: tensor(0.3304)\n",
      "26739 Training Loss: tensor(0.3272)\n",
      "26740 Training Loss: tensor(0.3264)\n",
      "26741 Training Loss: tensor(0.3272)\n",
      "26742 Training Loss: tensor(0.3264)\n",
      "26743 Training Loss: tensor(0.3274)\n",
      "26744 Training Loss: tensor(0.3267)\n",
      "26745 Training Loss: tensor(0.3268)\n",
      "26746 Training Loss: tensor(0.3264)\n",
      "26747 Training Loss: tensor(0.3278)\n",
      "26748 Training Loss: tensor(0.3271)\n",
      "26749 Training Loss: tensor(0.3296)\n",
      "26750 Training Loss: tensor(0.3269)\n",
      "26751 Training Loss: tensor(0.3274)\n",
      "26752 Training Loss: tensor(0.3267)\n",
      "26753 Training Loss: tensor(0.3271)\n",
      "26754 Training Loss: tensor(0.3269)\n",
      "26755 Training Loss: tensor(0.3271)\n",
      "26756 Training Loss: tensor(0.3269)\n",
      "26757 Training Loss: tensor(0.3271)\n",
      "26758 Training Loss: tensor(0.3272)\n",
      "26759 Training Loss: tensor(0.3266)\n",
      "26760 Training Loss: tensor(0.3269)\n",
      "26761 Training Loss: tensor(0.3279)\n",
      "26762 Training Loss: tensor(0.3276)\n",
      "26763 Training Loss: tensor(0.3274)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26764 Training Loss: tensor(0.3281)\n",
      "26765 Training Loss: tensor(0.3310)\n",
      "26766 Training Loss: tensor(0.3269)\n",
      "26767 Training Loss: tensor(0.3269)\n",
      "26768 Training Loss: tensor(0.3281)\n",
      "26769 Training Loss: tensor(0.3286)\n",
      "26770 Training Loss: tensor(0.3280)\n",
      "26771 Training Loss: tensor(0.3267)\n",
      "26772 Training Loss: tensor(0.3296)\n",
      "26773 Training Loss: tensor(0.3270)\n",
      "26774 Training Loss: tensor(0.3268)\n",
      "26775 Training Loss: tensor(0.3272)\n",
      "26776 Training Loss: tensor(0.3276)\n",
      "26777 Training Loss: tensor(0.3269)\n",
      "26778 Training Loss: tensor(0.3264)\n",
      "26779 Training Loss: tensor(0.3270)\n",
      "26780 Training Loss: tensor(0.3268)\n",
      "26781 Training Loss: tensor(0.3268)\n",
      "26782 Training Loss: tensor(0.3270)\n",
      "26783 Training Loss: tensor(0.3274)\n",
      "26784 Training Loss: tensor(0.3270)\n",
      "26785 Training Loss: tensor(0.3272)\n",
      "26786 Training Loss: tensor(0.3302)\n",
      "26787 Training Loss: tensor(0.3264)\n",
      "26788 Training Loss: tensor(0.3276)\n",
      "26789 Training Loss: tensor(0.3268)\n",
      "26790 Training Loss: tensor(0.3264)\n",
      "26791 Training Loss: tensor(0.3265)\n",
      "26792 Training Loss: tensor(0.3284)\n",
      "26793 Training Loss: tensor(0.3269)\n",
      "26794 Training Loss: tensor(0.3276)\n",
      "26795 Training Loss: tensor(0.3268)\n",
      "26796 Training Loss: tensor(0.3276)\n",
      "26797 Training Loss: tensor(0.3269)\n",
      "26798 Training Loss: tensor(0.3269)\n",
      "26799 Training Loss: tensor(0.3279)\n",
      "26800 Training Loss: tensor(0.3265)\n",
      "26801 Training Loss: tensor(0.3273)\n",
      "26802 Training Loss: tensor(0.3278)\n",
      "26803 Training Loss: tensor(0.3281)\n",
      "26804 Training Loss: tensor(0.3268)\n",
      "26805 Training Loss: tensor(0.3267)\n",
      "26806 Training Loss: tensor(0.3264)\n",
      "26807 Training Loss: tensor(0.3266)\n",
      "26808 Training Loss: tensor(0.3274)\n",
      "26809 Training Loss: tensor(0.3282)\n",
      "26810 Training Loss: tensor(0.3267)\n",
      "26811 Training Loss: tensor(0.3269)\n",
      "26812 Training Loss: tensor(0.3290)\n",
      "26813 Training Loss: tensor(0.3270)\n",
      "26814 Training Loss: tensor(0.3282)\n",
      "26815 Training Loss: tensor(0.3265)\n",
      "26816 Training Loss: tensor(0.3272)\n",
      "26817 Training Loss: tensor(0.3265)\n",
      "26818 Training Loss: tensor(0.3266)\n",
      "26819 Training Loss: tensor(0.3268)\n",
      "26820 Training Loss: tensor(0.3276)\n",
      "26821 Training Loss: tensor(0.3266)\n",
      "26822 Training Loss: tensor(0.3272)\n",
      "26823 Training Loss: tensor(0.3294)\n",
      "26824 Training Loss: tensor(0.3275)\n",
      "26825 Training Loss: tensor(0.3269)\n",
      "26826 Training Loss: tensor(0.3269)\n",
      "26827 Training Loss: tensor(0.3269)\n",
      "26828 Training Loss: tensor(0.3269)\n",
      "26829 Training Loss: tensor(0.3273)\n",
      "26830 Training Loss: tensor(0.3266)\n",
      "26831 Training Loss: tensor(0.3266)\n",
      "26832 Training Loss: tensor(0.3268)\n",
      "26833 Training Loss: tensor(0.3279)\n",
      "26834 Training Loss: tensor(0.3270)\n",
      "26835 Training Loss: tensor(0.3288)\n",
      "26836 Training Loss: tensor(0.3290)\n",
      "26837 Training Loss: tensor(0.3281)\n",
      "26838 Training Loss: tensor(0.3281)\n",
      "26839 Training Loss: tensor(0.3270)\n",
      "26840 Training Loss: tensor(0.3269)\n",
      "26841 Training Loss: tensor(0.3267)\n",
      "26842 Training Loss: tensor(0.3268)\n",
      "26843 Training Loss: tensor(0.3274)\n",
      "26844 Training Loss: tensor(0.3273)\n",
      "26845 Training Loss: tensor(0.3291)\n",
      "26846 Training Loss: tensor(0.3268)\n",
      "26847 Training Loss: tensor(0.3276)\n",
      "26848 Training Loss: tensor(0.3285)\n",
      "26849 Training Loss: tensor(0.3292)\n",
      "26850 Training Loss: tensor(0.3272)\n",
      "26851 Training Loss: tensor(0.3270)\n",
      "26852 Training Loss: tensor(0.3272)\n",
      "26853 Training Loss: tensor(0.3268)\n",
      "26854 Training Loss: tensor(0.3273)\n",
      "26855 Training Loss: tensor(0.3268)\n",
      "26856 Training Loss: tensor(0.3276)\n",
      "26857 Training Loss: tensor(0.3280)\n",
      "26858 Training Loss: tensor(0.3280)\n",
      "26859 Training Loss: tensor(0.3266)\n",
      "26860 Training Loss: tensor(0.3269)\n",
      "26861 Training Loss: tensor(0.3275)\n",
      "26862 Training Loss: tensor(0.3279)\n",
      "26863 Training Loss: tensor(0.3275)\n",
      "26864 Training Loss: tensor(0.3275)\n",
      "26865 Training Loss: tensor(0.3287)\n",
      "26866 Training Loss: tensor(0.3273)\n",
      "26867 Training Loss: tensor(0.3268)\n",
      "26868 Training Loss: tensor(0.3271)\n",
      "26869 Training Loss: tensor(0.3274)\n",
      "26870 Training Loss: tensor(0.3283)\n",
      "26871 Training Loss: tensor(0.3294)\n",
      "26872 Training Loss: tensor(0.3283)\n",
      "26873 Training Loss: tensor(0.3279)\n",
      "26874 Training Loss: tensor(0.3293)\n",
      "26875 Training Loss: tensor(0.3270)\n",
      "26876 Training Loss: tensor(0.3275)\n",
      "26877 Training Loss: tensor(0.3281)\n",
      "26878 Training Loss: tensor(0.3268)\n",
      "26879 Training Loss: tensor(0.3272)\n",
      "26880 Training Loss: tensor(0.3271)\n",
      "26881 Training Loss: tensor(0.3268)\n",
      "26882 Training Loss: tensor(0.3269)\n",
      "26883 Training Loss: tensor(0.3272)\n",
      "26884 Training Loss: tensor(0.3268)\n",
      "26885 Training Loss: tensor(0.3269)\n",
      "26886 Training Loss: tensor(0.3276)\n",
      "26887 Training Loss: tensor(0.3265)\n",
      "26888 Training Loss: tensor(0.3265)\n",
      "26889 Training Loss: tensor(0.3273)\n",
      "26890 Training Loss: tensor(0.3266)\n",
      "26891 Training Loss: tensor(0.3266)\n",
      "26892 Training Loss: tensor(0.3264)\n",
      "26893 Training Loss: tensor(0.3270)\n",
      "26894 Training Loss: tensor(0.3262)\n",
      "26895 Training Loss: tensor(0.3267)\n",
      "26896 Training Loss: tensor(0.3282)\n",
      "26897 Training Loss: tensor(0.3265)\n",
      "26898 Training Loss: tensor(0.3271)\n",
      "26899 Training Loss: tensor(0.3265)\n",
      "26900 Training Loss: tensor(0.3266)\n",
      "26901 Training Loss: tensor(0.3272)\n",
      "26902 Training Loss: tensor(0.3275)\n",
      "26903 Training Loss: tensor(0.3270)\n",
      "26904 Training Loss: tensor(0.3267)\n",
      "26905 Training Loss: tensor(0.3304)\n",
      "26906 Training Loss: tensor(0.3275)\n",
      "26907 Training Loss: tensor(0.3269)\n",
      "26908 Training Loss: tensor(0.3270)\n",
      "26909 Training Loss: tensor(0.3284)\n",
      "26910 Training Loss: tensor(0.3277)\n",
      "26911 Training Loss: tensor(0.3265)\n",
      "26912 Training Loss: tensor(0.3268)\n",
      "26913 Training Loss: tensor(0.3305)\n",
      "26914 Training Loss: tensor(0.3306)\n",
      "26915 Training Loss: tensor(0.3279)\n",
      "26916 Training Loss: tensor(0.3287)\n",
      "26917 Training Loss: tensor(0.3278)\n",
      "26918 Training Loss: tensor(0.3273)\n",
      "26919 Training Loss: tensor(0.3271)\n",
      "26920 Training Loss: tensor(0.3274)\n",
      "26921 Training Loss: tensor(0.3268)\n",
      "26922 Training Loss: tensor(0.3275)\n",
      "26923 Training Loss: tensor(0.3274)\n",
      "26924 Training Loss: tensor(0.3267)\n",
      "26925 Training Loss: tensor(0.3279)\n",
      "26926 Training Loss: tensor(0.3273)\n",
      "26927 Training Loss: tensor(0.3275)\n",
      "26928 Training Loss: tensor(0.3267)\n",
      "26929 Training Loss: tensor(0.3300)\n",
      "26930 Training Loss: tensor(0.3266)\n",
      "26931 Training Loss: tensor(0.3265)\n",
      "26932 Training Loss: tensor(0.3272)\n",
      "26933 Training Loss: tensor(0.3268)\n",
      "26934 Training Loss: tensor(0.3277)\n",
      "26935 Training Loss: tensor(0.3270)\n",
      "26936 Training Loss: tensor(0.3281)\n",
      "26937 Training Loss: tensor(0.3273)\n",
      "26938 Training Loss: tensor(0.3276)\n",
      "26939 Training Loss: tensor(0.3276)\n",
      "26940 Training Loss: tensor(0.3274)\n",
      "26941 Training Loss: tensor(0.3264)\n",
      "26942 Training Loss: tensor(0.3275)\n",
      "26943 Training Loss: tensor(0.3270)\n",
      "26944 Training Loss: tensor(0.3277)\n",
      "26945 Training Loss: tensor(0.3296)\n",
      "26946 Training Loss: tensor(0.3267)\n",
      "26947 Training Loss: tensor(0.3273)\n",
      "26948 Training Loss: tensor(0.3277)\n",
      "26949 Training Loss: tensor(0.3274)\n",
      "26950 Training Loss: tensor(0.3272)\n",
      "26951 Training Loss: tensor(0.3268)\n",
      "26952 Training Loss: tensor(0.3265)\n",
      "26953 Training Loss: tensor(0.3268)\n",
      "26954 Training Loss: tensor(0.3267)\n",
      "26955 Training Loss: tensor(0.3267)\n",
      "26956 Training Loss: tensor(0.3272)\n",
      "26957 Training Loss: tensor(0.3276)\n",
      "26958 Training Loss: tensor(0.3264)\n",
      "26959 Training Loss: tensor(0.3268)\n",
      "26960 Training Loss: tensor(0.3305)\n",
      "26961 Training Loss: tensor(0.3265)\n",
      "26962 Training Loss: tensor(0.3268)\n",
      "26963 Training Loss: tensor(0.3285)\n",
      "26964 Training Loss: tensor(0.3272)\n",
      "26965 Training Loss: tensor(0.3289)\n",
      "26966 Training Loss: tensor(0.3270)\n",
      "26967 Training Loss: tensor(0.3268)\n",
      "26968 Training Loss: tensor(0.3265)\n",
      "26969 Training Loss: tensor(0.3270)\n",
      "26970 Training Loss: tensor(0.3266)\n",
      "26971 Training Loss: tensor(0.3267)\n",
      "26972 Training Loss: tensor(0.3272)\n",
      "26973 Training Loss: tensor(0.3275)\n",
      "26974 Training Loss: tensor(0.3263)\n",
      "26975 Training Loss: tensor(0.3273)\n",
      "26976 Training Loss: tensor(0.3283)\n",
      "26977 Training Loss: tensor(0.3273)\n",
      "26978 Training Loss: tensor(0.3267)\n",
      "26979 Training Loss: tensor(0.3266)\n",
      "26980 Training Loss: tensor(0.3266)\n",
      "26981 Training Loss: tensor(0.3277)\n",
      "26982 Training Loss: tensor(0.3266)\n",
      "26983 Training Loss: tensor(0.3264)\n",
      "26984 Training Loss: tensor(0.3265)\n",
      "26985 Training Loss: tensor(0.3271)\n",
      "26986 Training Loss: tensor(0.3263)\n",
      "26987 Training Loss: tensor(0.3299)\n",
      "26988 Training Loss: tensor(0.3274)\n",
      "26989 Training Loss: tensor(0.3288)\n",
      "26990 Training Loss: tensor(0.3266)\n",
      "26991 Training Loss: tensor(0.3270)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26992 Training Loss: tensor(0.3277)\n",
      "26993 Training Loss: tensor(0.3287)\n",
      "26994 Training Loss: tensor(0.3275)\n",
      "26995 Training Loss: tensor(0.3275)\n",
      "26996 Training Loss: tensor(0.3276)\n",
      "26997 Training Loss: tensor(0.3269)\n",
      "26998 Training Loss: tensor(0.3295)\n",
      "26999 Training Loss: tensor(0.3268)\n",
      "27000 Training Loss: tensor(0.3276)\n",
      "27001 Training Loss: tensor(0.3270)\n",
      "27002 Training Loss: tensor(0.3264)\n",
      "27003 Training Loss: tensor(0.3269)\n",
      "27004 Training Loss: tensor(0.3275)\n",
      "27005 Training Loss: tensor(0.3269)\n",
      "27006 Training Loss: tensor(0.3268)\n",
      "27007 Training Loss: tensor(0.3294)\n",
      "27008 Training Loss: tensor(0.3266)\n",
      "27009 Training Loss: tensor(0.3273)\n",
      "27010 Training Loss: tensor(0.3275)\n",
      "27011 Training Loss: tensor(0.3267)\n",
      "27012 Training Loss: tensor(0.3278)\n",
      "27013 Training Loss: tensor(0.3267)\n",
      "27014 Training Loss: tensor(0.3269)\n",
      "27015 Training Loss: tensor(0.3263)\n",
      "27016 Training Loss: tensor(0.3270)\n",
      "27017 Training Loss: tensor(0.3297)\n",
      "27018 Training Loss: tensor(0.3271)\n",
      "27019 Training Loss: tensor(0.3268)\n",
      "27020 Training Loss: tensor(0.3266)\n",
      "27021 Training Loss: tensor(0.3271)\n",
      "27022 Training Loss: tensor(0.3266)\n",
      "27023 Training Loss: tensor(0.3266)\n",
      "27024 Training Loss: tensor(0.3272)\n",
      "27025 Training Loss: tensor(0.3266)\n",
      "27026 Training Loss: tensor(0.3275)\n",
      "27027 Training Loss: tensor(0.3277)\n",
      "27028 Training Loss: tensor(0.3267)\n",
      "27029 Training Loss: tensor(0.3263)\n",
      "27030 Training Loss: tensor(0.3264)\n",
      "27031 Training Loss: tensor(0.3268)\n",
      "27032 Training Loss: tensor(0.3281)\n",
      "27033 Training Loss: tensor(0.3285)\n",
      "27034 Training Loss: tensor(0.3267)\n",
      "27035 Training Loss: tensor(0.3265)\n",
      "27036 Training Loss: tensor(0.3270)\n",
      "27037 Training Loss: tensor(0.3267)\n",
      "27038 Training Loss: tensor(0.3276)\n",
      "27039 Training Loss: tensor(0.3263)\n",
      "27040 Training Loss: tensor(0.3264)\n",
      "27041 Training Loss: tensor(0.3299)\n",
      "27042 Training Loss: tensor(0.3269)\n",
      "27043 Training Loss: tensor(0.3262)\n",
      "27044 Training Loss: tensor(0.3267)\n",
      "27045 Training Loss: tensor(0.3267)\n",
      "27046 Training Loss: tensor(0.3278)\n",
      "27047 Training Loss: tensor(0.3272)\n",
      "27048 Training Loss: tensor(0.3266)\n",
      "27049 Training Loss: tensor(0.3267)\n",
      "27050 Training Loss: tensor(0.3264)\n",
      "27051 Training Loss: tensor(0.3280)\n",
      "27052 Training Loss: tensor(0.3269)\n",
      "27053 Training Loss: tensor(0.3269)\n",
      "27054 Training Loss: tensor(0.3269)\n",
      "27055 Training Loss: tensor(0.3266)\n",
      "27056 Training Loss: tensor(0.3266)\n",
      "27057 Training Loss: tensor(0.3267)\n",
      "27058 Training Loss: tensor(0.3263)\n",
      "27059 Training Loss: tensor(0.3265)\n",
      "27060 Training Loss: tensor(0.3265)\n",
      "27061 Training Loss: tensor(0.3291)\n",
      "27062 Training Loss: tensor(0.3266)\n",
      "27063 Training Loss: tensor(0.3269)\n",
      "27064 Training Loss: tensor(0.3269)\n",
      "27065 Training Loss: tensor(0.3272)\n",
      "27066 Training Loss: tensor(0.3267)\n",
      "27067 Training Loss: tensor(0.3272)\n",
      "27068 Training Loss: tensor(0.3267)\n",
      "27069 Training Loss: tensor(0.3267)\n",
      "27070 Training Loss: tensor(0.3273)\n",
      "27071 Training Loss: tensor(0.3272)\n",
      "27072 Training Loss: tensor(0.3263)\n",
      "27073 Training Loss: tensor(0.3263)\n",
      "27074 Training Loss: tensor(0.3262)\n",
      "27075 Training Loss: tensor(0.3263)\n",
      "27076 Training Loss: tensor(0.3281)\n",
      "27077 Training Loss: tensor(0.3272)\n",
      "27078 Training Loss: tensor(0.3261)\n",
      "27079 Training Loss: tensor(0.3263)\n",
      "27080 Training Loss: tensor(0.3264)\n",
      "27081 Training Loss: tensor(0.3274)\n",
      "27082 Training Loss: tensor(0.3266)\n",
      "27083 Training Loss: tensor(0.3261)\n",
      "27084 Training Loss: tensor(0.3280)\n",
      "27085 Training Loss: tensor(0.3282)\n",
      "27086 Training Loss: tensor(0.3265)\n",
      "27087 Training Loss: tensor(0.3264)\n",
      "27088 Training Loss: tensor(0.3265)\n",
      "27089 Training Loss: tensor(0.3278)\n",
      "27090 Training Loss: tensor(0.3268)\n",
      "27091 Training Loss: tensor(0.3266)\n",
      "27092 Training Loss: tensor(0.3267)\n",
      "27093 Training Loss: tensor(0.3273)\n",
      "27094 Training Loss: tensor(0.3269)\n",
      "27095 Training Loss: tensor(0.3264)\n",
      "27096 Training Loss: tensor(0.3269)\n",
      "27097 Training Loss: tensor(0.3270)\n",
      "27098 Training Loss: tensor(0.3268)\n",
      "27099 Training Loss: tensor(0.3267)\n",
      "27100 Training Loss: tensor(0.3269)\n",
      "27101 Training Loss: tensor(0.3270)\n",
      "27102 Training Loss: tensor(0.3267)\n",
      "27103 Training Loss: tensor(0.3302)\n",
      "27104 Training Loss: tensor(0.3264)\n",
      "27105 Training Loss: tensor(0.3263)\n",
      "27106 Training Loss: tensor(0.3266)\n",
      "27107 Training Loss: tensor(0.3269)\n",
      "27108 Training Loss: tensor(0.3278)\n",
      "27109 Training Loss: tensor(0.3268)\n",
      "27110 Training Loss: tensor(0.3272)\n",
      "27111 Training Loss: tensor(0.3260)\n",
      "27112 Training Loss: tensor(0.3268)\n",
      "27113 Training Loss: tensor(0.3266)\n",
      "27114 Training Loss: tensor(0.3267)\n",
      "27115 Training Loss: tensor(0.3267)\n",
      "27116 Training Loss: tensor(0.3315)\n",
      "27117 Training Loss: tensor(0.3280)\n",
      "27118 Training Loss: tensor(0.3270)\n",
      "27119 Training Loss: tensor(0.3265)\n",
      "27120 Training Loss: tensor(0.3286)\n",
      "27121 Training Loss: tensor(0.3275)\n",
      "27122 Training Loss: tensor(0.3273)\n",
      "27123 Training Loss: tensor(0.3268)\n",
      "27124 Training Loss: tensor(0.3274)\n",
      "27125 Training Loss: tensor(0.3269)\n",
      "27126 Training Loss: tensor(0.3267)\n",
      "27127 Training Loss: tensor(0.3273)\n",
      "27128 Training Loss: tensor(0.3265)\n",
      "27129 Training Loss: tensor(0.3278)\n",
      "27130 Training Loss: tensor(0.3264)\n",
      "27131 Training Loss: tensor(0.3263)\n",
      "27132 Training Loss: tensor(0.3267)\n",
      "27133 Training Loss: tensor(0.3268)\n",
      "27134 Training Loss: tensor(0.3301)\n",
      "27135 Training Loss: tensor(0.3266)\n",
      "27136 Training Loss: tensor(0.3263)\n",
      "27137 Training Loss: tensor(0.3269)\n",
      "27138 Training Loss: tensor(0.3273)\n",
      "27139 Training Loss: tensor(0.3270)\n",
      "27140 Training Loss: tensor(0.3273)\n",
      "27141 Training Loss: tensor(0.3267)\n",
      "27142 Training Loss: tensor(0.3270)\n",
      "27143 Training Loss: tensor(0.3263)\n",
      "27144 Training Loss: tensor(0.3281)\n",
      "27145 Training Loss: tensor(0.3279)\n",
      "27146 Training Loss: tensor(0.3271)\n",
      "27147 Training Loss: tensor(0.3269)\n",
      "27148 Training Loss: tensor(0.3272)\n",
      "27149 Training Loss: tensor(0.3289)\n",
      "27150 Training Loss: tensor(0.3270)\n",
      "27151 Training Loss: tensor(0.3270)\n",
      "27152 Training Loss: tensor(0.3271)\n",
      "27153 Training Loss: tensor(0.3272)\n",
      "27154 Training Loss: tensor(0.3266)\n",
      "27155 Training Loss: tensor(0.3283)\n",
      "27156 Training Loss: tensor(0.3265)\n",
      "27157 Training Loss: tensor(0.3282)\n",
      "27158 Training Loss: tensor(0.3263)\n",
      "27159 Training Loss: tensor(0.3269)\n",
      "27160 Training Loss: tensor(0.3263)\n",
      "27161 Training Loss: tensor(0.3265)\n",
      "27162 Training Loss: tensor(0.3276)\n",
      "27163 Training Loss: tensor(0.3263)\n",
      "27164 Training Loss: tensor(0.3272)\n",
      "27165 Training Loss: tensor(0.3264)\n",
      "27166 Training Loss: tensor(0.3269)\n",
      "27167 Training Loss: tensor(0.3273)\n",
      "27168 Training Loss: tensor(0.3264)\n",
      "27169 Training Loss: tensor(0.3272)\n",
      "27170 Training Loss: tensor(0.3266)\n",
      "27171 Training Loss: tensor(0.3264)\n",
      "27172 Training Loss: tensor(0.3266)\n",
      "27173 Training Loss: tensor(0.3262)\n",
      "27174 Training Loss: tensor(0.3271)\n",
      "27175 Training Loss: tensor(0.3265)\n",
      "27176 Training Loss: tensor(0.3262)\n",
      "27177 Training Loss: tensor(0.3262)\n",
      "27178 Training Loss: tensor(0.3275)\n",
      "27179 Training Loss: tensor(0.3263)\n",
      "27180 Training Loss: tensor(0.3266)\n",
      "27181 Training Loss: tensor(0.3270)\n",
      "27182 Training Loss: tensor(0.3307)\n",
      "27183 Training Loss: tensor(0.3266)\n",
      "27184 Training Loss: tensor(0.3265)\n",
      "27185 Training Loss: tensor(0.3268)\n",
      "27186 Training Loss: tensor(0.3299)\n",
      "27187 Training Loss: tensor(0.3274)\n",
      "27188 Training Loss: tensor(0.3267)\n",
      "27189 Training Loss: tensor(0.3268)\n",
      "27190 Training Loss: tensor(0.3271)\n",
      "27191 Training Loss: tensor(0.3270)\n",
      "27192 Training Loss: tensor(0.3267)\n",
      "27193 Training Loss: tensor(0.3271)\n",
      "27194 Training Loss: tensor(0.3288)\n",
      "27195 Training Loss: tensor(0.3265)\n",
      "27196 Training Loss: tensor(0.3271)\n",
      "27197 Training Loss: tensor(0.3266)\n",
      "27198 Training Loss: tensor(0.3265)\n",
      "27199 Training Loss: tensor(0.3266)\n",
      "27200 Training Loss: tensor(0.3276)\n",
      "27201 Training Loss: tensor(0.3264)\n",
      "27202 Training Loss: tensor(0.3264)\n",
      "27203 Training Loss: tensor(0.3264)\n",
      "27204 Training Loss: tensor(0.3265)\n",
      "27205 Training Loss: tensor(0.3266)\n",
      "27206 Training Loss: tensor(0.3264)\n",
      "27207 Training Loss: tensor(0.3291)\n",
      "27208 Training Loss: tensor(0.3262)\n",
      "27209 Training Loss: tensor(0.3283)\n",
      "27210 Training Loss: tensor(0.3266)\n",
      "27211 Training Loss: tensor(0.3262)\n",
      "27212 Training Loss: tensor(0.3266)\n",
      "27213 Training Loss: tensor(0.3266)\n",
      "27214 Training Loss: tensor(0.3269)\n",
      "27215 Training Loss: tensor(0.3268)\n",
      "27216 Training Loss: tensor(0.3261)\n",
      "27217 Training Loss: tensor(0.3301)\n",
      "27218 Training Loss: tensor(0.3281)\n",
      "27219 Training Loss: tensor(0.3263)\n",
      "27220 Training Loss: tensor(0.3263)\n",
      "27221 Training Loss: tensor(0.3275)\n",
      "27222 Training Loss: tensor(0.3265)\n",
      "27223 Training Loss: tensor(0.3265)\n",
      "27224 Training Loss: tensor(0.3280)\n",
      "27225 Training Loss: tensor(0.3278)\n",
      "27226 Training Loss: tensor(0.3264)\n",
      "27227 Training Loss: tensor(0.3265)\n",
      "27228 Training Loss: tensor(0.3273)\n",
      "27229 Training Loss: tensor(0.3265)\n",
      "27230 Training Loss: tensor(0.3272)\n",
      "27231 Training Loss: tensor(0.3281)\n",
      "27232 Training Loss: tensor(0.3262)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27233 Training Loss: tensor(0.3272)\n",
      "27234 Training Loss: tensor(0.3265)\n",
      "27235 Training Loss: tensor(0.3264)\n",
      "27236 Training Loss: tensor(0.3287)\n",
      "27237 Training Loss: tensor(0.3263)\n",
      "27238 Training Loss: tensor(0.3272)\n",
      "27239 Training Loss: tensor(0.3270)\n",
      "27240 Training Loss: tensor(0.3269)\n",
      "27241 Training Loss: tensor(0.3265)\n",
      "27242 Training Loss: tensor(0.3266)\n",
      "27243 Training Loss: tensor(0.3264)\n",
      "27244 Training Loss: tensor(0.3264)\n",
      "27245 Training Loss: tensor(0.3263)\n",
      "27246 Training Loss: tensor(0.3265)\n",
      "27247 Training Loss: tensor(0.3267)\n",
      "27248 Training Loss: tensor(0.3272)\n",
      "27249 Training Loss: tensor(0.3264)\n",
      "27250 Training Loss: tensor(0.3261)\n",
      "27251 Training Loss: tensor(0.3265)\n",
      "27252 Training Loss: tensor(0.3264)\n",
      "27253 Training Loss: tensor(0.3263)\n",
      "27254 Training Loss: tensor(0.3289)\n",
      "27255 Training Loss: tensor(0.3290)\n",
      "27256 Training Loss: tensor(0.3267)\n",
      "27257 Training Loss: tensor(0.3266)\n",
      "27258 Training Loss: tensor(0.3279)\n",
      "27259 Training Loss: tensor(0.3269)\n",
      "27260 Training Loss: tensor(0.3282)\n",
      "27261 Training Loss: tensor(0.3277)\n",
      "27262 Training Loss: tensor(0.3282)\n",
      "27263 Training Loss: tensor(0.3263)\n",
      "27264 Training Loss: tensor(0.3270)\n",
      "27265 Training Loss: tensor(0.3281)\n",
      "27266 Training Loss: tensor(0.3270)\n",
      "27267 Training Loss: tensor(0.3269)\n",
      "27268 Training Loss: tensor(0.3265)\n",
      "27269 Training Loss: tensor(0.3262)\n",
      "27270 Training Loss: tensor(0.3296)\n",
      "27271 Training Loss: tensor(0.3265)\n",
      "27272 Training Loss: tensor(0.3269)\n",
      "27273 Training Loss: tensor(0.3275)\n",
      "27274 Training Loss: tensor(0.3269)\n",
      "27275 Training Loss: tensor(0.3276)\n",
      "27276 Training Loss: tensor(0.3263)\n",
      "27277 Training Loss: tensor(0.3274)\n",
      "27278 Training Loss: tensor(0.3264)\n",
      "27279 Training Loss: tensor(0.3273)\n",
      "27280 Training Loss: tensor(0.3269)\n",
      "27281 Training Loss: tensor(0.3313)\n",
      "27282 Training Loss: tensor(0.3271)\n",
      "27283 Training Loss: tensor(0.3268)\n",
      "27284 Training Loss: tensor(0.3268)\n",
      "27285 Training Loss: tensor(0.3273)\n",
      "27286 Training Loss: tensor(0.3272)\n",
      "27287 Training Loss: tensor(0.3282)\n",
      "27288 Training Loss: tensor(0.3285)\n",
      "27289 Training Loss: tensor(0.3275)\n",
      "27290 Training Loss: tensor(0.3267)\n",
      "27291 Training Loss: tensor(0.3264)\n",
      "27292 Training Loss: tensor(0.3269)\n",
      "27293 Training Loss: tensor(0.3265)\n",
      "27294 Training Loss: tensor(0.3270)\n",
      "27295 Training Loss: tensor(0.3276)\n",
      "27296 Training Loss: tensor(0.3271)\n",
      "27297 Training Loss: tensor(0.3268)\n",
      "27298 Training Loss: tensor(0.3262)\n",
      "27299 Training Loss: tensor(0.3267)\n",
      "27300 Training Loss: tensor(0.3269)\n",
      "27301 Training Loss: tensor(0.3272)\n",
      "27302 Training Loss: tensor(0.3266)\n",
      "27303 Training Loss: tensor(0.3263)\n",
      "27304 Training Loss: tensor(0.3265)\n",
      "27305 Training Loss: tensor(0.3267)\n",
      "27306 Training Loss: tensor(0.3278)\n",
      "27307 Training Loss: tensor(0.3273)\n",
      "27308 Training Loss: tensor(0.3271)\n",
      "27309 Training Loss: tensor(0.3263)\n",
      "27310 Training Loss: tensor(0.3275)\n",
      "27311 Training Loss: tensor(0.3264)\n",
      "27312 Training Loss: tensor(0.3266)\n",
      "27313 Training Loss: tensor(0.3287)\n",
      "27314 Training Loss: tensor(0.3280)\n",
      "27315 Training Loss: tensor(0.3271)\n",
      "27316 Training Loss: tensor(0.3264)\n",
      "27317 Training Loss: tensor(0.3287)\n",
      "27318 Training Loss: tensor(0.3286)\n",
      "27319 Training Loss: tensor(0.3271)\n",
      "27320 Training Loss: tensor(0.3271)\n",
      "27321 Training Loss: tensor(0.3265)\n",
      "27322 Training Loss: tensor(0.3271)\n",
      "27323 Training Loss: tensor(0.3269)\n",
      "27324 Training Loss: tensor(0.3286)\n",
      "27325 Training Loss: tensor(0.3271)\n",
      "27326 Training Loss: tensor(0.3269)\n",
      "27327 Training Loss: tensor(0.3265)\n",
      "27328 Training Loss: tensor(0.3270)\n",
      "27329 Training Loss: tensor(0.3267)\n",
      "27330 Training Loss: tensor(0.3272)\n",
      "27331 Training Loss: tensor(0.3274)\n",
      "27332 Training Loss: tensor(0.3272)\n",
      "27333 Training Loss: tensor(0.3267)\n",
      "27334 Training Loss: tensor(0.3270)\n",
      "27335 Training Loss: tensor(0.3277)\n",
      "27336 Training Loss: tensor(0.3275)\n",
      "27337 Training Loss: tensor(0.3265)\n",
      "27338 Training Loss: tensor(0.3272)\n",
      "27339 Training Loss: tensor(0.3270)\n",
      "27340 Training Loss: tensor(0.3264)\n",
      "27341 Training Loss: tensor(0.3262)\n",
      "27342 Training Loss: tensor(0.3276)\n",
      "27343 Training Loss: tensor(0.3261)\n",
      "27344 Training Loss: tensor(0.3275)\n",
      "27345 Training Loss: tensor(0.3265)\n",
      "27346 Training Loss: tensor(0.3272)\n",
      "27347 Training Loss: tensor(0.3291)\n",
      "27348 Training Loss: tensor(0.3268)\n",
      "27349 Training Loss: tensor(0.3263)\n",
      "27350 Training Loss: tensor(0.3270)\n",
      "27351 Training Loss: tensor(0.3270)\n",
      "27352 Training Loss: tensor(0.3263)\n",
      "27353 Training Loss: tensor(0.3273)\n",
      "27354 Training Loss: tensor(0.3268)\n",
      "27355 Training Loss: tensor(0.3281)\n",
      "27356 Training Loss: tensor(0.3281)\n",
      "27357 Training Loss: tensor(0.3279)\n",
      "27358 Training Loss: tensor(0.3270)\n",
      "27359 Training Loss: tensor(0.3266)\n",
      "27360 Training Loss: tensor(0.3271)\n",
      "27361 Training Loss: tensor(0.3272)\n",
      "27362 Training Loss: tensor(0.3276)\n",
      "27363 Training Loss: tensor(0.3264)\n",
      "27364 Training Loss: tensor(0.3266)\n",
      "27365 Training Loss: tensor(0.3270)\n",
      "27366 Training Loss: tensor(0.3267)\n",
      "27367 Training Loss: tensor(0.3271)\n",
      "27368 Training Loss: tensor(0.3264)\n",
      "27369 Training Loss: tensor(0.3271)\n",
      "27370 Training Loss: tensor(0.3263)\n",
      "27371 Training Loss: tensor(0.3264)\n",
      "27372 Training Loss: tensor(0.3263)\n",
      "27373 Training Loss: tensor(0.3262)\n",
      "27374 Training Loss: tensor(0.3276)\n",
      "27375 Training Loss: tensor(0.3269)\n",
      "27376 Training Loss: tensor(0.3265)\n",
      "27377 Training Loss: tensor(0.3298)\n",
      "27378 Training Loss: tensor(0.3266)\n",
      "27379 Training Loss: tensor(0.3263)\n",
      "27380 Training Loss: tensor(0.3293)\n",
      "27381 Training Loss: tensor(0.3266)\n",
      "27382 Training Loss: tensor(0.3267)\n",
      "27383 Training Loss: tensor(0.3271)\n",
      "27384 Training Loss: tensor(0.3274)\n",
      "27385 Training Loss: tensor(0.3268)\n",
      "27386 Training Loss: tensor(0.3266)\n",
      "27387 Training Loss: tensor(0.3268)\n",
      "27388 Training Loss: tensor(0.3265)\n",
      "27389 Training Loss: tensor(0.3265)\n",
      "27390 Training Loss: tensor(0.3265)\n",
      "27391 Training Loss: tensor(0.3268)\n",
      "27392 Training Loss: tensor(0.3269)\n",
      "27393 Training Loss: tensor(0.3272)\n",
      "27394 Training Loss: tensor(0.3265)\n",
      "27395 Training Loss: tensor(0.3269)\n",
      "27396 Training Loss: tensor(0.3262)\n",
      "27397 Training Loss: tensor(0.3261)\n",
      "27398 Training Loss: tensor(0.3266)\n",
      "27399 Training Loss: tensor(0.3267)\n",
      "27400 Training Loss: tensor(0.3272)\n",
      "27401 Training Loss: tensor(0.3274)\n",
      "27402 Training Loss: tensor(0.3271)\n",
      "27403 Training Loss: tensor(0.3264)\n",
      "27404 Training Loss: tensor(0.3268)\n",
      "27405 Training Loss: tensor(0.3266)\n",
      "27406 Training Loss: tensor(0.3289)\n",
      "27407 Training Loss: tensor(0.3265)\n",
      "27408 Training Loss: tensor(0.3301)\n",
      "27409 Training Loss: tensor(0.3285)\n",
      "27410 Training Loss: tensor(0.3285)\n",
      "27411 Training Loss: tensor(0.3263)\n",
      "27412 Training Loss: tensor(0.3267)\n",
      "27413 Training Loss: tensor(0.3267)\n",
      "27414 Training Loss: tensor(0.3267)\n",
      "27415 Training Loss: tensor(0.3278)\n",
      "27416 Training Loss: tensor(0.3269)\n",
      "27417 Training Loss: tensor(0.3266)\n",
      "27418 Training Loss: tensor(0.3267)\n",
      "27419 Training Loss: tensor(0.3266)\n",
      "27420 Training Loss: tensor(0.3279)\n",
      "27421 Training Loss: tensor(0.3263)\n",
      "27422 Training Loss: tensor(0.3266)\n",
      "27423 Training Loss: tensor(0.3271)\n",
      "27424 Training Loss: tensor(0.3266)\n",
      "27425 Training Loss: tensor(0.3269)\n",
      "27426 Training Loss: tensor(0.3270)\n",
      "27427 Training Loss: tensor(0.3269)\n",
      "27428 Training Loss: tensor(0.3263)\n",
      "27429 Training Loss: tensor(0.3320)\n",
      "27430 Training Loss: tensor(0.3274)\n",
      "27431 Training Loss: tensor(0.3264)\n",
      "27432 Training Loss: tensor(0.3303)\n",
      "27433 Training Loss: tensor(0.3272)\n",
      "27434 Training Loss: tensor(0.3265)\n",
      "27435 Training Loss: tensor(0.3267)\n",
      "27436 Training Loss: tensor(0.3280)\n",
      "27437 Training Loss: tensor(0.3271)\n",
      "27438 Training Loss: tensor(0.3267)\n",
      "27439 Training Loss: tensor(0.3269)\n",
      "27440 Training Loss: tensor(0.3266)\n",
      "27441 Training Loss: tensor(0.3270)\n",
      "27442 Training Loss: tensor(0.3267)\n",
      "27443 Training Loss: tensor(0.3265)\n",
      "27444 Training Loss: tensor(0.3277)\n",
      "27445 Training Loss: tensor(0.3277)\n",
      "27446 Training Loss: tensor(0.3270)\n",
      "27447 Training Loss: tensor(0.3268)\n",
      "27448 Training Loss: tensor(0.3266)\n",
      "27449 Training Loss: tensor(0.3265)\n",
      "27450 Training Loss: tensor(0.3265)\n",
      "27451 Training Loss: tensor(0.3267)\n",
      "27452 Training Loss: tensor(0.3276)\n",
      "27453 Training Loss: tensor(0.3274)\n",
      "27454 Training Loss: tensor(0.3269)\n",
      "27455 Training Loss: tensor(0.3310)\n",
      "27456 Training Loss: tensor(0.3273)\n",
      "27457 Training Loss: tensor(0.3267)\n",
      "27458 Training Loss: tensor(0.3269)\n",
      "27459 Training Loss: tensor(0.3263)\n",
      "27460 Training Loss: tensor(0.3296)\n",
      "27461 Training Loss: tensor(0.3266)\n",
      "27462 Training Loss: tensor(0.3270)\n",
      "27463 Training Loss: tensor(0.3263)\n",
      "27464 Training Loss: tensor(0.3266)\n",
      "27465 Training Loss: tensor(0.3271)\n",
      "27466 Training Loss: tensor(0.3268)\n",
      "27467 Training Loss: tensor(0.3264)\n",
      "27468 Training Loss: tensor(0.3266)\n",
      "27469 Training Loss: tensor(0.3263)\n",
      "27470 Training Loss: tensor(0.3270)\n",
      "27471 Training Loss: tensor(0.3270)\n",
      "27472 Training Loss: tensor(0.3300)\n",
      "27473 Training Loss: tensor(0.3264)\n",
      "27474 Training Loss: tensor(0.3279)\n",
      "27475 Training Loss: tensor(0.3268)\n",
      "27476 Training Loss: tensor(0.3275)\n",
      "27477 Training Loss: tensor(0.3269)\n",
      "27478 Training Loss: tensor(0.3266)\n",
      "27479 Training Loss: tensor(0.3271)\n",
      "27480 Training Loss: tensor(0.3267)\n",
      "27481 Training Loss: tensor(0.3271)\n",
      "27482 Training Loss: tensor(0.3265)\n",
      "27483 Training Loss: tensor(0.3267)\n",
      "27484 Training Loss: tensor(0.3264)\n",
      "27485 Training Loss: tensor(0.3269)\n",
      "27486 Training Loss: tensor(0.3263)\n",
      "27487 Training Loss: tensor(0.3274)\n",
      "27488 Training Loss: tensor(0.3305)\n",
      "27489 Training Loss: tensor(0.3263)\n",
      "27490 Training Loss: tensor(0.3263)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27491 Training Loss: tensor(0.3272)\n",
      "27492 Training Loss: tensor(0.3268)\n",
      "27493 Training Loss: tensor(0.3268)\n",
      "27494 Training Loss: tensor(0.3272)\n",
      "27495 Training Loss: tensor(0.3277)\n",
      "27496 Training Loss: tensor(0.3272)\n",
      "27497 Training Loss: tensor(0.3266)\n",
      "27498 Training Loss: tensor(0.3273)\n",
      "27499 Training Loss: tensor(0.3271)\n",
      "27500 Training Loss: tensor(0.3264)\n",
      "27501 Training Loss: tensor(0.3264)\n",
      "27502 Training Loss: tensor(0.3264)\n",
      "27503 Training Loss: tensor(0.3268)\n",
      "27504 Training Loss: tensor(0.3265)\n",
      "27505 Training Loss: tensor(0.3264)\n",
      "27506 Training Loss: tensor(0.3265)\n",
      "27507 Training Loss: tensor(0.3267)\n",
      "27508 Training Loss: tensor(0.3266)\n",
      "27509 Training Loss: tensor(0.3263)\n",
      "27510 Training Loss: tensor(0.3276)\n",
      "27511 Training Loss: tensor(0.3260)\n",
      "27512 Training Loss: tensor(0.3277)\n",
      "27513 Training Loss: tensor(0.3263)\n",
      "27514 Training Loss: tensor(0.3261)\n",
      "27515 Training Loss: tensor(0.3270)\n",
      "27516 Training Loss: tensor(0.3268)\n",
      "27517 Training Loss: tensor(0.3265)\n",
      "27518 Training Loss: tensor(0.3271)\n",
      "27519 Training Loss: tensor(0.3285)\n",
      "27520 Training Loss: tensor(0.3264)\n",
      "27521 Training Loss: tensor(0.3284)\n",
      "27522 Training Loss: tensor(0.3263)\n",
      "27523 Training Loss: tensor(0.3267)\n",
      "27524 Training Loss: tensor(0.3269)\n",
      "27525 Training Loss: tensor(0.3278)\n",
      "27526 Training Loss: tensor(0.3265)\n",
      "27527 Training Loss: tensor(0.3266)\n",
      "27528 Training Loss: tensor(0.3266)\n",
      "27529 Training Loss: tensor(0.3313)\n",
      "27530 Training Loss: tensor(0.3282)\n",
      "27531 Training Loss: tensor(0.3267)\n",
      "27532 Training Loss: tensor(0.3271)\n",
      "27533 Training Loss: tensor(0.3277)\n",
      "27534 Training Loss: tensor(0.3270)\n",
      "27535 Training Loss: tensor(0.3267)\n",
      "27536 Training Loss: tensor(0.3264)\n",
      "27537 Training Loss: tensor(0.3264)\n",
      "27538 Training Loss: tensor(0.3268)\n",
      "27539 Training Loss: tensor(0.3265)\n",
      "27540 Training Loss: tensor(0.3266)\n",
      "27541 Training Loss: tensor(0.3264)\n",
      "27542 Training Loss: tensor(0.3289)\n",
      "27543 Training Loss: tensor(0.3297)\n",
      "27544 Training Loss: tensor(0.3272)\n",
      "27545 Training Loss: tensor(0.3275)\n",
      "27546 Training Loss: tensor(0.3272)\n",
      "27547 Training Loss: tensor(0.3283)\n",
      "27548 Training Loss: tensor(0.3277)\n",
      "27549 Training Loss: tensor(0.3270)\n",
      "27550 Training Loss: tensor(0.3275)\n",
      "27551 Training Loss: tensor(0.3276)\n",
      "27552 Training Loss: tensor(0.3274)\n",
      "27553 Training Loss: tensor(0.3277)\n",
      "27554 Training Loss: tensor(0.3271)\n",
      "27555 Training Loss: tensor(0.3263)\n",
      "27556 Training Loss: tensor(0.3269)\n",
      "27557 Training Loss: tensor(0.3271)\n",
      "27558 Training Loss: tensor(0.3274)\n",
      "27559 Training Loss: tensor(0.3275)\n",
      "27560 Training Loss: tensor(0.3265)\n",
      "27561 Training Loss: tensor(0.3267)\n",
      "27562 Training Loss: tensor(0.3271)\n",
      "27563 Training Loss: tensor(0.3273)\n",
      "27564 Training Loss: tensor(0.3281)\n",
      "27565 Training Loss: tensor(0.3267)\n",
      "27566 Training Loss: tensor(0.3269)\n",
      "27567 Training Loss: tensor(0.3272)\n",
      "27568 Training Loss: tensor(0.3273)\n",
      "27569 Training Loss: tensor(0.3269)\n",
      "27570 Training Loss: tensor(0.3266)\n",
      "27571 Training Loss: tensor(0.3278)\n",
      "27572 Training Loss: tensor(0.3268)\n",
      "27573 Training Loss: tensor(0.3265)\n",
      "27574 Training Loss: tensor(0.3267)\n",
      "27575 Training Loss: tensor(0.3265)\n",
      "27576 Training Loss: tensor(0.3269)\n",
      "27577 Training Loss: tensor(0.3270)\n",
      "27578 Training Loss: tensor(0.3278)\n",
      "27579 Training Loss: tensor(0.3261)\n",
      "27580 Training Loss: tensor(0.3261)\n",
      "27581 Training Loss: tensor(0.3285)\n",
      "27582 Training Loss: tensor(0.3266)\n",
      "27583 Training Loss: tensor(0.3266)\n",
      "27584 Training Loss: tensor(0.3281)\n",
      "27585 Training Loss: tensor(0.3278)\n",
      "27586 Training Loss: tensor(0.3266)\n",
      "27587 Training Loss: tensor(0.3265)\n",
      "27588 Training Loss: tensor(0.3264)\n",
      "27589 Training Loss: tensor(0.3262)\n",
      "27590 Training Loss: tensor(0.3263)\n",
      "27591 Training Loss: tensor(0.3268)\n",
      "27592 Training Loss: tensor(0.3266)\n",
      "27593 Training Loss: tensor(0.3261)\n",
      "27594 Training Loss: tensor(0.3266)\n",
      "27595 Training Loss: tensor(0.3276)\n",
      "27596 Training Loss: tensor(0.3266)\n",
      "27597 Training Loss: tensor(0.3267)\n",
      "27598 Training Loss: tensor(0.3272)\n",
      "27599 Training Loss: tensor(0.3264)\n",
      "27600 Training Loss: tensor(0.3266)\n",
      "27601 Training Loss: tensor(0.3267)\n",
      "27602 Training Loss: tensor(0.3268)\n",
      "27603 Training Loss: tensor(0.3263)\n",
      "27604 Training Loss: tensor(0.3262)\n",
      "27605 Training Loss: tensor(0.3268)\n",
      "27606 Training Loss: tensor(0.3266)\n",
      "27607 Training Loss: tensor(0.3261)\n",
      "27608 Training Loss: tensor(0.3269)\n",
      "27609 Training Loss: tensor(0.3268)\n",
      "27610 Training Loss: tensor(0.3268)\n",
      "27611 Training Loss: tensor(0.3267)\n",
      "27612 Training Loss: tensor(0.3284)\n",
      "27613 Training Loss: tensor(0.3262)\n",
      "27614 Training Loss: tensor(0.3272)\n",
      "27615 Training Loss: tensor(0.3276)\n",
      "27616 Training Loss: tensor(0.3267)\n",
      "27617 Training Loss: tensor(0.3271)\n",
      "27618 Training Loss: tensor(0.3264)\n",
      "27619 Training Loss: tensor(0.3263)\n",
      "27620 Training Loss: tensor(0.3270)\n",
      "27621 Training Loss: tensor(0.3262)\n",
      "27622 Training Loss: tensor(0.3263)\n",
      "27623 Training Loss: tensor(0.3291)\n",
      "27624 Training Loss: tensor(0.3277)\n",
      "27625 Training Loss: tensor(0.3279)\n",
      "27626 Training Loss: tensor(0.3290)\n",
      "27627 Training Loss: tensor(0.3273)\n",
      "27628 Training Loss: tensor(0.3270)\n",
      "27629 Training Loss: tensor(0.3266)\n",
      "27630 Training Loss: tensor(0.3263)\n",
      "27631 Training Loss: tensor(0.3269)\n",
      "27632 Training Loss: tensor(0.3267)\n",
      "27633 Training Loss: tensor(0.3265)\n",
      "27634 Training Loss: tensor(0.3265)\n",
      "27635 Training Loss: tensor(0.3267)\n",
      "27636 Training Loss: tensor(0.3273)\n",
      "27637 Training Loss: tensor(0.3269)\n",
      "27638 Training Loss: tensor(0.3269)\n",
      "27639 Training Loss: tensor(0.3272)\n",
      "27640 Training Loss: tensor(0.3274)\n",
      "27641 Training Loss: tensor(0.3264)\n",
      "27642 Training Loss: tensor(0.3263)\n",
      "27643 Training Loss: tensor(0.3268)\n",
      "27644 Training Loss: tensor(0.3262)\n",
      "27645 Training Loss: tensor(0.3290)\n",
      "27646 Training Loss: tensor(0.3267)\n",
      "27647 Training Loss: tensor(0.3267)\n",
      "27648 Training Loss: tensor(0.3263)\n",
      "27649 Training Loss: tensor(0.3266)\n",
      "27650 Training Loss: tensor(0.3277)\n",
      "27651 Training Loss: tensor(0.3262)\n",
      "27652 Training Loss: tensor(0.3263)\n",
      "27653 Training Loss: tensor(0.3264)\n",
      "27654 Training Loss: tensor(0.3266)\n",
      "27655 Training Loss: tensor(0.3264)\n",
      "27656 Training Loss: tensor(0.3280)\n",
      "27657 Training Loss: tensor(0.3262)\n",
      "27658 Training Loss: tensor(0.3262)\n",
      "27659 Training Loss: tensor(0.3262)\n",
      "27660 Training Loss: tensor(0.3260)\n",
      "27661 Training Loss: tensor(0.3263)\n",
      "27662 Training Loss: tensor(0.3270)\n",
      "27663 Training Loss: tensor(0.3273)\n",
      "27664 Training Loss: tensor(0.3261)\n",
      "27665 Training Loss: tensor(0.3264)\n",
      "27666 Training Loss: tensor(0.3265)\n",
      "27667 Training Loss: tensor(0.3263)\n",
      "27668 Training Loss: tensor(0.3261)\n",
      "27669 Training Loss: tensor(0.3281)\n",
      "27670 Training Loss: tensor(0.3276)\n",
      "27671 Training Loss: tensor(0.3261)\n",
      "27672 Training Loss: tensor(0.3276)\n",
      "27673 Training Loss: tensor(0.3267)\n",
      "27674 Training Loss: tensor(0.3267)\n",
      "27675 Training Loss: tensor(0.3264)\n",
      "27676 Training Loss: tensor(0.3267)\n",
      "27677 Training Loss: tensor(0.3268)\n",
      "27678 Training Loss: tensor(0.3277)\n",
      "27679 Training Loss: tensor(0.3265)\n",
      "27680 Training Loss: tensor(0.3265)\n",
      "27681 Training Loss: tensor(0.3277)\n",
      "27682 Training Loss: tensor(0.3301)\n",
      "27683 Training Loss: tensor(0.3270)\n",
      "27684 Training Loss: tensor(0.3266)\n",
      "27685 Training Loss: tensor(0.3267)\n",
      "27686 Training Loss: tensor(0.3271)\n",
      "27687 Training Loss: tensor(0.3285)\n",
      "27688 Training Loss: tensor(0.3264)\n",
      "27689 Training Loss: tensor(0.3279)\n",
      "27690 Training Loss: tensor(0.3266)\n",
      "27691 Training Loss: tensor(0.3272)\n",
      "27692 Training Loss: tensor(0.3264)\n",
      "27693 Training Loss: tensor(0.3266)\n",
      "27694 Training Loss: tensor(0.3265)\n",
      "27695 Training Loss: tensor(0.3262)\n",
      "27696 Training Loss: tensor(0.3261)\n",
      "27697 Training Loss: tensor(0.3264)\n",
      "27698 Training Loss: tensor(0.3306)\n",
      "27699 Training Loss: tensor(0.3260)\n",
      "27700 Training Loss: tensor(0.3260)\n",
      "27701 Training Loss: tensor(0.3262)\n",
      "27702 Training Loss: tensor(0.3266)\n",
      "27703 Training Loss: tensor(0.3261)\n",
      "27704 Training Loss: tensor(0.3261)\n",
      "27705 Training Loss: tensor(0.3262)\n",
      "27706 Training Loss: tensor(0.3274)\n",
      "27707 Training Loss: tensor(0.3260)\n",
      "27708 Training Loss: tensor(0.3264)\n",
      "27709 Training Loss: tensor(0.3283)\n",
      "27710 Training Loss: tensor(0.3271)\n",
      "27711 Training Loss: tensor(0.3264)\n",
      "27712 Training Loss: tensor(0.3265)\n",
      "27713 Training Loss: tensor(0.3266)\n",
      "27714 Training Loss: tensor(0.3263)\n",
      "27715 Training Loss: tensor(0.3264)\n",
      "27716 Training Loss: tensor(0.3280)\n",
      "27717 Training Loss: tensor(0.3293)\n",
      "27718 Training Loss: tensor(0.3265)\n",
      "27719 Training Loss: tensor(0.3263)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27720 Training Loss: tensor(0.3281)\n",
      "27721 Training Loss: tensor(0.3267)\n",
      "27722 Training Loss: tensor(0.3271)\n",
      "27723 Training Loss: tensor(0.3286)\n",
      "27724 Training Loss: tensor(0.3276)\n",
      "27725 Training Loss: tensor(0.3295)\n",
      "27726 Training Loss: tensor(0.3268)\n",
      "27727 Training Loss: tensor(0.3275)\n",
      "27728 Training Loss: tensor(0.3272)\n",
      "27729 Training Loss: tensor(0.3274)\n",
      "27730 Training Loss: tensor(0.3266)\n",
      "27731 Training Loss: tensor(0.3266)\n",
      "27732 Training Loss: tensor(0.3265)\n",
      "27733 Training Loss: tensor(0.3266)\n",
      "27734 Training Loss: tensor(0.3271)\n",
      "27735 Training Loss: tensor(0.3269)\n",
      "27736 Training Loss: tensor(0.3268)\n",
      "27737 Training Loss: tensor(0.3264)\n",
      "27738 Training Loss: tensor(0.3262)\n",
      "27739 Training Loss: tensor(0.3261)\n",
      "27740 Training Loss: tensor(0.3265)\n",
      "27741 Training Loss: tensor(0.3262)\n",
      "27742 Training Loss: tensor(0.3264)\n",
      "27743 Training Loss: tensor(0.3265)\n",
      "27744 Training Loss: tensor(0.3263)\n",
      "27745 Training Loss: tensor(0.3304)\n",
      "27746 Training Loss: tensor(0.3263)\n",
      "27747 Training Loss: tensor(0.3262)\n",
      "27748 Training Loss: tensor(0.3267)\n",
      "27749 Training Loss: tensor(0.3297)\n",
      "27750 Training Loss: tensor(0.3262)\n",
      "27751 Training Loss: tensor(0.3264)\n",
      "27752 Training Loss: tensor(0.3265)\n",
      "27753 Training Loss: tensor(0.3264)\n",
      "27754 Training Loss: tensor(0.3263)\n",
      "27755 Training Loss: tensor(0.3265)\n",
      "27756 Training Loss: tensor(0.3263)\n",
      "27757 Training Loss: tensor(0.3283)\n",
      "27758 Training Loss: tensor(0.3263)\n",
      "27759 Training Loss: tensor(0.3274)\n",
      "27760 Training Loss: tensor(0.3265)\n",
      "27761 Training Loss: tensor(0.3265)\n",
      "27762 Training Loss: tensor(0.3266)\n",
      "27763 Training Loss: tensor(0.3265)\n",
      "27764 Training Loss: tensor(0.3266)\n",
      "27765 Training Loss: tensor(0.3268)\n",
      "27766 Training Loss: tensor(0.3265)\n",
      "27767 Training Loss: tensor(0.3264)\n",
      "27768 Training Loss: tensor(0.3268)\n",
      "27769 Training Loss: tensor(0.3267)\n",
      "27770 Training Loss: tensor(0.3261)\n",
      "27771 Training Loss: tensor(0.3275)\n",
      "27772 Training Loss: tensor(0.3261)\n",
      "27773 Training Loss: tensor(0.3264)\n",
      "27774 Training Loss: tensor(0.3265)\n",
      "27775 Training Loss: tensor(0.3263)\n",
      "27776 Training Loss: tensor(0.3261)\n",
      "27777 Training Loss: tensor(0.3263)\n",
      "27778 Training Loss: tensor(0.3277)\n",
      "27779 Training Loss: tensor(0.3264)\n",
      "27780 Training Loss: tensor(0.3263)\n",
      "27781 Training Loss: tensor(0.3265)\n",
      "27782 Training Loss: tensor(0.3263)\n",
      "27783 Training Loss: tensor(0.3287)\n",
      "27784 Training Loss: tensor(0.3263)\n",
      "27785 Training Loss: tensor(0.3266)\n",
      "27786 Training Loss: tensor(0.3283)\n",
      "27787 Training Loss: tensor(0.3266)\n",
      "27788 Training Loss: tensor(0.3269)\n",
      "27789 Training Loss: tensor(0.3264)\n",
      "27790 Training Loss: tensor(0.3265)\n",
      "27791 Training Loss: tensor(0.3263)\n",
      "27792 Training Loss: tensor(0.3263)\n",
      "27793 Training Loss: tensor(0.3259)\n",
      "27794 Training Loss: tensor(0.3280)\n",
      "27795 Training Loss: tensor(0.3276)\n",
      "27796 Training Loss: tensor(0.3261)\n",
      "27797 Training Loss: tensor(0.3267)\n",
      "27798 Training Loss: tensor(0.3270)\n",
      "27799 Training Loss: tensor(0.3262)\n",
      "27800 Training Loss: tensor(0.3277)\n",
      "27801 Training Loss: tensor(0.3282)\n",
      "27802 Training Loss: tensor(0.3267)\n",
      "27803 Training Loss: tensor(0.3274)\n",
      "27804 Training Loss: tensor(0.3269)\n",
      "27805 Training Loss: tensor(0.3269)\n",
      "27806 Training Loss: tensor(0.3267)\n",
      "27807 Training Loss: tensor(0.3279)\n",
      "27808 Training Loss: tensor(0.3263)\n",
      "27809 Training Loss: tensor(0.3269)\n",
      "27810 Training Loss: tensor(0.3265)\n",
      "27811 Training Loss: tensor(0.3263)\n",
      "27812 Training Loss: tensor(0.3264)\n",
      "27813 Training Loss: tensor(0.3270)\n",
      "27814 Training Loss: tensor(0.3264)\n",
      "27815 Training Loss: tensor(0.3274)\n",
      "27816 Training Loss: tensor(0.3263)\n",
      "27817 Training Loss: tensor(0.3279)\n",
      "27818 Training Loss: tensor(0.3268)\n",
      "27819 Training Loss: tensor(0.3297)\n",
      "27820 Training Loss: tensor(0.3270)\n",
      "27821 Training Loss: tensor(0.3266)\n",
      "27822 Training Loss: tensor(0.3269)\n",
      "27823 Training Loss: tensor(0.3266)\n",
      "27824 Training Loss: tensor(0.3271)\n",
      "27825 Training Loss: tensor(0.3265)\n",
      "27826 Training Loss: tensor(0.3265)\n",
      "27827 Training Loss: tensor(0.3269)\n",
      "27828 Training Loss: tensor(0.3275)\n",
      "27829 Training Loss: tensor(0.3269)\n",
      "27830 Training Loss: tensor(0.3271)\n",
      "27831 Training Loss: tensor(0.3269)\n",
      "27832 Training Loss: tensor(0.3271)\n",
      "27833 Training Loss: tensor(0.3289)\n",
      "27834 Training Loss: tensor(0.3270)\n",
      "27835 Training Loss: tensor(0.3268)\n",
      "27836 Training Loss: tensor(0.3272)\n",
      "27837 Training Loss: tensor(0.3260)\n",
      "27838 Training Loss: tensor(0.3262)\n",
      "27839 Training Loss: tensor(0.3262)\n",
      "27840 Training Loss: tensor(0.3278)\n",
      "27841 Training Loss: tensor(0.3265)\n",
      "27842 Training Loss: tensor(0.3263)\n",
      "27843 Training Loss: tensor(0.3265)\n",
      "27844 Training Loss: tensor(0.3273)\n",
      "27845 Training Loss: tensor(0.3274)\n",
      "27846 Training Loss: tensor(0.3265)\n",
      "27847 Training Loss: tensor(0.3262)\n",
      "27848 Training Loss: tensor(0.3263)\n",
      "27849 Training Loss: tensor(0.3269)\n",
      "27850 Training Loss: tensor(0.3265)\n",
      "27851 Training Loss: tensor(0.3278)\n",
      "27852 Training Loss: tensor(0.3266)\n",
      "27853 Training Loss: tensor(0.3268)\n",
      "27854 Training Loss: tensor(0.3272)\n",
      "27855 Training Loss: tensor(0.3267)\n",
      "27856 Training Loss: tensor(0.3265)\n",
      "27857 Training Loss: tensor(0.3271)\n",
      "27858 Training Loss: tensor(0.3264)\n",
      "27859 Training Loss: tensor(0.3275)\n",
      "27860 Training Loss: tensor(0.3273)\n",
      "27861 Training Loss: tensor(0.3269)\n",
      "27862 Training Loss: tensor(0.3263)\n",
      "27863 Training Loss: tensor(0.3285)\n",
      "27864 Training Loss: tensor(0.3264)\n",
      "27865 Training Loss: tensor(0.3262)\n",
      "27866 Training Loss: tensor(0.3264)\n",
      "27867 Training Loss: tensor(0.3262)\n",
      "27868 Training Loss: tensor(0.3261)\n",
      "27869 Training Loss: tensor(0.3274)\n",
      "27870 Training Loss: tensor(0.3280)\n",
      "27871 Training Loss: tensor(0.3267)\n",
      "27872 Training Loss: tensor(0.3263)\n",
      "27873 Training Loss: tensor(0.3271)\n",
      "27874 Training Loss: tensor(0.3262)\n",
      "27875 Training Loss: tensor(0.3264)\n",
      "27876 Training Loss: tensor(0.3264)\n",
      "27877 Training Loss: tensor(0.3265)\n",
      "27878 Training Loss: tensor(0.3261)\n",
      "27879 Training Loss: tensor(0.3261)\n",
      "27880 Training Loss: tensor(0.3272)\n",
      "27881 Training Loss: tensor(0.3265)\n",
      "27882 Training Loss: tensor(0.3264)\n",
      "27883 Training Loss: tensor(0.3262)\n",
      "27884 Training Loss: tensor(0.3269)\n",
      "27885 Training Loss: tensor(0.3267)\n",
      "27886 Training Loss: tensor(0.3273)\n",
      "27887 Training Loss: tensor(0.3270)\n",
      "27888 Training Loss: tensor(0.3263)\n",
      "27889 Training Loss: tensor(0.3262)\n",
      "27890 Training Loss: tensor(0.3279)\n",
      "27891 Training Loss: tensor(0.3263)\n",
      "27892 Training Loss: tensor(0.3271)\n",
      "27893 Training Loss: tensor(0.3271)\n",
      "27894 Training Loss: tensor(0.3263)\n",
      "27895 Training Loss: tensor(0.3265)\n",
      "27896 Training Loss: tensor(0.3271)\n",
      "27897 Training Loss: tensor(0.3277)\n",
      "27898 Training Loss: tensor(0.3265)\n",
      "27899 Training Loss: tensor(0.3267)\n",
      "27900 Training Loss: tensor(0.3272)\n",
      "27901 Training Loss: tensor(0.3290)\n",
      "27902 Training Loss: tensor(0.3271)\n",
      "27903 Training Loss: tensor(0.3264)\n",
      "27904 Training Loss: tensor(0.3269)\n",
      "27905 Training Loss: tensor(0.3271)\n",
      "27906 Training Loss: tensor(0.3265)\n",
      "27907 Training Loss: tensor(0.3268)\n",
      "27908 Training Loss: tensor(0.3271)\n",
      "27909 Training Loss: tensor(0.3270)\n",
      "27910 Training Loss: tensor(0.3262)\n",
      "27911 Training Loss: tensor(0.3263)\n",
      "27912 Training Loss: tensor(0.3274)\n",
      "27913 Training Loss: tensor(0.3264)\n",
      "27914 Training Loss: tensor(0.3272)\n",
      "27915 Training Loss: tensor(0.3259)\n",
      "27916 Training Loss: tensor(0.3260)\n",
      "27917 Training Loss: tensor(0.3273)\n",
      "27918 Training Loss: tensor(0.3263)\n",
      "27919 Training Loss: tensor(0.3262)\n",
      "27920 Training Loss: tensor(0.3258)\n",
      "27921 Training Loss: tensor(0.3265)\n",
      "27922 Training Loss: tensor(0.3267)\n",
      "27923 Training Loss: tensor(0.3261)\n",
      "27924 Training Loss: tensor(0.3262)\n",
      "27925 Training Loss: tensor(0.3263)\n",
      "27926 Training Loss: tensor(0.3272)\n",
      "27927 Training Loss: tensor(0.3308)\n",
      "27928 Training Loss: tensor(0.3263)\n",
      "27929 Training Loss: tensor(0.3267)\n",
      "27930 Training Loss: tensor(0.3264)\n",
      "27931 Training Loss: tensor(0.3259)\n",
      "27932 Training Loss: tensor(0.3262)\n",
      "27933 Training Loss: tensor(0.3263)\n",
      "27934 Training Loss: tensor(0.3264)\n",
      "27935 Training Loss: tensor(0.3273)\n",
      "27936 Training Loss: tensor(0.3266)\n",
      "27937 Training Loss: tensor(0.3298)\n",
      "27938 Training Loss: tensor(0.3267)\n",
      "27939 Training Loss: tensor(0.3279)\n",
      "27940 Training Loss: tensor(0.3264)\n",
      "27941 Training Loss: tensor(0.3276)\n",
      "27942 Training Loss: tensor(0.3271)\n",
      "27943 Training Loss: tensor(0.3276)\n",
      "27944 Training Loss: tensor(0.3272)\n",
      "27945 Training Loss: tensor(0.3270)\n",
      "27946 Training Loss: tensor(0.3269)\n",
      "27947 Training Loss: tensor(0.3261)\n",
      "27948 Training Loss: tensor(0.3265)\n",
      "27949 Training Loss: tensor(0.3273)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27950 Training Loss: tensor(0.3259)\n",
      "27951 Training Loss: tensor(0.3260)\n",
      "27952 Training Loss: tensor(0.3279)\n",
      "27953 Training Loss: tensor(0.3267)\n",
      "27954 Training Loss: tensor(0.3265)\n",
      "27955 Training Loss: tensor(0.3267)\n",
      "27956 Training Loss: tensor(0.3281)\n",
      "27957 Training Loss: tensor(0.3267)\n",
      "27958 Training Loss: tensor(0.3264)\n",
      "27959 Training Loss: tensor(0.3267)\n",
      "27960 Training Loss: tensor(0.3267)\n",
      "27961 Training Loss: tensor(0.3262)\n",
      "27962 Training Loss: tensor(0.3270)\n",
      "27963 Training Loss: tensor(0.3272)\n",
      "27964 Training Loss: tensor(0.3263)\n",
      "27965 Training Loss: tensor(0.3265)\n",
      "27966 Training Loss: tensor(0.3262)\n",
      "27967 Training Loss: tensor(0.3264)\n",
      "27968 Training Loss: tensor(0.3266)\n",
      "27969 Training Loss: tensor(0.3267)\n",
      "27970 Training Loss: tensor(0.3285)\n",
      "27971 Training Loss: tensor(0.3270)\n",
      "27972 Training Loss: tensor(0.3268)\n",
      "27973 Training Loss: tensor(0.3266)\n",
      "27974 Training Loss: tensor(0.3268)\n",
      "27975 Training Loss: tensor(0.3266)\n",
      "27976 Training Loss: tensor(0.3265)\n",
      "27977 Training Loss: tensor(0.3263)\n",
      "27978 Training Loss: tensor(0.3261)\n",
      "27979 Training Loss: tensor(0.3263)\n",
      "27980 Training Loss: tensor(0.3264)\n",
      "27981 Training Loss: tensor(0.3266)\n",
      "27982 Training Loss: tensor(0.3268)\n",
      "27983 Training Loss: tensor(0.3264)\n",
      "27984 Training Loss: tensor(0.3273)\n",
      "27985 Training Loss: tensor(0.3262)\n",
      "27986 Training Loss: tensor(0.3263)\n",
      "27987 Training Loss: tensor(0.3261)\n",
      "27988 Training Loss: tensor(0.3266)\n",
      "27989 Training Loss: tensor(0.3270)\n",
      "27990 Training Loss: tensor(0.3262)\n",
      "27991 Training Loss: tensor(0.3261)\n",
      "27992 Training Loss: tensor(0.3262)\n",
      "27993 Training Loss: tensor(0.3265)\n",
      "27994 Training Loss: tensor(0.3275)\n",
      "27995 Training Loss: tensor(0.3265)\n",
      "27996 Training Loss: tensor(0.3259)\n",
      "27997 Training Loss: tensor(0.3259)\n",
      "27998 Training Loss: tensor(0.3265)\n",
      "27999 Training Loss: tensor(0.3277)\n",
      "28000 Training Loss: tensor(0.3270)\n",
      "28001 Training Loss: tensor(0.3263)\n",
      "28002 Training Loss: tensor(0.3273)\n",
      "28003 Training Loss: tensor(0.3260)\n",
      "28004 Training Loss: tensor(0.3262)\n",
      "28005 Training Loss: tensor(0.3260)\n",
      "28006 Training Loss: tensor(0.3261)\n",
      "28007 Training Loss: tensor(0.3272)\n",
      "28008 Training Loss: tensor(0.3263)\n",
      "28009 Training Loss: tensor(0.3271)\n",
      "28010 Training Loss: tensor(0.3271)\n",
      "28011 Training Loss: tensor(0.3263)\n",
      "28012 Training Loss: tensor(0.3260)\n",
      "28013 Training Loss: tensor(0.3264)\n",
      "28014 Training Loss: tensor(0.3270)\n",
      "28015 Training Loss: tensor(0.3262)\n",
      "28016 Training Loss: tensor(0.3262)\n",
      "28017 Training Loss: tensor(0.3263)\n",
      "28018 Training Loss: tensor(0.3261)\n",
      "28019 Training Loss: tensor(0.3267)\n",
      "28020 Training Loss: tensor(0.3261)\n",
      "28021 Training Loss: tensor(0.3270)\n",
      "28022 Training Loss: tensor(0.3260)\n",
      "28023 Training Loss: tensor(0.3263)\n",
      "28024 Training Loss: tensor(0.3271)\n",
      "28025 Training Loss: tensor(0.3264)\n",
      "28026 Training Loss: tensor(0.3261)\n",
      "28027 Training Loss: tensor(0.3269)\n",
      "28028 Training Loss: tensor(0.3263)\n",
      "28029 Training Loss: tensor(0.3262)\n",
      "28030 Training Loss: tensor(0.3268)\n",
      "28031 Training Loss: tensor(0.3270)\n",
      "28032 Training Loss: tensor(0.3261)\n",
      "28033 Training Loss: tensor(0.3272)\n",
      "28034 Training Loss: tensor(0.3261)\n",
      "28035 Training Loss: tensor(0.3259)\n",
      "28036 Training Loss: tensor(0.3264)\n",
      "28037 Training Loss: tensor(0.3271)\n",
      "28038 Training Loss: tensor(0.3262)\n",
      "28039 Training Loss: tensor(0.3262)\n",
      "28040 Training Loss: tensor(0.3278)\n",
      "28041 Training Loss: tensor(0.3263)\n",
      "28042 Training Loss: tensor(0.3263)\n",
      "28043 Training Loss: tensor(0.3264)\n",
      "28044 Training Loss: tensor(0.3267)\n",
      "28045 Training Loss: tensor(0.3296)\n",
      "28046 Training Loss: tensor(0.3262)\n",
      "28047 Training Loss: tensor(0.3271)\n",
      "28048 Training Loss: tensor(0.3262)\n",
      "28049 Training Loss: tensor(0.3274)\n",
      "28050 Training Loss: tensor(0.3269)\n",
      "28051 Training Loss: tensor(0.3264)\n",
      "28052 Training Loss: tensor(0.3266)\n",
      "28053 Training Loss: tensor(0.3277)\n",
      "28054 Training Loss: tensor(0.3263)\n",
      "28055 Training Loss: tensor(0.3264)\n",
      "28056 Training Loss: tensor(0.3267)\n",
      "28057 Training Loss: tensor(0.3263)\n",
      "28058 Training Loss: tensor(0.3267)\n",
      "28059 Training Loss: tensor(0.3273)\n",
      "28060 Training Loss: tensor(0.3263)\n",
      "28061 Training Loss: tensor(0.3264)\n",
      "28062 Training Loss: tensor(0.3274)\n",
      "28063 Training Loss: tensor(0.3266)\n",
      "28064 Training Loss: tensor(0.3261)\n",
      "28065 Training Loss: tensor(0.3263)\n",
      "28066 Training Loss: tensor(0.3262)\n",
      "28067 Training Loss: tensor(0.3278)\n",
      "28068 Training Loss: tensor(0.3283)\n",
      "28069 Training Loss: tensor(0.3265)\n",
      "28070 Training Loss: tensor(0.3263)\n",
      "28071 Training Loss: tensor(0.3260)\n",
      "28072 Training Loss: tensor(0.3263)\n",
      "28073 Training Loss: tensor(0.3271)\n",
      "28074 Training Loss: tensor(0.3263)\n",
      "28075 Training Loss: tensor(0.3278)\n",
      "28076 Training Loss: tensor(0.3295)\n",
      "28077 Training Loss: tensor(0.3270)\n",
      "28078 Training Loss: tensor(0.3278)\n",
      "28079 Training Loss: tensor(0.3267)\n",
      "28080 Training Loss: tensor(0.3283)\n",
      "28081 Training Loss: tensor(0.3263)\n",
      "28082 Training Loss: tensor(0.3267)\n",
      "28083 Training Loss: tensor(0.3270)\n",
      "28084 Training Loss: tensor(0.3266)\n",
      "28085 Training Loss: tensor(0.3285)\n",
      "28086 Training Loss: tensor(0.3268)\n",
      "28087 Training Loss: tensor(0.3279)\n",
      "28088 Training Loss: tensor(0.3263)\n",
      "28089 Training Loss: tensor(0.3265)\n",
      "28090 Training Loss: tensor(0.3268)\n",
      "28091 Training Loss: tensor(0.3265)\n",
      "28092 Training Loss: tensor(0.3266)\n",
      "28093 Training Loss: tensor(0.3274)\n",
      "28094 Training Loss: tensor(0.3265)\n",
      "28095 Training Loss: tensor(0.3262)\n",
      "28096 Training Loss: tensor(0.3261)\n",
      "28097 Training Loss: tensor(0.3275)\n",
      "28098 Training Loss: tensor(0.3281)\n",
      "28099 Training Loss: tensor(0.3264)\n",
      "28100 Training Loss: tensor(0.3264)\n",
      "28101 Training Loss: tensor(0.3263)\n",
      "28102 Training Loss: tensor(0.3282)\n",
      "28103 Training Loss: tensor(0.3259)\n",
      "28104 Training Loss: tensor(0.3263)\n",
      "28105 Training Loss: tensor(0.3260)\n",
      "28106 Training Loss: tensor(0.3269)\n",
      "28107 Training Loss: tensor(0.3283)\n",
      "28108 Training Loss: tensor(0.3265)\n",
      "28109 Training Loss: tensor(0.3265)\n",
      "28110 Training Loss: tensor(0.3266)\n",
      "28111 Training Loss: tensor(0.3279)\n",
      "28112 Training Loss: tensor(0.3263)\n",
      "28113 Training Loss: tensor(0.3271)\n",
      "28114 Training Loss: tensor(0.3265)\n",
      "28115 Training Loss: tensor(0.3261)\n",
      "28116 Training Loss: tensor(0.3267)\n",
      "28117 Training Loss: tensor(0.3265)\n",
      "28118 Training Loss: tensor(0.3263)\n",
      "28119 Training Loss: tensor(0.3268)\n",
      "28120 Training Loss: tensor(0.3275)\n",
      "28121 Training Loss: tensor(0.3262)\n",
      "28122 Training Loss: tensor(0.3263)\n",
      "28123 Training Loss: tensor(0.3262)\n",
      "28124 Training Loss: tensor(0.3267)\n",
      "28125 Training Loss: tensor(0.3269)\n",
      "28126 Training Loss: tensor(0.3263)\n",
      "28127 Training Loss: tensor(0.3260)\n",
      "28128 Training Loss: tensor(0.3263)\n",
      "28129 Training Loss: tensor(0.3276)\n",
      "28130 Training Loss: tensor(0.3259)\n",
      "28131 Training Loss: tensor(0.3279)\n",
      "28132 Training Loss: tensor(0.3272)\n",
      "28133 Training Loss: tensor(0.3263)\n",
      "28134 Training Loss: tensor(0.3269)\n",
      "28135 Training Loss: tensor(0.3270)\n",
      "28136 Training Loss: tensor(0.3273)\n",
      "28137 Training Loss: tensor(0.3269)\n",
      "28138 Training Loss: tensor(0.3263)\n",
      "28139 Training Loss: tensor(0.3265)\n",
      "28140 Training Loss: tensor(0.3266)\n",
      "28141 Training Loss: tensor(0.3264)\n",
      "28142 Training Loss: tensor(0.3269)\n",
      "28143 Training Loss: tensor(0.3261)\n",
      "28144 Training Loss: tensor(0.3264)\n",
      "28145 Training Loss: tensor(0.3264)\n",
      "28146 Training Loss: tensor(0.3313)\n",
      "28147 Training Loss: tensor(0.3262)\n",
      "28148 Training Loss: tensor(0.3278)\n",
      "28149 Training Loss: tensor(0.3268)\n",
      "28150 Training Loss: tensor(0.3268)\n",
      "28151 Training Loss: tensor(0.3259)\n",
      "28152 Training Loss: tensor(0.3262)\n",
      "28153 Training Loss: tensor(0.3267)\n",
      "28154 Training Loss: tensor(0.3271)\n",
      "28155 Training Loss: tensor(0.3265)\n",
      "28156 Training Loss: tensor(0.3265)\n",
      "28157 Training Loss: tensor(0.3266)\n",
      "28158 Training Loss: tensor(0.3266)\n",
      "28159 Training Loss: tensor(0.3265)\n",
      "28160 Training Loss: tensor(0.3263)\n",
      "28161 Training Loss: tensor(0.3265)\n",
      "28162 Training Loss: tensor(0.3301)\n",
      "28163 Training Loss: tensor(0.3260)\n",
      "28164 Training Loss: tensor(0.3265)\n",
      "28165 Training Loss: tensor(0.3268)\n",
      "28166 Training Loss: tensor(0.3261)\n",
      "28167 Training Loss: tensor(0.3285)\n",
      "28168 Training Loss: tensor(0.3261)\n",
      "28169 Training Loss: tensor(0.3269)\n",
      "28170 Training Loss: tensor(0.3269)\n",
      "28171 Training Loss: tensor(0.3266)\n",
      "28172 Training Loss: tensor(0.3268)\n",
      "28173 Training Loss: tensor(0.3271)\n",
      "28174 Training Loss: tensor(0.3263)\n",
      "28175 Training Loss: tensor(0.3262)\n",
      "28176 Training Loss: tensor(0.3264)\n",
      "28177 Training Loss: tensor(0.3264)\n",
      "28178 Training Loss: tensor(0.3263)\n",
      "28179 Training Loss: tensor(0.3263)\n",
      "28180 Training Loss: tensor(0.3283)\n",
      "28181 Training Loss: tensor(0.3260)\n",
      "28182 Training Loss: tensor(0.3271)\n",
      "28183 Training Loss: tensor(0.3262)\n",
      "28184 Training Loss: tensor(0.3276)\n",
      "28185 Training Loss: tensor(0.3267)\n",
      "28186 Training Loss: tensor(0.3268)\n",
      "28187 Training Loss: tensor(0.3260)\n",
      "28188 Training Loss: tensor(0.3262)\n",
      "28189 Training Loss: tensor(0.3272)\n",
      "28190 Training Loss: tensor(0.3267)\n",
      "28191 Training Loss: tensor(0.3266)\n",
      "28192 Training Loss: tensor(0.3267)\n",
      "28193 Training Loss: tensor(0.3270)\n",
      "28194 Training Loss: tensor(0.3265)\n",
      "28195 Training Loss: tensor(0.3272)\n",
      "28196 Training Loss: tensor(0.3260)\n",
      "28197 Training Loss: tensor(0.3265)\n",
      "28198 Training Loss: tensor(0.3259)\n",
      "28199 Training Loss: tensor(0.3275)\n",
      "28200 Training Loss: tensor(0.3283)\n",
      "28201 Training Loss: tensor(0.3264)\n",
      "28202 Training Loss: tensor(0.3261)\n",
      "28203 Training Loss: tensor(0.3263)\n",
      "28204 Training Loss: tensor(0.3291)\n",
      "28205 Training Loss: tensor(0.3261)\n",
      "28206 Training Loss: tensor(0.3295)\n",
      "28207 Training Loss: tensor(0.3277)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28208 Training Loss: tensor(0.3298)\n",
      "28209 Training Loss: tensor(0.3273)\n",
      "28210 Training Loss: tensor(0.3268)\n",
      "28211 Training Loss: tensor(0.3273)\n",
      "28212 Training Loss: tensor(0.3266)\n",
      "28213 Training Loss: tensor(0.3271)\n",
      "28214 Training Loss: tensor(0.3271)\n",
      "28215 Training Loss: tensor(0.3268)\n",
      "28216 Training Loss: tensor(0.3283)\n",
      "28217 Training Loss: tensor(0.3261)\n",
      "28218 Training Loss: tensor(0.3272)\n",
      "28219 Training Loss: tensor(0.3280)\n",
      "28220 Training Loss: tensor(0.3263)\n",
      "28221 Training Loss: tensor(0.3266)\n",
      "28222 Training Loss: tensor(0.3273)\n",
      "28223 Training Loss: tensor(0.3270)\n",
      "28224 Training Loss: tensor(0.3265)\n",
      "28225 Training Loss: tensor(0.3273)\n",
      "28226 Training Loss: tensor(0.3264)\n",
      "28227 Training Loss: tensor(0.3268)\n",
      "28228 Training Loss: tensor(0.3270)\n",
      "28229 Training Loss: tensor(0.3268)\n",
      "28230 Training Loss: tensor(0.3264)\n",
      "28231 Training Loss: tensor(0.3261)\n",
      "28232 Training Loss: tensor(0.3279)\n",
      "28233 Training Loss: tensor(0.3269)\n",
      "28234 Training Loss: tensor(0.3264)\n",
      "28235 Training Loss: tensor(0.3261)\n",
      "28236 Training Loss: tensor(0.3290)\n",
      "28237 Training Loss: tensor(0.3267)\n",
      "28238 Training Loss: tensor(0.3264)\n",
      "28239 Training Loss: tensor(0.3262)\n",
      "28240 Training Loss: tensor(0.3265)\n",
      "28241 Training Loss: tensor(0.3263)\n",
      "28242 Training Loss: tensor(0.3263)\n",
      "28243 Training Loss: tensor(0.3277)\n",
      "28244 Training Loss: tensor(0.3263)\n",
      "28245 Training Loss: tensor(0.3266)\n",
      "28246 Training Loss: tensor(0.3264)\n",
      "28247 Training Loss: tensor(0.3263)\n",
      "28248 Training Loss: tensor(0.3268)\n",
      "28249 Training Loss: tensor(0.3268)\n",
      "28250 Training Loss: tensor(0.3267)\n",
      "28251 Training Loss: tensor(0.3260)\n",
      "28252 Training Loss: tensor(0.3259)\n",
      "28253 Training Loss: tensor(0.3269)\n",
      "28254 Training Loss: tensor(0.3268)\n",
      "28255 Training Loss: tensor(0.3267)\n",
      "28256 Training Loss: tensor(0.3260)\n",
      "28257 Training Loss: tensor(0.3261)\n",
      "28258 Training Loss: tensor(0.3264)\n",
      "28259 Training Loss: tensor(0.3264)\n",
      "28260 Training Loss: tensor(0.3269)\n",
      "28261 Training Loss: tensor(0.3268)\n",
      "28262 Training Loss: tensor(0.3268)\n",
      "28263 Training Loss: tensor(0.3270)\n",
      "28264 Training Loss: tensor(0.3259)\n",
      "28265 Training Loss: tensor(0.3260)\n",
      "28266 Training Loss: tensor(0.3265)\n",
      "28267 Training Loss: tensor(0.3267)\n",
      "28268 Training Loss: tensor(0.3266)\n",
      "28269 Training Loss: tensor(0.3260)\n",
      "28270 Training Loss: tensor(0.3275)\n",
      "28271 Training Loss: tensor(0.3270)\n",
      "28272 Training Loss: tensor(0.3260)\n",
      "28273 Training Loss: tensor(0.3264)\n",
      "28274 Training Loss: tensor(0.3259)\n",
      "28275 Training Loss: tensor(0.3261)\n",
      "28276 Training Loss: tensor(0.3261)\n",
      "28277 Training Loss: tensor(0.3262)\n",
      "28278 Training Loss: tensor(0.3262)\n",
      "28279 Training Loss: tensor(0.3266)\n",
      "28280 Training Loss: tensor(0.3270)\n",
      "28281 Training Loss: tensor(0.3275)\n",
      "28282 Training Loss: tensor(0.3281)\n",
      "28283 Training Loss: tensor(0.3262)\n",
      "28284 Training Loss: tensor(0.3267)\n",
      "28285 Training Loss: tensor(0.3271)\n",
      "28286 Training Loss: tensor(0.3264)\n",
      "28287 Training Loss: tensor(0.3269)\n",
      "28288 Training Loss: tensor(0.3266)\n",
      "28289 Training Loss: tensor(0.3270)\n",
      "28290 Training Loss: tensor(0.3273)\n",
      "28291 Training Loss: tensor(0.3259)\n",
      "28292 Training Loss: tensor(0.3269)\n",
      "28293 Training Loss: tensor(0.3263)\n",
      "28294 Training Loss: tensor(0.3269)\n",
      "28295 Training Loss: tensor(0.3268)\n",
      "28296 Training Loss: tensor(0.3260)\n",
      "28297 Training Loss: tensor(0.3275)\n",
      "28298 Training Loss: tensor(0.3284)\n",
      "28299 Training Loss: tensor(0.3277)\n",
      "28300 Training Loss: tensor(0.3279)\n",
      "28301 Training Loss: tensor(0.3270)\n",
      "28302 Training Loss: tensor(0.3269)\n",
      "28303 Training Loss: tensor(0.3264)\n",
      "28304 Training Loss: tensor(0.3266)\n",
      "28305 Training Loss: tensor(0.3292)\n",
      "28306 Training Loss: tensor(0.3274)\n",
      "28307 Training Loss: tensor(0.3269)\n",
      "28308 Training Loss: tensor(0.3263)\n",
      "28309 Training Loss: tensor(0.3267)\n",
      "28310 Training Loss: tensor(0.3263)\n",
      "28311 Training Loss: tensor(0.3265)\n",
      "28312 Training Loss: tensor(0.3261)\n",
      "28313 Training Loss: tensor(0.3265)\n",
      "28314 Training Loss: tensor(0.3264)\n",
      "28315 Training Loss: tensor(0.3261)\n",
      "28316 Training Loss: tensor(0.3272)\n",
      "28317 Training Loss: tensor(0.3267)\n",
      "28318 Training Loss: tensor(0.3262)\n",
      "28319 Training Loss: tensor(0.3285)\n",
      "28320 Training Loss: tensor(0.3261)\n",
      "28321 Training Loss: tensor(0.3265)\n",
      "28322 Training Loss: tensor(0.3265)\n",
      "28323 Training Loss: tensor(0.3262)\n",
      "28324 Training Loss: tensor(0.3263)\n",
      "28325 Training Loss: tensor(0.3262)\n",
      "28326 Training Loss: tensor(0.3273)\n",
      "28327 Training Loss: tensor(0.3262)\n",
      "28328 Training Loss: tensor(0.3264)\n",
      "28329 Training Loss: tensor(0.3261)\n",
      "28330 Training Loss: tensor(0.3262)\n",
      "28331 Training Loss: tensor(0.3261)\n",
      "28332 Training Loss: tensor(0.3267)\n",
      "28333 Training Loss: tensor(0.3265)\n",
      "28334 Training Loss: tensor(0.3266)\n",
      "28335 Training Loss: tensor(0.3261)\n",
      "28336 Training Loss: tensor(0.3264)\n",
      "28337 Training Loss: tensor(0.3262)\n",
      "28338 Training Loss: tensor(0.3264)\n",
      "28339 Training Loss: tensor(0.3264)\n",
      "28340 Training Loss: tensor(0.3267)\n",
      "28341 Training Loss: tensor(0.3260)\n",
      "28342 Training Loss: tensor(0.3287)\n",
      "28343 Training Loss: tensor(0.3260)\n",
      "28344 Training Loss: tensor(0.3266)\n",
      "28345 Training Loss: tensor(0.3259)\n",
      "28346 Training Loss: tensor(0.3266)\n",
      "28347 Training Loss: tensor(0.3262)\n",
      "28348 Training Loss: tensor(0.3266)\n",
      "28349 Training Loss: tensor(0.3263)\n",
      "28350 Training Loss: tensor(0.3285)\n",
      "28351 Training Loss: tensor(0.3268)\n",
      "28352 Training Loss: tensor(0.3268)\n",
      "28353 Training Loss: tensor(0.3267)\n",
      "28354 Training Loss: tensor(0.3273)\n",
      "28355 Training Loss: tensor(0.3260)\n",
      "28356 Training Loss: tensor(0.3270)\n",
      "28357 Training Loss: tensor(0.3266)\n",
      "28358 Training Loss: tensor(0.3282)\n",
      "28359 Training Loss: tensor(0.3261)\n",
      "28360 Training Loss: tensor(0.3262)\n",
      "28361 Training Loss: tensor(0.3264)\n",
      "28362 Training Loss: tensor(0.3274)\n",
      "28363 Training Loss: tensor(0.3265)\n",
      "28364 Training Loss: tensor(0.3267)\n",
      "28365 Training Loss: tensor(0.3263)\n",
      "28366 Training Loss: tensor(0.3266)\n",
      "28367 Training Loss: tensor(0.3263)\n",
      "28368 Training Loss: tensor(0.3263)\n",
      "28369 Training Loss: tensor(0.3260)\n",
      "28370 Training Loss: tensor(0.3263)\n",
      "28371 Training Loss: tensor(0.3261)\n",
      "28372 Training Loss: tensor(0.3269)\n",
      "28373 Training Loss: tensor(0.3277)\n",
      "28374 Training Loss: tensor(0.3264)\n",
      "28375 Training Loss: tensor(0.3265)\n",
      "28376 Training Loss: tensor(0.3295)\n",
      "28377 Training Loss: tensor(0.3260)\n",
      "28378 Training Loss: tensor(0.3269)\n",
      "28379 Training Loss: tensor(0.3265)\n",
      "28380 Training Loss: tensor(0.3263)\n",
      "28381 Training Loss: tensor(0.3260)\n",
      "28382 Training Loss: tensor(0.3261)\n",
      "28383 Training Loss: tensor(0.3273)\n",
      "28384 Training Loss: tensor(0.3279)\n",
      "28385 Training Loss: tensor(0.3259)\n",
      "28386 Training Loss: tensor(0.3263)\n",
      "28387 Training Loss: tensor(0.3263)\n",
      "28388 Training Loss: tensor(0.3261)\n",
      "28389 Training Loss: tensor(0.3266)\n",
      "28390 Training Loss: tensor(0.3263)\n",
      "28391 Training Loss: tensor(0.3269)\n",
      "28392 Training Loss: tensor(0.3261)\n",
      "28393 Training Loss: tensor(0.3261)\n",
      "28394 Training Loss: tensor(0.3259)\n",
      "28395 Training Loss: tensor(0.3261)\n",
      "28396 Training Loss: tensor(0.3268)\n",
      "28397 Training Loss: tensor(0.3262)\n",
      "28398 Training Loss: tensor(0.3257)\n",
      "28399 Training Loss: tensor(0.3300)\n",
      "28400 Training Loss: tensor(0.3280)\n",
      "28401 Training Loss: tensor(0.3259)\n",
      "28402 Training Loss: tensor(0.3263)\n",
      "28403 Training Loss: tensor(0.3287)\n",
      "28404 Training Loss: tensor(0.3292)\n",
      "28405 Training Loss: tensor(0.3270)\n",
      "28406 Training Loss: tensor(0.3263)\n",
      "28407 Training Loss: tensor(0.3283)\n",
      "28408 Training Loss: tensor(0.3263)\n",
      "28409 Training Loss: tensor(0.3272)\n",
      "28410 Training Loss: tensor(0.3281)\n",
      "28411 Training Loss: tensor(0.3265)\n",
      "28412 Training Loss: tensor(0.3267)\n",
      "28413 Training Loss: tensor(0.3278)\n",
      "28414 Training Loss: tensor(0.3271)\n",
      "28415 Training Loss: tensor(0.3262)\n",
      "28416 Training Loss: tensor(0.3281)\n",
      "28417 Training Loss: tensor(0.3280)\n",
      "28418 Training Loss: tensor(0.3267)\n",
      "28419 Training Loss: tensor(0.3266)\n",
      "28420 Training Loss: tensor(0.3263)\n",
      "28421 Training Loss: tensor(0.3274)\n",
      "28422 Training Loss: tensor(0.3273)\n",
      "28423 Training Loss: tensor(0.3263)\n",
      "28424 Training Loss: tensor(0.3270)\n",
      "28425 Training Loss: tensor(0.3269)\n",
      "28426 Training Loss: tensor(0.3272)\n",
      "28427 Training Loss: tensor(0.3272)\n",
      "28428 Training Loss: tensor(0.3266)\n",
      "28429 Training Loss: tensor(0.3264)\n",
      "28430 Training Loss: tensor(0.3266)\n",
      "28431 Training Loss: tensor(0.3264)\n",
      "28432 Training Loss: tensor(0.3264)\n",
      "28433 Training Loss: tensor(0.3262)\n",
      "28434 Training Loss: tensor(0.3264)\n",
      "28435 Training Loss: tensor(0.3261)\n",
      "28436 Training Loss: tensor(0.3262)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28437 Training Loss: tensor(0.3266)\n",
      "28438 Training Loss: tensor(0.3260)\n",
      "28439 Training Loss: tensor(0.3276)\n",
      "28440 Training Loss: tensor(0.3289)\n",
      "28441 Training Loss: tensor(0.3270)\n",
      "28442 Training Loss: tensor(0.3266)\n",
      "28443 Training Loss: tensor(0.3287)\n",
      "28444 Training Loss: tensor(0.3265)\n",
      "28445 Training Loss: tensor(0.3271)\n",
      "28446 Training Loss: tensor(0.3266)\n",
      "28447 Training Loss: tensor(0.3264)\n",
      "28448 Training Loss: tensor(0.3275)\n",
      "28449 Training Loss: tensor(0.3261)\n",
      "28450 Training Loss: tensor(0.3274)\n",
      "28451 Training Loss: tensor(0.3270)\n",
      "28452 Training Loss: tensor(0.3267)\n",
      "28453 Training Loss: tensor(0.3264)\n",
      "28454 Training Loss: tensor(0.3273)\n",
      "28455 Training Loss: tensor(0.3262)\n",
      "28456 Training Loss: tensor(0.3263)\n",
      "28457 Training Loss: tensor(0.3264)\n",
      "28458 Training Loss: tensor(0.3262)\n",
      "28459 Training Loss: tensor(0.3265)\n",
      "28460 Training Loss: tensor(0.3270)\n",
      "28461 Training Loss: tensor(0.3271)\n",
      "28462 Training Loss: tensor(0.3264)\n",
      "28463 Training Loss: tensor(0.3260)\n",
      "28464 Training Loss: tensor(0.3285)\n",
      "28465 Training Loss: tensor(0.3275)\n",
      "28466 Training Loss: tensor(0.3261)\n",
      "28467 Training Loss: tensor(0.3261)\n",
      "28468 Training Loss: tensor(0.3262)\n",
      "28469 Training Loss: tensor(0.3268)\n",
      "28470 Training Loss: tensor(0.3262)\n",
      "28471 Training Loss: tensor(0.3263)\n",
      "28472 Training Loss: tensor(0.3262)\n",
      "28473 Training Loss: tensor(0.3262)\n",
      "28474 Training Loss: tensor(0.3263)\n",
      "28475 Training Loss: tensor(0.3263)\n",
      "28476 Training Loss: tensor(0.3261)\n",
      "28477 Training Loss: tensor(0.3261)\n",
      "28478 Training Loss: tensor(0.3261)\n",
      "28479 Training Loss: tensor(0.3277)\n",
      "28480 Training Loss: tensor(0.3262)\n",
      "28481 Training Loss: tensor(0.3262)\n",
      "28482 Training Loss: tensor(0.3261)\n",
      "28483 Training Loss: tensor(0.3271)\n",
      "28484 Training Loss: tensor(0.3259)\n",
      "28485 Training Loss: tensor(0.3264)\n",
      "28486 Training Loss: tensor(0.3264)\n",
      "28487 Training Loss: tensor(0.3260)\n",
      "28488 Training Loss: tensor(0.3271)\n",
      "28489 Training Loss: tensor(0.3262)\n",
      "28490 Training Loss: tensor(0.3261)\n",
      "28491 Training Loss: tensor(0.3258)\n",
      "28492 Training Loss: tensor(0.3261)\n",
      "28493 Training Loss: tensor(0.3273)\n",
      "28494 Training Loss: tensor(0.3275)\n",
      "28495 Training Loss: tensor(0.3259)\n",
      "28496 Training Loss: tensor(0.3260)\n",
      "28497 Training Loss: tensor(0.3279)\n",
      "28498 Training Loss: tensor(0.3278)\n",
      "28499 Training Loss: tensor(0.3268)\n",
      "28500 Training Loss: tensor(0.3262)\n",
      "28501 Training Loss: tensor(0.3268)\n",
      "28502 Training Loss: tensor(0.3265)\n",
      "28503 Training Loss: tensor(0.3262)\n",
      "28504 Training Loss: tensor(0.3260)\n",
      "28505 Training Loss: tensor(0.3263)\n",
      "28506 Training Loss: tensor(0.3262)\n",
      "28507 Training Loss: tensor(0.3271)\n",
      "28508 Training Loss: tensor(0.3260)\n",
      "28509 Training Loss: tensor(0.3297)\n",
      "28510 Training Loss: tensor(0.3263)\n",
      "28511 Training Loss: tensor(0.3272)\n",
      "28512 Training Loss: tensor(0.3274)\n",
      "28513 Training Loss: tensor(0.3264)\n",
      "28514 Training Loss: tensor(0.3262)\n",
      "28515 Training Loss: tensor(0.3264)\n",
      "28516 Training Loss: tensor(0.3264)\n",
      "28517 Training Loss: tensor(0.3262)\n",
      "28518 Training Loss: tensor(0.3275)\n",
      "28519 Training Loss: tensor(0.3266)\n",
      "28520 Training Loss: tensor(0.3263)\n",
      "28521 Training Loss: tensor(0.3262)\n",
      "28522 Training Loss: tensor(0.3264)\n",
      "28523 Training Loss: tensor(0.3261)\n",
      "28524 Training Loss: tensor(0.3276)\n",
      "28525 Training Loss: tensor(0.3268)\n",
      "28526 Training Loss: tensor(0.3263)\n",
      "28527 Training Loss: tensor(0.3269)\n",
      "28528 Training Loss: tensor(0.3284)\n",
      "28529 Training Loss: tensor(0.3266)\n",
      "28530 Training Loss: tensor(0.3263)\n",
      "28531 Training Loss: tensor(0.3263)\n",
      "28532 Training Loss: tensor(0.3259)\n",
      "28533 Training Loss: tensor(0.3266)\n",
      "28534 Training Loss: tensor(0.3267)\n",
      "28535 Training Loss: tensor(0.3263)\n",
      "28536 Training Loss: tensor(0.3263)\n",
      "28537 Training Loss: tensor(0.3264)\n",
      "28538 Training Loss: tensor(0.3266)\n",
      "28539 Training Loss: tensor(0.3263)\n",
      "28540 Training Loss: tensor(0.3260)\n",
      "28541 Training Loss: tensor(0.3260)\n",
      "28542 Training Loss: tensor(0.3260)\n",
      "28543 Training Loss: tensor(0.3280)\n",
      "28544 Training Loss: tensor(0.3262)\n",
      "28545 Training Loss: tensor(0.3282)\n",
      "28546 Training Loss: tensor(0.3260)\n",
      "28547 Training Loss: tensor(0.3262)\n",
      "28548 Training Loss: tensor(0.3263)\n",
      "28549 Training Loss: tensor(0.3268)\n",
      "28550 Training Loss: tensor(0.3262)\n",
      "28551 Training Loss: tensor(0.3273)\n",
      "28552 Training Loss: tensor(0.3265)\n",
      "28553 Training Loss: tensor(0.3261)\n",
      "28554 Training Loss: tensor(0.3262)\n",
      "28555 Training Loss: tensor(0.3263)\n",
      "28556 Training Loss: tensor(0.3268)\n",
      "28557 Training Loss: tensor(0.3263)\n",
      "28558 Training Loss: tensor(0.3263)\n",
      "28559 Training Loss: tensor(0.3265)\n",
      "28560 Training Loss: tensor(0.3263)\n",
      "28561 Training Loss: tensor(0.3263)\n",
      "28562 Training Loss: tensor(0.3260)\n",
      "28563 Training Loss: tensor(0.3279)\n",
      "28564 Training Loss: tensor(0.3269)\n",
      "28565 Training Loss: tensor(0.3270)\n",
      "28566 Training Loss: tensor(0.3264)\n",
      "28567 Training Loss: tensor(0.3261)\n",
      "28568 Training Loss: tensor(0.3260)\n",
      "28569 Training Loss: tensor(0.3272)\n",
      "28570 Training Loss: tensor(0.3260)\n",
      "28571 Training Loss: tensor(0.3283)\n",
      "28572 Training Loss: tensor(0.3267)\n",
      "28573 Training Loss: tensor(0.3259)\n",
      "28574 Training Loss: tensor(0.3259)\n",
      "28575 Training Loss: tensor(0.3268)\n",
      "28576 Training Loss: tensor(0.3260)\n",
      "28577 Training Loss: tensor(0.3262)\n",
      "28578 Training Loss: tensor(0.3261)\n",
      "28579 Training Loss: tensor(0.3266)\n",
      "28580 Training Loss: tensor(0.3260)\n",
      "28581 Training Loss: tensor(0.3263)\n",
      "28582 Training Loss: tensor(0.3265)\n",
      "28583 Training Loss: tensor(0.3263)\n",
      "28584 Training Loss: tensor(0.3280)\n",
      "28585 Training Loss: tensor(0.3261)\n",
      "28586 Training Loss: tensor(0.3262)\n",
      "28587 Training Loss: tensor(0.3259)\n",
      "28588 Training Loss: tensor(0.3260)\n",
      "28589 Training Loss: tensor(0.3288)\n",
      "28590 Training Loss: tensor(0.3274)\n",
      "28591 Training Loss: tensor(0.3274)\n",
      "28592 Training Loss: tensor(0.3265)\n",
      "28593 Training Loss: tensor(0.3272)\n",
      "28594 Training Loss: tensor(0.3262)\n",
      "28595 Training Loss: tensor(0.3263)\n",
      "28596 Training Loss: tensor(0.3264)\n",
      "28597 Training Loss: tensor(0.3265)\n",
      "28598 Training Loss: tensor(0.3258)\n",
      "28599 Training Loss: tensor(0.3260)\n",
      "28600 Training Loss: tensor(0.3265)\n",
      "28601 Training Loss: tensor(0.3265)\n",
      "28602 Training Loss: tensor(0.3280)\n",
      "28603 Training Loss: tensor(0.3267)\n",
      "28604 Training Loss: tensor(0.3258)\n",
      "28605 Training Loss: tensor(0.3259)\n",
      "28606 Training Loss: tensor(0.3261)\n",
      "28607 Training Loss: tensor(0.3265)\n",
      "28608 Training Loss: tensor(0.3259)\n",
      "28609 Training Loss: tensor(0.3271)\n",
      "28610 Training Loss: tensor(0.3261)\n",
      "28611 Training Loss: tensor(0.3261)\n",
      "28612 Training Loss: tensor(0.3261)\n",
      "28613 Training Loss: tensor(0.3271)\n",
      "28614 Training Loss: tensor(0.3259)\n",
      "28615 Training Loss: tensor(0.3260)\n",
      "28616 Training Loss: tensor(0.3269)\n",
      "28617 Training Loss: tensor(0.3260)\n",
      "28618 Training Loss: tensor(0.3263)\n",
      "28619 Training Loss: tensor(0.3259)\n",
      "28620 Training Loss: tensor(0.3265)\n",
      "28621 Training Loss: tensor(0.3264)\n",
      "28622 Training Loss: tensor(0.3266)\n",
      "28623 Training Loss: tensor(0.3260)\n",
      "28624 Training Loss: tensor(0.3260)\n",
      "28625 Training Loss: tensor(0.3285)\n",
      "28626 Training Loss: tensor(0.3293)\n",
      "28627 Training Loss: tensor(0.3260)\n",
      "28628 Training Loss: tensor(0.3263)\n",
      "28629 Training Loss: tensor(0.3263)\n",
      "28630 Training Loss: tensor(0.3259)\n",
      "28631 Training Loss: tensor(0.3270)\n",
      "28632 Training Loss: tensor(0.3261)\n",
      "28633 Training Loss: tensor(0.3260)\n",
      "28634 Training Loss: tensor(0.3262)\n",
      "28635 Training Loss: tensor(0.3288)\n",
      "28636 Training Loss: tensor(0.3274)\n",
      "28637 Training Loss: tensor(0.3267)\n",
      "28638 Training Loss: tensor(0.3262)\n",
      "28639 Training Loss: tensor(0.3260)\n",
      "28640 Training Loss: tensor(0.3271)\n",
      "28641 Training Loss: tensor(0.3276)\n",
      "28642 Training Loss: tensor(0.3267)\n",
      "28643 Training Loss: tensor(0.3262)\n",
      "28644 Training Loss: tensor(0.3260)\n",
      "28645 Training Loss: tensor(0.3259)\n",
      "28646 Training Loss: tensor(0.3264)\n",
      "28647 Training Loss: tensor(0.3259)\n",
      "28648 Training Loss: tensor(0.3263)\n",
      "28649 Training Loss: tensor(0.3260)\n",
      "28650 Training Loss: tensor(0.3259)\n",
      "28651 Training Loss: tensor(0.3265)\n",
      "28652 Training Loss: tensor(0.3261)\n",
      "28653 Training Loss: tensor(0.3265)\n",
      "28654 Training Loss: tensor(0.3266)\n",
      "28655 Training Loss: tensor(0.3257)\n",
      "28656 Training Loss: tensor(0.3300)\n",
      "28657 Training Loss: tensor(0.3262)\n",
      "28658 Training Loss: tensor(0.3274)\n",
      "28659 Training Loss: tensor(0.3259)\n",
      "28660 Training Loss: tensor(0.3284)\n",
      "28661 Training Loss: tensor(0.3266)\n",
      "28662 Training Loss: tensor(0.3264)\n",
      "28663 Training Loss: tensor(0.3275)\n",
      "28664 Training Loss: tensor(0.3266)\n",
      "28665 Training Loss: tensor(0.3267)\n",
      "28666 Training Loss: tensor(0.3264)\n",
      "28667 Training Loss: tensor(0.3295)\n",
      "28668 Training Loss: tensor(0.3269)\n",
      "28669 Training Loss: tensor(0.3266)\n",
      "28670 Training Loss: tensor(0.3265)\n",
      "28671 Training Loss: tensor(0.3265)\n",
      "28672 Training Loss: tensor(0.3267)\n",
      "28673 Training Loss: tensor(0.3264)\n",
      "28674 Training Loss: tensor(0.3260)\n",
      "28675 Training Loss: tensor(0.3265)\n",
      "28676 Training Loss: tensor(0.3268)\n",
      "28677 Training Loss: tensor(0.3262)\n",
      "28678 Training Loss: tensor(0.3270)\n",
      "28679 Training Loss: tensor(0.3262)\n",
      "28680 Training Loss: tensor(0.3265)\n",
      "28681 Training Loss: tensor(0.3258)\n",
      "28682 Training Loss: tensor(0.3261)\n",
      "28683 Training Loss: tensor(0.3265)\n",
      "28684 Training Loss: tensor(0.3272)\n",
      "28685 Training Loss: tensor(0.3262)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28686 Training Loss: tensor(0.3266)\n",
      "28687 Training Loss: tensor(0.3285)\n",
      "28688 Training Loss: tensor(0.3259)\n",
      "28689 Training Loss: tensor(0.3259)\n",
      "28690 Training Loss: tensor(0.3262)\n",
      "28691 Training Loss: tensor(0.3264)\n",
      "28692 Training Loss: tensor(0.3273)\n",
      "28693 Training Loss: tensor(0.3263)\n",
      "28694 Training Loss: tensor(0.3260)\n",
      "28695 Training Loss: tensor(0.3258)\n",
      "28696 Training Loss: tensor(0.3261)\n",
      "28697 Training Loss: tensor(0.3260)\n",
      "28698 Training Loss: tensor(0.3273)\n",
      "28699 Training Loss: tensor(0.3267)\n",
      "28700 Training Loss: tensor(0.3264)\n",
      "28701 Training Loss: tensor(0.3292)\n",
      "28702 Training Loss: tensor(0.3259)\n",
      "28703 Training Loss: tensor(0.3263)\n",
      "28704 Training Loss: tensor(0.3261)\n",
      "28705 Training Loss: tensor(0.3261)\n",
      "28706 Training Loss: tensor(0.3270)\n",
      "28707 Training Loss: tensor(0.3261)\n",
      "28708 Training Loss: tensor(0.3263)\n",
      "28709 Training Loss: tensor(0.3260)\n",
      "28710 Training Loss: tensor(0.3260)\n",
      "28711 Training Loss: tensor(0.3261)\n",
      "28712 Training Loss: tensor(0.3264)\n",
      "28713 Training Loss: tensor(0.3260)\n",
      "28714 Training Loss: tensor(0.3268)\n",
      "28715 Training Loss: tensor(0.3271)\n",
      "28716 Training Loss: tensor(0.3264)\n",
      "28717 Training Loss: tensor(0.3261)\n",
      "28718 Training Loss: tensor(0.3268)\n",
      "28719 Training Loss: tensor(0.3259)\n",
      "28720 Training Loss: tensor(0.3259)\n",
      "28721 Training Loss: tensor(0.3264)\n",
      "28722 Training Loss: tensor(0.3266)\n",
      "28723 Training Loss: tensor(0.3263)\n",
      "28724 Training Loss: tensor(0.3261)\n",
      "28725 Training Loss: tensor(0.3282)\n",
      "28726 Training Loss: tensor(0.3258)\n",
      "28727 Training Loss: tensor(0.3261)\n",
      "28728 Training Loss: tensor(0.3262)\n",
      "28729 Training Loss: tensor(0.3261)\n",
      "28730 Training Loss: tensor(0.3262)\n",
      "28731 Training Loss: tensor(0.3261)\n",
      "28732 Training Loss: tensor(0.3260)\n",
      "28733 Training Loss: tensor(0.3275)\n",
      "28734 Training Loss: tensor(0.3259)\n",
      "28735 Training Loss: tensor(0.3272)\n",
      "28736 Training Loss: tensor(0.3261)\n",
      "28737 Training Loss: tensor(0.3261)\n",
      "28738 Training Loss: tensor(0.3261)\n",
      "28739 Training Loss: tensor(0.3258)\n",
      "28740 Training Loss: tensor(0.3257)\n",
      "28741 Training Loss: tensor(0.3263)\n",
      "28742 Training Loss: tensor(0.3260)\n",
      "28743 Training Loss: tensor(0.3259)\n",
      "28744 Training Loss: tensor(0.3263)\n",
      "28745 Training Loss: tensor(0.3269)\n",
      "28746 Training Loss: tensor(0.3269)\n",
      "28747 Training Loss: tensor(0.3267)\n",
      "28748 Training Loss: tensor(0.3262)\n",
      "28749 Training Loss: tensor(0.3264)\n",
      "28750 Training Loss: tensor(0.3266)\n",
      "28751 Training Loss: tensor(0.3267)\n",
      "28752 Training Loss: tensor(0.3288)\n",
      "28753 Training Loss: tensor(0.3260)\n",
      "28754 Training Loss: tensor(0.3269)\n",
      "28755 Training Loss: tensor(0.3263)\n",
      "28756 Training Loss: tensor(0.3260)\n",
      "28757 Training Loss: tensor(0.3264)\n",
      "28758 Training Loss: tensor(0.3262)\n",
      "28759 Training Loss: tensor(0.3264)\n",
      "28760 Training Loss: tensor(0.3258)\n",
      "28761 Training Loss: tensor(0.3267)\n",
      "28762 Training Loss: tensor(0.3284)\n",
      "28763 Training Loss: tensor(0.3270)\n",
      "28764 Training Loss: tensor(0.3263)\n",
      "28765 Training Loss: tensor(0.3263)\n",
      "28766 Training Loss: tensor(0.3261)\n",
      "28767 Training Loss: tensor(0.3264)\n",
      "28768 Training Loss: tensor(0.3307)\n",
      "28769 Training Loss: tensor(0.3285)\n",
      "28770 Training Loss: tensor(0.3258)\n",
      "28771 Training Loss: tensor(0.3263)\n",
      "28772 Training Loss: tensor(0.3263)\n",
      "28773 Training Loss: tensor(0.3294)\n",
      "28774 Training Loss: tensor(0.3273)\n",
      "28775 Training Loss: tensor(0.3264)\n",
      "28776 Training Loss: tensor(0.3264)\n",
      "28777 Training Loss: tensor(0.3262)\n",
      "28778 Training Loss: tensor(0.3267)\n",
      "28779 Training Loss: tensor(0.3260)\n",
      "28780 Training Loss: tensor(0.3264)\n",
      "28781 Training Loss: tensor(0.3274)\n",
      "28782 Training Loss: tensor(0.3262)\n",
      "28783 Training Loss: tensor(0.3259)\n",
      "28784 Training Loss: tensor(0.3263)\n",
      "28785 Training Loss: tensor(0.3260)\n",
      "28786 Training Loss: tensor(0.3268)\n",
      "28787 Training Loss: tensor(0.3259)\n",
      "28788 Training Loss: tensor(0.3267)\n",
      "28789 Training Loss: tensor(0.3265)\n",
      "28790 Training Loss: tensor(0.3270)\n",
      "28791 Training Loss: tensor(0.3283)\n",
      "28792 Training Loss: tensor(0.3261)\n",
      "28793 Training Loss: tensor(0.3261)\n",
      "28794 Training Loss: tensor(0.3267)\n",
      "28795 Training Loss: tensor(0.3265)\n",
      "28796 Training Loss: tensor(0.3259)\n",
      "28797 Training Loss: tensor(0.3261)\n",
      "28798 Training Loss: tensor(0.3260)\n",
      "28799 Training Loss: tensor(0.3267)\n",
      "28800 Training Loss: tensor(0.3261)\n",
      "28801 Training Loss: tensor(0.3264)\n",
      "28802 Training Loss: tensor(0.3261)\n",
      "28803 Training Loss: tensor(0.3262)\n",
      "28804 Training Loss: tensor(0.3264)\n",
      "28805 Training Loss: tensor(0.3272)\n",
      "28806 Training Loss: tensor(0.3265)\n",
      "28807 Training Loss: tensor(0.3265)\n",
      "28808 Training Loss: tensor(0.3278)\n",
      "28809 Training Loss: tensor(0.3273)\n",
      "28810 Training Loss: tensor(0.3265)\n",
      "28811 Training Loss: tensor(0.3270)\n",
      "28812 Training Loss: tensor(0.3261)\n",
      "28813 Training Loss: tensor(0.3266)\n",
      "28814 Training Loss: tensor(0.3265)\n",
      "28815 Training Loss: tensor(0.3267)\n",
      "28816 Training Loss: tensor(0.3258)\n",
      "28817 Training Loss: tensor(0.3259)\n",
      "28818 Training Loss: tensor(0.3260)\n",
      "28819 Training Loss: tensor(0.3261)\n",
      "28820 Training Loss: tensor(0.3270)\n",
      "28821 Training Loss: tensor(0.3260)\n",
      "28822 Training Loss: tensor(0.3263)\n",
      "28823 Training Loss: tensor(0.3273)\n",
      "28824 Training Loss: tensor(0.3259)\n",
      "28825 Training Loss: tensor(0.3261)\n",
      "28826 Training Loss: tensor(0.3260)\n",
      "28827 Training Loss: tensor(0.3263)\n",
      "28828 Training Loss: tensor(0.3268)\n",
      "28829 Training Loss: tensor(0.3263)\n",
      "28830 Training Loss: tensor(0.3257)\n",
      "28831 Training Loss: tensor(0.3304)\n",
      "28832 Training Loss: tensor(0.3274)\n",
      "28833 Training Loss: tensor(0.3263)\n",
      "28834 Training Loss: tensor(0.3267)\n",
      "28835 Training Loss: tensor(0.3260)\n",
      "28836 Training Loss: tensor(0.3267)\n",
      "28837 Training Loss: tensor(0.3265)\n",
      "28838 Training Loss: tensor(0.3260)\n",
      "28839 Training Loss: tensor(0.3263)\n",
      "28840 Training Loss: tensor(0.3265)\n",
      "28841 Training Loss: tensor(0.3282)\n",
      "28842 Training Loss: tensor(0.3262)\n",
      "28843 Training Loss: tensor(0.3268)\n",
      "28844 Training Loss: tensor(0.3263)\n",
      "28845 Training Loss: tensor(0.3283)\n",
      "28846 Training Loss: tensor(0.3280)\n",
      "28847 Training Loss: tensor(0.3271)\n",
      "28848 Training Loss: tensor(0.3268)\n",
      "28849 Training Loss: tensor(0.3279)\n",
      "28850 Training Loss: tensor(0.3262)\n",
      "28851 Training Loss: tensor(0.3272)\n",
      "28852 Training Loss: tensor(0.3268)\n",
      "28853 Training Loss: tensor(0.3267)\n",
      "28854 Training Loss: tensor(0.3265)\n",
      "28855 Training Loss: tensor(0.3263)\n",
      "28856 Training Loss: tensor(0.3277)\n",
      "28857 Training Loss: tensor(0.3274)\n",
      "28858 Training Loss: tensor(0.3268)\n",
      "28859 Training Loss: tensor(0.3272)\n",
      "28860 Training Loss: tensor(0.3263)\n",
      "28861 Training Loss: tensor(0.3264)\n",
      "28862 Training Loss: tensor(0.3269)\n",
      "28863 Training Loss: tensor(0.3260)\n",
      "28864 Training Loss: tensor(0.3265)\n",
      "28865 Training Loss: tensor(0.3261)\n",
      "28866 Training Loss: tensor(0.3262)\n",
      "28867 Training Loss: tensor(0.3286)\n",
      "28868 Training Loss: tensor(0.3264)\n",
      "28869 Training Loss: tensor(0.3265)\n",
      "28870 Training Loss: tensor(0.3260)\n",
      "28871 Training Loss: tensor(0.3263)\n",
      "28872 Training Loss: tensor(0.3261)\n",
      "28873 Training Loss: tensor(0.3261)\n",
      "28874 Training Loss: tensor(0.3262)\n",
      "28875 Training Loss: tensor(0.3267)\n",
      "28876 Training Loss: tensor(0.3263)\n",
      "28877 Training Loss: tensor(0.3261)\n",
      "28878 Training Loss: tensor(0.3259)\n",
      "28879 Training Loss: tensor(0.3260)\n",
      "28880 Training Loss: tensor(0.3261)\n",
      "28881 Training Loss: tensor(0.3276)\n",
      "28882 Training Loss: tensor(0.3259)\n",
      "28883 Training Loss: tensor(0.3261)\n",
      "28884 Training Loss: tensor(0.3267)\n",
      "28885 Training Loss: tensor(0.3259)\n",
      "28886 Training Loss: tensor(0.3259)\n",
      "28887 Training Loss: tensor(0.3266)\n",
      "28888 Training Loss: tensor(0.3281)\n",
      "28889 Training Loss: tensor(0.3261)\n",
      "28890 Training Loss: tensor(0.3271)\n",
      "28891 Training Loss: tensor(0.3281)\n",
      "28892 Training Loss: tensor(0.3262)\n",
      "28893 Training Loss: tensor(0.3259)\n",
      "28894 Training Loss: tensor(0.3270)\n",
      "28895 Training Loss: tensor(0.3266)\n",
      "28896 Training Loss: tensor(0.3267)\n",
      "28897 Training Loss: tensor(0.3267)\n",
      "28898 Training Loss: tensor(0.3266)\n",
      "28899 Training Loss: tensor(0.3265)\n",
      "28900 Training Loss: tensor(0.3262)\n",
      "28901 Training Loss: tensor(0.3266)\n",
      "28902 Training Loss: tensor(0.3263)\n",
      "28903 Training Loss: tensor(0.3264)\n",
      "28904 Training Loss: tensor(0.3263)\n",
      "28905 Training Loss: tensor(0.3284)\n",
      "28906 Training Loss: tensor(0.3264)\n",
      "28907 Training Loss: tensor(0.3270)\n",
      "28908 Training Loss: tensor(0.3265)\n",
      "28909 Training Loss: tensor(0.3273)\n",
      "28910 Training Loss: tensor(0.3269)\n",
      "28911 Training Loss: tensor(0.3269)\n",
      "28912 Training Loss: tensor(0.3282)\n",
      "28913 Training Loss: tensor(0.3261)\n",
      "28914 Training Loss: tensor(0.3262)\n",
      "28915 Training Loss: tensor(0.3274)\n",
      "28916 Training Loss: tensor(0.3264)\n",
      "28917 Training Loss: tensor(0.3263)\n",
      "28918 Training Loss: tensor(0.3274)\n",
      "28919 Training Loss: tensor(0.3263)\n",
      "28920 Training Loss: tensor(0.3262)\n",
      "28921 Training Loss: tensor(0.3259)\n",
      "28922 Training Loss: tensor(0.3259)\n",
      "28923 Training Loss: tensor(0.3286)\n",
      "28924 Training Loss: tensor(0.3263)\n",
      "28925 Training Loss: tensor(0.3265)\n",
      "28926 Training Loss: tensor(0.3260)\n",
      "28927 Training Loss: tensor(0.3291)\n",
      "28928 Training Loss: tensor(0.3262)\n",
      "28929 Training Loss: tensor(0.3259)\n",
      "28930 Training Loss: tensor(0.3268)\n",
      "28931 Training Loss: tensor(0.3264)\n",
      "28932 Training Loss: tensor(0.3261)\n",
      "28933 Training Loss: tensor(0.3284)\n",
      "28934 Training Loss: tensor(0.3261)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28935 Training Loss: tensor(0.3259)\n",
      "28936 Training Loss: tensor(0.3266)\n",
      "28937 Training Loss: tensor(0.3265)\n",
      "28938 Training Loss: tensor(0.3260)\n",
      "28939 Training Loss: tensor(0.3266)\n",
      "28940 Training Loss: tensor(0.3275)\n",
      "28941 Training Loss: tensor(0.3264)\n",
      "28942 Training Loss: tensor(0.3264)\n",
      "28943 Training Loss: tensor(0.3264)\n",
      "28944 Training Loss: tensor(0.3274)\n",
      "28945 Training Loss: tensor(0.3264)\n",
      "28946 Training Loss: tensor(0.3282)\n",
      "28947 Training Loss: tensor(0.3262)\n",
      "28948 Training Loss: tensor(0.3263)\n",
      "28949 Training Loss: tensor(0.3259)\n",
      "28950 Training Loss: tensor(0.3264)\n",
      "28951 Training Loss: tensor(0.3261)\n",
      "28952 Training Loss: tensor(0.3271)\n",
      "28953 Training Loss: tensor(0.3259)\n",
      "28954 Training Loss: tensor(0.3268)\n",
      "28955 Training Loss: tensor(0.3272)\n",
      "28956 Training Loss: tensor(0.3275)\n",
      "28957 Training Loss: tensor(0.3261)\n",
      "28958 Training Loss: tensor(0.3261)\n",
      "28959 Training Loss: tensor(0.3266)\n",
      "28960 Training Loss: tensor(0.3264)\n",
      "28961 Training Loss: tensor(0.3276)\n",
      "28962 Training Loss: tensor(0.3263)\n",
      "28963 Training Loss: tensor(0.3259)\n",
      "28964 Training Loss: tensor(0.3259)\n",
      "28965 Training Loss: tensor(0.3264)\n",
      "28966 Training Loss: tensor(0.3261)\n",
      "28967 Training Loss: tensor(0.3273)\n",
      "28968 Training Loss: tensor(0.3257)\n",
      "28969 Training Loss: tensor(0.3263)\n",
      "28970 Training Loss: tensor(0.3259)\n",
      "28971 Training Loss: tensor(0.3278)\n",
      "28972 Training Loss: tensor(0.3269)\n",
      "28973 Training Loss: tensor(0.3261)\n",
      "28974 Training Loss: tensor(0.3263)\n",
      "28975 Training Loss: tensor(0.3259)\n",
      "28976 Training Loss: tensor(0.3265)\n",
      "28977 Training Loss: tensor(0.3265)\n",
      "28978 Training Loss: tensor(0.3263)\n",
      "28979 Training Loss: tensor(0.3261)\n",
      "28980 Training Loss: tensor(0.3272)\n",
      "28981 Training Loss: tensor(0.3290)\n",
      "28982 Training Loss: tensor(0.3259)\n",
      "28983 Training Loss: tensor(0.3260)\n",
      "28984 Training Loss: tensor(0.3263)\n",
      "28985 Training Loss: tensor(0.3265)\n",
      "28986 Training Loss: tensor(0.3263)\n",
      "28987 Training Loss: tensor(0.3273)\n",
      "28988 Training Loss: tensor(0.3259)\n",
      "28989 Training Loss: tensor(0.3260)\n",
      "28990 Training Loss: tensor(0.3262)\n",
      "28991 Training Loss: tensor(0.3267)\n",
      "28992 Training Loss: tensor(0.3267)\n",
      "28993 Training Loss: tensor(0.3258)\n",
      "28994 Training Loss: tensor(0.3261)\n",
      "28995 Training Loss: tensor(0.3259)\n",
      "28996 Training Loss: tensor(0.3261)\n",
      "28997 Training Loss: tensor(0.3264)\n",
      "28998 Training Loss: tensor(0.3268)\n",
      "28999 Training Loss: tensor(0.3264)\n",
      "29000 Training Loss: tensor(0.3259)\n",
      "29001 Training Loss: tensor(0.3260)\n",
      "29002 Training Loss: tensor(0.3258)\n",
      "29003 Training Loss: tensor(0.3259)\n",
      "29004 Training Loss: tensor(0.3259)\n",
      "29005 Training Loss: tensor(0.3278)\n",
      "29006 Training Loss: tensor(0.3287)\n",
      "29007 Training Loss: tensor(0.3258)\n",
      "29008 Training Loss: tensor(0.3259)\n",
      "29009 Training Loss: tensor(0.3263)\n",
      "29010 Training Loss: tensor(0.3259)\n",
      "29011 Training Loss: tensor(0.3260)\n",
      "29012 Training Loss: tensor(0.3261)\n",
      "29013 Training Loss: tensor(0.3259)\n",
      "29014 Training Loss: tensor(0.3259)\n",
      "29015 Training Loss: tensor(0.3265)\n",
      "29016 Training Loss: tensor(0.3262)\n",
      "29017 Training Loss: tensor(0.3272)\n",
      "29018 Training Loss: tensor(0.3260)\n",
      "29019 Training Loss: tensor(0.3259)\n",
      "29020 Training Loss: tensor(0.3266)\n",
      "29021 Training Loss: tensor(0.3264)\n",
      "29022 Training Loss: tensor(0.3268)\n",
      "29023 Training Loss: tensor(0.3267)\n",
      "29024 Training Loss: tensor(0.3260)\n",
      "29025 Training Loss: tensor(0.3260)\n",
      "29026 Training Loss: tensor(0.3263)\n",
      "29027 Training Loss: tensor(0.3264)\n",
      "29028 Training Loss: tensor(0.3257)\n",
      "29029 Training Loss: tensor(0.3260)\n",
      "29030 Training Loss: tensor(0.3272)\n",
      "29031 Training Loss: tensor(0.3258)\n",
      "29032 Training Loss: tensor(0.3261)\n",
      "29033 Training Loss: tensor(0.3256)\n",
      "29034 Training Loss: tensor(0.3260)\n",
      "29035 Training Loss: tensor(0.3262)\n",
      "29036 Training Loss: tensor(0.3259)\n",
      "29037 Training Loss: tensor(0.3261)\n",
      "29038 Training Loss: tensor(0.3257)\n",
      "29039 Training Loss: tensor(0.3258)\n",
      "29040 Training Loss: tensor(0.3260)\n",
      "29041 Training Loss: tensor(0.3256)\n",
      "29042 Training Loss: tensor(0.3260)\n",
      "29043 Training Loss: tensor(0.3268)\n",
      "29044 Training Loss: tensor(0.3256)\n",
      "29045 Training Loss: tensor(0.3257)\n",
      "29046 Training Loss: tensor(0.3261)\n",
      "29047 Training Loss: tensor(0.3278)\n",
      "29048 Training Loss: tensor(0.3259)\n",
      "29049 Training Loss: tensor(0.3259)\n",
      "29050 Training Loss: tensor(0.3260)\n",
      "29051 Training Loss: tensor(0.3259)\n",
      "29052 Training Loss: tensor(0.3257)\n",
      "29053 Training Loss: tensor(0.3259)\n",
      "29054 Training Loss: tensor(0.3261)\n",
      "29055 Training Loss: tensor(0.3277)\n",
      "29056 Training Loss: tensor(0.3258)\n",
      "29057 Training Loss: tensor(0.3261)\n",
      "29058 Training Loss: tensor(0.3262)\n",
      "29059 Training Loss: tensor(0.3259)\n",
      "29060 Training Loss: tensor(0.3261)\n",
      "29061 Training Loss: tensor(0.3260)\n",
      "29062 Training Loss: tensor(0.3274)\n",
      "29063 Training Loss: tensor(0.3258)\n",
      "29064 Training Loss: tensor(0.3257)\n",
      "29065 Training Loss: tensor(0.3258)\n",
      "29066 Training Loss: tensor(0.3261)\n",
      "29067 Training Loss: tensor(0.3260)\n",
      "29068 Training Loss: tensor(0.3260)\n",
      "29069 Training Loss: tensor(0.3259)\n",
      "29070 Training Loss: tensor(0.3261)\n",
      "29071 Training Loss: tensor(0.3256)\n",
      "29072 Training Loss: tensor(0.3271)\n",
      "29073 Training Loss: tensor(0.3256)\n",
      "29074 Training Loss: tensor(0.3260)\n",
      "29075 Training Loss: tensor(0.3282)\n",
      "29076 Training Loss: tensor(0.3285)\n",
      "29077 Training Loss: tensor(0.3256)\n",
      "29078 Training Loss: tensor(0.3270)\n",
      "29079 Training Loss: tensor(0.3262)\n",
      "29080 Training Loss: tensor(0.3261)\n",
      "29081 Training Loss: tensor(0.3263)\n",
      "29082 Training Loss: tensor(0.3261)\n",
      "29083 Training Loss: tensor(0.3258)\n",
      "29084 Training Loss: tensor(0.3257)\n",
      "29085 Training Loss: tensor(0.3275)\n",
      "29086 Training Loss: tensor(0.3260)\n",
      "29087 Training Loss: tensor(0.3259)\n",
      "29088 Training Loss: tensor(0.3261)\n",
      "29089 Training Loss: tensor(0.3267)\n",
      "29090 Training Loss: tensor(0.3276)\n",
      "29091 Training Loss: tensor(0.3263)\n",
      "29092 Training Loss: tensor(0.3258)\n",
      "29093 Training Loss: tensor(0.3260)\n",
      "29094 Training Loss: tensor(0.3290)\n",
      "29095 Training Loss: tensor(0.3259)\n",
      "29096 Training Loss: tensor(0.3261)\n",
      "29097 Training Loss: tensor(0.3260)\n",
      "29098 Training Loss: tensor(0.3260)\n",
      "29099 Training Loss: tensor(0.3262)\n",
      "29100 Training Loss: tensor(0.3260)\n",
      "29101 Training Loss: tensor(0.3272)\n",
      "29102 Training Loss: tensor(0.3275)\n",
      "29103 Training Loss: tensor(0.3282)\n",
      "29104 Training Loss: tensor(0.3260)\n",
      "29105 Training Loss: tensor(0.3264)\n",
      "29106 Training Loss: tensor(0.3264)\n",
      "29107 Training Loss: tensor(0.3261)\n",
      "29108 Training Loss: tensor(0.3264)\n",
      "29109 Training Loss: tensor(0.3264)\n",
      "29110 Training Loss: tensor(0.3262)\n",
      "29111 Training Loss: tensor(0.3263)\n",
      "29112 Training Loss: tensor(0.3263)\n",
      "29113 Training Loss: tensor(0.3270)\n",
      "29114 Training Loss: tensor(0.3276)\n",
      "29115 Training Loss: tensor(0.3270)\n",
      "29116 Training Loss: tensor(0.3261)\n",
      "29117 Training Loss: tensor(0.3257)\n",
      "29118 Training Loss: tensor(0.3259)\n",
      "29119 Training Loss: tensor(0.3259)\n",
      "29120 Training Loss: tensor(0.3266)\n",
      "29121 Training Loss: tensor(0.3263)\n",
      "29122 Training Loss: tensor(0.3265)\n",
      "29123 Training Loss: tensor(0.3264)\n",
      "29124 Training Loss: tensor(0.3264)\n",
      "29125 Training Loss: tensor(0.3264)\n",
      "29126 Training Loss: tensor(0.3264)\n",
      "29127 Training Loss: tensor(0.3261)\n",
      "29128 Training Loss: tensor(0.3259)\n",
      "29129 Training Loss: tensor(0.3267)\n",
      "29130 Training Loss: tensor(0.3259)\n",
      "29131 Training Loss: tensor(0.3267)\n",
      "29132 Training Loss: tensor(0.3267)\n",
      "29133 Training Loss: tensor(0.3258)\n",
      "29134 Training Loss: tensor(0.3260)\n",
      "29135 Training Loss: tensor(0.3259)\n",
      "29136 Training Loss: tensor(0.3266)\n",
      "29137 Training Loss: tensor(0.3262)\n",
      "29138 Training Loss: tensor(0.3259)\n",
      "29139 Training Loss: tensor(0.3261)\n",
      "29140 Training Loss: tensor(0.3261)\n",
      "29141 Training Loss: tensor(0.3262)\n",
      "29142 Training Loss: tensor(0.3262)\n",
      "29143 Training Loss: tensor(0.3265)\n",
      "29144 Training Loss: tensor(0.3269)\n",
      "29145 Training Loss: tensor(0.3261)\n",
      "29146 Training Loss: tensor(0.3259)\n",
      "29147 Training Loss: tensor(0.3268)\n",
      "29148 Training Loss: tensor(0.3261)\n",
      "29149 Training Loss: tensor(0.3260)\n",
      "29150 Training Loss: tensor(0.3291)\n",
      "29151 Training Loss: tensor(0.3259)\n",
      "29152 Training Loss: tensor(0.3258)\n",
      "29153 Training Loss: tensor(0.3257)\n",
      "29154 Training Loss: tensor(0.3262)\n",
      "29155 Training Loss: tensor(0.3260)\n",
      "29156 Training Loss: tensor(0.3267)\n",
      "29157 Training Loss: tensor(0.3263)\n",
      "29158 Training Loss: tensor(0.3260)\n",
      "29159 Training Loss: tensor(0.3260)\n",
      "29160 Training Loss: tensor(0.3266)\n",
      "29161 Training Loss: tensor(0.3265)\n",
      "29162 Training Loss: tensor(0.3273)\n",
      "29163 Training Loss: tensor(0.3258)\n",
      "29164 Training Loss: tensor(0.3264)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29165 Training Loss: tensor(0.3257)\n",
      "29166 Training Loss: tensor(0.3262)\n",
      "29167 Training Loss: tensor(0.3273)\n",
      "29168 Training Loss: tensor(0.3260)\n",
      "29169 Training Loss: tensor(0.3259)\n",
      "29170 Training Loss: tensor(0.3290)\n",
      "29171 Training Loss: tensor(0.3260)\n",
      "29172 Training Loss: tensor(0.3264)\n",
      "29173 Training Loss: tensor(0.3262)\n",
      "29174 Training Loss: tensor(0.3263)\n",
      "29175 Training Loss: tensor(0.3260)\n",
      "29176 Training Loss: tensor(0.3264)\n",
      "29177 Training Loss: tensor(0.3264)\n",
      "29178 Training Loss: tensor(0.3268)\n",
      "29179 Training Loss: tensor(0.3277)\n",
      "29180 Training Loss: tensor(0.3262)\n",
      "29181 Training Loss: tensor(0.3259)\n",
      "29182 Training Loss: tensor(0.3261)\n",
      "29183 Training Loss: tensor(0.3259)\n",
      "29184 Training Loss: tensor(0.3260)\n",
      "29185 Training Loss: tensor(0.3261)\n",
      "29186 Training Loss: tensor(0.3258)\n",
      "29187 Training Loss: tensor(0.3258)\n",
      "29188 Training Loss: tensor(0.3266)\n",
      "29189 Training Loss: tensor(0.3264)\n",
      "29190 Training Loss: tensor(0.3272)\n",
      "29191 Training Loss: tensor(0.3286)\n",
      "29192 Training Loss: tensor(0.3258)\n",
      "29193 Training Loss: tensor(0.3260)\n",
      "29194 Training Loss: tensor(0.3278)\n",
      "29195 Training Loss: tensor(0.3265)\n",
      "29196 Training Loss: tensor(0.3261)\n",
      "29197 Training Loss: tensor(0.3268)\n",
      "29198 Training Loss: tensor(0.3259)\n",
      "29199 Training Loss: tensor(0.3262)\n",
      "29200 Training Loss: tensor(0.3265)\n",
      "29201 Training Loss: tensor(0.3264)\n",
      "29202 Training Loss: tensor(0.3261)\n",
      "29203 Training Loss: tensor(0.3260)\n",
      "29204 Training Loss: tensor(0.3273)\n",
      "29205 Training Loss: tensor(0.3260)\n",
      "29206 Training Loss: tensor(0.3263)\n",
      "29207 Training Loss: tensor(0.3262)\n",
      "29208 Training Loss: tensor(0.3258)\n",
      "29209 Training Loss: tensor(0.3260)\n",
      "29210 Training Loss: tensor(0.3270)\n",
      "29211 Training Loss: tensor(0.3279)\n",
      "29212 Training Loss: tensor(0.3262)\n",
      "29213 Training Loss: tensor(0.3259)\n",
      "29214 Training Loss: tensor(0.3260)\n",
      "29215 Training Loss: tensor(0.3260)\n",
      "29216 Training Loss: tensor(0.3259)\n",
      "29217 Training Loss: tensor(0.3260)\n",
      "29218 Training Loss: tensor(0.3263)\n",
      "29219 Training Loss: tensor(0.3261)\n",
      "29220 Training Loss: tensor(0.3264)\n",
      "29221 Training Loss: tensor(0.3260)\n",
      "29222 Training Loss: tensor(0.3269)\n",
      "29223 Training Loss: tensor(0.3261)\n",
      "29224 Training Loss: tensor(0.3261)\n",
      "29225 Training Loss: tensor(0.3287)\n",
      "29226 Training Loss: tensor(0.3267)\n",
      "29227 Training Loss: tensor(0.3259)\n",
      "29228 Training Loss: tensor(0.3262)\n",
      "29229 Training Loss: tensor(0.3269)\n",
      "29230 Training Loss: tensor(0.3264)\n",
      "29231 Training Loss: tensor(0.3258)\n",
      "29232 Training Loss: tensor(0.3262)\n",
      "29233 Training Loss: tensor(0.3258)\n",
      "29234 Training Loss: tensor(0.3257)\n",
      "29235 Training Loss: tensor(0.3258)\n",
      "29236 Training Loss: tensor(0.3268)\n",
      "29237 Training Loss: tensor(0.3264)\n",
      "29238 Training Loss: tensor(0.3268)\n",
      "29239 Training Loss: tensor(0.3258)\n",
      "29240 Training Loss: tensor(0.3257)\n",
      "29241 Training Loss: tensor(0.3274)\n",
      "29242 Training Loss: tensor(0.3274)\n",
      "29243 Training Loss: tensor(0.3287)\n",
      "29244 Training Loss: tensor(0.3259)\n",
      "29245 Training Loss: tensor(0.3265)\n",
      "29246 Training Loss: tensor(0.3264)\n",
      "29247 Training Loss: tensor(0.3263)\n",
      "29248 Training Loss: tensor(0.3258)\n",
      "29249 Training Loss: tensor(0.3263)\n",
      "29250 Training Loss: tensor(0.3259)\n",
      "29251 Training Loss: tensor(0.3259)\n",
      "29252 Training Loss: tensor(0.3260)\n",
      "29253 Training Loss: tensor(0.3264)\n",
      "29254 Training Loss: tensor(0.3260)\n",
      "29255 Training Loss: tensor(0.3266)\n",
      "29256 Training Loss: tensor(0.3271)\n",
      "29257 Training Loss: tensor(0.3285)\n",
      "29258 Training Loss: tensor(0.3262)\n",
      "29259 Training Loss: tensor(0.3260)\n",
      "29260 Training Loss: tensor(0.3263)\n",
      "29261 Training Loss: tensor(0.3261)\n",
      "29262 Training Loss: tensor(0.3264)\n",
      "29263 Training Loss: tensor(0.3281)\n",
      "29264 Training Loss: tensor(0.3265)\n",
      "29265 Training Loss: tensor(0.3259)\n",
      "29266 Training Loss: tensor(0.3261)\n",
      "29267 Training Loss: tensor(0.3264)\n",
      "29268 Training Loss: tensor(0.3271)\n",
      "29269 Training Loss: tensor(0.3272)\n",
      "29270 Training Loss: tensor(0.3278)\n",
      "29271 Training Loss: tensor(0.3268)\n",
      "29272 Training Loss: tensor(0.3264)\n",
      "29273 Training Loss: tensor(0.3257)\n",
      "29274 Training Loss: tensor(0.3261)\n",
      "29275 Training Loss: tensor(0.3261)\n",
      "29276 Training Loss: tensor(0.3265)\n",
      "29277 Training Loss: tensor(0.3259)\n",
      "29278 Training Loss: tensor(0.3274)\n",
      "29279 Training Loss: tensor(0.3293)\n",
      "29280 Training Loss: tensor(0.3269)\n",
      "29281 Training Loss: tensor(0.3269)\n",
      "29282 Training Loss: tensor(0.3261)\n",
      "29283 Training Loss: tensor(0.3272)\n",
      "29284 Training Loss: tensor(0.3265)\n",
      "29285 Training Loss: tensor(0.3271)\n",
      "29286 Training Loss: tensor(0.3265)\n",
      "29287 Training Loss: tensor(0.3264)\n",
      "29288 Training Loss: tensor(0.3265)\n",
      "29289 Training Loss: tensor(0.3269)\n",
      "29290 Training Loss: tensor(0.3263)\n",
      "29291 Training Loss: tensor(0.3266)\n",
      "29292 Training Loss: tensor(0.3283)\n",
      "29293 Training Loss: tensor(0.3272)\n",
      "29294 Training Loss: tensor(0.3260)\n",
      "29295 Training Loss: tensor(0.3261)\n",
      "29296 Training Loss: tensor(0.3271)\n",
      "29297 Training Loss: tensor(0.3261)\n",
      "29298 Training Loss: tensor(0.3261)\n",
      "29299 Training Loss: tensor(0.3262)\n",
      "29300 Training Loss: tensor(0.3270)\n",
      "29301 Training Loss: tensor(0.3281)\n",
      "29302 Training Loss: tensor(0.3260)\n",
      "29303 Training Loss: tensor(0.3260)\n",
      "29304 Training Loss: tensor(0.3271)\n",
      "29305 Training Loss: tensor(0.3269)\n",
      "29306 Training Loss: tensor(0.3261)\n",
      "29307 Training Loss: tensor(0.3266)\n",
      "29308 Training Loss: tensor(0.3259)\n",
      "29309 Training Loss: tensor(0.3279)\n",
      "29310 Training Loss: tensor(0.3260)\n",
      "29311 Training Loss: tensor(0.3263)\n",
      "29312 Training Loss: tensor(0.3262)\n",
      "29313 Training Loss: tensor(0.3283)\n",
      "29314 Training Loss: tensor(0.3267)\n",
      "29315 Training Loss: tensor(0.3263)\n",
      "29316 Training Loss: tensor(0.3274)\n",
      "29317 Training Loss: tensor(0.3270)\n",
      "29318 Training Loss: tensor(0.3283)\n",
      "29319 Training Loss: tensor(0.3268)\n",
      "29320 Training Loss: tensor(0.3262)\n",
      "29321 Training Loss: tensor(0.3264)\n",
      "29322 Training Loss: tensor(0.3268)\n",
      "29323 Training Loss: tensor(0.3273)\n",
      "29324 Training Loss: tensor(0.3267)\n",
      "29325 Training Loss: tensor(0.3262)\n",
      "29326 Training Loss: tensor(0.3259)\n",
      "29327 Training Loss: tensor(0.3276)\n",
      "29328 Training Loss: tensor(0.3259)\n",
      "29329 Training Loss: tensor(0.3264)\n",
      "29330 Training Loss: tensor(0.3269)\n",
      "29331 Training Loss: tensor(0.3271)\n",
      "29332 Training Loss: tensor(0.3269)\n",
      "29333 Training Loss: tensor(0.3263)\n",
      "29334 Training Loss: tensor(0.3259)\n",
      "29335 Training Loss: tensor(0.3266)\n",
      "29336 Training Loss: tensor(0.3264)\n",
      "29337 Training Loss: tensor(0.3263)\n",
      "29338 Training Loss: tensor(0.3262)\n",
      "29339 Training Loss: tensor(0.3265)\n",
      "29340 Training Loss: tensor(0.3262)\n",
      "29341 Training Loss: tensor(0.3259)\n",
      "29342 Training Loss: tensor(0.3259)\n",
      "29343 Training Loss: tensor(0.3269)\n",
      "29344 Training Loss: tensor(0.3266)\n",
      "29345 Training Loss: tensor(0.3259)\n",
      "29346 Training Loss: tensor(0.3260)\n",
      "29347 Training Loss: tensor(0.3260)\n",
      "29348 Training Loss: tensor(0.3273)\n",
      "29349 Training Loss: tensor(0.3271)\n",
      "29350 Training Loss: tensor(0.3271)\n",
      "29351 Training Loss: tensor(0.3259)\n",
      "29352 Training Loss: tensor(0.3259)\n",
      "29353 Training Loss: tensor(0.3257)\n",
      "29354 Training Loss: tensor(0.3258)\n",
      "29355 Training Loss: tensor(0.3263)\n",
      "29356 Training Loss: tensor(0.3260)\n",
      "29357 Training Loss: tensor(0.3269)\n",
      "29358 Training Loss: tensor(0.3275)\n",
      "29359 Training Loss: tensor(0.3261)\n",
      "29360 Training Loss: tensor(0.3259)\n",
      "29361 Training Loss: tensor(0.3263)\n",
      "29362 Training Loss: tensor(0.3262)\n",
      "29363 Training Loss: tensor(0.3258)\n",
      "29364 Training Loss: tensor(0.3257)\n",
      "29365 Training Loss: tensor(0.3261)\n",
      "29366 Training Loss: tensor(0.3260)\n",
      "29367 Training Loss: tensor(0.3263)\n",
      "29368 Training Loss: tensor(0.3265)\n",
      "29369 Training Loss: tensor(0.3264)\n",
      "29370 Training Loss: tensor(0.3260)\n",
      "29371 Training Loss: tensor(0.3266)\n",
      "29372 Training Loss: tensor(0.3257)\n",
      "29373 Training Loss: tensor(0.3256)\n",
      "29374 Training Loss: tensor(0.3259)\n",
      "29375 Training Loss: tensor(0.3260)\n",
      "29376 Training Loss: tensor(0.3260)\n",
      "29377 Training Loss: tensor(0.3263)\n",
      "29378 Training Loss: tensor(0.3271)\n",
      "29379 Training Loss: tensor(0.3263)\n",
      "29380 Training Loss: tensor(0.3279)\n",
      "29381 Training Loss: tensor(0.3268)\n",
      "29382 Training Loss: tensor(0.3258)\n",
      "29383 Training Loss: tensor(0.3259)\n",
      "29384 Training Loss: tensor(0.3262)\n",
      "29385 Training Loss: tensor(0.3278)\n",
      "29386 Training Loss: tensor(0.3258)\n",
      "29387 Training Loss: tensor(0.3261)\n",
      "29388 Training Loss: tensor(0.3258)\n",
      "29389 Training Loss: tensor(0.3260)\n",
      "29390 Training Loss: tensor(0.3260)\n",
      "29391 Training Loss: tensor(0.3259)\n",
      "29392 Training Loss: tensor(0.3275)\n",
      "29393 Training Loss: tensor(0.3258)\n",
      "29394 Training Loss: tensor(0.3264)\n",
      "29395 Training Loss: tensor(0.3258)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29396 Training Loss: tensor(0.3257)\n",
      "29397 Training Loss: tensor(0.3286)\n",
      "29398 Training Loss: tensor(0.3271)\n",
      "29399 Training Loss: tensor(0.3259)\n",
      "29400 Training Loss: tensor(0.3266)\n",
      "29401 Training Loss: tensor(0.3265)\n",
      "29402 Training Loss: tensor(0.3265)\n",
      "29403 Training Loss: tensor(0.3260)\n",
      "29404 Training Loss: tensor(0.3269)\n",
      "29405 Training Loss: tensor(0.3258)\n",
      "29406 Training Loss: tensor(0.3266)\n",
      "29407 Training Loss: tensor(0.3263)\n",
      "29408 Training Loss: tensor(0.3257)\n",
      "29409 Training Loss: tensor(0.3263)\n",
      "29410 Training Loss: tensor(0.3259)\n",
      "29411 Training Loss: tensor(0.3259)\n",
      "29412 Training Loss: tensor(0.3263)\n",
      "29413 Training Loss: tensor(0.3267)\n",
      "29414 Training Loss: tensor(0.3258)\n",
      "29415 Training Loss: tensor(0.3257)\n",
      "29416 Training Loss: tensor(0.3267)\n",
      "29417 Training Loss: tensor(0.3261)\n",
      "29418 Training Loss: tensor(0.3271)\n",
      "29419 Training Loss: tensor(0.3270)\n",
      "29420 Training Loss: tensor(0.3293)\n",
      "29421 Training Loss: tensor(0.3272)\n",
      "29422 Training Loss: tensor(0.3269)\n",
      "29423 Training Loss: tensor(0.3262)\n",
      "29424 Training Loss: tensor(0.3262)\n",
      "29425 Training Loss: tensor(0.3262)\n",
      "29426 Training Loss: tensor(0.3262)\n",
      "29427 Training Loss: tensor(0.3260)\n",
      "29428 Training Loss: tensor(0.3275)\n",
      "29429 Training Loss: tensor(0.3261)\n",
      "29430 Training Loss: tensor(0.3262)\n",
      "29431 Training Loss: tensor(0.3264)\n",
      "29432 Training Loss: tensor(0.3273)\n",
      "29433 Training Loss: tensor(0.3266)\n",
      "29434 Training Loss: tensor(0.3264)\n",
      "29435 Training Loss: tensor(0.3259)\n",
      "29436 Training Loss: tensor(0.3261)\n",
      "29437 Training Loss: tensor(0.3262)\n",
      "29438 Training Loss: tensor(0.3260)\n",
      "29439 Training Loss: tensor(0.3262)\n",
      "29440 Training Loss: tensor(0.3259)\n",
      "29441 Training Loss: tensor(0.3262)\n",
      "29442 Training Loss: tensor(0.3264)\n",
      "29443 Training Loss: tensor(0.3261)\n",
      "29444 Training Loss: tensor(0.3262)\n",
      "29445 Training Loss: tensor(0.3265)\n",
      "29446 Training Loss: tensor(0.3258)\n",
      "29447 Training Loss: tensor(0.3264)\n",
      "29448 Training Loss: tensor(0.3259)\n",
      "29449 Training Loss: tensor(0.3259)\n",
      "29450 Training Loss: tensor(0.3264)\n",
      "29451 Training Loss: tensor(0.3259)\n",
      "29452 Training Loss: tensor(0.3257)\n",
      "29453 Training Loss: tensor(0.3275)\n",
      "29454 Training Loss: tensor(0.3282)\n",
      "29455 Training Loss: tensor(0.3264)\n",
      "29456 Training Loss: tensor(0.3258)\n",
      "29457 Training Loss: tensor(0.3259)\n",
      "29458 Training Loss: tensor(0.3258)\n",
      "29459 Training Loss: tensor(0.3257)\n",
      "29460 Training Loss: tensor(0.3272)\n",
      "29461 Training Loss: tensor(0.3271)\n",
      "29462 Training Loss: tensor(0.3265)\n",
      "29463 Training Loss: tensor(0.3260)\n",
      "29464 Training Loss: tensor(0.3259)\n",
      "29465 Training Loss: tensor(0.3265)\n",
      "29466 Training Loss: tensor(0.3260)\n",
      "29467 Training Loss: tensor(0.3259)\n",
      "29468 Training Loss: tensor(0.3261)\n",
      "29469 Training Loss: tensor(0.3274)\n",
      "29470 Training Loss: tensor(0.3259)\n",
      "29471 Training Loss: tensor(0.3263)\n",
      "29472 Training Loss: tensor(0.3261)\n",
      "29473 Training Loss: tensor(0.3267)\n",
      "29474 Training Loss: tensor(0.3260)\n",
      "29475 Training Loss: tensor(0.3259)\n",
      "29476 Training Loss: tensor(0.3257)\n",
      "29477 Training Loss: tensor(0.3259)\n",
      "29478 Training Loss: tensor(0.3268)\n",
      "29479 Training Loss: tensor(0.3266)\n",
      "29480 Training Loss: tensor(0.3259)\n",
      "29481 Training Loss: tensor(0.3257)\n",
      "29482 Training Loss: tensor(0.3258)\n",
      "29483 Training Loss: tensor(0.3262)\n",
      "29484 Training Loss: tensor(0.3259)\n",
      "29485 Training Loss: tensor(0.3260)\n",
      "29486 Training Loss: tensor(0.3262)\n",
      "29487 Training Loss: tensor(0.3261)\n",
      "29488 Training Loss: tensor(0.3283)\n",
      "29489 Training Loss: tensor(0.3263)\n",
      "29490 Training Loss: tensor(0.3273)\n",
      "29491 Training Loss: tensor(0.3289)\n",
      "29492 Training Loss: tensor(0.3261)\n",
      "29493 Training Loss: tensor(0.3257)\n",
      "29494 Training Loss: tensor(0.3264)\n",
      "29495 Training Loss: tensor(0.3260)\n",
      "29496 Training Loss: tensor(0.3274)\n",
      "29497 Training Loss: tensor(0.3267)\n",
      "29498 Training Loss: tensor(0.3267)\n",
      "29499 Training Loss: tensor(0.3265)\n",
      "29500 Training Loss: tensor(0.3265)\n",
      "29501 Training Loss: tensor(0.3262)\n",
      "29502 Training Loss: tensor(0.3272)\n",
      "29503 Training Loss: tensor(0.3261)\n",
      "29504 Training Loss: tensor(0.3267)\n",
      "29505 Training Loss: tensor(0.3265)\n",
      "29506 Training Loss: tensor(0.3264)\n",
      "29507 Training Loss: tensor(0.3270)\n",
      "29508 Training Loss: tensor(0.3262)\n",
      "29509 Training Loss: tensor(0.3262)\n",
      "29510 Training Loss: tensor(0.3259)\n",
      "29511 Training Loss: tensor(0.3273)\n",
      "29512 Training Loss: tensor(0.3265)\n",
      "29513 Training Loss: tensor(0.3275)\n",
      "29514 Training Loss: tensor(0.3270)\n",
      "29515 Training Loss: tensor(0.3264)\n",
      "29516 Training Loss: tensor(0.3261)\n",
      "29517 Training Loss: tensor(0.3262)\n",
      "29518 Training Loss: tensor(0.3308)\n",
      "29519 Training Loss: tensor(0.3262)\n",
      "29520 Training Loss: tensor(0.3258)\n",
      "29521 Training Loss: tensor(0.3261)\n",
      "29522 Training Loss: tensor(0.3269)\n",
      "29523 Training Loss: tensor(0.3284)\n",
      "29524 Training Loss: tensor(0.3265)\n",
      "29525 Training Loss: tensor(0.3264)\n",
      "29526 Training Loss: tensor(0.3269)\n",
      "29527 Training Loss: tensor(0.3272)\n",
      "29528 Training Loss: tensor(0.3261)\n",
      "29529 Training Loss: tensor(0.3259)\n",
      "29530 Training Loss: tensor(0.3267)\n",
      "29531 Training Loss: tensor(0.3260)\n",
      "29532 Training Loss: tensor(0.3269)\n",
      "29533 Training Loss: tensor(0.3261)\n",
      "29534 Training Loss: tensor(0.3264)\n",
      "29535 Training Loss: tensor(0.3271)\n",
      "29536 Training Loss: tensor(0.3259)\n",
      "29537 Training Loss: tensor(0.3259)\n",
      "29538 Training Loss: tensor(0.3259)\n",
      "29539 Training Loss: tensor(0.3283)\n",
      "29540 Training Loss: tensor(0.3270)\n",
      "29541 Training Loss: tensor(0.3264)\n",
      "29542 Training Loss: tensor(0.3259)\n",
      "29543 Training Loss: tensor(0.3262)\n",
      "29544 Training Loss: tensor(0.3257)\n",
      "29545 Training Loss: tensor(0.3258)\n",
      "29546 Training Loss: tensor(0.3262)\n",
      "29547 Training Loss: tensor(0.3258)\n",
      "29548 Training Loss: tensor(0.3260)\n",
      "29549 Training Loss: tensor(0.3273)\n",
      "29550 Training Loss: tensor(0.3259)\n",
      "29551 Training Loss: tensor(0.3261)\n",
      "29552 Training Loss: tensor(0.3264)\n",
      "29553 Training Loss: tensor(0.3260)\n",
      "29554 Training Loss: tensor(0.3284)\n",
      "29555 Training Loss: tensor(0.3279)\n",
      "29556 Training Loss: tensor(0.3258)\n",
      "29557 Training Loss: tensor(0.3259)\n",
      "29558 Training Loss: tensor(0.3267)\n",
      "29559 Training Loss: tensor(0.3263)\n",
      "29560 Training Loss: tensor(0.3263)\n",
      "29561 Training Loss: tensor(0.3262)\n",
      "29562 Training Loss: tensor(0.3261)\n",
      "29563 Training Loss: tensor(0.3261)\n",
      "29564 Training Loss: tensor(0.3261)\n",
      "29565 Training Loss: tensor(0.3265)\n",
      "29566 Training Loss: tensor(0.3261)\n",
      "29567 Training Loss: tensor(0.3268)\n",
      "29568 Training Loss: tensor(0.3260)\n",
      "29569 Training Loss: tensor(0.3258)\n",
      "29570 Training Loss: tensor(0.3259)\n",
      "29571 Training Loss: tensor(0.3263)\n",
      "29572 Training Loss: tensor(0.3267)\n",
      "29573 Training Loss: tensor(0.3267)\n",
      "29574 Training Loss: tensor(0.3261)\n",
      "29575 Training Loss: tensor(0.3258)\n",
      "29576 Training Loss: tensor(0.3264)\n",
      "29577 Training Loss: tensor(0.3261)\n",
      "29578 Training Loss: tensor(0.3257)\n",
      "29579 Training Loss: tensor(0.3258)\n",
      "29580 Training Loss: tensor(0.3274)\n",
      "29581 Training Loss: tensor(0.3264)\n",
      "29582 Training Loss: tensor(0.3258)\n",
      "29583 Training Loss: tensor(0.3259)\n",
      "29584 Training Loss: tensor(0.3279)\n",
      "29585 Training Loss: tensor(0.3270)\n",
      "29586 Training Loss: tensor(0.3258)\n",
      "29587 Training Loss: tensor(0.3257)\n",
      "29588 Training Loss: tensor(0.3272)\n",
      "29589 Training Loss: tensor(0.3257)\n",
      "29590 Training Loss: tensor(0.3267)\n",
      "29591 Training Loss: tensor(0.3264)\n",
      "29592 Training Loss: tensor(0.3267)\n",
      "29593 Training Loss: tensor(0.3259)\n",
      "29594 Training Loss: tensor(0.3262)\n",
      "29595 Training Loss: tensor(0.3257)\n",
      "29596 Training Loss: tensor(0.3260)\n",
      "29597 Training Loss: tensor(0.3257)\n",
      "29598 Training Loss: tensor(0.3260)\n",
      "29599 Training Loss: tensor(0.3259)\n",
      "29600 Training Loss: tensor(0.3258)\n",
      "29601 Training Loss: tensor(0.3258)\n",
      "29602 Training Loss: tensor(0.3259)\n",
      "29603 Training Loss: tensor(0.3260)\n",
      "29604 Training Loss: tensor(0.3257)\n",
      "29605 Training Loss: tensor(0.3257)\n",
      "29606 Training Loss: tensor(0.3262)\n",
      "29607 Training Loss: tensor(0.3255)\n",
      "29608 Training Loss: tensor(0.3256)\n",
      "29609 Training Loss: tensor(0.3261)\n",
      "29610 Training Loss: tensor(0.3265)\n",
      "29611 Training Loss: tensor(0.3255)\n",
      "29612 Training Loss: tensor(0.3265)\n",
      "29613 Training Loss: tensor(0.3272)\n",
      "29614 Training Loss: tensor(0.3258)\n",
      "29615 Training Loss: tensor(0.3259)\n",
      "29616 Training Loss: tensor(0.3261)\n",
      "29617 Training Loss: tensor(0.3273)\n",
      "29618 Training Loss: tensor(0.3262)\n",
      "29619 Training Loss: tensor(0.3256)\n",
      "29620 Training Loss: tensor(0.3260)\n",
      "29621 Training Loss: tensor(0.3259)\n",
      "29622 Training Loss: tensor(0.3260)\n",
      "29623 Training Loss: tensor(0.3267)\n",
      "29624 Training Loss: tensor(0.3259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29625 Training Loss: tensor(0.3263)\n",
      "29626 Training Loss: tensor(0.3263)\n",
      "29627 Training Loss: tensor(0.3259)\n",
      "29628 Training Loss: tensor(0.3277)\n",
      "29629 Training Loss: tensor(0.3267)\n",
      "29630 Training Loss: tensor(0.3266)\n",
      "29631 Training Loss: tensor(0.3257)\n",
      "29632 Training Loss: tensor(0.3264)\n",
      "29633 Training Loss: tensor(0.3271)\n",
      "29634 Training Loss: tensor(0.3261)\n",
      "29635 Training Loss: tensor(0.3260)\n",
      "29636 Training Loss: tensor(0.3265)\n",
      "29637 Training Loss: tensor(0.3260)\n",
      "29638 Training Loss: tensor(0.3258)\n",
      "29639 Training Loss: tensor(0.3259)\n",
      "29640 Training Loss: tensor(0.3259)\n",
      "29641 Training Loss: tensor(0.3257)\n",
      "29642 Training Loss: tensor(0.3259)\n",
      "29643 Training Loss: tensor(0.3260)\n",
      "29644 Training Loss: tensor(0.3260)\n",
      "29645 Training Loss: tensor(0.3255)\n",
      "29646 Training Loss: tensor(0.3267)\n",
      "29647 Training Loss: tensor(0.3259)\n",
      "29648 Training Loss: tensor(0.3258)\n",
      "29649 Training Loss: tensor(0.3258)\n",
      "29650 Training Loss: tensor(0.3257)\n",
      "29651 Training Loss: tensor(0.3257)\n",
      "29652 Training Loss: tensor(0.3263)\n",
      "29653 Training Loss: tensor(0.3263)\n",
      "29654 Training Loss: tensor(0.3283)\n",
      "29655 Training Loss: tensor(0.3258)\n",
      "29656 Training Loss: tensor(0.3260)\n",
      "29657 Training Loss: tensor(0.3278)\n",
      "29658 Training Loss: tensor(0.3261)\n",
      "29659 Training Loss: tensor(0.3258)\n",
      "29660 Training Loss: tensor(0.3260)\n",
      "29661 Training Loss: tensor(0.3258)\n",
      "29662 Training Loss: tensor(0.3256)\n",
      "29663 Training Loss: tensor(0.3262)\n",
      "29664 Training Loss: tensor(0.3260)\n",
      "29665 Training Loss: tensor(0.3264)\n",
      "29666 Training Loss: tensor(0.3270)\n",
      "29667 Training Loss: tensor(0.3270)\n",
      "29668 Training Loss: tensor(0.3256)\n",
      "29669 Training Loss: tensor(0.3281)\n",
      "29670 Training Loss: tensor(0.3257)\n",
      "29671 Training Loss: tensor(0.3259)\n",
      "29672 Training Loss: tensor(0.3265)\n",
      "29673 Training Loss: tensor(0.3258)\n",
      "29674 Training Loss: tensor(0.3279)\n",
      "29675 Training Loss: tensor(0.3258)\n",
      "29676 Training Loss: tensor(0.3258)\n",
      "29677 Training Loss: tensor(0.3267)\n",
      "29678 Training Loss: tensor(0.3261)\n",
      "29679 Training Loss: tensor(0.3257)\n",
      "29680 Training Loss: tensor(0.3262)\n",
      "29681 Training Loss: tensor(0.3259)\n",
      "29682 Training Loss: tensor(0.3263)\n",
      "29683 Training Loss: tensor(0.3258)\n",
      "29684 Training Loss: tensor(0.3259)\n",
      "29685 Training Loss: tensor(0.3268)\n",
      "29686 Training Loss: tensor(0.3260)\n",
      "29687 Training Loss: tensor(0.3258)\n",
      "29688 Training Loss: tensor(0.3261)\n",
      "29689 Training Loss: tensor(0.3281)\n",
      "29690 Training Loss: tensor(0.3258)\n",
      "29691 Training Loss: tensor(0.3260)\n",
      "29692 Training Loss: tensor(0.3264)\n",
      "29693 Training Loss: tensor(0.3264)\n",
      "29694 Training Loss: tensor(0.3256)\n",
      "29695 Training Loss: tensor(0.3258)\n",
      "29696 Training Loss: tensor(0.3258)\n",
      "29697 Training Loss: tensor(0.3275)\n",
      "29698 Training Loss: tensor(0.3261)\n",
      "29699 Training Loss: tensor(0.3261)\n",
      "29700 Training Loss: tensor(0.3257)\n",
      "29701 Training Loss: tensor(0.3259)\n",
      "29702 Training Loss: tensor(0.3290)\n",
      "29703 Training Loss: tensor(0.3261)\n",
      "29704 Training Loss: tensor(0.3269)\n",
      "29705 Training Loss: tensor(0.3265)\n",
      "29706 Training Loss: tensor(0.3256)\n",
      "29707 Training Loss: tensor(0.3258)\n",
      "29708 Training Loss: tensor(0.3259)\n",
      "29709 Training Loss: tensor(0.3272)\n",
      "29710 Training Loss: tensor(0.3261)\n",
      "29711 Training Loss: tensor(0.3259)\n",
      "29712 Training Loss: tensor(0.3258)\n",
      "29713 Training Loss: tensor(0.3279)\n",
      "29714 Training Loss: tensor(0.3258)\n",
      "29715 Training Loss: tensor(0.3259)\n",
      "29716 Training Loss: tensor(0.3258)\n",
      "29717 Training Loss: tensor(0.3259)\n",
      "29718 Training Loss: tensor(0.3260)\n",
      "29719 Training Loss: tensor(0.3257)\n",
      "29720 Training Loss: tensor(0.3260)\n",
      "29721 Training Loss: tensor(0.3259)\n",
      "29722 Training Loss: tensor(0.3262)\n",
      "29723 Training Loss: tensor(0.3256)\n",
      "29724 Training Loss: tensor(0.3286)\n",
      "29725 Training Loss: tensor(0.3264)\n",
      "29726 Training Loss: tensor(0.3257)\n",
      "29727 Training Loss: tensor(0.3257)\n",
      "29728 Training Loss: tensor(0.3273)\n",
      "29729 Training Loss: tensor(0.3276)\n",
      "29730 Training Loss: tensor(0.3258)\n",
      "29731 Training Loss: tensor(0.3271)\n",
      "29732 Training Loss: tensor(0.3272)\n",
      "29733 Training Loss: tensor(0.3265)\n",
      "29734 Training Loss: tensor(0.3263)\n",
      "29735 Training Loss: tensor(0.3264)\n",
      "29736 Training Loss: tensor(0.3259)\n",
      "29737 Training Loss: tensor(0.3261)\n",
      "29738 Training Loss: tensor(0.3263)\n",
      "29739 Training Loss: tensor(0.3263)\n",
      "29740 Training Loss: tensor(0.3259)\n",
      "29741 Training Loss: tensor(0.3258)\n",
      "29742 Training Loss: tensor(0.3256)\n",
      "29743 Training Loss: tensor(0.3258)\n",
      "29744 Training Loss: tensor(0.3260)\n",
      "29745 Training Loss: tensor(0.3269)\n",
      "29746 Training Loss: tensor(0.3259)\n",
      "29747 Training Loss: tensor(0.3262)\n",
      "29748 Training Loss: tensor(0.3258)\n",
      "29749 Training Loss: tensor(0.3262)\n",
      "29750 Training Loss: tensor(0.3264)\n",
      "29751 Training Loss: tensor(0.3274)\n",
      "29752 Training Loss: tensor(0.3260)\n",
      "29753 Training Loss: tensor(0.3258)\n",
      "29754 Training Loss: tensor(0.3258)\n",
      "29755 Training Loss: tensor(0.3273)\n",
      "29756 Training Loss: tensor(0.3261)\n",
      "29757 Training Loss: tensor(0.3262)\n",
      "29758 Training Loss: tensor(0.3273)\n",
      "29759 Training Loss: tensor(0.3261)\n",
      "29760 Training Loss: tensor(0.3261)\n",
      "29761 Training Loss: tensor(0.3257)\n",
      "29762 Training Loss: tensor(0.3257)\n",
      "29763 Training Loss: tensor(0.3257)\n",
      "29764 Training Loss: tensor(0.3260)\n",
      "29765 Training Loss: tensor(0.3269)\n",
      "29766 Training Loss: tensor(0.3258)\n",
      "29767 Training Loss: tensor(0.3256)\n",
      "29768 Training Loss: tensor(0.3269)\n",
      "29769 Training Loss: tensor(0.3261)\n",
      "29770 Training Loss: tensor(0.3271)\n",
      "29771 Training Loss: tensor(0.3258)\n",
      "29772 Training Loss: tensor(0.3269)\n",
      "29773 Training Loss: tensor(0.3259)\n",
      "29774 Training Loss: tensor(0.3257)\n",
      "29775 Training Loss: tensor(0.3264)\n",
      "29776 Training Loss: tensor(0.3260)\n",
      "29777 Training Loss: tensor(0.3257)\n",
      "29778 Training Loss: tensor(0.3260)\n",
      "29779 Training Loss: tensor(0.3280)\n",
      "29780 Training Loss: tensor(0.3262)\n",
      "29781 Training Loss: tensor(0.3256)\n",
      "29782 Training Loss: tensor(0.3265)\n",
      "29783 Training Loss: tensor(0.3257)\n",
      "29784 Training Loss: tensor(0.3257)\n",
      "29785 Training Loss: tensor(0.3260)\n",
      "29786 Training Loss: tensor(0.3267)\n",
      "29787 Training Loss: tensor(0.3257)\n",
      "29788 Training Loss: tensor(0.3260)\n",
      "29789 Training Loss: tensor(0.3261)\n",
      "29790 Training Loss: tensor(0.3265)\n",
      "29791 Training Loss: tensor(0.3261)\n",
      "29792 Training Loss: tensor(0.3260)\n",
      "29793 Training Loss: tensor(0.3258)\n",
      "29794 Training Loss: tensor(0.3257)\n",
      "29795 Training Loss: tensor(0.3256)\n",
      "29796 Training Loss: tensor(0.3260)\n",
      "29797 Training Loss: tensor(0.3259)\n",
      "29798 Training Loss: tensor(0.3285)\n",
      "29799 Training Loss: tensor(0.3256)\n",
      "29800 Training Loss: tensor(0.3258)\n",
      "29801 Training Loss: tensor(0.3260)\n",
      "29802 Training Loss: tensor(0.3260)\n",
      "29803 Training Loss: tensor(0.3258)\n",
      "29804 Training Loss: tensor(0.3260)\n",
      "29805 Training Loss: tensor(0.3257)\n",
      "29806 Training Loss: tensor(0.3264)\n",
      "29807 Training Loss: tensor(0.3259)\n",
      "29808 Training Loss: tensor(0.3275)\n",
      "29809 Training Loss: tensor(0.3257)\n",
      "29810 Training Loss: tensor(0.3263)\n",
      "29811 Training Loss: tensor(0.3268)\n",
      "29812 Training Loss: tensor(0.3265)\n",
      "29813 Training Loss: tensor(0.3261)\n",
      "29814 Training Loss: tensor(0.3259)\n",
      "29815 Training Loss: tensor(0.3261)\n",
      "29816 Training Loss: tensor(0.3261)\n",
      "29817 Training Loss: tensor(0.3261)\n",
      "29818 Training Loss: tensor(0.3261)\n",
      "29819 Training Loss: tensor(0.3257)\n",
      "29820 Training Loss: tensor(0.3268)\n",
      "29821 Training Loss: tensor(0.3261)\n",
      "29822 Training Loss: tensor(0.3268)\n",
      "29823 Training Loss: tensor(0.3256)\n",
      "29824 Training Loss: tensor(0.3273)\n",
      "29825 Training Loss: tensor(0.3256)\n",
      "29826 Training Loss: tensor(0.3258)\n",
      "29827 Training Loss: tensor(0.3271)\n",
      "29828 Training Loss: tensor(0.3256)\n",
      "29829 Training Loss: tensor(0.3259)\n",
      "29830 Training Loss: tensor(0.3270)\n",
      "29831 Training Loss: tensor(0.3256)\n",
      "29832 Training Loss: tensor(0.3292)\n",
      "29833 Training Loss: tensor(0.3258)\n",
      "29834 Training Loss: tensor(0.3264)\n",
      "29835 Training Loss: tensor(0.3270)\n",
      "29836 Training Loss: tensor(0.3268)\n",
      "29837 Training Loss: tensor(0.3264)\n",
      "29838 Training Loss: tensor(0.3261)\n",
      "29839 Training Loss: tensor(0.3272)\n",
      "29840 Training Loss: tensor(0.3263)\n",
      "29841 Training Loss: tensor(0.3259)\n",
      "29842 Training Loss: tensor(0.3261)\n",
      "29843 Training Loss: tensor(0.3271)\n",
      "29844 Training Loss: tensor(0.3260)\n",
      "29845 Training Loss: tensor(0.3261)\n",
      "29846 Training Loss: tensor(0.3260)\n",
      "29847 Training Loss: tensor(0.3258)\n",
      "29848 Training Loss: tensor(0.3267)\n",
      "29849 Training Loss: tensor(0.3257)\n",
      "29850 Training Loss: tensor(0.3260)\n",
      "29851 Training Loss: tensor(0.3255)\n",
      "29852 Training Loss: tensor(0.3258)\n",
      "29853 Training Loss: tensor(0.3266)\n",
      "29854 Training Loss: tensor(0.3257)\n",
      "29855 Training Loss: tensor(0.3259)\n",
      "29856 Training Loss: tensor(0.3280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29857 Training Loss: tensor(0.3257)\n",
      "29858 Training Loss: tensor(0.3281)\n",
      "29859 Training Loss: tensor(0.3268)\n",
      "29860 Training Loss: tensor(0.3277)\n",
      "29861 Training Loss: tensor(0.3261)\n",
      "29862 Training Loss: tensor(0.3264)\n",
      "29863 Training Loss: tensor(0.3272)\n",
      "29864 Training Loss: tensor(0.3265)\n",
      "29865 Training Loss: tensor(0.3261)\n",
      "29866 Training Loss: tensor(0.3284)\n",
      "29867 Training Loss: tensor(0.3263)\n",
      "29868 Training Loss: tensor(0.3258)\n",
      "29869 Training Loss: tensor(0.3268)\n",
      "29870 Training Loss: tensor(0.3262)\n",
      "29871 Training Loss: tensor(0.3264)\n",
      "29872 Training Loss: tensor(0.3261)\n",
      "29873 Training Loss: tensor(0.3260)\n",
      "29874 Training Loss: tensor(0.3267)\n",
      "29875 Training Loss: tensor(0.3259)\n",
      "29876 Training Loss: tensor(0.3265)\n",
      "29877 Training Loss: tensor(0.3260)\n",
      "29878 Training Loss: tensor(0.3264)\n",
      "29879 Training Loss: tensor(0.3272)\n",
      "29880 Training Loss: tensor(0.3260)\n",
      "29881 Training Loss: tensor(0.3259)\n",
      "29882 Training Loss: tensor(0.3258)\n",
      "29883 Training Loss: tensor(0.3296)\n",
      "29884 Training Loss: tensor(0.3257)\n",
      "29885 Training Loss: tensor(0.3260)\n",
      "29886 Training Loss: tensor(0.3260)\n",
      "29887 Training Loss: tensor(0.3265)\n",
      "29888 Training Loss: tensor(0.3260)\n",
      "29889 Training Loss: tensor(0.3260)\n",
      "29890 Training Loss: tensor(0.3260)\n",
      "29891 Training Loss: tensor(0.3261)\n",
      "29892 Training Loss: tensor(0.3258)\n",
      "29893 Training Loss: tensor(0.3262)\n",
      "29894 Training Loss: tensor(0.3257)\n",
      "29895 Training Loss: tensor(0.3262)\n",
      "29896 Training Loss: tensor(0.3257)\n",
      "29897 Training Loss: tensor(0.3257)\n",
      "29898 Training Loss: tensor(0.3258)\n",
      "29899 Training Loss: tensor(0.3261)\n",
      "29900 Training Loss: tensor(0.3256)\n",
      "29901 Training Loss: tensor(0.3256)\n",
      "29902 Training Loss: tensor(0.3270)\n",
      "29903 Training Loss: tensor(0.3258)\n",
      "29904 Training Loss: tensor(0.3295)\n",
      "29905 Training Loss: tensor(0.3257)\n",
      "29906 Training Loss: tensor(0.3276)\n",
      "29907 Training Loss: tensor(0.3256)\n",
      "29908 Training Loss: tensor(0.3261)\n",
      "29909 Training Loss: tensor(0.3259)\n",
      "29910 Training Loss: tensor(0.3259)\n",
      "29911 Training Loss: tensor(0.3256)\n",
      "29912 Training Loss: tensor(0.3257)\n",
      "29913 Training Loss: tensor(0.3261)\n",
      "29914 Training Loss: tensor(0.3264)\n",
      "29915 Training Loss: tensor(0.3260)\n",
      "29916 Training Loss: tensor(0.3259)\n",
      "29917 Training Loss: tensor(0.3257)\n",
      "29918 Training Loss: tensor(0.3256)\n",
      "29919 Training Loss: tensor(0.3277)\n",
      "29920 Training Loss: tensor(0.3262)\n",
      "29921 Training Loss: tensor(0.3267)\n",
      "29922 Training Loss: tensor(0.3256)\n",
      "29923 Training Loss: tensor(0.3274)\n",
      "29924 Training Loss: tensor(0.3258)\n",
      "29925 Training Loss: tensor(0.3259)\n",
      "29926 Training Loss: tensor(0.3258)\n",
      "29927 Training Loss: tensor(0.3264)\n",
      "29928 Training Loss: tensor(0.3259)\n",
      "29929 Training Loss: tensor(0.3260)\n",
      "29930 Training Loss: tensor(0.3258)\n",
      "29931 Training Loss: tensor(0.3288)\n",
      "29932 Training Loss: tensor(0.3260)\n",
      "29933 Training Loss: tensor(0.3271)\n",
      "29934 Training Loss: tensor(0.3260)\n",
      "29935 Training Loss: tensor(0.3259)\n",
      "29936 Training Loss: tensor(0.3262)\n",
      "29937 Training Loss: tensor(0.3256)\n",
      "29938 Training Loss: tensor(0.3268)\n",
      "29939 Training Loss: tensor(0.3269)\n",
      "29940 Training Loss: tensor(0.3257)\n",
      "29941 Training Loss: tensor(0.3279)\n",
      "29942 Training Loss: tensor(0.3258)\n",
      "29943 Training Loss: tensor(0.3265)\n",
      "29944 Training Loss: tensor(0.3263)\n",
      "29945 Training Loss: tensor(0.3257)\n",
      "29946 Training Loss: tensor(0.3261)\n",
      "29947 Training Loss: tensor(0.3263)\n",
      "29948 Training Loss: tensor(0.3260)\n",
      "29949 Training Loss: tensor(0.3259)\n",
      "29950 Training Loss: tensor(0.3262)\n",
      "29951 Training Loss: tensor(0.3262)\n",
      "29952 Training Loss: tensor(0.3267)\n",
      "29953 Training Loss: tensor(0.3260)\n",
      "29954 Training Loss: tensor(0.3260)\n",
      "29955 Training Loss: tensor(0.3267)\n",
      "29956 Training Loss: tensor(0.3272)\n",
      "29957 Training Loss: tensor(0.3274)\n",
      "29958 Training Loss: tensor(0.3260)\n",
      "29959 Training Loss: tensor(0.3262)\n",
      "29960 Training Loss: tensor(0.3258)\n",
      "29961 Training Loss: tensor(0.3263)\n",
      "29962 Training Loss: tensor(0.3264)\n",
      "29963 Training Loss: tensor(0.3264)\n",
      "29964 Training Loss: tensor(0.3257)\n",
      "29965 Training Loss: tensor(0.3257)\n",
      "29966 Training Loss: tensor(0.3259)\n",
      "29967 Training Loss: tensor(0.3259)\n",
      "29968 Training Loss: tensor(0.3259)\n",
      "29969 Training Loss: tensor(0.3260)\n",
      "29970 Training Loss: tensor(0.3257)\n",
      "29971 Training Loss: tensor(0.3261)\n",
      "29972 Training Loss: tensor(0.3263)\n",
      "29973 Training Loss: tensor(0.3260)\n",
      "29974 Training Loss: tensor(0.3256)\n",
      "29975 Training Loss: tensor(0.3271)\n",
      "29976 Training Loss: tensor(0.3256)\n",
      "29977 Training Loss: tensor(0.3263)\n",
      "29978 Training Loss: tensor(0.3267)\n",
      "29979 Training Loss: tensor(0.3257)\n",
      "29980 Training Loss: tensor(0.3259)\n",
      "29981 Training Loss: tensor(0.3265)\n",
      "29982 Training Loss: tensor(0.3259)\n",
      "29983 Training Loss: tensor(0.3258)\n",
      "29984 Training Loss: tensor(0.3269)\n",
      "29985 Training Loss: tensor(0.3257)\n",
      "29986 Training Loss: tensor(0.3261)\n",
      "29987 Training Loss: tensor(0.3258)\n",
      "29988 Training Loss: tensor(0.3262)\n",
      "29989 Training Loss: tensor(0.3259)\n",
      "29990 Training Loss: tensor(0.3263)\n",
      "29991 Training Loss: tensor(0.3261)\n",
      "29992 Training Loss: tensor(0.3281)\n",
      "29993 Training Loss: tensor(0.3259)\n",
      "29994 Training Loss: tensor(0.3259)\n",
      "29995 Training Loss: tensor(0.3260)\n",
      "29996 Training Loss: tensor(0.3257)\n",
      "29997 Training Loss: tensor(0.3259)\n",
      "29998 Training Loss: tensor(0.3265)\n",
      "29999 Training Loss: tensor(0.3260)\n",
      "30000 Training Loss: tensor(0.3259)\n",
      "30001 Training Loss: tensor(0.3263)\n",
      "30002 Training Loss: tensor(0.3265)\n",
      "30003 Training Loss: tensor(0.3258)\n",
      "30004 Training Loss: tensor(0.3259)\n",
      "30005 Training Loss: tensor(0.3257)\n",
      "30006 Training Loss: tensor(0.3260)\n",
      "30007 Training Loss: tensor(0.3259)\n",
      "30008 Training Loss: tensor(0.3285)\n",
      "30009 Training Loss: tensor(0.3257)\n",
      "30010 Training Loss: tensor(0.3258)\n",
      "30011 Training Loss: tensor(0.3257)\n",
      "30012 Training Loss: tensor(0.3259)\n",
      "30013 Training Loss: tensor(0.3265)\n",
      "30014 Training Loss: tensor(0.3259)\n",
      "30015 Training Loss: tensor(0.3271)\n",
      "30016 Training Loss: tensor(0.3258)\n",
      "30017 Training Loss: tensor(0.3262)\n",
      "30018 Training Loss: tensor(0.3261)\n",
      "30019 Training Loss: tensor(0.3259)\n",
      "30020 Training Loss: tensor(0.3259)\n",
      "30021 Training Loss: tensor(0.3265)\n",
      "30022 Training Loss: tensor(0.3261)\n",
      "30023 Training Loss: tensor(0.3258)\n",
      "30024 Training Loss: tensor(0.3260)\n",
      "30025 Training Loss: tensor(0.3265)\n",
      "30026 Training Loss: tensor(0.3259)\n",
      "30027 Training Loss: tensor(0.3261)\n",
      "30028 Training Loss: tensor(0.3260)\n",
      "30029 Training Loss: tensor(0.3267)\n",
      "30030 Training Loss: tensor(0.3257)\n",
      "30031 Training Loss: tensor(0.3258)\n",
      "30032 Training Loss: tensor(0.3257)\n",
      "30033 Training Loss: tensor(0.3265)\n",
      "30034 Training Loss: tensor(0.3266)\n",
      "30035 Training Loss: tensor(0.3278)\n",
      "30036 Training Loss: tensor(0.3267)\n",
      "30037 Training Loss: tensor(0.3268)\n",
      "30038 Training Loss: tensor(0.3255)\n",
      "30039 Training Loss: tensor(0.3258)\n",
      "30040 Training Loss: tensor(0.3257)\n",
      "30041 Training Loss: tensor(0.3258)\n",
      "30042 Training Loss: tensor(0.3259)\n",
      "30043 Training Loss: tensor(0.3259)\n",
      "30044 Training Loss: tensor(0.3256)\n",
      "30045 Training Loss: tensor(0.3267)\n",
      "30046 Training Loss: tensor(0.3256)\n",
      "30047 Training Loss: tensor(0.3262)\n",
      "30048 Training Loss: tensor(0.3259)\n",
      "30049 Training Loss: tensor(0.3259)\n",
      "30050 Training Loss: tensor(0.3266)\n",
      "30051 Training Loss: tensor(0.3262)\n",
      "30052 Training Loss: tensor(0.3268)\n",
      "30053 Training Loss: tensor(0.3264)\n",
      "30054 Training Loss: tensor(0.3258)\n",
      "30055 Training Loss: tensor(0.3258)\n",
      "30056 Training Loss: tensor(0.3260)\n",
      "30057 Training Loss: tensor(0.3258)\n",
      "30058 Training Loss: tensor(0.3258)\n",
      "30059 Training Loss: tensor(0.3257)\n",
      "30060 Training Loss: tensor(0.3277)\n",
      "30061 Training Loss: tensor(0.3275)\n",
      "30062 Training Loss: tensor(0.3259)\n",
      "30063 Training Loss: tensor(0.3258)\n",
      "30064 Training Loss: tensor(0.3267)\n",
      "30065 Training Loss: tensor(0.3260)\n",
      "30066 Training Loss: tensor(0.3257)\n",
      "30067 Training Loss: tensor(0.3255)\n",
      "30068 Training Loss: tensor(0.3276)\n",
      "30069 Training Loss: tensor(0.3257)\n",
      "30070 Training Loss: tensor(0.3260)\n",
      "30071 Training Loss: tensor(0.3262)\n",
      "30072 Training Loss: tensor(0.3262)\n",
      "30073 Training Loss: tensor(0.3259)\n",
      "30074 Training Loss: tensor(0.3265)\n",
      "30075 Training Loss: tensor(0.3258)\n",
      "30076 Training Loss: tensor(0.3264)\n",
      "30077 Training Loss: tensor(0.3262)\n",
      "30078 Training Loss: tensor(0.3259)\n",
      "30079 Training Loss: tensor(0.3265)\n",
      "30080 Training Loss: tensor(0.3265)\n",
      "30081 Training Loss: tensor(0.3258)\n",
      "30082 Training Loss: tensor(0.3256)\n",
      "30083 Training Loss: tensor(0.3258)\n",
      "30084 Training Loss: tensor(0.3260)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30085 Training Loss: tensor(0.3261)\n",
      "30086 Training Loss: tensor(0.3258)\n",
      "30087 Training Loss: tensor(0.3257)\n",
      "30088 Training Loss: tensor(0.3256)\n",
      "30089 Training Loss: tensor(0.3262)\n",
      "30090 Training Loss: tensor(0.3256)\n",
      "30091 Training Loss: tensor(0.3263)\n",
      "30092 Training Loss: tensor(0.3273)\n",
      "30093 Training Loss: tensor(0.3255)\n",
      "30094 Training Loss: tensor(0.3255)\n",
      "30095 Training Loss: tensor(0.3258)\n",
      "30096 Training Loss: tensor(0.3264)\n",
      "30097 Training Loss: tensor(0.3263)\n",
      "30098 Training Loss: tensor(0.3258)\n",
      "30099 Training Loss: tensor(0.3263)\n",
      "30100 Training Loss: tensor(0.3261)\n",
      "30101 Training Loss: tensor(0.3260)\n",
      "30102 Training Loss: tensor(0.3258)\n",
      "30103 Training Loss: tensor(0.3260)\n",
      "30104 Training Loss: tensor(0.3259)\n",
      "30105 Training Loss: tensor(0.3259)\n",
      "30106 Training Loss: tensor(0.3258)\n",
      "30107 Training Loss: tensor(0.3271)\n",
      "30108 Training Loss: tensor(0.3258)\n",
      "30109 Training Loss: tensor(0.3301)\n",
      "30110 Training Loss: tensor(0.3258)\n",
      "30111 Training Loss: tensor(0.3257)\n",
      "30112 Training Loss: tensor(0.3257)\n",
      "30113 Training Loss: tensor(0.3261)\n",
      "30114 Training Loss: tensor(0.3257)\n",
      "30115 Training Loss: tensor(0.3282)\n",
      "30116 Training Loss: tensor(0.3256)\n",
      "30117 Training Loss: tensor(0.3277)\n",
      "30118 Training Loss: tensor(0.3260)\n",
      "30119 Training Loss: tensor(0.3259)\n",
      "30120 Training Loss: tensor(0.3260)\n",
      "30121 Training Loss: tensor(0.3267)\n",
      "30122 Training Loss: tensor(0.3270)\n",
      "30123 Training Loss: tensor(0.3267)\n",
      "30124 Training Loss: tensor(0.3275)\n",
      "30125 Training Loss: tensor(0.3263)\n",
      "30126 Training Loss: tensor(0.3261)\n",
      "30127 Training Loss: tensor(0.3259)\n",
      "30128 Training Loss: tensor(0.3259)\n",
      "30129 Training Loss: tensor(0.3265)\n",
      "30130 Training Loss: tensor(0.3266)\n",
      "30131 Training Loss: tensor(0.3270)\n",
      "30132 Training Loss: tensor(0.3262)\n",
      "30133 Training Loss: tensor(0.3262)\n",
      "30134 Training Loss: tensor(0.3270)\n",
      "30135 Training Loss: tensor(0.3260)\n",
      "30136 Training Loss: tensor(0.3273)\n",
      "30137 Training Loss: tensor(0.3263)\n",
      "30138 Training Loss: tensor(0.3258)\n",
      "30139 Training Loss: tensor(0.3263)\n",
      "30140 Training Loss: tensor(0.3256)\n",
      "30141 Training Loss: tensor(0.3258)\n",
      "30142 Training Loss: tensor(0.3272)\n",
      "30143 Training Loss: tensor(0.3263)\n",
      "30144 Training Loss: tensor(0.3260)\n",
      "30145 Training Loss: tensor(0.3261)\n",
      "30146 Training Loss: tensor(0.3261)\n",
      "30147 Training Loss: tensor(0.3259)\n",
      "30148 Training Loss: tensor(0.3256)\n",
      "30149 Training Loss: tensor(0.3260)\n",
      "30150 Training Loss: tensor(0.3260)\n",
      "30151 Training Loss: tensor(0.3283)\n",
      "30152 Training Loss: tensor(0.3259)\n",
      "30153 Training Loss: tensor(0.3258)\n",
      "30154 Training Loss: tensor(0.3256)\n",
      "30155 Training Loss: tensor(0.3275)\n",
      "30156 Training Loss: tensor(0.3257)\n",
      "30157 Training Loss: tensor(0.3282)\n",
      "30158 Training Loss: tensor(0.3261)\n",
      "30159 Training Loss: tensor(0.3261)\n",
      "30160 Training Loss: tensor(0.3284)\n",
      "30161 Training Loss: tensor(0.3268)\n",
      "30162 Training Loss: tensor(0.3259)\n",
      "30163 Training Loss: tensor(0.3261)\n",
      "30164 Training Loss: tensor(0.3264)\n",
      "30165 Training Loss: tensor(0.3265)\n",
      "30166 Training Loss: tensor(0.3274)\n",
      "30167 Training Loss: tensor(0.3259)\n",
      "30168 Training Loss: tensor(0.3261)\n",
      "30169 Training Loss: tensor(0.3263)\n",
      "30170 Training Loss: tensor(0.3265)\n",
      "30171 Training Loss: tensor(0.3263)\n",
      "30172 Training Loss: tensor(0.3261)\n",
      "30173 Training Loss: tensor(0.3265)\n",
      "30174 Training Loss: tensor(0.3265)\n",
      "30175 Training Loss: tensor(0.3257)\n",
      "30176 Training Loss: tensor(0.3279)\n",
      "30177 Training Loss: tensor(0.3260)\n",
      "30178 Training Loss: tensor(0.3270)\n",
      "30179 Training Loss: tensor(0.3268)\n",
      "30180 Training Loss: tensor(0.3257)\n",
      "30181 Training Loss: tensor(0.3273)\n",
      "30182 Training Loss: tensor(0.3260)\n",
      "30183 Training Loss: tensor(0.3264)\n",
      "30184 Training Loss: tensor(0.3277)\n",
      "30185 Training Loss: tensor(0.3273)\n",
      "30186 Training Loss: tensor(0.3262)\n",
      "30187 Training Loss: tensor(0.3259)\n",
      "30188 Training Loss: tensor(0.3262)\n",
      "30189 Training Loss: tensor(0.3263)\n",
      "30190 Training Loss: tensor(0.3265)\n",
      "30191 Training Loss: tensor(0.3277)\n",
      "30192 Training Loss: tensor(0.3257)\n",
      "30193 Training Loss: tensor(0.3260)\n",
      "30194 Training Loss: tensor(0.3265)\n",
      "30195 Training Loss: tensor(0.3257)\n",
      "30196 Training Loss: tensor(0.3265)\n",
      "30197 Training Loss: tensor(0.3267)\n",
      "30198 Training Loss: tensor(0.3267)\n",
      "30199 Training Loss: tensor(0.3261)\n",
      "30200 Training Loss: tensor(0.3260)\n",
      "30201 Training Loss: tensor(0.3261)\n",
      "30202 Training Loss: tensor(0.3258)\n",
      "30203 Training Loss: tensor(0.3258)\n",
      "30204 Training Loss: tensor(0.3258)\n",
      "30205 Training Loss: tensor(0.3260)\n",
      "30206 Training Loss: tensor(0.3259)\n",
      "30207 Training Loss: tensor(0.3262)\n",
      "30208 Training Loss: tensor(0.3263)\n",
      "30209 Training Loss: tensor(0.3259)\n",
      "30210 Training Loss: tensor(0.3258)\n",
      "30211 Training Loss: tensor(0.3257)\n",
      "30212 Training Loss: tensor(0.3259)\n",
      "30213 Training Loss: tensor(0.3266)\n",
      "30214 Training Loss: tensor(0.3259)\n",
      "30215 Training Loss: tensor(0.3259)\n",
      "30216 Training Loss: tensor(0.3256)\n",
      "30217 Training Loss: tensor(0.3259)\n",
      "30218 Training Loss: tensor(0.3257)\n",
      "30219 Training Loss: tensor(0.3261)\n",
      "30220 Training Loss: tensor(0.3260)\n",
      "30221 Training Loss: tensor(0.3265)\n",
      "30222 Training Loss: tensor(0.3256)\n",
      "30223 Training Loss: tensor(0.3269)\n",
      "30224 Training Loss: tensor(0.3256)\n",
      "30225 Training Loss: tensor(0.3257)\n",
      "30226 Training Loss: tensor(0.3266)\n",
      "30227 Training Loss: tensor(0.3269)\n",
      "30228 Training Loss: tensor(0.3260)\n",
      "30229 Training Loss: tensor(0.3258)\n",
      "30230 Training Loss: tensor(0.3267)\n",
      "30231 Training Loss: tensor(0.3265)\n",
      "30232 Training Loss: tensor(0.3264)\n",
      "30233 Training Loss: tensor(0.3258)\n",
      "30234 Training Loss: tensor(0.3270)\n",
      "30235 Training Loss: tensor(0.3259)\n",
      "30236 Training Loss: tensor(0.3262)\n",
      "30237 Training Loss: tensor(0.3264)\n",
      "30238 Training Loss: tensor(0.3262)\n",
      "30239 Training Loss: tensor(0.3258)\n",
      "30240 Training Loss: tensor(0.3274)\n",
      "30241 Training Loss: tensor(0.3268)\n",
      "30242 Training Loss: tensor(0.3261)\n",
      "30243 Training Loss: tensor(0.3269)\n",
      "30244 Training Loss: tensor(0.3258)\n",
      "30245 Training Loss: tensor(0.3263)\n",
      "30246 Training Loss: tensor(0.3261)\n",
      "30247 Training Loss: tensor(0.3260)\n",
      "30248 Training Loss: tensor(0.3262)\n",
      "30249 Training Loss: tensor(0.3260)\n",
      "30250 Training Loss: tensor(0.3260)\n",
      "30251 Training Loss: tensor(0.3259)\n",
      "30252 Training Loss: tensor(0.3266)\n",
      "30253 Training Loss: tensor(0.3259)\n",
      "30254 Training Loss: tensor(0.3260)\n",
      "30255 Training Loss: tensor(0.3256)\n",
      "30256 Training Loss: tensor(0.3258)\n",
      "30257 Training Loss: tensor(0.3259)\n",
      "30258 Training Loss: tensor(0.3257)\n",
      "30259 Training Loss: tensor(0.3260)\n",
      "30260 Training Loss: tensor(0.3259)\n",
      "30261 Training Loss: tensor(0.3258)\n",
      "30262 Training Loss: tensor(0.3256)\n",
      "30263 Training Loss: tensor(0.3259)\n",
      "30264 Training Loss: tensor(0.3254)\n",
      "30265 Training Loss: tensor(0.3259)\n",
      "30266 Training Loss: tensor(0.3267)\n",
      "30267 Training Loss: tensor(0.3265)\n",
      "30268 Training Loss: tensor(0.3265)\n",
      "30269 Training Loss: tensor(0.3259)\n",
      "30270 Training Loss: tensor(0.3257)\n",
      "30271 Training Loss: tensor(0.3278)\n",
      "30272 Training Loss: tensor(0.3259)\n",
      "30273 Training Loss: tensor(0.3258)\n",
      "30274 Training Loss: tensor(0.3261)\n",
      "30275 Training Loss: tensor(0.3265)\n",
      "30276 Training Loss: tensor(0.3261)\n",
      "30277 Training Loss: tensor(0.3260)\n",
      "30278 Training Loss: tensor(0.3263)\n",
      "30279 Training Loss: tensor(0.3259)\n",
      "30280 Training Loss: tensor(0.3258)\n",
      "30281 Training Loss: tensor(0.3263)\n",
      "30282 Training Loss: tensor(0.3269)\n",
      "30283 Training Loss: tensor(0.3260)\n",
      "30284 Training Loss: tensor(0.3263)\n",
      "30285 Training Loss: tensor(0.3255)\n",
      "30286 Training Loss: tensor(0.3259)\n",
      "30287 Training Loss: tensor(0.3259)\n",
      "30288 Training Loss: tensor(0.3262)\n",
      "30289 Training Loss: tensor(0.3262)\n",
      "30290 Training Loss: tensor(0.3258)\n",
      "30291 Training Loss: tensor(0.3261)\n",
      "30292 Training Loss: tensor(0.3256)\n",
      "30293 Training Loss: tensor(0.3263)\n",
      "30294 Training Loss: tensor(0.3256)\n",
      "30295 Training Loss: tensor(0.3256)\n",
      "30296 Training Loss: tensor(0.3255)\n",
      "30297 Training Loss: tensor(0.3276)\n",
      "30298 Training Loss: tensor(0.3265)\n",
      "30299 Training Loss: tensor(0.3257)\n",
      "30300 Training Loss: tensor(0.3264)\n",
      "30301 Training Loss: tensor(0.3275)\n",
      "30302 Training Loss: tensor(0.3299)\n",
      "30303 Training Loss: tensor(0.3257)\n",
      "30304 Training Loss: tensor(0.3271)\n",
      "30305 Training Loss: tensor(0.3267)\n",
      "30306 Training Loss: tensor(0.3261)\n",
      "30307 Training Loss: tensor(0.3266)\n",
      "30308 Training Loss: tensor(0.3261)\n",
      "30309 Training Loss: tensor(0.3264)\n",
      "30310 Training Loss: tensor(0.3265)\n",
      "30311 Training Loss: tensor(0.3256)\n",
      "30312 Training Loss: tensor(0.3258)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30313 Training Loss: tensor(0.3262)\n",
      "30314 Training Loss: tensor(0.3267)\n",
      "30315 Training Loss: tensor(0.3259)\n",
      "30316 Training Loss: tensor(0.3257)\n",
      "30317 Training Loss: tensor(0.3259)\n",
      "30318 Training Loss: tensor(0.3262)\n",
      "30319 Training Loss: tensor(0.3272)\n",
      "30320 Training Loss: tensor(0.3257)\n",
      "30321 Training Loss: tensor(0.3257)\n",
      "30322 Training Loss: tensor(0.3270)\n",
      "30323 Training Loss: tensor(0.3276)\n",
      "30324 Training Loss: tensor(0.3259)\n",
      "30325 Training Loss: tensor(0.3259)\n",
      "30326 Training Loss: tensor(0.3268)\n",
      "30327 Training Loss: tensor(0.3264)\n",
      "30328 Training Loss: tensor(0.3256)\n",
      "30329 Training Loss: tensor(0.3259)\n",
      "30330 Training Loss: tensor(0.3259)\n",
      "30331 Training Loss: tensor(0.3267)\n",
      "30332 Training Loss: tensor(0.3260)\n",
      "30333 Training Loss: tensor(0.3258)\n",
      "30334 Training Loss: tensor(0.3257)\n",
      "30335 Training Loss: tensor(0.3261)\n",
      "30336 Training Loss: tensor(0.3257)\n",
      "30337 Training Loss: tensor(0.3266)\n",
      "30338 Training Loss: tensor(0.3260)\n",
      "30339 Training Loss: tensor(0.3257)\n",
      "30340 Training Loss: tensor(0.3258)\n",
      "30341 Training Loss: tensor(0.3258)\n",
      "30342 Training Loss: tensor(0.3254)\n",
      "30343 Training Loss: tensor(0.3258)\n",
      "30344 Training Loss: tensor(0.3257)\n",
      "30345 Training Loss: tensor(0.3273)\n",
      "30346 Training Loss: tensor(0.3256)\n",
      "30347 Training Loss: tensor(0.3261)\n",
      "30348 Training Loss: tensor(0.3260)\n",
      "30349 Training Loss: tensor(0.3261)\n",
      "30350 Training Loss: tensor(0.3258)\n",
      "30351 Training Loss: tensor(0.3256)\n",
      "30352 Training Loss: tensor(0.3258)\n",
      "30353 Training Loss: tensor(0.3261)\n",
      "30354 Training Loss: tensor(0.3256)\n",
      "30355 Training Loss: tensor(0.3258)\n",
      "30356 Training Loss: tensor(0.3258)\n",
      "30357 Training Loss: tensor(0.3256)\n",
      "30358 Training Loss: tensor(0.3268)\n",
      "30359 Training Loss: tensor(0.3256)\n",
      "30360 Training Loss: tensor(0.3259)\n",
      "30361 Training Loss: tensor(0.3260)\n",
      "30362 Training Loss: tensor(0.3261)\n",
      "30363 Training Loss: tensor(0.3266)\n",
      "30364 Training Loss: tensor(0.3280)\n",
      "30365 Training Loss: tensor(0.3264)\n",
      "30366 Training Loss: tensor(0.3256)\n",
      "30367 Training Loss: tensor(0.3259)\n",
      "30368 Training Loss: tensor(0.3259)\n",
      "30369 Training Loss: tensor(0.3258)\n",
      "30370 Training Loss: tensor(0.3256)\n",
      "30371 Training Loss: tensor(0.3258)\n",
      "30372 Training Loss: tensor(0.3267)\n",
      "30373 Training Loss: tensor(0.3264)\n",
      "30374 Training Loss: tensor(0.3257)\n",
      "30375 Training Loss: tensor(0.3265)\n",
      "30376 Training Loss: tensor(0.3257)\n",
      "30377 Training Loss: tensor(0.3256)\n",
      "30378 Training Loss: tensor(0.3256)\n",
      "30379 Training Loss: tensor(0.3257)\n",
      "30380 Training Loss: tensor(0.3256)\n",
      "30381 Training Loss: tensor(0.3258)\n",
      "30382 Training Loss: tensor(0.3265)\n",
      "30383 Training Loss: tensor(0.3257)\n",
      "30384 Training Loss: tensor(0.3262)\n",
      "30385 Training Loss: tensor(0.3264)\n",
      "30386 Training Loss: tensor(0.3256)\n",
      "30387 Training Loss: tensor(0.3255)\n",
      "30388 Training Loss: tensor(0.3278)\n",
      "30389 Training Loss: tensor(0.3256)\n",
      "30390 Training Loss: tensor(0.3264)\n",
      "30391 Training Loss: tensor(0.3257)\n",
      "30392 Training Loss: tensor(0.3258)\n",
      "30393 Training Loss: tensor(0.3257)\n",
      "30394 Training Loss: tensor(0.3263)\n",
      "30395 Training Loss: tensor(0.3260)\n",
      "30396 Training Loss: tensor(0.3258)\n",
      "30397 Training Loss: tensor(0.3261)\n",
      "30398 Training Loss: tensor(0.3260)\n",
      "30399 Training Loss: tensor(0.3258)\n",
      "30400 Training Loss: tensor(0.3257)\n",
      "30401 Training Loss: tensor(0.3261)\n",
      "30402 Training Loss: tensor(0.3266)\n",
      "30403 Training Loss: tensor(0.3258)\n",
      "30404 Training Loss: tensor(0.3258)\n",
      "30405 Training Loss: tensor(0.3260)\n",
      "30406 Training Loss: tensor(0.3265)\n",
      "30407 Training Loss: tensor(0.3260)\n",
      "30408 Training Loss: tensor(0.3257)\n",
      "30409 Training Loss: tensor(0.3269)\n",
      "30410 Training Loss: tensor(0.3257)\n",
      "30411 Training Loss: tensor(0.3266)\n",
      "30412 Training Loss: tensor(0.3260)\n",
      "30413 Training Loss: tensor(0.3255)\n",
      "30414 Training Loss: tensor(0.3256)\n",
      "30415 Training Loss: tensor(0.3256)\n",
      "30416 Training Loss: tensor(0.3267)\n",
      "30417 Training Loss: tensor(0.3258)\n",
      "30418 Training Loss: tensor(0.3270)\n",
      "30419 Training Loss: tensor(0.3261)\n",
      "30420 Training Loss: tensor(0.3257)\n",
      "30421 Training Loss: tensor(0.3257)\n",
      "30422 Training Loss: tensor(0.3256)\n",
      "30423 Training Loss: tensor(0.3263)\n",
      "30424 Training Loss: tensor(0.3263)\n",
      "30425 Training Loss: tensor(0.3259)\n",
      "30426 Training Loss: tensor(0.3256)\n",
      "30427 Training Loss: tensor(0.3259)\n",
      "30428 Training Loss: tensor(0.3257)\n",
      "30429 Training Loss: tensor(0.3258)\n",
      "30430 Training Loss: tensor(0.3255)\n",
      "30431 Training Loss: tensor(0.3255)\n",
      "30432 Training Loss: tensor(0.3261)\n",
      "30433 Training Loss: tensor(0.3256)\n",
      "30434 Training Loss: tensor(0.3268)\n",
      "30435 Training Loss: tensor(0.3263)\n",
      "30436 Training Loss: tensor(0.3259)\n",
      "30437 Training Loss: tensor(0.3287)\n",
      "30438 Training Loss: tensor(0.3277)\n",
      "30439 Training Loss: tensor(0.3260)\n",
      "30440 Training Loss: tensor(0.3257)\n",
      "30441 Training Loss: tensor(0.3255)\n",
      "30442 Training Loss: tensor(0.3260)\n",
      "30443 Training Loss: tensor(0.3262)\n",
      "30444 Training Loss: tensor(0.3256)\n",
      "30445 Training Loss: tensor(0.3263)\n",
      "30446 Training Loss: tensor(0.3261)\n",
      "30447 Training Loss: tensor(0.3269)\n",
      "30448 Training Loss: tensor(0.3258)\n",
      "30449 Training Loss: tensor(0.3255)\n",
      "30450 Training Loss: tensor(0.3265)\n",
      "30451 Training Loss: tensor(0.3258)\n",
      "30452 Training Loss: tensor(0.3293)\n",
      "30453 Training Loss: tensor(0.3265)\n",
      "30454 Training Loss: tensor(0.3266)\n",
      "30455 Training Loss: tensor(0.3265)\n",
      "30456 Training Loss: tensor(0.3265)\n",
      "30457 Training Loss: tensor(0.3265)\n",
      "30458 Training Loss: tensor(0.3259)\n",
      "30459 Training Loss: tensor(0.3258)\n",
      "30460 Training Loss: tensor(0.3262)\n",
      "30461 Training Loss: tensor(0.3266)\n",
      "30462 Training Loss: tensor(0.3261)\n",
      "30463 Training Loss: tensor(0.3259)\n",
      "30464 Training Loss: tensor(0.3258)\n",
      "30465 Training Loss: tensor(0.3257)\n",
      "30466 Training Loss: tensor(0.3279)\n",
      "30467 Training Loss: tensor(0.3264)\n",
      "30468 Training Loss: tensor(0.3263)\n",
      "30469 Training Loss: tensor(0.3273)\n",
      "30470 Training Loss: tensor(0.3263)\n",
      "30471 Training Loss: tensor(0.3260)\n",
      "30472 Training Loss: tensor(0.3259)\n",
      "30473 Training Loss: tensor(0.3257)\n",
      "30474 Training Loss: tensor(0.3262)\n",
      "30475 Training Loss: tensor(0.3258)\n",
      "30476 Training Loss: tensor(0.3257)\n",
      "30477 Training Loss: tensor(0.3260)\n",
      "30478 Training Loss: tensor(0.3256)\n",
      "30479 Training Loss: tensor(0.3258)\n",
      "30480 Training Loss: tensor(0.3261)\n",
      "30481 Training Loss: tensor(0.3260)\n",
      "30482 Training Loss: tensor(0.3258)\n",
      "30483 Training Loss: tensor(0.3256)\n",
      "30484 Training Loss: tensor(0.3265)\n",
      "30485 Training Loss: tensor(0.3259)\n",
      "30486 Training Loss: tensor(0.3270)\n",
      "30487 Training Loss: tensor(0.3261)\n",
      "30488 Training Loss: tensor(0.3255)\n",
      "30489 Training Loss: tensor(0.3292)\n",
      "30490 Training Loss: tensor(0.3262)\n",
      "30491 Training Loss: tensor(0.3257)\n",
      "30492 Training Loss: tensor(0.3264)\n",
      "30493 Training Loss: tensor(0.3266)\n",
      "30494 Training Loss: tensor(0.3262)\n",
      "30495 Training Loss: tensor(0.3272)\n",
      "30496 Training Loss: tensor(0.3257)\n",
      "30497 Training Loss: tensor(0.3258)\n",
      "30498 Training Loss: tensor(0.3258)\n",
      "30499 Training Loss: tensor(0.3263)\n",
      "30500 Training Loss: tensor(0.3258)\n",
      "30501 Training Loss: tensor(0.3258)\n",
      "30502 Training Loss: tensor(0.3261)\n",
      "30503 Training Loss: tensor(0.3265)\n",
      "30504 Training Loss: tensor(0.3267)\n",
      "30505 Training Loss: tensor(0.3256)\n",
      "30506 Training Loss: tensor(0.3264)\n",
      "30507 Training Loss: tensor(0.3257)\n",
      "30508 Training Loss: tensor(0.3258)\n",
      "30509 Training Loss: tensor(0.3276)\n",
      "30510 Training Loss: tensor(0.3265)\n",
      "30511 Training Loss: tensor(0.3260)\n",
      "30512 Training Loss: tensor(0.3260)\n",
      "30513 Training Loss: tensor(0.3257)\n",
      "30514 Training Loss: tensor(0.3267)\n",
      "30515 Training Loss: tensor(0.3273)\n",
      "30516 Training Loss: tensor(0.3258)\n",
      "30517 Training Loss: tensor(0.3257)\n",
      "30518 Training Loss: tensor(0.3258)\n",
      "30519 Training Loss: tensor(0.3303)\n",
      "30520 Training Loss: tensor(0.3257)\n",
      "30521 Training Loss: tensor(0.3262)\n",
      "30522 Training Loss: tensor(0.3265)\n",
      "30523 Training Loss: tensor(0.3277)\n",
      "30524 Training Loss: tensor(0.3273)\n",
      "30525 Training Loss: tensor(0.3261)\n",
      "30526 Training Loss: tensor(0.3263)\n",
      "30527 Training Loss: tensor(0.3256)\n",
      "30528 Training Loss: tensor(0.3259)\n",
      "30529 Training Loss: tensor(0.3265)\n",
      "30530 Training Loss: tensor(0.3266)\n",
      "30531 Training Loss: tensor(0.3260)\n",
      "30532 Training Loss: tensor(0.3257)\n",
      "30533 Training Loss: tensor(0.3257)\n",
      "30534 Training Loss: tensor(0.3269)\n",
      "30535 Training Loss: tensor(0.3258)\n",
      "30536 Training Loss: tensor(0.3259)\n",
      "30537 Training Loss: tensor(0.3270)\n",
      "30538 Training Loss: tensor(0.3269)\n",
      "30539 Training Loss: tensor(0.3258)\n",
      "30540 Training Loss: tensor(0.3260)\n",
      "30541 Training Loss: tensor(0.3259)\n",
      "30542 Training Loss: tensor(0.3260)\n",
      "30543 Training Loss: tensor(0.3256)\n",
      "30544 Training Loss: tensor(0.3256)\n",
      "30545 Training Loss: tensor(0.3257)\n",
      "30546 Training Loss: tensor(0.3256)\n",
      "30547 Training Loss: tensor(0.3257)\n",
      "30548 Training Loss: tensor(0.3261)\n",
      "30549 Training Loss: tensor(0.3259)\n",
      "30550 Training Loss: tensor(0.3261)\n",
      "30551 Training Loss: tensor(0.3261)\n",
      "30552 Training Loss: tensor(0.3256)\n",
      "30553 Training Loss: tensor(0.3257)\n",
      "30554 Training Loss: tensor(0.3258)\n",
      "30555 Training Loss: tensor(0.3258)\n",
      "30556 Training Loss: tensor(0.3258)\n",
      "30557 Training Loss: tensor(0.3262)\n",
      "30558 Training Loss: tensor(0.3256)\n",
      "30559 Training Loss: tensor(0.3258)\n",
      "30560 Training Loss: tensor(0.3268)\n",
      "30561 Training Loss: tensor(0.3256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30562 Training Loss: tensor(0.3273)\n",
      "30563 Training Loss: tensor(0.3276)\n",
      "30564 Training Loss: tensor(0.3256)\n",
      "30565 Training Loss: tensor(0.3259)\n",
      "30566 Training Loss: tensor(0.3259)\n",
      "30567 Training Loss: tensor(0.3260)\n",
      "30568 Training Loss: tensor(0.3262)\n",
      "30569 Training Loss: tensor(0.3260)\n",
      "30570 Training Loss: tensor(0.3263)\n",
      "30571 Training Loss: tensor(0.3260)\n",
      "30572 Training Loss: tensor(0.3257)\n",
      "30573 Training Loss: tensor(0.3258)\n",
      "30574 Training Loss: tensor(0.3266)\n",
      "30575 Training Loss: tensor(0.3258)\n",
      "30576 Training Loss: tensor(0.3257)\n",
      "30577 Training Loss: tensor(0.3255)\n",
      "30578 Training Loss: tensor(0.3269)\n",
      "30579 Training Loss: tensor(0.3258)\n",
      "30580 Training Loss: tensor(0.3255)\n",
      "30581 Training Loss: tensor(0.3264)\n",
      "30582 Training Loss: tensor(0.3275)\n",
      "30583 Training Loss: tensor(0.3255)\n",
      "30584 Training Loss: tensor(0.3262)\n",
      "30585 Training Loss: tensor(0.3261)\n",
      "30586 Training Loss: tensor(0.3259)\n",
      "30587 Training Loss: tensor(0.3261)\n",
      "30588 Training Loss: tensor(0.3259)\n",
      "30589 Training Loss: tensor(0.3260)\n",
      "30590 Training Loss: tensor(0.3264)\n",
      "30591 Training Loss: tensor(0.3257)\n",
      "30592 Training Loss: tensor(0.3262)\n",
      "30593 Training Loss: tensor(0.3263)\n",
      "30594 Training Loss: tensor(0.3262)\n",
      "30595 Training Loss: tensor(0.3273)\n",
      "30596 Training Loss: tensor(0.3258)\n",
      "30597 Training Loss: tensor(0.3258)\n",
      "30598 Training Loss: tensor(0.3257)\n",
      "30599 Training Loss: tensor(0.3263)\n",
      "30600 Training Loss: tensor(0.3264)\n",
      "30601 Training Loss: tensor(0.3281)\n",
      "30602 Training Loss: tensor(0.3274)\n",
      "30603 Training Loss: tensor(0.3265)\n",
      "30604 Training Loss: tensor(0.3262)\n",
      "30605 Training Loss: tensor(0.3257)\n",
      "30606 Training Loss: tensor(0.3261)\n",
      "30607 Training Loss: tensor(0.3257)\n",
      "30608 Training Loss: tensor(0.3262)\n",
      "30609 Training Loss: tensor(0.3255)\n",
      "30610 Training Loss: tensor(0.3259)\n",
      "30611 Training Loss: tensor(0.3258)\n",
      "30612 Training Loss: tensor(0.3257)\n",
      "30613 Training Loss: tensor(0.3260)\n",
      "30614 Training Loss: tensor(0.3259)\n",
      "30615 Training Loss: tensor(0.3260)\n",
      "30616 Training Loss: tensor(0.3256)\n",
      "30617 Training Loss: tensor(0.3256)\n",
      "30618 Training Loss: tensor(0.3260)\n",
      "30619 Training Loss: tensor(0.3259)\n",
      "30620 Training Loss: tensor(0.3264)\n",
      "30621 Training Loss: tensor(0.3255)\n",
      "30622 Training Loss: tensor(0.3279)\n",
      "30623 Training Loss: tensor(0.3263)\n",
      "30624 Training Loss: tensor(0.3264)\n",
      "30625 Training Loss: tensor(0.3257)\n",
      "30626 Training Loss: tensor(0.3266)\n",
      "30627 Training Loss: tensor(0.3255)\n",
      "30628 Training Loss: tensor(0.3260)\n",
      "30629 Training Loss: tensor(0.3270)\n",
      "30630 Training Loss: tensor(0.3263)\n",
      "30631 Training Loss: tensor(0.3256)\n",
      "30632 Training Loss: tensor(0.3264)\n",
      "30633 Training Loss: tensor(0.3267)\n",
      "30634 Training Loss: tensor(0.3258)\n",
      "30635 Training Loss: tensor(0.3256)\n",
      "30636 Training Loss: tensor(0.3258)\n",
      "30637 Training Loss: tensor(0.3273)\n",
      "30638 Training Loss: tensor(0.3256)\n",
      "30639 Training Loss: tensor(0.3258)\n",
      "30640 Training Loss: tensor(0.3258)\n",
      "30641 Training Loss: tensor(0.3256)\n",
      "30642 Training Loss: tensor(0.3259)\n",
      "30643 Training Loss: tensor(0.3263)\n",
      "30644 Training Loss: tensor(0.3258)\n",
      "30645 Training Loss: tensor(0.3259)\n",
      "30646 Training Loss: tensor(0.3255)\n",
      "30647 Training Loss: tensor(0.3262)\n",
      "30648 Training Loss: tensor(0.3260)\n",
      "30649 Training Loss: tensor(0.3270)\n",
      "30650 Training Loss: tensor(0.3256)\n",
      "30651 Training Loss: tensor(0.3255)\n",
      "30652 Training Loss: tensor(0.3278)\n",
      "30653 Training Loss: tensor(0.3258)\n",
      "30654 Training Loss: tensor(0.3292)\n",
      "30655 Training Loss: tensor(0.3259)\n",
      "30656 Training Loss: tensor(0.3260)\n",
      "30657 Training Loss: tensor(0.3263)\n",
      "30658 Training Loss: tensor(0.3266)\n",
      "30659 Training Loss: tensor(0.3265)\n",
      "30660 Training Loss: tensor(0.3259)\n",
      "30661 Training Loss: tensor(0.3267)\n",
      "30662 Training Loss: tensor(0.3263)\n",
      "30663 Training Loss: tensor(0.3264)\n",
      "30664 Training Loss: tensor(0.3268)\n",
      "30665 Training Loss: tensor(0.3258)\n",
      "30666 Training Loss: tensor(0.3257)\n",
      "30667 Training Loss: tensor(0.3265)\n",
      "30668 Training Loss: tensor(0.3260)\n",
      "30669 Training Loss: tensor(0.3262)\n",
      "30670 Training Loss: tensor(0.3270)\n",
      "30671 Training Loss: tensor(0.3258)\n",
      "30672 Training Loss: tensor(0.3258)\n",
      "30673 Training Loss: tensor(0.3262)\n",
      "30674 Training Loss: tensor(0.3255)\n",
      "30675 Training Loss: tensor(0.3264)\n",
      "30676 Training Loss: tensor(0.3267)\n",
      "30677 Training Loss: tensor(0.3260)\n",
      "30678 Training Loss: tensor(0.3256)\n",
      "30679 Training Loss: tensor(0.3256)\n",
      "30680 Training Loss: tensor(0.3257)\n",
      "30681 Training Loss: tensor(0.3257)\n",
      "30682 Training Loss: tensor(0.3259)\n",
      "30683 Training Loss: tensor(0.3279)\n",
      "30684 Training Loss: tensor(0.3267)\n",
      "30685 Training Loss: tensor(0.3256)\n",
      "30686 Training Loss: tensor(0.3258)\n",
      "30687 Training Loss: tensor(0.3256)\n",
      "30688 Training Loss: tensor(0.3272)\n",
      "30689 Training Loss: tensor(0.3259)\n",
      "30690 Training Loss: tensor(0.3261)\n",
      "30691 Training Loss: tensor(0.3257)\n",
      "30692 Training Loss: tensor(0.3257)\n",
      "30693 Training Loss: tensor(0.3260)\n",
      "30694 Training Loss: tensor(0.3256)\n",
      "30695 Training Loss: tensor(0.3258)\n",
      "30696 Training Loss: tensor(0.3263)\n",
      "30697 Training Loss: tensor(0.3258)\n",
      "30698 Training Loss: tensor(0.3260)\n",
      "30699 Training Loss: tensor(0.3257)\n",
      "30700 Training Loss: tensor(0.3267)\n",
      "30701 Training Loss: tensor(0.3260)\n",
      "30702 Training Loss: tensor(0.3268)\n",
      "30703 Training Loss: tensor(0.3257)\n",
      "30704 Training Loss: tensor(0.3262)\n",
      "30705 Training Loss: tensor(0.3258)\n",
      "30706 Training Loss: tensor(0.3262)\n",
      "30707 Training Loss: tensor(0.3260)\n",
      "30708 Training Loss: tensor(0.3258)\n",
      "30709 Training Loss: tensor(0.3262)\n",
      "30710 Training Loss: tensor(0.3255)\n",
      "30711 Training Loss: tensor(0.3258)\n",
      "30712 Training Loss: tensor(0.3263)\n",
      "30713 Training Loss: tensor(0.3261)\n",
      "30714 Training Loss: tensor(0.3264)\n",
      "30715 Training Loss: tensor(0.3259)\n",
      "30716 Training Loss: tensor(0.3257)\n",
      "30717 Training Loss: tensor(0.3259)\n",
      "30718 Training Loss: tensor(0.3257)\n",
      "30719 Training Loss: tensor(0.3256)\n",
      "30720 Training Loss: tensor(0.3258)\n",
      "30721 Training Loss: tensor(0.3282)\n",
      "30722 Training Loss: tensor(0.3257)\n",
      "30723 Training Loss: tensor(0.3256)\n",
      "30724 Training Loss: tensor(0.3257)\n",
      "30725 Training Loss: tensor(0.3259)\n",
      "30726 Training Loss: tensor(0.3261)\n",
      "30727 Training Loss: tensor(0.3258)\n",
      "30728 Training Loss: tensor(0.3256)\n",
      "30729 Training Loss: tensor(0.3262)\n",
      "30730 Training Loss: tensor(0.3262)\n",
      "30731 Training Loss: tensor(0.3257)\n",
      "30732 Training Loss: tensor(0.3259)\n",
      "30733 Training Loss: tensor(0.3269)\n",
      "30734 Training Loss: tensor(0.3257)\n",
      "30735 Training Loss: tensor(0.3258)\n",
      "30736 Training Loss: tensor(0.3269)\n",
      "30737 Training Loss: tensor(0.3279)\n",
      "30738 Training Loss: tensor(0.3257)\n",
      "30739 Training Loss: tensor(0.3259)\n",
      "30740 Training Loss: tensor(0.3262)\n",
      "30741 Training Loss: tensor(0.3259)\n",
      "30742 Training Loss: tensor(0.3267)\n",
      "30743 Training Loss: tensor(0.3268)\n",
      "30744 Training Loss: tensor(0.3261)\n",
      "30745 Training Loss: tensor(0.3261)\n",
      "30746 Training Loss: tensor(0.3260)\n",
      "30747 Training Loss: tensor(0.3261)\n",
      "30748 Training Loss: tensor(0.3259)\n",
      "30749 Training Loss: tensor(0.3265)\n",
      "30750 Training Loss: tensor(0.3264)\n",
      "30751 Training Loss: tensor(0.3257)\n",
      "30752 Training Loss: tensor(0.3257)\n",
      "30753 Training Loss: tensor(0.3257)\n",
      "30754 Training Loss: tensor(0.3263)\n",
      "30755 Training Loss: tensor(0.3258)\n",
      "30756 Training Loss: tensor(0.3259)\n",
      "30757 Training Loss: tensor(0.3258)\n",
      "30758 Training Loss: tensor(0.3276)\n",
      "30759 Training Loss: tensor(0.3263)\n",
      "30760 Training Loss: tensor(0.3262)\n",
      "30761 Training Loss: tensor(0.3259)\n",
      "30762 Training Loss: tensor(0.3255)\n",
      "30763 Training Loss: tensor(0.3254)\n",
      "30764 Training Loss: tensor(0.3258)\n",
      "30765 Training Loss: tensor(0.3266)\n",
      "30766 Training Loss: tensor(0.3258)\n",
      "30767 Training Loss: tensor(0.3273)\n",
      "30768 Training Loss: tensor(0.3257)\n",
      "30769 Training Loss: tensor(0.3258)\n",
      "30770 Training Loss: tensor(0.3260)\n",
      "30771 Training Loss: tensor(0.3259)\n",
      "30772 Training Loss: tensor(0.3260)\n",
      "30773 Training Loss: tensor(0.3258)\n",
      "30774 Training Loss: tensor(0.3271)\n",
      "30775 Training Loss: tensor(0.3263)\n",
      "30776 Training Loss: tensor(0.3259)\n",
      "30777 Training Loss: tensor(0.3263)\n",
      "30778 Training Loss: tensor(0.3255)\n",
      "30779 Training Loss: tensor(0.3259)\n",
      "30780 Training Loss: tensor(0.3259)\n",
      "30781 Training Loss: tensor(0.3256)\n",
      "30782 Training Loss: tensor(0.3261)\n",
      "30783 Training Loss: tensor(0.3255)\n",
      "30784 Training Loss: tensor(0.3272)\n",
      "30785 Training Loss: tensor(0.3257)\n",
      "30786 Training Loss: tensor(0.3261)\n",
      "30787 Training Loss: tensor(0.3258)\n",
      "30788 Training Loss: tensor(0.3262)\n",
      "30789 Training Loss: tensor(0.3271)\n",
      "30790 Training Loss: tensor(0.3256)\n",
      "30791 Training Loss: tensor(0.3254)\n",
      "30792 Training Loss: tensor(0.3263)\n",
      "30793 Training Loss: tensor(0.3273)\n",
      "30794 Training Loss: tensor(0.3256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30795 Training Loss: tensor(0.3264)\n",
      "30796 Training Loss: tensor(0.3256)\n",
      "30797 Training Loss: tensor(0.3260)\n",
      "30798 Training Loss: tensor(0.3258)\n",
      "30799 Training Loss: tensor(0.3262)\n",
      "30800 Training Loss: tensor(0.3256)\n",
      "30801 Training Loss: tensor(0.3258)\n",
      "30802 Training Loss: tensor(0.3256)\n",
      "30803 Training Loss: tensor(0.3258)\n",
      "30804 Training Loss: tensor(0.3258)\n",
      "30805 Training Loss: tensor(0.3265)\n",
      "30806 Training Loss: tensor(0.3258)\n",
      "30807 Training Loss: tensor(0.3264)\n",
      "30808 Training Loss: tensor(0.3257)\n",
      "30809 Training Loss: tensor(0.3264)\n",
      "30810 Training Loss: tensor(0.3257)\n",
      "30811 Training Loss: tensor(0.3267)\n",
      "30812 Training Loss: tensor(0.3261)\n",
      "30813 Training Loss: tensor(0.3262)\n",
      "30814 Training Loss: tensor(0.3265)\n",
      "30815 Training Loss: tensor(0.3256)\n",
      "30816 Training Loss: tensor(0.3256)\n",
      "30817 Training Loss: tensor(0.3258)\n",
      "30818 Training Loss: tensor(0.3259)\n",
      "30819 Training Loss: tensor(0.3261)\n",
      "30820 Training Loss: tensor(0.3255)\n",
      "30821 Training Loss: tensor(0.3261)\n",
      "30822 Training Loss: tensor(0.3256)\n",
      "30823 Training Loss: tensor(0.3256)\n",
      "30824 Training Loss: tensor(0.3277)\n",
      "30825 Training Loss: tensor(0.3268)\n",
      "30826 Training Loss: tensor(0.3254)\n",
      "30827 Training Loss: tensor(0.3257)\n",
      "30828 Training Loss: tensor(0.3259)\n",
      "30829 Training Loss: tensor(0.3260)\n",
      "30830 Training Loss: tensor(0.3261)\n",
      "30831 Training Loss: tensor(0.3265)\n",
      "30832 Training Loss: tensor(0.3270)\n",
      "30833 Training Loss: tensor(0.3258)\n",
      "30834 Training Loss: tensor(0.3256)\n",
      "30835 Training Loss: tensor(0.3261)\n",
      "30836 Training Loss: tensor(0.3257)\n",
      "30837 Training Loss: tensor(0.3265)\n",
      "30838 Training Loss: tensor(0.3268)\n",
      "30839 Training Loss: tensor(0.3256)\n",
      "30840 Training Loss: tensor(0.3257)\n",
      "30841 Training Loss: tensor(0.3270)\n",
      "30842 Training Loss: tensor(0.3263)\n",
      "30843 Training Loss: tensor(0.3258)\n",
      "30844 Training Loss: tensor(0.3263)\n",
      "30845 Training Loss: tensor(0.3264)\n",
      "30846 Training Loss: tensor(0.3259)\n",
      "30847 Training Loss: tensor(0.3263)\n",
      "30848 Training Loss: tensor(0.3256)\n",
      "30849 Training Loss: tensor(0.3265)\n",
      "30850 Training Loss: tensor(0.3258)\n",
      "30851 Training Loss: tensor(0.3260)\n",
      "30852 Training Loss: tensor(0.3258)\n",
      "30853 Training Loss: tensor(0.3257)\n",
      "30854 Training Loss: tensor(0.3256)\n",
      "30855 Training Loss: tensor(0.3256)\n",
      "30856 Training Loss: tensor(0.3265)\n",
      "30857 Training Loss: tensor(0.3258)\n",
      "30858 Training Loss: tensor(0.3256)\n",
      "30859 Training Loss: tensor(0.3253)\n",
      "30860 Training Loss: tensor(0.3257)\n",
      "30861 Training Loss: tensor(0.3256)\n",
      "30862 Training Loss: tensor(0.3256)\n",
      "30863 Training Loss: tensor(0.3254)\n",
      "30864 Training Loss: tensor(0.3254)\n",
      "30865 Training Loss: tensor(0.3257)\n",
      "30866 Training Loss: tensor(0.3258)\n",
      "30867 Training Loss: tensor(0.3255)\n",
      "30868 Training Loss: tensor(0.3257)\n",
      "30869 Training Loss: tensor(0.3267)\n",
      "30870 Training Loss: tensor(0.3257)\n",
      "30871 Training Loss: tensor(0.3268)\n",
      "30872 Training Loss: tensor(0.3256)\n",
      "30873 Training Loss: tensor(0.3262)\n",
      "30874 Training Loss: tensor(0.3259)\n",
      "30875 Training Loss: tensor(0.3257)\n",
      "30876 Training Loss: tensor(0.3259)\n",
      "30877 Training Loss: tensor(0.3263)\n",
      "30878 Training Loss: tensor(0.3260)\n",
      "30879 Training Loss: tensor(0.3261)\n",
      "30880 Training Loss: tensor(0.3257)\n",
      "30881 Training Loss: tensor(0.3252)\n",
      "30882 Training Loss: tensor(0.3258)\n",
      "30883 Training Loss: tensor(0.3256)\n",
      "30884 Training Loss: tensor(0.3255)\n",
      "30885 Training Loss: tensor(0.3262)\n",
      "30886 Training Loss: tensor(0.3262)\n",
      "30887 Training Loss: tensor(0.3260)\n",
      "30888 Training Loss: tensor(0.3265)\n",
      "30889 Training Loss: tensor(0.3261)\n",
      "30890 Training Loss: tensor(0.3263)\n",
      "30891 Training Loss: tensor(0.3257)\n",
      "30892 Training Loss: tensor(0.3276)\n",
      "30893 Training Loss: tensor(0.3256)\n",
      "30894 Training Loss: tensor(0.3267)\n",
      "30895 Training Loss: tensor(0.3260)\n",
      "30896 Training Loss: tensor(0.3261)\n",
      "30897 Training Loss: tensor(0.3261)\n",
      "30898 Training Loss: tensor(0.3262)\n",
      "30899 Training Loss: tensor(0.3263)\n",
      "30900 Training Loss: tensor(0.3256)\n",
      "30901 Training Loss: tensor(0.3254)\n",
      "30902 Training Loss: tensor(0.3264)\n",
      "30903 Training Loss: tensor(0.3262)\n",
      "30904 Training Loss: tensor(0.3255)\n",
      "30905 Training Loss: tensor(0.3258)\n",
      "30906 Training Loss: tensor(0.3260)\n",
      "30907 Training Loss: tensor(0.3258)\n",
      "30908 Training Loss: tensor(0.3257)\n",
      "30909 Training Loss: tensor(0.3255)\n",
      "30910 Training Loss: tensor(0.3257)\n",
      "30911 Training Loss: tensor(0.3262)\n",
      "30912 Training Loss: tensor(0.3270)\n",
      "30913 Training Loss: tensor(0.3255)\n",
      "30914 Training Loss: tensor(0.3262)\n",
      "30915 Training Loss: tensor(0.3256)\n",
      "30916 Training Loss: tensor(0.3258)\n",
      "30917 Training Loss: tensor(0.3260)\n",
      "30918 Training Loss: tensor(0.3255)\n",
      "30919 Training Loss: tensor(0.3254)\n",
      "30920 Training Loss: tensor(0.3261)\n",
      "30921 Training Loss: tensor(0.3258)\n",
      "30922 Training Loss: tensor(0.3256)\n",
      "30923 Training Loss: tensor(0.3268)\n",
      "30924 Training Loss: tensor(0.3259)\n",
      "30925 Training Loss: tensor(0.3260)\n",
      "30926 Training Loss: tensor(0.3254)\n",
      "30927 Training Loss: tensor(0.3256)\n",
      "30928 Training Loss: tensor(0.3260)\n",
      "30929 Training Loss: tensor(0.3268)\n",
      "30930 Training Loss: tensor(0.3264)\n",
      "30931 Training Loss: tensor(0.3259)\n",
      "30932 Training Loss: tensor(0.3273)\n",
      "30933 Training Loss: tensor(0.3259)\n",
      "30934 Training Loss: tensor(0.3262)\n",
      "30935 Training Loss: tensor(0.3264)\n",
      "30936 Training Loss: tensor(0.3259)\n",
      "30937 Training Loss: tensor(0.3265)\n",
      "30938 Training Loss: tensor(0.3259)\n",
      "30939 Training Loss: tensor(0.3261)\n",
      "30940 Training Loss: tensor(0.3265)\n",
      "30941 Training Loss: tensor(0.3257)\n",
      "30942 Training Loss: tensor(0.3263)\n",
      "30943 Training Loss: tensor(0.3256)\n",
      "30944 Training Loss: tensor(0.3262)\n",
      "30945 Training Loss: tensor(0.3268)\n",
      "30946 Training Loss: tensor(0.3258)\n",
      "30947 Training Loss: tensor(0.3256)\n",
      "30948 Training Loss: tensor(0.3256)\n",
      "30949 Training Loss: tensor(0.3270)\n",
      "30950 Training Loss: tensor(0.3259)\n",
      "30951 Training Loss: tensor(0.3258)\n",
      "30952 Training Loss: tensor(0.3265)\n",
      "30953 Training Loss: tensor(0.3256)\n",
      "30954 Training Loss: tensor(0.3265)\n",
      "30955 Training Loss: tensor(0.3268)\n",
      "30956 Training Loss: tensor(0.3258)\n",
      "30957 Training Loss: tensor(0.3259)\n",
      "30958 Training Loss: tensor(0.3262)\n",
      "30959 Training Loss: tensor(0.3265)\n",
      "30960 Training Loss: tensor(0.3258)\n",
      "30961 Training Loss: tensor(0.3262)\n",
      "30962 Training Loss: tensor(0.3259)\n",
      "30963 Training Loss: tensor(0.3257)\n",
      "30964 Training Loss: tensor(0.3261)\n",
      "30965 Training Loss: tensor(0.3265)\n",
      "30966 Training Loss: tensor(0.3259)\n",
      "30967 Training Loss: tensor(0.3262)\n",
      "30968 Training Loss: tensor(0.3257)\n",
      "30969 Training Loss: tensor(0.3255)\n",
      "30970 Training Loss: tensor(0.3258)\n",
      "30971 Training Loss: tensor(0.3272)\n",
      "30972 Training Loss: tensor(0.3259)\n",
      "30973 Training Loss: tensor(0.3258)\n",
      "30974 Training Loss: tensor(0.3255)\n",
      "30975 Training Loss: tensor(0.3261)\n",
      "30976 Training Loss: tensor(0.3257)\n",
      "30977 Training Loss: tensor(0.3256)\n",
      "30978 Training Loss: tensor(0.3263)\n",
      "30979 Training Loss: tensor(0.3265)\n",
      "30980 Training Loss: tensor(0.3285)\n",
      "30981 Training Loss: tensor(0.3259)\n",
      "30982 Training Loss: tensor(0.3261)\n",
      "30983 Training Loss: tensor(0.3262)\n",
      "30984 Training Loss: tensor(0.3269)\n",
      "30985 Training Loss: tensor(0.3261)\n",
      "30986 Training Loss: tensor(0.3258)\n",
      "30987 Training Loss: tensor(0.3266)\n",
      "30988 Training Loss: tensor(0.3260)\n",
      "30989 Training Loss: tensor(0.3265)\n",
      "30990 Training Loss: tensor(0.3261)\n",
      "30991 Training Loss: tensor(0.3262)\n",
      "30992 Training Loss: tensor(0.3256)\n",
      "30993 Training Loss: tensor(0.3257)\n",
      "30994 Training Loss: tensor(0.3254)\n",
      "30995 Training Loss: tensor(0.3257)\n",
      "30996 Training Loss: tensor(0.3263)\n",
      "30997 Training Loss: tensor(0.3265)\n",
      "30998 Training Loss: tensor(0.3270)\n",
      "30999 Training Loss: tensor(0.3262)\n",
      "31000 Training Loss: tensor(0.3260)\n",
      "31001 Training Loss: tensor(0.3264)\n",
      "31002 Training Loss: tensor(0.3260)\n",
      "31003 Training Loss: tensor(0.3256)\n",
      "31004 Training Loss: tensor(0.3275)\n",
      "31005 Training Loss: tensor(0.3261)\n",
      "31006 Training Loss: tensor(0.3260)\n",
      "31007 Training Loss: tensor(0.3257)\n",
      "31008 Training Loss: tensor(0.3260)\n",
      "31009 Training Loss: tensor(0.3262)\n",
      "31010 Training Loss: tensor(0.3266)\n",
      "31011 Training Loss: tensor(0.3256)\n",
      "31012 Training Loss: tensor(0.3260)\n",
      "31013 Training Loss: tensor(0.3258)\n",
      "31014 Training Loss: tensor(0.3260)\n",
      "31015 Training Loss: tensor(0.3257)\n",
      "31016 Training Loss: tensor(0.3261)\n",
      "31017 Training Loss: tensor(0.3256)\n",
      "31018 Training Loss: tensor(0.3256)\n",
      "31019 Training Loss: tensor(0.3255)\n",
      "31020 Training Loss: tensor(0.3265)\n",
      "31021 Training Loss: tensor(0.3259)\n",
      "31022 Training Loss: tensor(0.3255)\n",
      "31023 Training Loss: tensor(0.3256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31024 Training Loss: tensor(0.3258)\n",
      "31025 Training Loss: tensor(0.3260)\n",
      "31026 Training Loss: tensor(0.3255)\n",
      "31027 Training Loss: tensor(0.3257)\n",
      "31028 Training Loss: tensor(0.3277)\n",
      "31029 Training Loss: tensor(0.3259)\n",
      "31030 Training Loss: tensor(0.3282)\n",
      "31031 Training Loss: tensor(0.3258)\n",
      "31032 Training Loss: tensor(0.3258)\n",
      "31033 Training Loss: tensor(0.3261)\n",
      "31034 Training Loss: tensor(0.3256)\n",
      "31035 Training Loss: tensor(0.3255)\n",
      "31036 Training Loss: tensor(0.3253)\n",
      "31037 Training Loss: tensor(0.3266)\n",
      "31038 Training Loss: tensor(0.3259)\n",
      "31039 Training Loss: tensor(0.3263)\n",
      "31040 Training Loss: tensor(0.3266)\n",
      "31041 Training Loss: tensor(0.3256)\n",
      "31042 Training Loss: tensor(0.3267)\n",
      "31043 Training Loss: tensor(0.3258)\n",
      "31044 Training Loss: tensor(0.3257)\n",
      "31045 Training Loss: tensor(0.3261)\n",
      "31046 Training Loss: tensor(0.3256)\n",
      "31047 Training Loss: tensor(0.3261)\n",
      "31048 Training Loss: tensor(0.3259)\n",
      "31049 Training Loss: tensor(0.3256)\n",
      "31050 Training Loss: tensor(0.3261)\n",
      "31051 Training Loss: tensor(0.3256)\n",
      "31052 Training Loss: tensor(0.3256)\n",
      "31053 Training Loss: tensor(0.3263)\n",
      "31054 Training Loss: tensor(0.3256)\n",
      "31055 Training Loss: tensor(0.3267)\n",
      "31056 Training Loss: tensor(0.3270)\n",
      "31057 Training Loss: tensor(0.3259)\n",
      "31058 Training Loss: tensor(0.3258)\n",
      "31059 Training Loss: tensor(0.3259)\n",
      "31060 Training Loss: tensor(0.3267)\n",
      "31061 Training Loss: tensor(0.3281)\n",
      "31062 Training Loss: tensor(0.3260)\n",
      "31063 Training Loss: tensor(0.3284)\n",
      "31064 Training Loss: tensor(0.3265)\n",
      "31065 Training Loss: tensor(0.3259)\n",
      "31066 Training Loss: tensor(0.3256)\n",
      "31067 Training Loss: tensor(0.3267)\n",
      "31068 Training Loss: tensor(0.3262)\n",
      "31069 Training Loss: tensor(0.3257)\n",
      "31070 Training Loss: tensor(0.3265)\n",
      "31071 Training Loss: tensor(0.3258)\n",
      "31072 Training Loss: tensor(0.3266)\n",
      "31073 Training Loss: tensor(0.3262)\n",
      "31074 Training Loss: tensor(0.3263)\n",
      "31075 Training Loss: tensor(0.3257)\n",
      "31076 Training Loss: tensor(0.3257)\n",
      "31077 Training Loss: tensor(0.3257)\n",
      "31078 Training Loss: tensor(0.3256)\n",
      "31079 Training Loss: tensor(0.3255)\n",
      "31080 Training Loss: tensor(0.3259)\n",
      "31081 Training Loss: tensor(0.3259)\n",
      "31082 Training Loss: tensor(0.3257)\n",
      "31083 Training Loss: tensor(0.3257)\n",
      "31084 Training Loss: tensor(0.3258)\n",
      "31085 Training Loss: tensor(0.3276)\n",
      "31086 Training Loss: tensor(0.3262)\n",
      "31087 Training Loss: tensor(0.3265)\n",
      "31088 Training Loss: tensor(0.3258)\n",
      "31089 Training Loss: tensor(0.3254)\n",
      "31090 Training Loss: tensor(0.3259)\n",
      "31091 Training Loss: tensor(0.3263)\n",
      "31092 Training Loss: tensor(0.3257)\n",
      "31093 Training Loss: tensor(0.3255)\n",
      "31094 Training Loss: tensor(0.3256)\n",
      "31095 Training Loss: tensor(0.3263)\n",
      "31096 Training Loss: tensor(0.3266)\n",
      "31097 Training Loss: tensor(0.3254)\n",
      "31098 Training Loss: tensor(0.3253)\n",
      "31099 Training Loss: tensor(0.3255)\n",
      "31100 Training Loss: tensor(0.3255)\n",
      "31101 Training Loss: tensor(0.3270)\n",
      "31102 Training Loss: tensor(0.3263)\n",
      "31103 Training Loss: tensor(0.3256)\n",
      "31104 Training Loss: tensor(0.3258)\n",
      "31105 Training Loss: tensor(0.3255)\n",
      "31106 Training Loss: tensor(0.3267)\n",
      "31107 Training Loss: tensor(0.3259)\n",
      "31108 Training Loss: tensor(0.3294)\n",
      "31109 Training Loss: tensor(0.3257)\n",
      "31110 Training Loss: tensor(0.3259)\n",
      "31111 Training Loss: tensor(0.3256)\n",
      "31112 Training Loss: tensor(0.3262)\n",
      "31113 Training Loss: tensor(0.3261)\n",
      "31114 Training Loss: tensor(0.3262)\n",
      "31115 Training Loss: tensor(0.3257)\n",
      "31116 Training Loss: tensor(0.3257)\n",
      "31117 Training Loss: tensor(0.3257)\n",
      "31118 Training Loss: tensor(0.3262)\n",
      "31119 Training Loss: tensor(0.3259)\n",
      "31120 Training Loss: tensor(0.3261)\n",
      "31121 Training Loss: tensor(0.3255)\n",
      "31122 Training Loss: tensor(0.3261)\n",
      "31123 Training Loss: tensor(0.3274)\n",
      "31124 Training Loss: tensor(0.3261)\n",
      "31125 Training Loss: tensor(0.3265)\n",
      "31126 Training Loss: tensor(0.3263)\n",
      "31127 Training Loss: tensor(0.3254)\n",
      "31128 Training Loss: tensor(0.3266)\n",
      "31129 Training Loss: tensor(0.3267)\n",
      "31130 Training Loss: tensor(0.3263)\n",
      "31131 Training Loss: tensor(0.3259)\n",
      "31132 Training Loss: tensor(0.3260)\n",
      "31133 Training Loss: tensor(0.3258)\n",
      "31134 Training Loss: tensor(0.3257)\n",
      "31135 Training Loss: tensor(0.3257)\n",
      "31136 Training Loss: tensor(0.3258)\n",
      "31137 Training Loss: tensor(0.3272)\n",
      "31138 Training Loss: tensor(0.3257)\n",
      "31139 Training Loss: tensor(0.3259)\n",
      "31140 Training Loss: tensor(0.3258)\n",
      "31141 Training Loss: tensor(0.3254)\n",
      "31142 Training Loss: tensor(0.3274)\n",
      "31143 Training Loss: tensor(0.3256)\n",
      "31144 Training Loss: tensor(0.3256)\n",
      "31145 Training Loss: tensor(0.3259)\n",
      "31146 Training Loss: tensor(0.3258)\n",
      "31147 Training Loss: tensor(0.3256)\n",
      "31148 Training Loss: tensor(0.3253)\n",
      "31149 Training Loss: tensor(0.3303)\n",
      "31150 Training Loss: tensor(0.3263)\n",
      "31151 Training Loss: tensor(0.3256)\n",
      "31152 Training Loss: tensor(0.3257)\n",
      "31153 Training Loss: tensor(0.3262)\n",
      "31154 Training Loss: tensor(0.3267)\n",
      "31155 Training Loss: tensor(0.3254)\n",
      "31156 Training Loss: tensor(0.3255)\n",
      "31157 Training Loss: tensor(0.3258)\n",
      "31158 Training Loss: tensor(0.3261)\n",
      "31159 Training Loss: tensor(0.3260)\n",
      "31160 Training Loss: tensor(0.3265)\n",
      "31161 Training Loss: tensor(0.3256)\n",
      "31162 Training Loss: tensor(0.3259)\n",
      "31163 Training Loss: tensor(0.3267)\n",
      "31164 Training Loss: tensor(0.3255)\n",
      "31165 Training Loss: tensor(0.3265)\n",
      "31166 Training Loss: tensor(0.3258)\n",
      "31167 Training Loss: tensor(0.3256)\n",
      "31168 Training Loss: tensor(0.3254)\n",
      "31169 Training Loss: tensor(0.3258)\n",
      "31170 Training Loss: tensor(0.3257)\n",
      "31171 Training Loss: tensor(0.3261)\n",
      "31172 Training Loss: tensor(0.3257)\n",
      "31173 Training Loss: tensor(0.3257)\n",
      "31174 Training Loss: tensor(0.3254)\n",
      "31175 Training Loss: tensor(0.3254)\n",
      "31176 Training Loss: tensor(0.3271)\n",
      "31177 Training Loss: tensor(0.3260)\n",
      "31178 Training Loss: tensor(0.3262)\n",
      "31179 Training Loss: tensor(0.3266)\n",
      "31180 Training Loss: tensor(0.3267)\n",
      "31181 Training Loss: tensor(0.3261)\n",
      "31182 Training Loss: tensor(0.3257)\n",
      "31183 Training Loss: tensor(0.3258)\n",
      "31184 Training Loss: tensor(0.3257)\n",
      "31185 Training Loss: tensor(0.3257)\n",
      "31186 Training Loss: tensor(0.3264)\n",
      "31187 Training Loss: tensor(0.3258)\n",
      "31188 Training Loss: tensor(0.3254)\n",
      "31189 Training Loss: tensor(0.3260)\n",
      "31190 Training Loss: tensor(0.3257)\n",
      "31191 Training Loss: tensor(0.3258)\n",
      "31192 Training Loss: tensor(0.3255)\n",
      "31193 Training Loss: tensor(0.3256)\n",
      "31194 Training Loss: tensor(0.3255)\n",
      "31195 Training Loss: tensor(0.3268)\n",
      "31196 Training Loss: tensor(0.3254)\n",
      "31197 Training Loss: tensor(0.3257)\n",
      "31198 Training Loss: tensor(0.3262)\n",
      "31199 Training Loss: tensor(0.3262)\n",
      "31200 Training Loss: tensor(0.3256)\n",
      "31201 Training Loss: tensor(0.3263)\n",
      "31202 Training Loss: tensor(0.3254)\n",
      "31203 Training Loss: tensor(0.3257)\n",
      "31204 Training Loss: tensor(0.3256)\n",
      "31205 Training Loss: tensor(0.3267)\n",
      "31206 Training Loss: tensor(0.3267)\n",
      "31207 Training Loss: tensor(0.3255)\n",
      "31208 Training Loss: tensor(0.3255)\n",
      "31209 Training Loss: tensor(0.3257)\n",
      "31210 Training Loss: tensor(0.3258)\n",
      "31211 Training Loss: tensor(0.3271)\n",
      "31212 Training Loss: tensor(0.3269)\n",
      "31213 Training Loss: tensor(0.3263)\n",
      "31214 Training Loss: tensor(0.3256)\n",
      "31215 Training Loss: tensor(0.3264)\n",
      "31216 Training Loss: tensor(0.3261)\n",
      "31217 Training Loss: tensor(0.3259)\n",
      "31218 Training Loss: tensor(0.3262)\n",
      "31219 Training Loss: tensor(0.3257)\n",
      "31220 Training Loss: tensor(0.3254)\n",
      "31221 Training Loss: tensor(0.3258)\n",
      "31222 Training Loss: tensor(0.3255)\n",
      "31223 Training Loss: tensor(0.3259)\n",
      "31224 Training Loss: tensor(0.3255)\n",
      "31225 Training Loss: tensor(0.3259)\n",
      "31226 Training Loss: tensor(0.3255)\n",
      "31227 Training Loss: tensor(0.3260)\n",
      "31228 Training Loss: tensor(0.3263)\n",
      "31229 Training Loss: tensor(0.3285)\n",
      "31230 Training Loss: tensor(0.3254)\n",
      "31231 Training Loss: tensor(0.3269)\n",
      "31232 Training Loss: tensor(0.3259)\n",
      "31233 Training Loss: tensor(0.3262)\n",
      "31234 Training Loss: tensor(0.3269)\n",
      "31235 Training Loss: tensor(0.3256)\n",
      "31236 Training Loss: tensor(0.3259)\n",
      "31237 Training Loss: tensor(0.3262)\n",
      "31238 Training Loss: tensor(0.3255)\n",
      "31239 Training Loss: tensor(0.3259)\n",
      "31240 Training Loss: tensor(0.3264)\n",
      "31241 Training Loss: tensor(0.3258)\n",
      "31242 Training Loss: tensor(0.3257)\n",
      "31243 Training Loss: tensor(0.3256)\n",
      "31244 Training Loss: tensor(0.3260)\n",
      "31245 Training Loss: tensor(0.3255)\n",
      "31246 Training Loss: tensor(0.3255)\n",
      "31247 Training Loss: tensor(0.3260)\n",
      "31248 Training Loss: tensor(0.3258)\n",
      "31249 Training Loss: tensor(0.3269)\n",
      "31250 Training Loss: tensor(0.3262)\n",
      "31251 Training Loss: tensor(0.3263)\n",
      "31252 Training Loss: tensor(0.3257)\n",
      "31253 Training Loss: tensor(0.3283)\n",
      "31254 Training Loss: tensor(0.3256)\n",
      "31255 Training Loss: tensor(0.3260)\n",
      "31256 Training Loss: tensor(0.3271)\n",
      "31257 Training Loss: tensor(0.3265)\n",
      "31258 Training Loss: tensor(0.3273)\n",
      "31259 Training Loss: tensor(0.3264)\n",
      "31260 Training Loss: tensor(0.3267)\n",
      "31261 Training Loss: tensor(0.3267)\n",
      "31262 Training Loss: tensor(0.3260)\n",
      "31263 Training Loss: tensor(0.3262)\n",
      "31264 Training Loss: tensor(0.3258)\n",
      "31265 Training Loss: tensor(0.3258)\n",
      "31266 Training Loss: tensor(0.3268)\n",
      "31267 Training Loss: tensor(0.3256)\n",
      "31268 Training Loss: tensor(0.3270)\n",
      "31269 Training Loss: tensor(0.3256)\n",
      "31270 Training Loss: tensor(0.3259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31271 Training Loss: tensor(0.3259)\n",
      "31272 Training Loss: tensor(0.3257)\n",
      "31273 Training Loss: tensor(0.3278)\n",
      "31274 Training Loss: tensor(0.3259)\n",
      "31275 Training Loss: tensor(0.3263)\n",
      "31276 Training Loss: tensor(0.3255)\n",
      "31277 Training Loss: tensor(0.3256)\n",
      "31278 Training Loss: tensor(0.3256)\n",
      "31279 Training Loss: tensor(0.3262)\n",
      "31280 Training Loss: tensor(0.3257)\n",
      "31281 Training Loss: tensor(0.3259)\n",
      "31282 Training Loss: tensor(0.3262)\n",
      "31283 Training Loss: tensor(0.3261)\n",
      "31284 Training Loss: tensor(0.3261)\n",
      "31285 Training Loss: tensor(0.3272)\n",
      "31286 Training Loss: tensor(0.3266)\n",
      "31287 Training Loss: tensor(0.3262)\n",
      "31288 Training Loss: tensor(0.3258)\n",
      "31289 Training Loss: tensor(0.3257)\n",
      "31290 Training Loss: tensor(0.3269)\n",
      "31291 Training Loss: tensor(0.3262)\n",
      "31292 Training Loss: tensor(0.3255)\n",
      "31293 Training Loss: tensor(0.3268)\n",
      "31294 Training Loss: tensor(0.3259)\n",
      "31295 Training Loss: tensor(0.3271)\n",
      "31296 Training Loss: tensor(0.3259)\n",
      "31297 Training Loss: tensor(0.3255)\n",
      "31298 Training Loss: tensor(0.3262)\n",
      "31299 Training Loss: tensor(0.3265)\n",
      "31300 Training Loss: tensor(0.3264)\n",
      "31301 Training Loss: tensor(0.3255)\n",
      "31302 Training Loss: tensor(0.3256)\n",
      "31303 Training Loss: tensor(0.3263)\n",
      "31304 Training Loss: tensor(0.3270)\n",
      "31305 Training Loss: tensor(0.3263)\n",
      "31306 Training Loss: tensor(0.3258)\n",
      "31307 Training Loss: tensor(0.3266)\n",
      "31308 Training Loss: tensor(0.3254)\n",
      "31309 Training Loss: tensor(0.3258)\n",
      "31310 Training Loss: tensor(0.3258)\n",
      "31311 Training Loss: tensor(0.3257)\n",
      "31312 Training Loss: tensor(0.3258)\n",
      "31313 Training Loss: tensor(0.3259)\n",
      "31314 Training Loss: tensor(0.3255)\n",
      "31315 Training Loss: tensor(0.3255)\n",
      "31316 Training Loss: tensor(0.3256)\n",
      "31317 Training Loss: tensor(0.3257)\n",
      "31318 Training Loss: tensor(0.3254)\n",
      "31319 Training Loss: tensor(0.3261)\n",
      "31320 Training Loss: tensor(0.3277)\n",
      "31321 Training Loss: tensor(0.3253)\n",
      "31322 Training Loss: tensor(0.3264)\n",
      "31323 Training Loss: tensor(0.3256)\n",
      "31324 Training Loss: tensor(0.3256)\n",
      "31325 Training Loss: tensor(0.3257)\n",
      "31326 Training Loss: tensor(0.3257)\n",
      "31327 Training Loss: tensor(0.3256)\n",
      "31328 Training Loss: tensor(0.3259)\n",
      "31329 Training Loss: tensor(0.3258)\n",
      "31330 Training Loss: tensor(0.3256)\n",
      "31331 Training Loss: tensor(0.3259)\n",
      "31332 Training Loss: tensor(0.3258)\n",
      "31333 Training Loss: tensor(0.3256)\n",
      "31334 Training Loss: tensor(0.3259)\n",
      "31335 Training Loss: tensor(0.3262)\n",
      "31336 Training Loss: tensor(0.3252)\n",
      "31337 Training Loss: tensor(0.3254)\n",
      "31338 Training Loss: tensor(0.3255)\n",
      "31339 Training Loss: tensor(0.3256)\n",
      "31340 Training Loss: tensor(0.3256)\n",
      "31341 Training Loss: tensor(0.3259)\n",
      "31342 Training Loss: tensor(0.3257)\n",
      "31343 Training Loss: tensor(0.3254)\n",
      "31344 Training Loss: tensor(0.3258)\n",
      "31345 Training Loss: tensor(0.3253)\n",
      "31346 Training Loss: tensor(0.3259)\n",
      "31347 Training Loss: tensor(0.3254)\n",
      "31348 Training Loss: tensor(0.3262)\n",
      "31349 Training Loss: tensor(0.3254)\n",
      "31350 Training Loss: tensor(0.3255)\n",
      "31351 Training Loss: tensor(0.3266)\n",
      "31352 Training Loss: tensor(0.3257)\n",
      "31353 Training Loss: tensor(0.3262)\n",
      "31354 Training Loss: tensor(0.3255)\n",
      "31355 Training Loss: tensor(0.3256)\n",
      "31356 Training Loss: tensor(0.3252)\n",
      "31357 Training Loss: tensor(0.3279)\n",
      "31358 Training Loss: tensor(0.3254)\n",
      "31359 Training Loss: tensor(0.3271)\n",
      "31360 Training Loss: tensor(0.3257)\n",
      "31361 Training Loss: tensor(0.3255)\n",
      "31362 Training Loss: tensor(0.3256)\n",
      "31363 Training Loss: tensor(0.3257)\n",
      "31364 Training Loss: tensor(0.3254)\n",
      "31365 Training Loss: tensor(0.3260)\n",
      "31366 Training Loss: tensor(0.3255)\n",
      "31367 Training Loss: tensor(0.3257)\n",
      "31368 Training Loss: tensor(0.3259)\n",
      "31369 Training Loss: tensor(0.3269)\n",
      "31370 Training Loss: tensor(0.3261)\n",
      "31371 Training Loss: tensor(0.3271)\n",
      "31372 Training Loss: tensor(0.3280)\n",
      "31373 Training Loss: tensor(0.3257)\n",
      "31374 Training Loss: tensor(0.3263)\n",
      "31375 Training Loss: tensor(0.3257)\n",
      "31376 Training Loss: tensor(0.3255)\n",
      "31377 Training Loss: tensor(0.3257)\n",
      "31378 Training Loss: tensor(0.3253)\n",
      "31379 Training Loss: tensor(0.3258)\n",
      "31380 Training Loss: tensor(0.3260)\n",
      "31381 Training Loss: tensor(0.3253)\n",
      "31382 Training Loss: tensor(0.3256)\n",
      "31383 Training Loss: tensor(0.3258)\n",
      "31384 Training Loss: tensor(0.3262)\n",
      "31385 Training Loss: tensor(0.3261)\n",
      "31386 Training Loss: tensor(0.3261)\n",
      "31387 Training Loss: tensor(0.3258)\n",
      "31388 Training Loss: tensor(0.3255)\n",
      "31389 Training Loss: tensor(0.3262)\n",
      "31390 Training Loss: tensor(0.3265)\n",
      "31391 Training Loss: tensor(0.3254)\n",
      "31392 Training Loss: tensor(0.3260)\n",
      "31393 Training Loss: tensor(0.3259)\n",
      "31394 Training Loss: tensor(0.3272)\n",
      "31395 Training Loss: tensor(0.3278)\n",
      "31396 Training Loss: tensor(0.3257)\n",
      "31397 Training Loss: tensor(0.3266)\n",
      "31398 Training Loss: tensor(0.3257)\n",
      "31399 Training Loss: tensor(0.3261)\n",
      "31400 Training Loss: tensor(0.3257)\n",
      "31401 Training Loss: tensor(0.3257)\n",
      "31402 Training Loss: tensor(0.3256)\n",
      "31403 Training Loss: tensor(0.3254)\n",
      "31404 Training Loss: tensor(0.3258)\n",
      "31405 Training Loss: tensor(0.3266)\n",
      "31406 Training Loss: tensor(0.3259)\n",
      "31407 Training Loss: tensor(0.3262)\n",
      "31408 Training Loss: tensor(0.3275)\n",
      "31409 Training Loss: tensor(0.3263)\n",
      "31410 Training Loss: tensor(0.3255)\n",
      "31411 Training Loss: tensor(0.3268)\n",
      "31412 Training Loss: tensor(0.3266)\n",
      "31413 Training Loss: tensor(0.3259)\n",
      "31414 Training Loss: tensor(0.3256)\n",
      "31415 Training Loss: tensor(0.3255)\n",
      "31416 Training Loss: tensor(0.3261)\n",
      "31417 Training Loss: tensor(0.3254)\n",
      "31418 Training Loss: tensor(0.3270)\n",
      "31419 Training Loss: tensor(0.3256)\n",
      "31420 Training Loss: tensor(0.3257)\n",
      "31421 Training Loss: tensor(0.3265)\n",
      "31422 Training Loss: tensor(0.3259)\n",
      "31423 Training Loss: tensor(0.3268)\n",
      "31424 Training Loss: tensor(0.3255)\n",
      "31425 Training Loss: tensor(0.3257)\n",
      "31426 Training Loss: tensor(0.3257)\n",
      "31427 Training Loss: tensor(0.3261)\n",
      "31428 Training Loss: tensor(0.3256)\n",
      "31429 Training Loss: tensor(0.3261)\n",
      "31430 Training Loss: tensor(0.3261)\n",
      "31431 Training Loss: tensor(0.3261)\n",
      "31432 Training Loss: tensor(0.3257)\n",
      "31433 Training Loss: tensor(0.3264)\n",
      "31434 Training Loss: tensor(0.3263)\n",
      "31435 Training Loss: tensor(0.3261)\n",
      "31436 Training Loss: tensor(0.3267)\n",
      "31437 Training Loss: tensor(0.3261)\n",
      "31438 Training Loss: tensor(0.3262)\n",
      "31439 Training Loss: tensor(0.3255)\n",
      "31440 Training Loss: tensor(0.3256)\n",
      "31441 Training Loss: tensor(0.3255)\n",
      "31442 Training Loss: tensor(0.3268)\n",
      "31443 Training Loss: tensor(0.3263)\n",
      "31444 Training Loss: tensor(0.3256)\n",
      "31445 Training Loss: tensor(0.3256)\n",
      "31446 Training Loss: tensor(0.3264)\n",
      "31447 Training Loss: tensor(0.3257)\n",
      "31448 Training Loss: tensor(0.3257)\n",
      "31449 Training Loss: tensor(0.3257)\n",
      "31450 Training Loss: tensor(0.3257)\n",
      "31451 Training Loss: tensor(0.3256)\n",
      "31452 Training Loss: tensor(0.3262)\n",
      "31453 Training Loss: tensor(0.3262)\n",
      "31454 Training Loss: tensor(0.3256)\n",
      "31455 Training Loss: tensor(0.3262)\n",
      "31456 Training Loss: tensor(0.3255)\n",
      "31457 Training Loss: tensor(0.3267)\n",
      "31458 Training Loss: tensor(0.3255)\n",
      "31459 Training Loss: tensor(0.3268)\n",
      "31460 Training Loss: tensor(0.3254)\n",
      "31461 Training Loss: tensor(0.3260)\n",
      "31462 Training Loss: tensor(0.3254)\n",
      "31463 Training Loss: tensor(0.3266)\n",
      "31464 Training Loss: tensor(0.3257)\n",
      "31465 Training Loss: tensor(0.3254)\n",
      "31466 Training Loss: tensor(0.3273)\n",
      "31467 Training Loss: tensor(0.3259)\n",
      "31468 Training Loss: tensor(0.3253)\n",
      "31469 Training Loss: tensor(0.3258)\n",
      "31470 Training Loss: tensor(0.3258)\n",
      "31471 Training Loss: tensor(0.3268)\n",
      "31472 Training Loss: tensor(0.3256)\n",
      "31473 Training Loss: tensor(0.3262)\n",
      "31474 Training Loss: tensor(0.3257)\n",
      "31475 Training Loss: tensor(0.3256)\n",
      "31476 Training Loss: tensor(0.3257)\n",
      "31477 Training Loss: tensor(0.3253)\n",
      "31478 Training Loss: tensor(0.3262)\n",
      "31479 Training Loss: tensor(0.3256)\n",
      "31480 Training Loss: tensor(0.3265)\n",
      "31481 Training Loss: tensor(0.3255)\n",
      "31482 Training Loss: tensor(0.3255)\n",
      "31483 Training Loss: tensor(0.3254)\n",
      "31484 Training Loss: tensor(0.3257)\n",
      "31485 Training Loss: tensor(0.3256)\n",
      "31486 Training Loss: tensor(0.3254)\n",
      "31487 Training Loss: tensor(0.3266)\n",
      "31488 Training Loss: tensor(0.3258)\n",
      "31489 Training Loss: tensor(0.3255)\n",
      "31490 Training Loss: tensor(0.3261)\n",
      "31491 Training Loss: tensor(0.3258)\n",
      "31492 Training Loss: tensor(0.3259)\n",
      "31493 Training Loss: tensor(0.3267)\n",
      "31494 Training Loss: tensor(0.3256)\n",
      "31495 Training Loss: tensor(0.3254)\n",
      "31496 Training Loss: tensor(0.3256)\n",
      "31497 Training Loss: tensor(0.3258)\n",
      "31498 Training Loss: tensor(0.3276)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31499 Training Loss: tensor(0.3262)\n",
      "31500 Training Loss: tensor(0.3254)\n",
      "31501 Training Loss: tensor(0.3264)\n",
      "31502 Training Loss: tensor(0.3258)\n",
      "31503 Training Loss: tensor(0.3258)\n",
      "31504 Training Loss: tensor(0.3259)\n",
      "31505 Training Loss: tensor(0.3257)\n",
      "31506 Training Loss: tensor(0.3260)\n",
      "31507 Training Loss: tensor(0.3258)\n",
      "31508 Training Loss: tensor(0.3259)\n",
      "31509 Training Loss: tensor(0.3258)\n",
      "31510 Training Loss: tensor(0.3256)\n",
      "31511 Training Loss: tensor(0.3286)\n",
      "31512 Training Loss: tensor(0.3264)\n",
      "31513 Training Loss: tensor(0.3258)\n",
      "31514 Training Loss: tensor(0.3255)\n",
      "31515 Training Loss: tensor(0.3259)\n",
      "31516 Training Loss: tensor(0.3276)\n",
      "31517 Training Loss: tensor(0.3258)\n",
      "31518 Training Loss: tensor(0.3258)\n",
      "31519 Training Loss: tensor(0.3258)\n",
      "31520 Training Loss: tensor(0.3259)\n",
      "31521 Training Loss: tensor(0.3266)\n",
      "31522 Training Loss: tensor(0.3257)\n",
      "31523 Training Loss: tensor(0.3257)\n",
      "31524 Training Loss: tensor(0.3255)\n",
      "31525 Training Loss: tensor(0.3260)\n",
      "31526 Training Loss: tensor(0.3257)\n",
      "31527 Training Loss: tensor(0.3266)\n",
      "31528 Training Loss: tensor(0.3255)\n",
      "31529 Training Loss: tensor(0.3257)\n",
      "31530 Training Loss: tensor(0.3261)\n",
      "31531 Training Loss: tensor(0.3267)\n",
      "31532 Training Loss: tensor(0.3256)\n",
      "31533 Training Loss: tensor(0.3255)\n",
      "31534 Training Loss: tensor(0.3260)\n",
      "31535 Training Loss: tensor(0.3259)\n",
      "31536 Training Loss: tensor(0.3267)\n",
      "31537 Training Loss: tensor(0.3256)\n",
      "31538 Training Loss: tensor(0.3255)\n",
      "31539 Training Loss: tensor(0.3271)\n",
      "31540 Training Loss: tensor(0.3256)\n",
      "31541 Training Loss: tensor(0.3260)\n",
      "31542 Training Loss: tensor(0.3254)\n",
      "31543 Training Loss: tensor(0.3257)\n",
      "31544 Training Loss: tensor(0.3258)\n",
      "31545 Training Loss: tensor(0.3253)\n",
      "31546 Training Loss: tensor(0.3259)\n",
      "31547 Training Loss: tensor(0.3256)\n",
      "31548 Training Loss: tensor(0.3262)\n",
      "31549 Training Loss: tensor(0.3259)\n",
      "31550 Training Loss: tensor(0.3257)\n",
      "31551 Training Loss: tensor(0.3257)\n",
      "31552 Training Loss: tensor(0.3254)\n",
      "31553 Training Loss: tensor(0.3263)\n",
      "31554 Training Loss: tensor(0.3255)\n",
      "31555 Training Loss: tensor(0.3260)\n",
      "31556 Training Loss: tensor(0.3255)\n",
      "31557 Training Loss: tensor(0.3253)\n",
      "31558 Training Loss: tensor(0.3256)\n",
      "31559 Training Loss: tensor(0.3262)\n",
      "31560 Training Loss: tensor(0.3263)\n",
      "31561 Training Loss: tensor(0.3260)\n",
      "31562 Training Loss: tensor(0.3256)\n",
      "31563 Training Loss: tensor(0.3262)\n",
      "31564 Training Loss: tensor(0.3256)\n",
      "31565 Training Loss: tensor(0.3257)\n",
      "31566 Training Loss: tensor(0.3264)\n",
      "31567 Training Loss: tensor(0.3258)\n",
      "31568 Training Loss: tensor(0.3263)\n",
      "31569 Training Loss: tensor(0.3255)\n",
      "31570 Training Loss: tensor(0.3255)\n",
      "31571 Training Loss: tensor(0.3258)\n",
      "31572 Training Loss: tensor(0.3261)\n",
      "31573 Training Loss: tensor(0.3265)\n",
      "31574 Training Loss: tensor(0.3258)\n",
      "31575 Training Loss: tensor(0.3255)\n",
      "31576 Training Loss: tensor(0.3255)\n",
      "31577 Training Loss: tensor(0.3256)\n",
      "31578 Training Loss: tensor(0.3258)\n",
      "31579 Training Loss: tensor(0.3272)\n",
      "31580 Training Loss: tensor(0.3256)\n",
      "31581 Training Loss: tensor(0.3258)\n",
      "31582 Training Loss: tensor(0.3253)\n",
      "31583 Training Loss: tensor(0.3254)\n",
      "31584 Training Loss: tensor(0.3255)\n",
      "31585 Training Loss: tensor(0.3252)\n",
      "31586 Training Loss: tensor(0.3268)\n",
      "31587 Training Loss: tensor(0.3253)\n",
      "31588 Training Loss: tensor(0.3263)\n",
      "31589 Training Loss: tensor(0.3253)\n",
      "31590 Training Loss: tensor(0.3254)\n",
      "31591 Training Loss: tensor(0.3253)\n",
      "31592 Training Loss: tensor(0.3252)\n",
      "31593 Training Loss: tensor(0.3253)\n",
      "31594 Training Loss: tensor(0.3270)\n",
      "31595 Training Loss: tensor(0.3254)\n",
      "31596 Training Loss: tensor(0.3274)\n",
      "31597 Training Loss: tensor(0.3256)\n",
      "31598 Training Loss: tensor(0.3257)\n",
      "31599 Training Loss: tensor(0.3253)\n",
      "31600 Training Loss: tensor(0.3255)\n",
      "31601 Training Loss: tensor(0.3255)\n",
      "31602 Training Loss: tensor(0.3259)\n",
      "31603 Training Loss: tensor(0.3254)\n",
      "31604 Training Loss: tensor(0.3255)\n",
      "31605 Training Loss: tensor(0.3256)\n",
      "31606 Training Loss: tensor(0.3261)\n",
      "31607 Training Loss: tensor(0.3254)\n",
      "31608 Training Loss: tensor(0.3255)\n",
      "31609 Training Loss: tensor(0.3255)\n",
      "31610 Training Loss: tensor(0.3256)\n",
      "31611 Training Loss: tensor(0.3259)\n",
      "31612 Training Loss: tensor(0.3255)\n",
      "31613 Training Loss: tensor(0.3256)\n",
      "31614 Training Loss: tensor(0.3255)\n",
      "31615 Training Loss: tensor(0.3254)\n",
      "31616 Training Loss: tensor(0.3254)\n",
      "31617 Training Loss: tensor(0.3254)\n",
      "31618 Training Loss: tensor(0.3257)\n",
      "31619 Training Loss: tensor(0.3259)\n",
      "31620 Training Loss: tensor(0.3256)\n",
      "31621 Training Loss: tensor(0.3267)\n",
      "31622 Training Loss: tensor(0.3255)\n",
      "31623 Training Loss: tensor(0.3252)\n",
      "31624 Training Loss: tensor(0.3255)\n",
      "31625 Training Loss: tensor(0.3279)\n",
      "31626 Training Loss: tensor(0.3254)\n",
      "31627 Training Loss: tensor(0.3256)\n",
      "31628 Training Loss: tensor(0.3260)\n",
      "31629 Training Loss: tensor(0.3256)\n",
      "31630 Training Loss: tensor(0.3263)\n",
      "31631 Training Loss: tensor(0.3256)\n",
      "31632 Training Loss: tensor(0.3254)\n",
      "31633 Training Loss: tensor(0.3259)\n",
      "31634 Training Loss: tensor(0.3252)\n",
      "31635 Training Loss: tensor(0.3256)\n",
      "31636 Training Loss: tensor(0.3253)\n",
      "31637 Training Loss: tensor(0.3260)\n",
      "31638 Training Loss: tensor(0.3258)\n",
      "31639 Training Loss: tensor(0.3253)\n",
      "31640 Training Loss: tensor(0.3255)\n",
      "31641 Training Loss: tensor(0.3258)\n",
      "31642 Training Loss: tensor(0.3256)\n",
      "31643 Training Loss: tensor(0.3264)\n",
      "31644 Training Loss: tensor(0.3268)\n",
      "31645 Training Loss: tensor(0.3261)\n",
      "31646 Training Loss: tensor(0.3263)\n",
      "31647 Training Loss: tensor(0.3253)\n",
      "31648 Training Loss: tensor(0.3260)\n",
      "31649 Training Loss: tensor(0.3254)\n",
      "31650 Training Loss: tensor(0.3259)\n",
      "31651 Training Loss: tensor(0.3260)\n",
      "31652 Training Loss: tensor(0.3265)\n",
      "31653 Training Loss: tensor(0.3258)\n",
      "31654 Training Loss: tensor(0.3258)\n",
      "31655 Training Loss: tensor(0.3254)\n",
      "31656 Training Loss: tensor(0.3269)\n",
      "31657 Training Loss: tensor(0.3260)\n",
      "31658 Training Loss: tensor(0.3255)\n",
      "31659 Training Loss: tensor(0.3260)\n",
      "31660 Training Loss: tensor(0.3258)\n",
      "31661 Training Loss: tensor(0.3257)\n",
      "31662 Training Loss: tensor(0.3256)\n",
      "31663 Training Loss: tensor(0.3256)\n",
      "31664 Training Loss: tensor(0.3265)\n",
      "31665 Training Loss: tensor(0.3255)\n",
      "31666 Training Loss: tensor(0.3254)\n",
      "31667 Training Loss: tensor(0.3260)\n",
      "31668 Training Loss: tensor(0.3268)\n",
      "31669 Training Loss: tensor(0.3279)\n",
      "31670 Training Loss: tensor(0.3254)\n",
      "31671 Training Loss: tensor(0.3255)\n",
      "31672 Training Loss: tensor(0.3262)\n",
      "31673 Training Loss: tensor(0.3254)\n",
      "31674 Training Loss: tensor(0.3260)\n",
      "31675 Training Loss: tensor(0.3253)\n",
      "31676 Training Loss: tensor(0.3266)\n",
      "31677 Training Loss: tensor(0.3259)\n",
      "31678 Training Loss: tensor(0.3252)\n",
      "31679 Training Loss: tensor(0.3257)\n",
      "31680 Training Loss: tensor(0.3254)\n",
      "31681 Training Loss: tensor(0.3254)\n",
      "31682 Training Loss: tensor(0.3252)\n",
      "31683 Training Loss: tensor(0.3258)\n",
      "31684 Training Loss: tensor(0.3259)\n",
      "31685 Training Loss: tensor(0.3266)\n",
      "31686 Training Loss: tensor(0.3253)\n",
      "31687 Training Loss: tensor(0.3258)\n",
      "31688 Training Loss: tensor(0.3259)\n",
      "31689 Training Loss: tensor(0.3255)\n",
      "31690 Training Loss: tensor(0.3254)\n",
      "31691 Training Loss: tensor(0.3261)\n",
      "31692 Training Loss: tensor(0.3257)\n",
      "31693 Training Loss: tensor(0.3258)\n",
      "31694 Training Loss: tensor(0.3254)\n",
      "31695 Training Loss: tensor(0.3256)\n",
      "31696 Training Loss: tensor(0.3270)\n",
      "31697 Training Loss: tensor(0.3260)\n",
      "31698 Training Loss: tensor(0.3269)\n",
      "31699 Training Loss: tensor(0.3256)\n",
      "31700 Training Loss: tensor(0.3259)\n",
      "31701 Training Loss: tensor(0.3266)\n",
      "31702 Training Loss: tensor(0.3263)\n",
      "31703 Training Loss: tensor(0.3267)\n",
      "31704 Training Loss: tensor(0.3269)\n",
      "31705 Training Loss: tensor(0.3258)\n",
      "31706 Training Loss: tensor(0.3259)\n",
      "31707 Training Loss: tensor(0.3262)\n",
      "31708 Training Loss: tensor(0.3257)\n",
      "31709 Training Loss: tensor(0.3261)\n",
      "31710 Training Loss: tensor(0.3260)\n",
      "31711 Training Loss: tensor(0.3262)\n",
      "31712 Training Loss: tensor(0.3268)\n",
      "31713 Training Loss: tensor(0.3260)\n",
      "31714 Training Loss: tensor(0.3256)\n",
      "31715 Training Loss: tensor(0.3261)\n",
      "31716 Training Loss: tensor(0.3257)\n",
      "31717 Training Loss: tensor(0.3269)\n",
      "31718 Training Loss: tensor(0.3256)\n",
      "31719 Training Loss: tensor(0.3257)\n",
      "31720 Training Loss: tensor(0.3255)\n",
      "31721 Training Loss: tensor(0.3265)\n",
      "31722 Training Loss: tensor(0.3260)\n",
      "31723 Training Loss: tensor(0.3266)\n",
      "31724 Training Loss: tensor(0.3257)\n",
      "31725 Training Loss: tensor(0.3258)\n",
      "31726 Training Loss: tensor(0.3256)\n",
      "31727 Training Loss: tensor(0.3257)\n",
      "31728 Training Loss: tensor(0.3256)\n",
      "31729 Training Loss: tensor(0.3278)\n",
      "31730 Training Loss: tensor(0.3257)\n",
      "31731 Training Loss: tensor(0.3259)\n",
      "31732 Training Loss: tensor(0.3258)\n",
      "31733 Training Loss: tensor(0.3258)\n",
      "31734 Training Loss: tensor(0.3261)\n",
      "31735 Training Loss: tensor(0.3275)\n",
      "31736 Training Loss: tensor(0.3258)\n",
      "31737 Training Loss: tensor(0.3253)\n",
      "31738 Training Loss: tensor(0.3266)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31739 Training Loss: tensor(0.3255)\n",
      "31740 Training Loss: tensor(0.3253)\n",
      "31741 Training Loss: tensor(0.3252)\n",
      "31742 Training Loss: tensor(0.3268)\n",
      "31743 Training Loss: tensor(0.3255)\n",
      "31744 Training Loss: tensor(0.3264)\n",
      "31745 Training Loss: tensor(0.3254)\n",
      "31746 Training Loss: tensor(0.3259)\n",
      "31747 Training Loss: tensor(0.3259)\n",
      "31748 Training Loss: tensor(0.3280)\n",
      "31749 Training Loss: tensor(0.3256)\n",
      "31750 Training Loss: tensor(0.3254)\n",
      "31751 Training Loss: tensor(0.3259)\n",
      "31752 Training Loss: tensor(0.3253)\n",
      "31753 Training Loss: tensor(0.3263)\n",
      "31754 Training Loss: tensor(0.3256)\n",
      "31755 Training Loss: tensor(0.3262)\n",
      "31756 Training Loss: tensor(0.3258)\n",
      "31757 Training Loss: tensor(0.3256)\n",
      "31758 Training Loss: tensor(0.3258)\n",
      "31759 Training Loss: tensor(0.3260)\n",
      "31760 Training Loss: tensor(0.3258)\n",
      "31761 Training Loss: tensor(0.3256)\n",
      "31762 Training Loss: tensor(0.3253)\n",
      "31763 Training Loss: tensor(0.3260)\n",
      "31764 Training Loss: tensor(0.3254)\n",
      "31765 Training Loss: tensor(0.3254)\n",
      "31766 Training Loss: tensor(0.3255)\n",
      "31767 Training Loss: tensor(0.3255)\n",
      "31768 Training Loss: tensor(0.3254)\n",
      "31769 Training Loss: tensor(0.3254)\n",
      "31770 Training Loss: tensor(0.3264)\n",
      "31771 Training Loss: tensor(0.3259)\n",
      "31772 Training Loss: tensor(0.3253)\n",
      "31773 Training Loss: tensor(0.3260)\n",
      "31774 Training Loss: tensor(0.3264)\n",
      "31775 Training Loss: tensor(0.3272)\n",
      "31776 Training Loss: tensor(0.3257)\n",
      "31777 Training Loss: tensor(0.3262)\n",
      "31778 Training Loss: tensor(0.3254)\n",
      "31779 Training Loss: tensor(0.3254)\n",
      "31780 Training Loss: tensor(0.3262)\n",
      "31781 Training Loss: tensor(0.3255)\n",
      "31782 Training Loss: tensor(0.3252)\n",
      "31783 Training Loss: tensor(0.3254)\n",
      "31784 Training Loss: tensor(0.3261)\n",
      "31785 Training Loss: tensor(0.3257)\n",
      "31786 Training Loss: tensor(0.3253)\n",
      "31787 Training Loss: tensor(0.3261)\n",
      "31788 Training Loss: tensor(0.3268)\n",
      "31789 Training Loss: tensor(0.3261)\n",
      "31790 Training Loss: tensor(0.3267)\n",
      "31791 Training Loss: tensor(0.3285)\n",
      "31792 Training Loss: tensor(0.3255)\n",
      "31793 Training Loss: tensor(0.3260)\n",
      "31794 Training Loss: tensor(0.3256)\n",
      "31795 Training Loss: tensor(0.3267)\n",
      "31796 Training Loss: tensor(0.3257)\n",
      "31797 Training Loss: tensor(0.3258)\n",
      "31798 Training Loss: tensor(0.3260)\n",
      "31799 Training Loss: tensor(0.3258)\n",
      "31800 Training Loss: tensor(0.3258)\n",
      "31801 Training Loss: tensor(0.3263)\n",
      "31802 Training Loss: tensor(0.3255)\n",
      "31803 Training Loss: tensor(0.3261)\n",
      "31804 Training Loss: tensor(0.3257)\n",
      "31805 Training Loss: tensor(0.3255)\n",
      "31806 Training Loss: tensor(0.3264)\n",
      "31807 Training Loss: tensor(0.3255)\n",
      "31808 Training Loss: tensor(0.3257)\n",
      "31809 Training Loss: tensor(0.3262)\n",
      "31810 Training Loss: tensor(0.3272)\n",
      "31811 Training Loss: tensor(0.3265)\n",
      "31812 Training Loss: tensor(0.3257)\n",
      "31813 Training Loss: tensor(0.3258)\n",
      "31814 Training Loss: tensor(0.3256)\n",
      "31815 Training Loss: tensor(0.3256)\n",
      "31816 Training Loss: tensor(0.3261)\n",
      "31817 Training Loss: tensor(0.3261)\n",
      "31818 Training Loss: tensor(0.3261)\n",
      "31819 Training Loss: tensor(0.3254)\n",
      "31820 Training Loss: tensor(0.3256)\n",
      "31821 Training Loss: tensor(0.3254)\n",
      "31822 Training Loss: tensor(0.3254)\n",
      "31823 Training Loss: tensor(0.3263)\n",
      "31824 Training Loss: tensor(0.3258)\n",
      "31825 Training Loss: tensor(0.3272)\n",
      "31826 Training Loss: tensor(0.3263)\n",
      "31827 Training Loss: tensor(0.3253)\n",
      "31828 Training Loss: tensor(0.3262)\n",
      "31829 Training Loss: tensor(0.3260)\n",
      "31830 Training Loss: tensor(0.3268)\n",
      "31831 Training Loss: tensor(0.3257)\n",
      "31832 Training Loss: tensor(0.3256)\n",
      "31833 Training Loss: tensor(0.3259)\n",
      "31834 Training Loss: tensor(0.3262)\n",
      "31835 Training Loss: tensor(0.3255)\n",
      "31836 Training Loss: tensor(0.3256)\n",
      "31837 Training Loss: tensor(0.3257)\n",
      "31838 Training Loss: tensor(0.3258)\n",
      "31839 Training Loss: tensor(0.3255)\n",
      "31840 Training Loss: tensor(0.3258)\n",
      "31841 Training Loss: tensor(0.3259)\n",
      "31842 Training Loss: tensor(0.3265)\n",
      "31843 Training Loss: tensor(0.3254)\n",
      "31844 Training Loss: tensor(0.3260)\n",
      "31845 Training Loss: tensor(0.3256)\n",
      "31846 Training Loss: tensor(0.3259)\n",
      "31847 Training Loss: tensor(0.3257)\n",
      "31848 Training Loss: tensor(0.3256)\n",
      "31849 Training Loss: tensor(0.3255)\n",
      "31850 Training Loss: tensor(0.3262)\n",
      "31851 Training Loss: tensor(0.3254)\n",
      "31852 Training Loss: tensor(0.3254)\n",
      "31853 Training Loss: tensor(0.3255)\n",
      "31854 Training Loss: tensor(0.3259)\n",
      "31855 Training Loss: tensor(0.3255)\n",
      "31856 Training Loss: tensor(0.3256)\n",
      "31857 Training Loss: tensor(0.3253)\n",
      "31858 Training Loss: tensor(0.3255)\n",
      "31859 Training Loss: tensor(0.3253)\n",
      "31860 Training Loss: tensor(0.3266)\n",
      "31861 Training Loss: tensor(0.3255)\n",
      "31862 Training Loss: tensor(0.3271)\n",
      "31863 Training Loss: tensor(0.3256)\n",
      "31864 Training Loss: tensor(0.3259)\n",
      "31865 Training Loss: tensor(0.3265)\n",
      "31866 Training Loss: tensor(0.3265)\n",
      "31867 Training Loss: tensor(0.3256)\n",
      "31868 Training Loss: tensor(0.3253)\n",
      "31869 Training Loss: tensor(0.3259)\n",
      "31870 Training Loss: tensor(0.3264)\n",
      "31871 Training Loss: tensor(0.3264)\n",
      "31872 Training Loss: tensor(0.3260)\n",
      "31873 Training Loss: tensor(0.3255)\n",
      "31874 Training Loss: tensor(0.3264)\n",
      "31875 Training Loss: tensor(0.3254)\n",
      "31876 Training Loss: tensor(0.3271)\n",
      "31877 Training Loss: tensor(0.3262)\n",
      "31878 Training Loss: tensor(0.3258)\n",
      "31879 Training Loss: tensor(0.3256)\n",
      "31880 Training Loss: tensor(0.3255)\n",
      "31881 Training Loss: tensor(0.3270)\n",
      "31882 Training Loss: tensor(0.3254)\n",
      "31883 Training Loss: tensor(0.3262)\n",
      "31884 Training Loss: tensor(0.3263)\n",
      "31885 Training Loss: tensor(0.3257)\n",
      "31886 Training Loss: tensor(0.3257)\n",
      "31887 Training Loss: tensor(0.3254)\n",
      "31888 Training Loss: tensor(0.3256)\n",
      "31889 Training Loss: tensor(0.3269)\n",
      "31890 Training Loss: tensor(0.3259)\n",
      "31891 Training Loss: tensor(0.3280)\n",
      "31892 Training Loss: tensor(0.3254)\n",
      "31893 Training Loss: tensor(0.3263)\n",
      "31894 Training Loss: tensor(0.3264)\n",
      "31895 Training Loss: tensor(0.3257)\n",
      "31896 Training Loss: tensor(0.3255)\n",
      "31897 Training Loss: tensor(0.3253)\n",
      "31898 Training Loss: tensor(0.3266)\n",
      "31899 Training Loss: tensor(0.3255)\n",
      "31900 Training Loss: tensor(0.3257)\n",
      "31901 Training Loss: tensor(0.3257)\n",
      "31902 Training Loss: tensor(0.3257)\n",
      "31903 Training Loss: tensor(0.3264)\n",
      "31904 Training Loss: tensor(0.3260)\n",
      "31905 Training Loss: tensor(0.3257)\n",
      "31906 Training Loss: tensor(0.3256)\n",
      "31907 Training Loss: tensor(0.3256)\n",
      "31908 Training Loss: tensor(0.3254)\n",
      "31909 Training Loss: tensor(0.3257)\n",
      "31910 Training Loss: tensor(0.3257)\n",
      "31911 Training Loss: tensor(0.3274)\n",
      "31912 Training Loss: tensor(0.3257)\n",
      "31913 Training Loss: tensor(0.3254)\n",
      "31914 Training Loss: tensor(0.3255)\n",
      "31915 Training Loss: tensor(0.3257)\n",
      "31916 Training Loss: tensor(0.3255)\n",
      "31917 Training Loss: tensor(0.3262)\n",
      "31918 Training Loss: tensor(0.3264)\n",
      "31919 Training Loss: tensor(0.3257)\n",
      "31920 Training Loss: tensor(0.3259)\n",
      "31921 Training Loss: tensor(0.3262)\n",
      "31922 Training Loss: tensor(0.3259)\n",
      "31923 Training Loss: tensor(0.3255)\n",
      "31924 Training Loss: tensor(0.3255)\n",
      "31925 Training Loss: tensor(0.3255)\n",
      "31926 Training Loss: tensor(0.3255)\n",
      "31927 Training Loss: tensor(0.3260)\n",
      "31928 Training Loss: tensor(0.3255)\n",
      "31929 Training Loss: tensor(0.3263)\n",
      "31930 Training Loss: tensor(0.3258)\n",
      "31931 Training Loss: tensor(0.3264)\n",
      "31932 Training Loss: tensor(0.3256)\n",
      "31933 Training Loss: tensor(0.3268)\n",
      "31934 Training Loss: tensor(0.3258)\n",
      "31935 Training Loss: tensor(0.3266)\n",
      "31936 Training Loss: tensor(0.3262)\n",
      "31937 Training Loss: tensor(0.3255)\n",
      "31938 Training Loss: tensor(0.3258)\n",
      "31939 Training Loss: tensor(0.3258)\n",
      "31940 Training Loss: tensor(0.3263)\n",
      "31941 Training Loss: tensor(0.3257)\n",
      "31942 Training Loss: tensor(0.3255)\n",
      "31943 Training Loss: tensor(0.3257)\n",
      "31944 Training Loss: tensor(0.3254)\n",
      "31945 Training Loss: tensor(0.3261)\n",
      "31946 Training Loss: tensor(0.3255)\n",
      "31947 Training Loss: tensor(0.3252)\n",
      "31948 Training Loss: tensor(0.3254)\n",
      "31949 Training Loss: tensor(0.3257)\n",
      "31950 Training Loss: tensor(0.3257)\n",
      "31951 Training Loss: tensor(0.3257)\n",
      "31952 Training Loss: tensor(0.3262)\n",
      "31953 Training Loss: tensor(0.3255)\n",
      "31954 Training Loss: tensor(0.3256)\n",
      "31955 Training Loss: tensor(0.3257)\n",
      "31956 Training Loss: tensor(0.3255)\n",
      "31957 Training Loss: tensor(0.3256)\n",
      "31958 Training Loss: tensor(0.3264)\n",
      "31959 Training Loss: tensor(0.3262)\n",
      "31960 Training Loss: tensor(0.3256)\n",
      "31961 Training Loss: tensor(0.3257)\n",
      "31962 Training Loss: tensor(0.3254)\n",
      "31963 Training Loss: tensor(0.3263)\n",
      "31964 Training Loss: tensor(0.3272)\n",
      "31965 Training Loss: tensor(0.3253)\n",
      "31966 Training Loss: tensor(0.3262)\n",
      "31967 Training Loss: tensor(0.3271)\n",
      "31968 Training Loss: tensor(0.3257)\n",
      "31969 Training Loss: tensor(0.3254)\n",
      "31970 Training Loss: tensor(0.3253)\n",
      "31971 Training Loss: tensor(0.3258)\n",
      "31972 Training Loss: tensor(0.3261)\n",
      "31973 Training Loss: tensor(0.3256)\n",
      "31974 Training Loss: tensor(0.3258)\n",
      "31975 Training Loss: tensor(0.3272)\n",
      "31976 Training Loss: tensor(0.3255)\n",
      "31977 Training Loss: tensor(0.3261)\n",
      "31978 Training Loss: tensor(0.3259)\n",
      "31979 Training Loss: tensor(0.3257)\n",
      "31980 Training Loss: tensor(0.3273)\n",
      "31981 Training Loss: tensor(0.3260)\n",
      "31982 Training Loss: tensor(0.3253)\n",
      "31983 Training Loss: tensor(0.3253)\n",
      "31984 Training Loss: tensor(0.3257)\n",
      "31985 Training Loss: tensor(0.3258)\n",
      "31986 Training Loss: tensor(0.3255)\n",
      "31987 Training Loss: tensor(0.3254)\n",
      "31988 Training Loss: tensor(0.3254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31989 Training Loss: tensor(0.3290)\n",
      "31990 Training Loss: tensor(0.3253)\n",
      "31991 Training Loss: tensor(0.3266)\n",
      "31992 Training Loss: tensor(0.3254)\n",
      "31993 Training Loss: tensor(0.3259)\n",
      "31994 Training Loss: tensor(0.3257)\n",
      "31995 Training Loss: tensor(0.3257)\n",
      "31996 Training Loss: tensor(0.3262)\n",
      "31997 Training Loss: tensor(0.3265)\n",
      "31998 Training Loss: tensor(0.3260)\n",
      "31999 Training Loss: tensor(0.3255)\n",
      "32000 Training Loss: tensor(0.3257)\n",
      "32001 Training Loss: tensor(0.3256)\n",
      "32002 Training Loss: tensor(0.3254)\n",
      "32003 Training Loss: tensor(0.3257)\n",
      "32004 Training Loss: tensor(0.3262)\n",
      "32005 Training Loss: tensor(0.3262)\n",
      "32006 Training Loss: tensor(0.3254)\n",
      "32007 Training Loss: tensor(0.3255)\n",
      "32008 Training Loss: tensor(0.3254)\n",
      "32009 Training Loss: tensor(0.3261)\n",
      "32010 Training Loss: tensor(0.3255)\n",
      "32011 Training Loss: tensor(0.3257)\n",
      "32012 Training Loss: tensor(0.3253)\n",
      "32013 Training Loss: tensor(0.3256)\n",
      "32014 Training Loss: tensor(0.3258)\n",
      "32015 Training Loss: tensor(0.3263)\n",
      "32016 Training Loss: tensor(0.3256)\n",
      "32017 Training Loss: tensor(0.3253)\n",
      "32018 Training Loss: tensor(0.3253)\n",
      "32019 Training Loss: tensor(0.3271)\n",
      "32020 Training Loss: tensor(0.3255)\n",
      "32021 Training Loss: tensor(0.3253)\n",
      "32022 Training Loss: tensor(0.3280)\n",
      "32023 Training Loss: tensor(0.3266)\n",
      "32024 Training Loss: tensor(0.3267)\n",
      "32025 Training Loss: tensor(0.3257)\n",
      "32026 Training Loss: tensor(0.3256)\n",
      "32027 Training Loss: tensor(0.3262)\n",
      "32028 Training Loss: tensor(0.3267)\n",
      "32029 Training Loss: tensor(0.3260)\n",
      "32030 Training Loss: tensor(0.3262)\n",
      "32031 Training Loss: tensor(0.3259)\n",
      "32032 Training Loss: tensor(0.3258)\n",
      "32033 Training Loss: tensor(0.3258)\n",
      "32034 Training Loss: tensor(0.3260)\n",
      "32035 Training Loss: tensor(0.3256)\n",
      "32036 Training Loss: tensor(0.3255)\n",
      "32037 Training Loss: tensor(0.3253)\n",
      "32038 Training Loss: tensor(0.3253)\n",
      "32039 Training Loss: tensor(0.3256)\n",
      "32040 Training Loss: tensor(0.3285)\n",
      "32041 Training Loss: tensor(0.3263)\n",
      "32042 Training Loss: tensor(0.3258)\n",
      "32043 Training Loss: tensor(0.3255)\n",
      "32044 Training Loss: tensor(0.3252)\n",
      "32045 Training Loss: tensor(0.3254)\n",
      "32046 Training Loss: tensor(0.3268)\n",
      "32047 Training Loss: tensor(0.3255)\n",
      "32048 Training Loss: tensor(0.3256)\n",
      "32049 Training Loss: tensor(0.3254)\n",
      "32050 Training Loss: tensor(0.3261)\n",
      "32051 Training Loss: tensor(0.3257)\n",
      "32052 Training Loss: tensor(0.3254)\n",
      "32053 Training Loss: tensor(0.3262)\n",
      "32054 Training Loss: tensor(0.3254)\n",
      "32055 Training Loss: tensor(0.3262)\n",
      "32056 Training Loss: tensor(0.3262)\n",
      "32057 Training Loss: tensor(0.3266)\n",
      "32058 Training Loss: tensor(0.3265)\n",
      "32059 Training Loss: tensor(0.3257)\n",
      "32060 Training Loss: tensor(0.3260)\n",
      "32061 Training Loss: tensor(0.3256)\n",
      "32062 Training Loss: tensor(0.3262)\n",
      "32063 Training Loss: tensor(0.3269)\n",
      "32064 Training Loss: tensor(0.3256)\n",
      "32065 Training Loss: tensor(0.3258)\n",
      "32066 Training Loss: tensor(0.3262)\n",
      "32067 Training Loss: tensor(0.3271)\n",
      "32068 Training Loss: tensor(0.3261)\n",
      "32069 Training Loss: tensor(0.3255)\n",
      "32070 Training Loss: tensor(0.3260)\n",
      "32071 Training Loss: tensor(0.3256)\n",
      "32072 Training Loss: tensor(0.3261)\n",
      "32073 Training Loss: tensor(0.3256)\n",
      "32074 Training Loss: tensor(0.3266)\n",
      "32075 Training Loss: tensor(0.3267)\n",
      "32076 Training Loss: tensor(0.3263)\n",
      "32077 Training Loss: tensor(0.3259)\n",
      "32078 Training Loss: tensor(0.3260)\n",
      "32079 Training Loss: tensor(0.3257)\n",
      "32080 Training Loss: tensor(0.3260)\n",
      "32081 Training Loss: tensor(0.3255)\n",
      "32082 Training Loss: tensor(0.3256)\n",
      "32083 Training Loss: tensor(0.3263)\n",
      "32084 Training Loss: tensor(0.3253)\n",
      "32085 Training Loss: tensor(0.3259)\n",
      "32086 Training Loss: tensor(0.3257)\n",
      "32087 Training Loss: tensor(0.3255)\n",
      "32088 Training Loss: tensor(0.3252)\n",
      "32089 Training Loss: tensor(0.3259)\n",
      "32090 Training Loss: tensor(0.3274)\n",
      "32091 Training Loss: tensor(0.3253)\n",
      "32092 Training Loss: tensor(0.3252)\n",
      "32093 Training Loss: tensor(0.3260)\n",
      "32094 Training Loss: tensor(0.3266)\n",
      "32095 Training Loss: tensor(0.3261)\n",
      "32096 Training Loss: tensor(0.3261)\n",
      "32097 Training Loss: tensor(0.3259)\n",
      "32098 Training Loss: tensor(0.3253)\n",
      "32099 Training Loss: tensor(0.3266)\n",
      "32100 Training Loss: tensor(0.3258)\n",
      "32101 Training Loss: tensor(0.3257)\n",
      "32102 Training Loss: tensor(0.3259)\n",
      "32103 Training Loss: tensor(0.3258)\n",
      "32104 Training Loss: tensor(0.3259)\n",
      "32105 Training Loss: tensor(0.3253)\n",
      "32106 Training Loss: tensor(0.3253)\n",
      "32107 Training Loss: tensor(0.3258)\n",
      "32108 Training Loss: tensor(0.3258)\n",
      "32109 Training Loss: tensor(0.3265)\n",
      "32110 Training Loss: tensor(0.3256)\n",
      "32111 Training Loss: tensor(0.3256)\n",
      "32112 Training Loss: tensor(0.3255)\n",
      "32113 Training Loss: tensor(0.3256)\n",
      "32114 Training Loss: tensor(0.3259)\n",
      "32115 Training Loss: tensor(0.3258)\n",
      "32116 Training Loss: tensor(0.3259)\n",
      "32117 Training Loss: tensor(0.3254)\n",
      "32118 Training Loss: tensor(0.3267)\n",
      "32119 Training Loss: tensor(0.3253)\n",
      "32120 Training Loss: tensor(0.3255)\n",
      "32121 Training Loss: tensor(0.3256)\n",
      "32122 Training Loss: tensor(0.3264)\n",
      "32123 Training Loss: tensor(0.3262)\n",
      "32124 Training Loss: tensor(0.3258)\n",
      "32125 Training Loss: tensor(0.3255)\n",
      "32126 Training Loss: tensor(0.3257)\n",
      "32127 Training Loss: tensor(0.3253)\n",
      "32128 Training Loss: tensor(0.3256)\n",
      "32129 Training Loss: tensor(0.3254)\n",
      "32130 Training Loss: tensor(0.3265)\n",
      "32131 Training Loss: tensor(0.3255)\n",
      "32132 Training Loss: tensor(0.3258)\n",
      "32133 Training Loss: tensor(0.3263)\n",
      "32134 Training Loss: tensor(0.3256)\n",
      "32135 Training Loss: tensor(0.3257)\n",
      "32136 Training Loss: tensor(0.3255)\n",
      "32137 Training Loss: tensor(0.3253)\n",
      "32138 Training Loss: tensor(0.3256)\n",
      "32139 Training Loss: tensor(0.3257)\n",
      "32140 Training Loss: tensor(0.3256)\n",
      "32141 Training Loss: tensor(0.3254)\n",
      "32142 Training Loss: tensor(0.3251)\n",
      "32143 Training Loss: tensor(0.3258)\n",
      "32144 Training Loss: tensor(0.3269)\n",
      "32145 Training Loss: tensor(0.3257)\n",
      "32146 Training Loss: tensor(0.3259)\n",
      "32147 Training Loss: tensor(0.3260)\n",
      "32148 Training Loss: tensor(0.3257)\n",
      "32149 Training Loss: tensor(0.3277)\n",
      "32150 Training Loss: tensor(0.3256)\n",
      "32151 Training Loss: tensor(0.3256)\n",
      "32152 Training Loss: tensor(0.3260)\n",
      "32153 Training Loss: tensor(0.3258)\n",
      "32154 Training Loss: tensor(0.3261)\n",
      "32155 Training Loss: tensor(0.3253)\n",
      "32156 Training Loss: tensor(0.3253)\n",
      "32157 Training Loss: tensor(0.3255)\n",
      "32158 Training Loss: tensor(0.3263)\n",
      "32159 Training Loss: tensor(0.3257)\n",
      "32160 Training Loss: tensor(0.3253)\n",
      "32161 Training Loss: tensor(0.3253)\n",
      "32162 Training Loss: tensor(0.3254)\n",
      "32163 Training Loss: tensor(0.3259)\n",
      "32164 Training Loss: tensor(0.3254)\n",
      "32165 Training Loss: tensor(0.3256)\n",
      "32166 Training Loss: tensor(0.3261)\n",
      "32167 Training Loss: tensor(0.3255)\n",
      "32168 Training Loss: tensor(0.3258)\n",
      "32169 Training Loss: tensor(0.3266)\n",
      "32170 Training Loss: tensor(0.3253)\n",
      "32171 Training Loss: tensor(0.3254)\n",
      "32172 Training Loss: tensor(0.3255)\n",
      "32173 Training Loss: tensor(0.3258)\n",
      "32174 Training Loss: tensor(0.3256)\n",
      "32175 Training Loss: tensor(0.3263)\n",
      "32176 Training Loss: tensor(0.3259)\n",
      "32177 Training Loss: tensor(0.3267)\n",
      "32178 Training Loss: tensor(0.3266)\n",
      "32179 Training Loss: tensor(0.3254)\n",
      "32180 Training Loss: tensor(0.3258)\n",
      "32181 Training Loss: tensor(0.3257)\n",
      "32182 Training Loss: tensor(0.3251)\n",
      "32183 Training Loss: tensor(0.3253)\n",
      "32184 Training Loss: tensor(0.3256)\n",
      "32185 Training Loss: tensor(0.3255)\n",
      "32186 Training Loss: tensor(0.3260)\n",
      "32187 Training Loss: tensor(0.3254)\n",
      "32188 Training Loss: tensor(0.3259)\n",
      "32189 Training Loss: tensor(0.3265)\n",
      "32190 Training Loss: tensor(0.3255)\n",
      "32191 Training Loss: tensor(0.3267)\n",
      "32192 Training Loss: tensor(0.3265)\n",
      "32193 Training Loss: tensor(0.3267)\n",
      "32194 Training Loss: tensor(0.3258)\n",
      "32195 Training Loss: tensor(0.3267)\n",
      "32196 Training Loss: tensor(0.3259)\n",
      "32197 Training Loss: tensor(0.3254)\n",
      "32198 Training Loss: tensor(0.3259)\n",
      "32199 Training Loss: tensor(0.3260)\n",
      "32200 Training Loss: tensor(0.3259)\n",
      "32201 Training Loss: tensor(0.3281)\n",
      "32202 Training Loss: tensor(0.3261)\n",
      "32203 Training Loss: tensor(0.3259)\n",
      "32204 Training Loss: tensor(0.3258)\n",
      "32205 Training Loss: tensor(0.3254)\n",
      "32206 Training Loss: tensor(0.3254)\n",
      "32207 Training Loss: tensor(0.3272)\n",
      "32208 Training Loss: tensor(0.3254)\n",
      "32209 Training Loss: tensor(0.3256)\n",
      "32210 Training Loss: tensor(0.3256)\n",
      "32211 Training Loss: tensor(0.3254)\n",
      "32212 Training Loss: tensor(0.3258)\n",
      "32213 Training Loss: tensor(0.3260)\n",
      "32214 Training Loss: tensor(0.3256)\n",
      "32215 Training Loss: tensor(0.3257)\n",
      "32216 Training Loss: tensor(0.3259)\n",
      "32217 Training Loss: tensor(0.3260)\n",
      "32218 Training Loss: tensor(0.3260)\n",
      "32219 Training Loss: tensor(0.3260)\n",
      "32220 Training Loss: tensor(0.3264)\n",
      "32221 Training Loss: tensor(0.3253)\n",
      "32222 Training Loss: tensor(0.3266)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32223 Training Loss: tensor(0.3253)\n",
      "32224 Training Loss: tensor(0.3256)\n",
      "32225 Training Loss: tensor(0.3253)\n",
      "32226 Training Loss: tensor(0.3255)\n",
      "32227 Training Loss: tensor(0.3253)\n",
      "32228 Training Loss: tensor(0.3266)\n",
      "32229 Training Loss: tensor(0.3260)\n",
      "32230 Training Loss: tensor(0.3257)\n",
      "32231 Training Loss: tensor(0.3267)\n",
      "32232 Training Loss: tensor(0.3257)\n",
      "32233 Training Loss: tensor(0.3274)\n",
      "32234 Training Loss: tensor(0.3264)\n",
      "32235 Training Loss: tensor(0.3262)\n",
      "32236 Training Loss: tensor(0.3256)\n",
      "32237 Training Loss: tensor(0.3259)\n",
      "32238 Training Loss: tensor(0.3257)\n",
      "32239 Training Loss: tensor(0.3255)\n",
      "32240 Training Loss: tensor(0.3261)\n",
      "32241 Training Loss: tensor(0.3260)\n",
      "32242 Training Loss: tensor(0.3258)\n",
      "32243 Training Loss: tensor(0.3257)\n",
      "32244 Training Loss: tensor(0.3256)\n",
      "32245 Training Loss: tensor(0.3260)\n",
      "32246 Training Loss: tensor(0.3273)\n",
      "32247 Training Loss: tensor(0.3255)\n",
      "32248 Training Loss: tensor(0.3253)\n",
      "32249 Training Loss: tensor(0.3261)\n",
      "32250 Training Loss: tensor(0.3256)\n",
      "32251 Training Loss: tensor(0.3255)\n",
      "32252 Training Loss: tensor(0.3256)\n",
      "32253 Training Loss: tensor(0.3254)\n",
      "32254 Training Loss: tensor(0.3269)\n",
      "32255 Training Loss: tensor(0.3269)\n",
      "32256 Training Loss: tensor(0.3259)\n",
      "32257 Training Loss: tensor(0.3256)\n",
      "32258 Training Loss: tensor(0.3255)\n",
      "32259 Training Loss: tensor(0.3256)\n",
      "32260 Training Loss: tensor(0.3260)\n",
      "32261 Training Loss: tensor(0.3252)\n",
      "32262 Training Loss: tensor(0.3255)\n",
      "32263 Training Loss: tensor(0.3255)\n",
      "32264 Training Loss: tensor(0.3256)\n",
      "32265 Training Loss: tensor(0.3260)\n",
      "32266 Training Loss: tensor(0.3253)\n",
      "32267 Training Loss: tensor(0.3269)\n",
      "32268 Training Loss: tensor(0.3255)\n",
      "32269 Training Loss: tensor(0.3266)\n",
      "32270 Training Loss: tensor(0.3253)\n",
      "32271 Training Loss: tensor(0.3255)\n",
      "32272 Training Loss: tensor(0.3265)\n",
      "32273 Training Loss: tensor(0.3264)\n",
      "32274 Training Loss: tensor(0.3258)\n",
      "32275 Training Loss: tensor(0.3253)\n",
      "32276 Training Loss: tensor(0.3264)\n",
      "32277 Training Loss: tensor(0.3255)\n",
      "32278 Training Loss: tensor(0.3258)\n",
      "32279 Training Loss: tensor(0.3261)\n",
      "32280 Training Loss: tensor(0.3256)\n",
      "32281 Training Loss: tensor(0.3260)\n",
      "32282 Training Loss: tensor(0.3260)\n",
      "32283 Training Loss: tensor(0.3255)\n",
      "32284 Training Loss: tensor(0.3254)\n",
      "32285 Training Loss: tensor(0.3259)\n",
      "32286 Training Loss: tensor(0.3254)\n",
      "32287 Training Loss: tensor(0.3274)\n",
      "32288 Training Loss: tensor(0.3259)\n",
      "32289 Training Loss: tensor(0.3253)\n",
      "32290 Training Loss: tensor(0.3255)\n",
      "32291 Training Loss: tensor(0.3258)\n",
      "32292 Training Loss: tensor(0.3267)\n",
      "32293 Training Loss: tensor(0.3270)\n",
      "32294 Training Loss: tensor(0.3258)\n",
      "32295 Training Loss: tensor(0.3255)\n",
      "32296 Training Loss: tensor(0.3264)\n",
      "32297 Training Loss: tensor(0.3261)\n",
      "32298 Training Loss: tensor(0.3263)\n",
      "32299 Training Loss: tensor(0.3255)\n",
      "32300 Training Loss: tensor(0.3264)\n",
      "32301 Training Loss: tensor(0.3256)\n",
      "32302 Training Loss: tensor(0.3261)\n",
      "32303 Training Loss: tensor(0.3259)\n",
      "32304 Training Loss: tensor(0.3256)\n",
      "32305 Training Loss: tensor(0.3254)\n",
      "32306 Training Loss: tensor(0.3265)\n",
      "32307 Training Loss: tensor(0.3253)\n",
      "32308 Training Loss: tensor(0.3253)\n",
      "32309 Training Loss: tensor(0.3255)\n",
      "32310 Training Loss: tensor(0.3254)\n",
      "32311 Training Loss: tensor(0.3253)\n",
      "32312 Training Loss: tensor(0.3257)\n",
      "32313 Training Loss: tensor(0.3261)\n",
      "32314 Training Loss: tensor(0.3257)\n",
      "32315 Training Loss: tensor(0.3254)\n",
      "32316 Training Loss: tensor(0.3253)\n",
      "32317 Training Loss: tensor(0.3258)\n",
      "32318 Training Loss: tensor(0.3255)\n",
      "32319 Training Loss: tensor(0.3255)\n",
      "32320 Training Loss: tensor(0.3257)\n",
      "32321 Training Loss: tensor(0.3259)\n",
      "32322 Training Loss: tensor(0.3262)\n",
      "32323 Training Loss: tensor(0.3259)\n",
      "32324 Training Loss: tensor(0.3255)\n",
      "32325 Training Loss: tensor(0.3260)\n",
      "32326 Training Loss: tensor(0.3252)\n",
      "32327 Training Loss: tensor(0.3253)\n",
      "32328 Training Loss: tensor(0.3257)\n",
      "32329 Training Loss: tensor(0.3256)\n",
      "32330 Training Loss: tensor(0.3262)\n",
      "32331 Training Loss: tensor(0.3253)\n",
      "32332 Training Loss: tensor(0.3262)\n",
      "32333 Training Loss: tensor(0.3253)\n",
      "32334 Training Loss: tensor(0.3273)\n",
      "32335 Training Loss: tensor(0.3257)\n",
      "32336 Training Loss: tensor(0.3263)\n",
      "32337 Training Loss: tensor(0.3256)\n",
      "32338 Training Loss: tensor(0.3254)\n",
      "32339 Training Loss: tensor(0.3264)\n",
      "32340 Training Loss: tensor(0.3260)\n",
      "32341 Training Loss: tensor(0.3253)\n",
      "32342 Training Loss: tensor(0.3254)\n",
      "32343 Training Loss: tensor(0.3255)\n",
      "32344 Training Loss: tensor(0.3258)\n",
      "32345 Training Loss: tensor(0.3262)\n",
      "32346 Training Loss: tensor(0.3263)\n",
      "32347 Training Loss: tensor(0.3259)\n",
      "32348 Training Loss: tensor(0.3254)\n",
      "32349 Training Loss: tensor(0.3253)\n",
      "32350 Training Loss: tensor(0.3255)\n",
      "32351 Training Loss: tensor(0.3256)\n",
      "32352 Training Loss: tensor(0.3255)\n",
      "32353 Training Loss: tensor(0.3255)\n",
      "32354 Training Loss: tensor(0.3254)\n",
      "32355 Training Loss: tensor(0.3259)\n",
      "32356 Training Loss: tensor(0.3252)\n",
      "32357 Training Loss: tensor(0.3253)\n",
      "32358 Training Loss: tensor(0.3283)\n",
      "32359 Training Loss: tensor(0.3265)\n",
      "32360 Training Loss: tensor(0.3258)\n",
      "32361 Training Loss: tensor(0.3262)\n",
      "32362 Training Loss: tensor(0.3264)\n",
      "32363 Training Loss: tensor(0.3256)\n",
      "32364 Training Loss: tensor(0.3258)\n",
      "32365 Training Loss: tensor(0.3258)\n",
      "32366 Training Loss: tensor(0.3254)\n",
      "32367 Training Loss: tensor(0.3263)\n",
      "32368 Training Loss: tensor(0.3254)\n",
      "32369 Training Loss: tensor(0.3262)\n",
      "32370 Training Loss: tensor(0.3258)\n",
      "32371 Training Loss: tensor(0.3254)\n",
      "32372 Training Loss: tensor(0.3256)\n",
      "32373 Training Loss: tensor(0.3256)\n",
      "32374 Training Loss: tensor(0.3257)\n",
      "32375 Training Loss: tensor(0.3253)\n",
      "32376 Training Loss: tensor(0.3254)\n",
      "32377 Training Loss: tensor(0.3254)\n",
      "32378 Training Loss: tensor(0.3256)\n",
      "32379 Training Loss: tensor(0.3256)\n",
      "32380 Training Loss: tensor(0.3255)\n",
      "32381 Training Loss: tensor(0.3254)\n",
      "32382 Training Loss: tensor(0.3265)\n",
      "32383 Training Loss: tensor(0.3294)\n",
      "32384 Training Loss: tensor(0.3259)\n",
      "32385 Training Loss: tensor(0.3258)\n",
      "32386 Training Loss: tensor(0.3256)\n",
      "32387 Training Loss: tensor(0.3256)\n",
      "32388 Training Loss: tensor(0.3255)\n",
      "32389 Training Loss: tensor(0.3256)\n",
      "32390 Training Loss: tensor(0.3253)\n",
      "32391 Training Loss: tensor(0.3253)\n",
      "32392 Training Loss: tensor(0.3258)\n",
      "32393 Training Loss: tensor(0.3256)\n",
      "32394 Training Loss: tensor(0.3253)\n",
      "32395 Training Loss: tensor(0.3265)\n",
      "32396 Training Loss: tensor(0.3254)\n",
      "32397 Training Loss: tensor(0.3259)\n",
      "32398 Training Loss: tensor(0.3263)\n",
      "32399 Training Loss: tensor(0.3261)\n",
      "32400 Training Loss: tensor(0.3254)\n",
      "32401 Training Loss: tensor(0.3260)\n",
      "32402 Training Loss: tensor(0.3257)\n",
      "32403 Training Loss: tensor(0.3263)\n",
      "32404 Training Loss: tensor(0.3262)\n",
      "32405 Training Loss: tensor(0.3257)\n",
      "32406 Training Loss: tensor(0.3257)\n",
      "32407 Training Loss: tensor(0.3260)\n",
      "32408 Training Loss: tensor(0.3252)\n",
      "32409 Training Loss: tensor(0.3251)\n",
      "32410 Training Loss: tensor(0.3290)\n",
      "32411 Training Loss: tensor(0.3254)\n",
      "32412 Training Loss: tensor(0.3255)\n",
      "32413 Training Loss: tensor(0.3258)\n",
      "32414 Training Loss: tensor(0.3256)\n",
      "32415 Training Loss: tensor(0.3256)\n",
      "32416 Training Loss: tensor(0.3258)\n",
      "32417 Training Loss: tensor(0.3254)\n",
      "32418 Training Loss: tensor(0.3255)\n",
      "32419 Training Loss: tensor(0.3260)\n",
      "32420 Training Loss: tensor(0.3257)\n",
      "32421 Training Loss: tensor(0.3254)\n",
      "32422 Training Loss: tensor(0.3258)\n",
      "32423 Training Loss: tensor(0.3252)\n",
      "32424 Training Loss: tensor(0.3254)\n",
      "32425 Training Loss: tensor(0.3254)\n",
      "32426 Training Loss: tensor(0.3259)\n",
      "32427 Training Loss: tensor(0.3254)\n",
      "32428 Training Loss: tensor(0.3263)\n",
      "32429 Training Loss: tensor(0.3253)\n",
      "32430 Training Loss: tensor(0.3253)\n",
      "32431 Training Loss: tensor(0.3257)\n",
      "32432 Training Loss: tensor(0.3255)\n",
      "32433 Training Loss: tensor(0.3256)\n",
      "32434 Training Loss: tensor(0.3260)\n",
      "32435 Training Loss: tensor(0.3255)\n",
      "32436 Training Loss: tensor(0.3253)\n",
      "32437 Training Loss: tensor(0.3255)\n",
      "32438 Training Loss: tensor(0.3257)\n",
      "32439 Training Loss: tensor(0.3257)\n",
      "32440 Training Loss: tensor(0.3253)\n",
      "32441 Training Loss: tensor(0.3253)\n",
      "32442 Training Loss: tensor(0.3252)\n",
      "32443 Training Loss: tensor(0.3258)\n",
      "32444 Training Loss: tensor(0.3253)\n",
      "32445 Training Loss: tensor(0.3259)\n",
      "32446 Training Loss: tensor(0.3253)\n",
      "32447 Training Loss: tensor(0.3254)\n",
      "32448 Training Loss: tensor(0.3257)\n",
      "32449 Training Loss: tensor(0.3260)\n",
      "32450 Training Loss: tensor(0.3255)\n",
      "32451 Training Loss: tensor(0.3255)\n",
      "32452 Training Loss: tensor(0.3257)\n",
      "32453 Training Loss: tensor(0.3261)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32454 Training Loss: tensor(0.3256)\n",
      "32455 Training Loss: tensor(0.3253)\n",
      "32456 Training Loss: tensor(0.3256)\n",
      "32457 Training Loss: tensor(0.3255)\n",
      "32458 Training Loss: tensor(0.3261)\n",
      "32459 Training Loss: tensor(0.3252)\n",
      "32460 Training Loss: tensor(0.3261)\n",
      "32461 Training Loss: tensor(0.3256)\n",
      "32462 Training Loss: tensor(0.3256)\n",
      "32463 Training Loss: tensor(0.3256)\n",
      "32464 Training Loss: tensor(0.3255)\n",
      "32465 Training Loss: tensor(0.3255)\n",
      "32466 Training Loss: tensor(0.3267)\n",
      "32467 Training Loss: tensor(0.3253)\n",
      "32468 Training Loss: tensor(0.3261)\n",
      "32469 Training Loss: tensor(0.3259)\n",
      "32470 Training Loss: tensor(0.3255)\n",
      "32471 Training Loss: tensor(0.3257)\n",
      "32472 Training Loss: tensor(0.3259)\n",
      "32473 Training Loss: tensor(0.3255)\n",
      "32474 Training Loss: tensor(0.3252)\n",
      "32475 Training Loss: tensor(0.3257)\n",
      "32476 Training Loss: tensor(0.3253)\n",
      "32477 Training Loss: tensor(0.3252)\n",
      "32478 Training Loss: tensor(0.3254)\n",
      "32479 Training Loss: tensor(0.3264)\n",
      "32480 Training Loss: tensor(0.3253)\n",
      "32481 Training Loss: tensor(0.3251)\n",
      "32482 Training Loss: tensor(0.3263)\n",
      "32483 Training Loss: tensor(0.3258)\n",
      "32484 Training Loss: tensor(0.3257)\n",
      "32485 Training Loss: tensor(0.3260)\n",
      "32486 Training Loss: tensor(0.3260)\n",
      "32487 Training Loss: tensor(0.3270)\n",
      "32488 Training Loss: tensor(0.3258)\n",
      "32489 Training Loss: tensor(0.3252)\n",
      "32490 Training Loss: tensor(0.3255)\n",
      "32491 Training Loss: tensor(0.3257)\n",
      "32492 Training Loss: tensor(0.3266)\n",
      "32493 Training Loss: tensor(0.3254)\n",
      "32494 Training Loss: tensor(0.3254)\n",
      "32495 Training Loss: tensor(0.3254)\n",
      "32496 Training Loss: tensor(0.3252)\n",
      "32497 Training Loss: tensor(0.3265)\n",
      "32498 Training Loss: tensor(0.3271)\n",
      "32499 Training Loss: tensor(0.3254)\n",
      "32500 Training Loss: tensor(0.3270)\n",
      "32501 Training Loss: tensor(0.3253)\n",
      "32502 Training Loss: tensor(0.3253)\n",
      "32503 Training Loss: tensor(0.3260)\n",
      "32504 Training Loss: tensor(0.3252)\n",
      "32505 Training Loss: tensor(0.3258)\n",
      "32506 Training Loss: tensor(0.3254)\n",
      "32507 Training Loss: tensor(0.3255)\n",
      "32508 Training Loss: tensor(0.3264)\n",
      "32509 Training Loss: tensor(0.3253)\n",
      "32510 Training Loss: tensor(0.3255)\n",
      "32511 Training Loss: tensor(0.3259)\n",
      "32512 Training Loss: tensor(0.3265)\n",
      "32513 Training Loss: tensor(0.3257)\n",
      "32514 Training Loss: tensor(0.3267)\n",
      "32515 Training Loss: tensor(0.3253)\n",
      "32516 Training Loss: tensor(0.3254)\n",
      "32517 Training Loss: tensor(0.3254)\n",
      "32518 Training Loss: tensor(0.3257)\n",
      "32519 Training Loss: tensor(0.3262)\n",
      "32520 Training Loss: tensor(0.3263)\n",
      "32521 Training Loss: tensor(0.3255)\n",
      "32522 Training Loss: tensor(0.3253)\n",
      "32523 Training Loss: tensor(0.3255)\n",
      "32524 Training Loss: tensor(0.3259)\n",
      "32525 Training Loss: tensor(0.3256)\n",
      "32526 Training Loss: tensor(0.3260)\n",
      "32527 Training Loss: tensor(0.3261)\n",
      "32528 Training Loss: tensor(0.3270)\n",
      "32529 Training Loss: tensor(0.3252)\n",
      "32530 Training Loss: tensor(0.3252)\n",
      "32531 Training Loss: tensor(0.3257)\n",
      "32532 Training Loss: tensor(0.3259)\n",
      "32533 Training Loss: tensor(0.3254)\n",
      "32534 Training Loss: tensor(0.3261)\n",
      "32535 Training Loss: tensor(0.3255)\n",
      "32536 Training Loss: tensor(0.3259)\n",
      "32537 Training Loss: tensor(0.3253)\n",
      "32538 Training Loss: tensor(0.3253)\n",
      "32539 Training Loss: tensor(0.3256)\n",
      "32540 Training Loss: tensor(0.3260)\n",
      "32541 Training Loss: tensor(0.3255)\n",
      "32542 Training Loss: tensor(0.3252)\n",
      "32543 Training Loss: tensor(0.3273)\n",
      "32544 Training Loss: tensor(0.3255)\n",
      "32545 Training Loss: tensor(0.3273)\n",
      "32546 Training Loss: tensor(0.3273)\n",
      "32547 Training Loss: tensor(0.3262)\n",
      "32548 Training Loss: tensor(0.3254)\n",
      "32549 Training Loss: tensor(0.3255)\n",
      "32550 Training Loss: tensor(0.3268)\n",
      "32551 Training Loss: tensor(0.3256)\n",
      "32552 Training Loss: tensor(0.3257)\n",
      "32553 Training Loss: tensor(0.3259)\n",
      "32554 Training Loss: tensor(0.3254)\n",
      "32555 Training Loss: tensor(0.3255)\n",
      "32556 Training Loss: tensor(0.3254)\n",
      "32557 Training Loss: tensor(0.3257)\n",
      "32558 Training Loss: tensor(0.3266)\n",
      "32559 Training Loss: tensor(0.3257)\n",
      "32560 Training Loss: tensor(0.3261)\n",
      "32561 Training Loss: tensor(0.3261)\n",
      "32562 Training Loss: tensor(0.3274)\n",
      "32563 Training Loss: tensor(0.3254)\n",
      "32564 Training Loss: tensor(0.3262)\n",
      "32565 Training Loss: tensor(0.3258)\n",
      "32566 Training Loss: tensor(0.3257)\n",
      "32567 Training Loss: tensor(0.3259)\n",
      "32568 Training Loss: tensor(0.3262)\n",
      "32569 Training Loss: tensor(0.3280)\n",
      "32570 Training Loss: tensor(0.3259)\n",
      "32571 Training Loss: tensor(0.3257)\n",
      "32572 Training Loss: tensor(0.3262)\n",
      "32573 Training Loss: tensor(0.3261)\n",
      "32574 Training Loss: tensor(0.3259)\n",
      "32575 Training Loss: tensor(0.3259)\n",
      "32576 Training Loss: tensor(0.3255)\n",
      "32577 Training Loss: tensor(0.3258)\n",
      "32578 Training Loss: tensor(0.3260)\n",
      "32579 Training Loss: tensor(0.3263)\n",
      "32580 Training Loss: tensor(0.3266)\n",
      "32581 Training Loss: tensor(0.3253)\n",
      "32582 Training Loss: tensor(0.3254)\n",
      "32583 Training Loss: tensor(0.3258)\n",
      "32584 Training Loss: tensor(0.3269)\n",
      "32585 Training Loss: tensor(0.3262)\n",
      "32586 Training Loss: tensor(0.3260)\n",
      "32587 Training Loss: tensor(0.3258)\n",
      "32588 Training Loss: tensor(0.3255)\n",
      "32589 Training Loss: tensor(0.3254)\n",
      "32590 Training Loss: tensor(0.3261)\n",
      "32591 Training Loss: tensor(0.3257)\n",
      "32592 Training Loss: tensor(0.3264)\n",
      "32593 Training Loss: tensor(0.3272)\n",
      "32594 Training Loss: tensor(0.3265)\n",
      "32595 Training Loss: tensor(0.3257)\n",
      "32596 Training Loss: tensor(0.3258)\n",
      "32597 Training Loss: tensor(0.3253)\n",
      "32598 Training Loss: tensor(0.3270)\n",
      "32599 Training Loss: tensor(0.3262)\n",
      "32600 Training Loss: tensor(0.3265)\n",
      "32601 Training Loss: tensor(0.3262)\n",
      "32602 Training Loss: tensor(0.3270)\n",
      "32603 Training Loss: tensor(0.3260)\n",
      "32604 Training Loss: tensor(0.3260)\n",
      "32605 Training Loss: tensor(0.3260)\n",
      "32606 Training Loss: tensor(0.3258)\n",
      "32607 Training Loss: tensor(0.3258)\n",
      "32608 Training Loss: tensor(0.3264)\n",
      "32609 Training Loss: tensor(0.3270)\n",
      "32610 Training Loss: tensor(0.3261)\n",
      "32611 Training Loss: tensor(0.3260)\n",
      "32612 Training Loss: tensor(0.3253)\n",
      "32613 Training Loss: tensor(0.3255)\n",
      "32614 Training Loss: tensor(0.3266)\n",
      "32615 Training Loss: tensor(0.3257)\n",
      "32616 Training Loss: tensor(0.3263)\n",
      "32617 Training Loss: tensor(0.3257)\n",
      "32618 Training Loss: tensor(0.3262)\n",
      "32619 Training Loss: tensor(0.3255)\n",
      "32620 Training Loss: tensor(0.3262)\n",
      "32621 Training Loss: tensor(0.3257)\n",
      "32622 Training Loss: tensor(0.3269)\n",
      "32623 Training Loss: tensor(0.3261)\n",
      "32624 Training Loss: tensor(0.3253)\n",
      "32625 Training Loss: tensor(0.3256)\n",
      "32626 Training Loss: tensor(0.3261)\n",
      "32627 Training Loss: tensor(0.3258)\n",
      "32628 Training Loss: tensor(0.3270)\n",
      "32629 Training Loss: tensor(0.3257)\n",
      "32630 Training Loss: tensor(0.3261)\n",
      "32631 Training Loss: tensor(0.3279)\n",
      "32632 Training Loss: tensor(0.3273)\n",
      "32633 Training Loss: tensor(0.3254)\n",
      "32634 Training Loss: tensor(0.3254)\n",
      "32635 Training Loss: tensor(0.3252)\n",
      "32636 Training Loss: tensor(0.3255)\n",
      "32637 Training Loss: tensor(0.3258)\n",
      "32638 Training Loss: tensor(0.3255)\n",
      "32639 Training Loss: tensor(0.3257)\n",
      "32640 Training Loss: tensor(0.3263)\n",
      "32641 Training Loss: tensor(0.3253)\n",
      "32642 Training Loss: tensor(0.3260)\n",
      "32643 Training Loss: tensor(0.3254)\n",
      "32644 Training Loss: tensor(0.3253)\n",
      "32645 Training Loss: tensor(0.3255)\n",
      "32646 Training Loss: tensor(0.3255)\n",
      "32647 Training Loss: tensor(0.3260)\n",
      "32648 Training Loss: tensor(0.3254)\n",
      "32649 Training Loss: tensor(0.3258)\n",
      "32650 Training Loss: tensor(0.3257)\n",
      "32651 Training Loss: tensor(0.3255)\n",
      "32652 Training Loss: tensor(0.3257)\n",
      "32653 Training Loss: tensor(0.3253)\n",
      "32654 Training Loss: tensor(0.3260)\n",
      "32655 Training Loss: tensor(0.3258)\n",
      "32656 Training Loss: tensor(0.3252)\n",
      "32657 Training Loss: tensor(0.3262)\n",
      "32658 Training Loss: tensor(0.3263)\n",
      "32659 Training Loss: tensor(0.3255)\n",
      "32660 Training Loss: tensor(0.3256)\n",
      "32661 Training Loss: tensor(0.3257)\n",
      "32662 Training Loss: tensor(0.3256)\n",
      "32663 Training Loss: tensor(0.3259)\n",
      "32664 Training Loss: tensor(0.3261)\n",
      "32665 Training Loss: tensor(0.3257)\n",
      "32666 Training Loss: tensor(0.3256)\n",
      "32667 Training Loss: tensor(0.3257)\n",
      "32668 Training Loss: tensor(0.3252)\n",
      "32669 Training Loss: tensor(0.3255)\n",
      "32670 Training Loss: tensor(0.3254)\n",
      "32671 Training Loss: tensor(0.3253)\n",
      "32672 Training Loss: tensor(0.3260)\n",
      "32673 Training Loss: tensor(0.3255)\n",
      "32674 Training Loss: tensor(0.3258)\n",
      "32675 Training Loss: tensor(0.3252)\n",
      "32676 Training Loss: tensor(0.3257)\n",
      "32677 Training Loss: tensor(0.3259)\n",
      "32678 Training Loss: tensor(0.3267)\n",
      "32679 Training Loss: tensor(0.3253)\n",
      "32680 Training Loss: tensor(0.3251)\n",
      "32681 Training Loss: tensor(0.3251)\n",
      "32682 Training Loss: tensor(0.3267)\n",
      "32683 Training Loss: tensor(0.3253)\n",
      "32684 Training Loss: tensor(0.3254)\n",
      "32685 Training Loss: tensor(0.3259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32686 Training Loss: tensor(0.3270)\n",
      "32687 Training Loss: tensor(0.3254)\n",
      "32688 Training Loss: tensor(0.3254)\n",
      "32689 Training Loss: tensor(0.3255)\n",
      "32690 Training Loss: tensor(0.3255)\n",
      "32691 Training Loss: tensor(0.3255)\n",
      "32692 Training Loss: tensor(0.3261)\n",
      "32693 Training Loss: tensor(0.3257)\n",
      "32694 Training Loss: tensor(0.3253)\n",
      "32695 Training Loss: tensor(0.3253)\n",
      "32696 Training Loss: tensor(0.3264)\n",
      "32697 Training Loss: tensor(0.3253)\n",
      "32698 Training Loss: tensor(0.3253)\n",
      "32699 Training Loss: tensor(0.3258)\n",
      "32700 Training Loss: tensor(0.3252)\n",
      "32701 Training Loss: tensor(0.3251)\n",
      "32702 Training Loss: tensor(0.3253)\n",
      "32703 Training Loss: tensor(0.3258)\n",
      "32704 Training Loss: tensor(0.3254)\n",
      "32705 Training Loss: tensor(0.3264)\n",
      "32706 Training Loss: tensor(0.3252)\n",
      "32707 Training Loss: tensor(0.3264)\n",
      "32708 Training Loss: tensor(0.3260)\n",
      "32709 Training Loss: tensor(0.3254)\n",
      "32710 Training Loss: tensor(0.3259)\n",
      "32711 Training Loss: tensor(0.3257)\n",
      "32712 Training Loss: tensor(0.3258)\n",
      "32713 Training Loss: tensor(0.3255)\n",
      "32714 Training Loss: tensor(0.3255)\n",
      "32715 Training Loss: tensor(0.3259)\n",
      "32716 Training Loss: tensor(0.3254)\n",
      "32717 Training Loss: tensor(0.3255)\n",
      "32718 Training Loss: tensor(0.3257)\n",
      "32719 Training Loss: tensor(0.3258)\n",
      "32720 Training Loss: tensor(0.3256)\n",
      "32721 Training Loss: tensor(0.3261)\n",
      "32722 Training Loss: tensor(0.3260)\n",
      "32723 Training Loss: tensor(0.3255)\n",
      "32724 Training Loss: tensor(0.3251)\n",
      "32725 Training Loss: tensor(0.3256)\n",
      "32726 Training Loss: tensor(0.3255)\n",
      "32727 Training Loss: tensor(0.3253)\n",
      "32728 Training Loss: tensor(0.3259)\n",
      "32729 Training Loss: tensor(0.3254)\n",
      "32730 Training Loss: tensor(0.3255)\n",
      "32731 Training Loss: tensor(0.3253)\n",
      "32732 Training Loss: tensor(0.3255)\n",
      "32733 Training Loss: tensor(0.3253)\n",
      "32734 Training Loss: tensor(0.3265)\n",
      "32735 Training Loss: tensor(0.3269)\n",
      "32736 Training Loss: tensor(0.3258)\n",
      "32737 Training Loss: tensor(0.3256)\n",
      "32738 Training Loss: tensor(0.3256)\n",
      "32739 Training Loss: tensor(0.3257)\n",
      "32740 Training Loss: tensor(0.3259)\n",
      "32741 Training Loss: tensor(0.3258)\n",
      "32742 Training Loss: tensor(0.3262)\n",
      "32743 Training Loss: tensor(0.3256)\n",
      "32744 Training Loss: tensor(0.3258)\n",
      "32745 Training Loss: tensor(0.3258)\n",
      "32746 Training Loss: tensor(0.3256)\n",
      "32747 Training Loss: tensor(0.3254)\n",
      "32748 Training Loss: tensor(0.3253)\n",
      "32749 Training Loss: tensor(0.3261)\n",
      "32750 Training Loss: tensor(0.3255)\n",
      "32751 Training Loss: tensor(0.3253)\n",
      "32752 Training Loss: tensor(0.3264)\n",
      "32753 Training Loss: tensor(0.3253)\n",
      "32754 Training Loss: tensor(0.3258)\n",
      "32755 Training Loss: tensor(0.3253)\n",
      "32756 Training Loss: tensor(0.3258)\n",
      "32757 Training Loss: tensor(0.3273)\n",
      "32758 Training Loss: tensor(0.3254)\n",
      "32759 Training Loss: tensor(0.3257)\n",
      "32760 Training Loss: tensor(0.3258)\n",
      "32761 Training Loss: tensor(0.3258)\n",
      "32762 Training Loss: tensor(0.3259)\n",
      "32763 Training Loss: tensor(0.3254)\n",
      "32764 Training Loss: tensor(0.3267)\n",
      "32765 Training Loss: tensor(0.3253)\n",
      "32766 Training Loss: tensor(0.3253)\n",
      "32767 Training Loss: tensor(0.3268)\n",
      "32768 Training Loss: tensor(0.3253)\n",
      "32769 Training Loss: tensor(0.3252)\n",
      "32770 Training Loss: tensor(0.3257)\n",
      "32771 Training Loss: tensor(0.3258)\n",
      "32772 Training Loss: tensor(0.3256)\n",
      "32773 Training Loss: tensor(0.3255)\n",
      "32774 Training Loss: tensor(0.3260)\n",
      "32775 Training Loss: tensor(0.3255)\n",
      "32776 Training Loss: tensor(0.3256)\n",
      "32777 Training Loss: tensor(0.3268)\n",
      "32778 Training Loss: tensor(0.3261)\n",
      "32779 Training Loss: tensor(0.3265)\n",
      "32780 Training Loss: tensor(0.3255)\n",
      "32781 Training Loss: tensor(0.3256)\n",
      "32782 Training Loss: tensor(0.3264)\n",
      "32783 Training Loss: tensor(0.3262)\n",
      "32784 Training Loss: tensor(0.3255)\n",
      "32785 Training Loss: tensor(0.3258)\n",
      "32786 Training Loss: tensor(0.3260)\n",
      "32787 Training Loss: tensor(0.3256)\n",
      "32788 Training Loss: tensor(0.3254)\n",
      "32789 Training Loss: tensor(0.3261)\n",
      "32790 Training Loss: tensor(0.3256)\n",
      "32791 Training Loss: tensor(0.3257)\n",
      "32792 Training Loss: tensor(0.3260)\n",
      "32793 Training Loss: tensor(0.3253)\n",
      "32794 Training Loss: tensor(0.3251)\n",
      "32795 Training Loss: tensor(0.3255)\n",
      "32796 Training Loss: tensor(0.3257)\n",
      "32797 Training Loss: tensor(0.3258)\n",
      "32798 Training Loss: tensor(0.3256)\n",
      "32799 Training Loss: tensor(0.3257)\n",
      "32800 Training Loss: tensor(0.3258)\n",
      "32801 Training Loss: tensor(0.3260)\n",
      "32802 Training Loss: tensor(0.3256)\n",
      "32803 Training Loss: tensor(0.3262)\n",
      "32804 Training Loss: tensor(0.3271)\n",
      "32805 Training Loss: tensor(0.3255)\n",
      "32806 Training Loss: tensor(0.3256)\n",
      "32807 Training Loss: tensor(0.3251)\n",
      "32808 Training Loss: tensor(0.3253)\n",
      "32809 Training Loss: tensor(0.3256)\n",
      "32810 Training Loss: tensor(0.3256)\n",
      "32811 Training Loss: tensor(0.3257)\n",
      "32812 Training Loss: tensor(0.3257)\n",
      "32813 Training Loss: tensor(0.3262)\n",
      "32814 Training Loss: tensor(0.3253)\n",
      "32815 Training Loss: tensor(0.3252)\n",
      "32816 Training Loss: tensor(0.3253)\n",
      "32817 Training Loss: tensor(0.3251)\n",
      "32818 Training Loss: tensor(0.3255)\n",
      "32819 Training Loss: tensor(0.3254)\n",
      "32820 Training Loss: tensor(0.3254)\n",
      "32821 Training Loss: tensor(0.3256)\n",
      "32822 Training Loss: tensor(0.3263)\n",
      "32823 Training Loss: tensor(0.3258)\n",
      "32824 Training Loss: tensor(0.3255)\n",
      "32825 Training Loss: tensor(0.3253)\n",
      "32826 Training Loss: tensor(0.3271)\n",
      "32827 Training Loss: tensor(0.3257)\n",
      "32828 Training Loss: tensor(0.3254)\n",
      "32829 Training Loss: tensor(0.3256)\n",
      "32830 Training Loss: tensor(0.3265)\n",
      "32831 Training Loss: tensor(0.3253)\n",
      "32832 Training Loss: tensor(0.3253)\n",
      "32833 Training Loss: tensor(0.3255)\n",
      "32834 Training Loss: tensor(0.3255)\n",
      "32835 Training Loss: tensor(0.3255)\n",
      "32836 Training Loss: tensor(0.3256)\n",
      "32837 Training Loss: tensor(0.3256)\n",
      "32838 Training Loss: tensor(0.3252)\n",
      "32839 Training Loss: tensor(0.3252)\n",
      "32840 Training Loss: tensor(0.3254)\n",
      "32841 Training Loss: tensor(0.3256)\n",
      "32842 Training Loss: tensor(0.3268)\n",
      "32843 Training Loss: tensor(0.3252)\n",
      "32844 Training Loss: tensor(0.3257)\n",
      "32845 Training Loss: tensor(0.3252)\n",
      "32846 Training Loss: tensor(0.3256)\n",
      "32847 Training Loss: tensor(0.3252)\n",
      "32848 Training Loss: tensor(0.3253)\n",
      "32849 Training Loss: tensor(0.3259)\n",
      "32850 Training Loss: tensor(0.3261)\n",
      "32851 Training Loss: tensor(0.3255)\n",
      "32852 Training Loss: tensor(0.3253)\n",
      "32853 Training Loss: tensor(0.3261)\n",
      "32854 Training Loss: tensor(0.3254)\n",
      "32855 Training Loss: tensor(0.3256)\n",
      "32856 Training Loss: tensor(0.3253)\n",
      "32857 Training Loss: tensor(0.3265)\n",
      "32858 Training Loss: tensor(0.3252)\n",
      "32859 Training Loss: tensor(0.3254)\n",
      "32860 Training Loss: tensor(0.3255)\n",
      "32861 Training Loss: tensor(0.3255)\n",
      "32862 Training Loss: tensor(0.3253)\n",
      "32863 Training Loss: tensor(0.3259)\n",
      "32864 Training Loss: tensor(0.3263)\n",
      "32865 Training Loss: tensor(0.3257)\n",
      "32866 Training Loss: tensor(0.3255)\n",
      "32867 Training Loss: tensor(0.3256)\n",
      "32868 Training Loss: tensor(0.3253)\n",
      "32869 Training Loss: tensor(0.3259)\n",
      "32870 Training Loss: tensor(0.3254)\n",
      "32871 Training Loss: tensor(0.3256)\n",
      "32872 Training Loss: tensor(0.3257)\n",
      "32873 Training Loss: tensor(0.3260)\n",
      "32874 Training Loss: tensor(0.3254)\n",
      "32875 Training Loss: tensor(0.3253)\n",
      "32876 Training Loss: tensor(0.3257)\n",
      "32877 Training Loss: tensor(0.3260)\n",
      "32878 Training Loss: tensor(0.3265)\n",
      "32879 Training Loss: tensor(0.3255)\n",
      "32880 Training Loss: tensor(0.3258)\n",
      "32881 Training Loss: tensor(0.3250)\n",
      "32882 Training Loss: tensor(0.3251)\n",
      "32883 Training Loss: tensor(0.3260)\n",
      "32884 Training Loss: tensor(0.3257)\n",
      "32885 Training Loss: tensor(0.3254)\n",
      "32886 Training Loss: tensor(0.3263)\n",
      "32887 Training Loss: tensor(0.3252)\n",
      "32888 Training Loss: tensor(0.3256)\n",
      "32889 Training Loss: tensor(0.3254)\n",
      "32890 Training Loss: tensor(0.3251)\n",
      "32891 Training Loss: tensor(0.3253)\n",
      "32892 Training Loss: tensor(0.3259)\n",
      "32893 Training Loss: tensor(0.3265)\n",
      "32894 Training Loss: tensor(0.3252)\n",
      "32895 Training Loss: tensor(0.3264)\n",
      "32896 Training Loss: tensor(0.3255)\n",
      "32897 Training Loss: tensor(0.3253)\n",
      "32898 Training Loss: tensor(0.3252)\n",
      "32899 Training Loss: tensor(0.3253)\n",
      "32900 Training Loss: tensor(0.3255)\n",
      "32901 Training Loss: tensor(0.3255)\n",
      "32902 Training Loss: tensor(0.3253)\n",
      "32903 Training Loss: tensor(0.3254)\n",
      "32904 Training Loss: tensor(0.3271)\n",
      "32905 Training Loss: tensor(0.3251)\n",
      "32906 Training Loss: tensor(0.3255)\n",
      "32907 Training Loss: tensor(0.3255)\n",
      "32908 Training Loss: tensor(0.3254)\n",
      "32909 Training Loss: tensor(0.3259)\n",
      "32910 Training Loss: tensor(0.3255)\n",
      "32911 Training Loss: tensor(0.3252)\n",
      "32912 Training Loss: tensor(0.3254)\n",
      "32913 Training Loss: tensor(0.3252)\n",
      "32914 Training Loss: tensor(0.3254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32915 Training Loss: tensor(0.3265)\n",
      "32916 Training Loss: tensor(0.3261)\n",
      "32917 Training Loss: tensor(0.3254)\n",
      "32918 Training Loss: tensor(0.3261)\n",
      "32919 Training Loss: tensor(0.3255)\n",
      "32920 Training Loss: tensor(0.3268)\n",
      "32921 Training Loss: tensor(0.3256)\n",
      "32922 Training Loss: tensor(0.3254)\n",
      "32923 Training Loss: tensor(0.3255)\n",
      "32924 Training Loss: tensor(0.3252)\n",
      "32925 Training Loss: tensor(0.3256)\n",
      "32926 Training Loss: tensor(0.3256)\n",
      "32927 Training Loss: tensor(0.3259)\n",
      "32928 Training Loss: tensor(0.3256)\n",
      "32929 Training Loss: tensor(0.3258)\n",
      "32930 Training Loss: tensor(0.3258)\n",
      "32931 Training Loss: tensor(0.3254)\n",
      "32932 Training Loss: tensor(0.3263)\n",
      "32933 Training Loss: tensor(0.3260)\n",
      "32934 Training Loss: tensor(0.3254)\n",
      "32935 Training Loss: tensor(0.3261)\n",
      "32936 Training Loss: tensor(0.3261)\n",
      "32937 Training Loss: tensor(0.3258)\n",
      "32938 Training Loss: tensor(0.3261)\n",
      "32939 Training Loss: tensor(0.3255)\n",
      "32940 Training Loss: tensor(0.3256)\n",
      "32941 Training Loss: tensor(0.3258)\n",
      "32942 Training Loss: tensor(0.3254)\n",
      "32943 Training Loss: tensor(0.3255)\n",
      "32944 Training Loss: tensor(0.3253)\n",
      "32945 Training Loss: tensor(0.3253)\n",
      "32946 Training Loss: tensor(0.3251)\n",
      "32947 Training Loss: tensor(0.3251)\n",
      "32948 Training Loss: tensor(0.3254)\n",
      "32949 Training Loss: tensor(0.3253)\n",
      "32950 Training Loss: tensor(0.3257)\n",
      "32951 Training Loss: tensor(0.3255)\n",
      "32952 Training Loss: tensor(0.3253)\n",
      "32953 Training Loss: tensor(0.3265)\n",
      "32954 Training Loss: tensor(0.3267)\n",
      "32955 Training Loss: tensor(0.3268)\n",
      "32956 Training Loss: tensor(0.3251)\n",
      "32957 Training Loss: tensor(0.3257)\n",
      "32958 Training Loss: tensor(0.3254)\n",
      "32959 Training Loss: tensor(0.3253)\n",
      "32960 Training Loss: tensor(0.3262)\n",
      "32961 Training Loss: tensor(0.3252)\n",
      "32962 Training Loss: tensor(0.3272)\n",
      "32963 Training Loss: tensor(0.3254)\n",
      "32964 Training Loss: tensor(0.3254)\n",
      "32965 Training Loss: tensor(0.3254)\n",
      "32966 Training Loss: tensor(0.3251)\n",
      "32967 Training Loss: tensor(0.3261)\n",
      "32968 Training Loss: tensor(0.3253)\n",
      "32969 Training Loss: tensor(0.3257)\n",
      "32970 Training Loss: tensor(0.3258)\n",
      "32971 Training Loss: tensor(0.3254)\n",
      "32972 Training Loss: tensor(0.3257)\n",
      "32973 Training Loss: tensor(0.3273)\n",
      "32974 Training Loss: tensor(0.3258)\n",
      "32975 Training Loss: tensor(0.3271)\n",
      "32976 Training Loss: tensor(0.3255)\n",
      "32977 Training Loss: tensor(0.3266)\n",
      "32978 Training Loss: tensor(0.3255)\n",
      "32979 Training Loss: tensor(0.3254)\n",
      "32980 Training Loss: tensor(0.3255)\n",
      "32981 Training Loss: tensor(0.3256)\n",
      "32982 Training Loss: tensor(0.3257)\n",
      "32983 Training Loss: tensor(0.3254)\n",
      "32984 Training Loss: tensor(0.3269)\n",
      "32985 Training Loss: tensor(0.3254)\n",
      "32986 Training Loss: tensor(0.3253)\n",
      "32987 Training Loss: tensor(0.3271)\n",
      "32988 Training Loss: tensor(0.3255)\n",
      "32989 Training Loss: tensor(0.3255)\n",
      "32990 Training Loss: tensor(0.3255)\n",
      "32991 Training Loss: tensor(0.3256)\n",
      "32992 Training Loss: tensor(0.3258)\n",
      "32993 Training Loss: tensor(0.3257)\n",
      "32994 Training Loss: tensor(0.3259)\n",
      "32995 Training Loss: tensor(0.3256)\n",
      "32996 Training Loss: tensor(0.3254)\n",
      "32997 Training Loss: tensor(0.3273)\n",
      "32998 Training Loss: tensor(0.3254)\n",
      "32999 Training Loss: tensor(0.3252)\n",
      "33000 Training Loss: tensor(0.3253)\n",
      "33001 Training Loss: tensor(0.3257)\n",
      "33002 Training Loss: tensor(0.3253)\n",
      "33003 Training Loss: tensor(0.3253)\n",
      "33004 Training Loss: tensor(0.3255)\n",
      "33005 Training Loss: tensor(0.3261)\n",
      "33006 Training Loss: tensor(0.3253)\n",
      "33007 Training Loss: tensor(0.3257)\n",
      "33008 Training Loss: tensor(0.3258)\n",
      "33009 Training Loss: tensor(0.3259)\n",
      "33010 Training Loss: tensor(0.3253)\n",
      "33011 Training Loss: tensor(0.3256)\n",
      "33012 Training Loss: tensor(0.3252)\n",
      "33013 Training Loss: tensor(0.3259)\n",
      "33014 Training Loss: tensor(0.3255)\n",
      "33015 Training Loss: tensor(0.3253)\n",
      "33016 Training Loss: tensor(0.3254)\n",
      "33017 Training Loss: tensor(0.3253)\n",
      "33018 Training Loss: tensor(0.3255)\n",
      "33019 Training Loss: tensor(0.3256)\n",
      "33020 Training Loss: tensor(0.3252)\n",
      "33021 Training Loss: tensor(0.3254)\n",
      "33022 Training Loss: tensor(0.3252)\n",
      "33023 Training Loss: tensor(0.3255)\n",
      "33024 Training Loss: tensor(0.3252)\n",
      "33025 Training Loss: tensor(0.3256)\n",
      "33026 Training Loss: tensor(0.3272)\n",
      "33027 Training Loss: tensor(0.3259)\n",
      "33028 Training Loss: tensor(0.3253)\n",
      "33029 Training Loss: tensor(0.3253)\n",
      "33030 Training Loss: tensor(0.3253)\n",
      "33031 Training Loss: tensor(0.3252)\n",
      "33032 Training Loss: tensor(0.3253)\n",
      "33033 Training Loss: tensor(0.3252)\n",
      "33034 Training Loss: tensor(0.3254)\n",
      "33035 Training Loss: tensor(0.3254)\n",
      "33036 Training Loss: tensor(0.3256)\n",
      "33037 Training Loss: tensor(0.3252)\n",
      "33038 Training Loss: tensor(0.3252)\n",
      "33039 Training Loss: tensor(0.3253)\n",
      "33040 Training Loss: tensor(0.3256)\n",
      "33041 Training Loss: tensor(0.3257)\n",
      "33042 Training Loss: tensor(0.3255)\n",
      "33043 Training Loss: tensor(0.3252)\n",
      "33044 Training Loss: tensor(0.3252)\n",
      "33045 Training Loss: tensor(0.3253)\n",
      "33046 Training Loss: tensor(0.3254)\n",
      "33047 Training Loss: tensor(0.3251)\n",
      "33048 Training Loss: tensor(0.3252)\n",
      "33049 Training Loss: tensor(0.3260)\n",
      "33050 Training Loss: tensor(0.3252)\n",
      "33051 Training Loss: tensor(0.3252)\n",
      "33052 Training Loss: tensor(0.3256)\n",
      "33053 Training Loss: tensor(0.3253)\n",
      "33054 Training Loss: tensor(0.3258)\n",
      "33055 Training Loss: tensor(0.3258)\n",
      "33056 Training Loss: tensor(0.3255)\n",
      "33057 Training Loss: tensor(0.3251)\n",
      "33058 Training Loss: tensor(0.3254)\n",
      "33059 Training Loss: tensor(0.3253)\n",
      "33060 Training Loss: tensor(0.3273)\n",
      "33061 Training Loss: tensor(0.3250)\n",
      "33062 Training Loss: tensor(0.3257)\n",
      "33063 Training Loss: tensor(0.3254)\n",
      "33064 Training Loss: tensor(0.3259)\n",
      "33065 Training Loss: tensor(0.3255)\n",
      "33066 Training Loss: tensor(0.3258)\n",
      "33067 Training Loss: tensor(0.3253)\n",
      "33068 Training Loss: tensor(0.3257)\n",
      "33069 Training Loss: tensor(0.3253)\n",
      "33070 Training Loss: tensor(0.3252)\n",
      "33071 Training Loss: tensor(0.3252)\n",
      "33072 Training Loss: tensor(0.3254)\n",
      "33073 Training Loss: tensor(0.3265)\n",
      "33074 Training Loss: tensor(0.3255)\n",
      "33075 Training Loss: tensor(0.3255)\n",
      "33076 Training Loss: tensor(0.3253)\n",
      "33077 Training Loss: tensor(0.3257)\n",
      "33078 Training Loss: tensor(0.3253)\n",
      "33079 Training Loss: tensor(0.3256)\n",
      "33080 Training Loss: tensor(0.3266)\n",
      "33081 Training Loss: tensor(0.3258)\n",
      "33082 Training Loss: tensor(0.3263)\n",
      "33083 Training Loss: tensor(0.3253)\n",
      "33084 Training Loss: tensor(0.3253)\n",
      "33085 Training Loss: tensor(0.3260)\n",
      "33086 Training Loss: tensor(0.3255)\n",
      "33087 Training Loss: tensor(0.3259)\n",
      "33088 Training Loss: tensor(0.3252)\n",
      "33089 Training Loss: tensor(0.3252)\n",
      "33090 Training Loss: tensor(0.3265)\n",
      "33091 Training Loss: tensor(0.3261)\n",
      "33092 Training Loss: tensor(0.3253)\n",
      "33093 Training Loss: tensor(0.3254)\n",
      "33094 Training Loss: tensor(0.3263)\n",
      "33095 Training Loss: tensor(0.3259)\n",
      "33096 Training Loss: tensor(0.3252)\n",
      "33097 Training Loss: tensor(0.3254)\n",
      "33098 Training Loss: tensor(0.3251)\n",
      "33099 Training Loss: tensor(0.3255)\n",
      "33100 Training Loss: tensor(0.3269)\n",
      "33101 Training Loss: tensor(0.3255)\n",
      "33102 Training Loss: tensor(0.3262)\n",
      "33103 Training Loss: tensor(0.3254)\n",
      "33104 Training Loss: tensor(0.3272)\n",
      "33105 Training Loss: tensor(0.3255)\n",
      "33106 Training Loss: tensor(0.3257)\n",
      "33107 Training Loss: tensor(0.3255)\n",
      "33108 Training Loss: tensor(0.3258)\n",
      "33109 Training Loss: tensor(0.3255)\n",
      "33110 Training Loss: tensor(0.3255)\n",
      "33111 Training Loss: tensor(0.3257)\n",
      "33112 Training Loss: tensor(0.3269)\n",
      "33113 Training Loss: tensor(0.3258)\n",
      "33114 Training Loss: tensor(0.3274)\n",
      "33115 Training Loss: tensor(0.3255)\n",
      "33116 Training Loss: tensor(0.3264)\n",
      "33117 Training Loss: tensor(0.3259)\n",
      "33118 Training Loss: tensor(0.3261)\n",
      "33119 Training Loss: tensor(0.3254)\n",
      "33120 Training Loss: tensor(0.3255)\n",
      "33121 Training Loss: tensor(0.3258)\n",
      "33122 Training Loss: tensor(0.3262)\n",
      "33123 Training Loss: tensor(0.3255)\n",
      "33124 Training Loss: tensor(0.3253)\n",
      "33125 Training Loss: tensor(0.3254)\n",
      "33126 Training Loss: tensor(0.3259)\n",
      "33127 Training Loss: tensor(0.3253)\n",
      "33128 Training Loss: tensor(0.3259)\n",
      "33129 Training Loss: tensor(0.3263)\n",
      "33130 Training Loss: tensor(0.3265)\n",
      "33131 Training Loss: tensor(0.3253)\n",
      "33132 Training Loss: tensor(0.3260)\n",
      "33133 Training Loss: tensor(0.3258)\n",
      "33134 Training Loss: tensor(0.3259)\n",
      "33135 Training Loss: tensor(0.3263)\n",
      "33136 Training Loss: tensor(0.3252)\n",
      "33137 Training Loss: tensor(0.3255)\n",
      "33138 Training Loss: tensor(0.3261)\n",
      "33139 Training Loss: tensor(0.3259)\n",
      "33140 Training Loss: tensor(0.3254)\n",
      "33141 Training Loss: tensor(0.3253)\n",
      "33142 Training Loss: tensor(0.3257)\n",
      "33143 Training Loss: tensor(0.3253)\n",
      "33144 Training Loss: tensor(0.3252)\n",
      "33145 Training Loss: tensor(0.3258)\n",
      "33146 Training Loss: tensor(0.3253)\n",
      "33147 Training Loss: tensor(0.3272)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33148 Training Loss: tensor(0.3258)\n",
      "33149 Training Loss: tensor(0.3265)\n",
      "33150 Training Loss: tensor(0.3260)\n",
      "33151 Training Loss: tensor(0.3254)\n",
      "33152 Training Loss: tensor(0.3258)\n",
      "33153 Training Loss: tensor(0.3252)\n",
      "33154 Training Loss: tensor(0.3254)\n",
      "33155 Training Loss: tensor(0.3259)\n",
      "33156 Training Loss: tensor(0.3255)\n",
      "33157 Training Loss: tensor(0.3255)\n",
      "33158 Training Loss: tensor(0.3257)\n",
      "33159 Training Loss: tensor(0.3259)\n",
      "33160 Training Loss: tensor(0.3256)\n",
      "33161 Training Loss: tensor(0.3256)\n",
      "33162 Training Loss: tensor(0.3259)\n",
      "33163 Training Loss: tensor(0.3252)\n",
      "33164 Training Loss: tensor(0.3254)\n",
      "33165 Training Loss: tensor(0.3254)\n",
      "33166 Training Loss: tensor(0.3253)\n",
      "33167 Training Loss: tensor(0.3254)\n",
      "33168 Training Loss: tensor(0.3255)\n",
      "33169 Training Loss: tensor(0.3253)\n",
      "33170 Training Loss: tensor(0.3252)\n",
      "33171 Training Loss: tensor(0.3254)\n",
      "33172 Training Loss: tensor(0.3252)\n",
      "33173 Training Loss: tensor(0.3251)\n",
      "33174 Training Loss: tensor(0.3268)\n",
      "33175 Training Loss: tensor(0.3253)\n",
      "33176 Training Loss: tensor(0.3257)\n",
      "33177 Training Loss: tensor(0.3252)\n",
      "33178 Training Loss: tensor(0.3251)\n",
      "33179 Training Loss: tensor(0.3262)\n",
      "33180 Training Loss: tensor(0.3253)\n",
      "33181 Training Loss: tensor(0.3252)\n",
      "33182 Training Loss: tensor(0.3253)\n",
      "33183 Training Loss: tensor(0.3261)\n",
      "33184 Training Loss: tensor(0.3252)\n",
      "33185 Training Loss: tensor(0.3260)\n",
      "33186 Training Loss: tensor(0.3254)\n",
      "33187 Training Loss: tensor(0.3278)\n",
      "33188 Training Loss: tensor(0.3266)\n",
      "33189 Training Loss: tensor(0.3255)\n",
      "33190 Training Loss: tensor(0.3260)\n",
      "33191 Training Loss: tensor(0.3257)\n",
      "33192 Training Loss: tensor(0.3256)\n",
      "33193 Training Loss: tensor(0.3257)\n",
      "33194 Training Loss: tensor(0.3252)\n",
      "33195 Training Loss: tensor(0.3256)\n",
      "33196 Training Loss: tensor(0.3262)\n",
      "33197 Training Loss: tensor(0.3254)\n",
      "33198 Training Loss: tensor(0.3254)\n",
      "33199 Training Loss: tensor(0.3269)\n",
      "33200 Training Loss: tensor(0.3257)\n",
      "33201 Training Loss: tensor(0.3260)\n",
      "33202 Training Loss: tensor(0.3258)\n",
      "33203 Training Loss: tensor(0.3254)\n",
      "33204 Training Loss: tensor(0.3253)\n",
      "33205 Training Loss: tensor(0.3260)\n",
      "33206 Training Loss: tensor(0.3254)\n",
      "33207 Training Loss: tensor(0.3256)\n",
      "33208 Training Loss: tensor(0.3257)\n",
      "33209 Training Loss: tensor(0.3267)\n",
      "33210 Training Loss: tensor(0.3254)\n",
      "33211 Training Loss: tensor(0.3256)\n",
      "33212 Training Loss: tensor(0.3256)\n",
      "33213 Training Loss: tensor(0.3252)\n",
      "33214 Training Loss: tensor(0.3258)\n",
      "33215 Training Loss: tensor(0.3256)\n",
      "33216 Training Loss: tensor(0.3263)\n",
      "33217 Training Loss: tensor(0.3262)\n",
      "33218 Training Loss: tensor(0.3254)\n",
      "33219 Training Loss: tensor(0.3256)\n",
      "33220 Training Loss: tensor(0.3260)\n",
      "33221 Training Loss: tensor(0.3255)\n",
      "33222 Training Loss: tensor(0.3272)\n",
      "33223 Training Loss: tensor(0.3252)\n",
      "33224 Training Loss: tensor(0.3252)\n",
      "33225 Training Loss: tensor(0.3253)\n",
      "33226 Training Loss: tensor(0.3254)\n",
      "33227 Training Loss: tensor(0.3254)\n",
      "33228 Training Loss: tensor(0.3251)\n",
      "33229 Training Loss: tensor(0.3262)\n",
      "33230 Training Loss: tensor(0.3265)\n",
      "33231 Training Loss: tensor(0.3255)\n",
      "33232 Training Loss: tensor(0.3252)\n",
      "33233 Training Loss: tensor(0.3253)\n",
      "33234 Training Loss: tensor(0.3261)\n",
      "33235 Training Loss: tensor(0.3253)\n",
      "33236 Training Loss: tensor(0.3259)\n",
      "33237 Training Loss: tensor(0.3262)\n",
      "33238 Training Loss: tensor(0.3258)\n",
      "33239 Training Loss: tensor(0.3255)\n",
      "33240 Training Loss: tensor(0.3255)\n",
      "33241 Training Loss: tensor(0.3258)\n",
      "33242 Training Loss: tensor(0.3252)\n",
      "33243 Training Loss: tensor(0.3263)\n",
      "33244 Training Loss: tensor(0.3252)\n",
      "33245 Training Loss: tensor(0.3265)\n",
      "33246 Training Loss: tensor(0.3263)\n",
      "33247 Training Loss: tensor(0.3262)\n",
      "33248 Training Loss: tensor(0.3252)\n",
      "33249 Training Loss: tensor(0.3251)\n",
      "33250 Training Loss: tensor(0.3251)\n",
      "33251 Training Loss: tensor(0.3253)\n",
      "33252 Training Loss: tensor(0.3252)\n",
      "33253 Training Loss: tensor(0.3257)\n",
      "33254 Training Loss: tensor(0.3255)\n",
      "33255 Training Loss: tensor(0.3259)\n",
      "33256 Training Loss: tensor(0.3253)\n",
      "33257 Training Loss: tensor(0.3254)\n",
      "33258 Training Loss: tensor(0.3251)\n",
      "33259 Training Loss: tensor(0.3258)\n",
      "33260 Training Loss: tensor(0.3255)\n",
      "33261 Training Loss: tensor(0.3258)\n",
      "33262 Training Loss: tensor(0.3256)\n",
      "33263 Training Loss: tensor(0.3254)\n",
      "33264 Training Loss: tensor(0.3261)\n",
      "33265 Training Loss: tensor(0.3251)\n",
      "33266 Training Loss: tensor(0.3257)\n",
      "33267 Training Loss: tensor(0.3251)\n",
      "33268 Training Loss: tensor(0.3255)\n",
      "33269 Training Loss: tensor(0.3251)\n",
      "33270 Training Loss: tensor(0.3260)\n",
      "33271 Training Loss: tensor(0.3252)\n",
      "33272 Training Loss: tensor(0.3253)\n",
      "33273 Training Loss: tensor(0.3253)\n",
      "33274 Training Loss: tensor(0.3251)\n",
      "33275 Training Loss: tensor(0.3252)\n",
      "33276 Training Loss: tensor(0.3255)\n",
      "33277 Training Loss: tensor(0.3252)\n",
      "33278 Training Loss: tensor(0.3256)\n",
      "33279 Training Loss: tensor(0.3259)\n",
      "33280 Training Loss: tensor(0.3251)\n",
      "33281 Training Loss: tensor(0.3258)\n",
      "33282 Training Loss: tensor(0.3262)\n",
      "33283 Training Loss: tensor(0.3273)\n",
      "33284 Training Loss: tensor(0.3252)\n",
      "33285 Training Loss: tensor(0.3256)\n",
      "33286 Training Loss: tensor(0.3256)\n",
      "33287 Training Loss: tensor(0.3253)\n",
      "33288 Training Loss: tensor(0.3263)\n",
      "33289 Training Loss: tensor(0.3255)\n",
      "33290 Training Loss: tensor(0.3259)\n",
      "33291 Training Loss: tensor(0.3256)\n",
      "33292 Training Loss: tensor(0.3259)\n",
      "33293 Training Loss: tensor(0.3257)\n",
      "33294 Training Loss: tensor(0.3253)\n",
      "33295 Training Loss: tensor(0.3265)\n",
      "33296 Training Loss: tensor(0.3263)\n",
      "33297 Training Loss: tensor(0.3257)\n",
      "33298 Training Loss: tensor(0.3257)\n",
      "33299 Training Loss: tensor(0.3255)\n",
      "33300 Training Loss: tensor(0.3256)\n",
      "33301 Training Loss: tensor(0.3253)\n",
      "33302 Training Loss: tensor(0.3251)\n",
      "33303 Training Loss: tensor(0.3253)\n",
      "33304 Training Loss: tensor(0.3254)\n",
      "33305 Training Loss: tensor(0.3272)\n",
      "33306 Training Loss: tensor(0.3265)\n",
      "33307 Training Loss: tensor(0.3254)\n",
      "33308 Training Loss: tensor(0.3267)\n",
      "33309 Training Loss: tensor(0.3254)\n",
      "33310 Training Loss: tensor(0.3259)\n",
      "33311 Training Loss: tensor(0.3257)\n",
      "33312 Training Loss: tensor(0.3255)\n",
      "33313 Training Loss: tensor(0.3256)\n",
      "33314 Training Loss: tensor(0.3252)\n",
      "33315 Training Loss: tensor(0.3257)\n",
      "33316 Training Loss: tensor(0.3256)\n",
      "33317 Training Loss: tensor(0.3266)\n",
      "33318 Training Loss: tensor(0.3252)\n",
      "33319 Training Loss: tensor(0.3253)\n",
      "33320 Training Loss: tensor(0.3256)\n",
      "33321 Training Loss: tensor(0.3257)\n",
      "33322 Training Loss: tensor(0.3254)\n",
      "33323 Training Loss: tensor(0.3256)\n",
      "33324 Training Loss: tensor(0.3257)\n",
      "33325 Training Loss: tensor(0.3253)\n",
      "33326 Training Loss: tensor(0.3255)\n",
      "33327 Training Loss: tensor(0.3251)\n",
      "33328 Training Loss: tensor(0.3255)\n",
      "33329 Training Loss: tensor(0.3253)\n",
      "33330 Training Loss: tensor(0.3255)\n",
      "33331 Training Loss: tensor(0.3258)\n",
      "33332 Training Loss: tensor(0.3258)\n",
      "33333 Training Loss: tensor(0.3255)\n",
      "33334 Training Loss: tensor(0.3252)\n",
      "33335 Training Loss: tensor(0.3255)\n",
      "33336 Training Loss: tensor(0.3257)\n",
      "33337 Training Loss: tensor(0.3256)\n",
      "33338 Training Loss: tensor(0.3255)\n",
      "33339 Training Loss: tensor(0.3252)\n",
      "33340 Training Loss: tensor(0.3252)\n",
      "33341 Training Loss: tensor(0.3258)\n",
      "33342 Training Loss: tensor(0.3257)\n",
      "33343 Training Loss: tensor(0.3273)\n",
      "33344 Training Loss: tensor(0.3254)\n",
      "33345 Training Loss: tensor(0.3251)\n",
      "33346 Training Loss: tensor(0.3252)\n",
      "33347 Training Loss: tensor(0.3255)\n",
      "33348 Training Loss: tensor(0.3261)\n",
      "33349 Training Loss: tensor(0.3252)\n",
      "33350 Training Loss: tensor(0.3255)\n",
      "33351 Training Loss: tensor(0.3257)\n",
      "33352 Training Loss: tensor(0.3255)\n",
      "33353 Training Loss: tensor(0.3250)\n",
      "33354 Training Loss: tensor(0.3258)\n",
      "33355 Training Loss: tensor(0.3252)\n",
      "33356 Training Loss: tensor(0.3265)\n",
      "33357 Training Loss: tensor(0.3261)\n",
      "33358 Training Loss: tensor(0.3254)\n",
      "33359 Training Loss: tensor(0.3252)\n",
      "33360 Training Loss: tensor(0.3262)\n",
      "33361 Training Loss: tensor(0.3252)\n",
      "33362 Training Loss: tensor(0.3255)\n",
      "33363 Training Loss: tensor(0.3255)\n",
      "33364 Training Loss: tensor(0.3251)\n",
      "33365 Training Loss: tensor(0.3255)\n",
      "33366 Training Loss: tensor(0.3251)\n",
      "33367 Training Loss: tensor(0.3255)\n",
      "33368 Training Loss: tensor(0.3252)\n",
      "33369 Training Loss: tensor(0.3260)\n",
      "33370 Training Loss: tensor(0.3258)\n",
      "33371 Training Loss: tensor(0.3253)\n",
      "33372 Training Loss: tensor(0.3258)\n",
      "33373 Training Loss: tensor(0.3257)\n",
      "33374 Training Loss: tensor(0.3257)\n",
      "33375 Training Loss: tensor(0.3258)\n",
      "33376 Training Loss: tensor(0.3253)\n",
      "33377 Training Loss: tensor(0.3258)\n",
      "33378 Training Loss: tensor(0.3268)\n",
      "33379 Training Loss: tensor(0.3260)\n",
      "33380 Training Loss: tensor(0.3258)\n",
      "33381 Training Loss: tensor(0.3255)\n",
      "33382 Training Loss: tensor(0.3263)\n",
      "33383 Training Loss: tensor(0.3272)\n",
      "33384 Training Loss: tensor(0.3261)\n",
      "33385 Training Loss: tensor(0.3257)\n",
      "33386 Training Loss: tensor(0.3254)\n",
      "33387 Training Loss: tensor(0.3262)\n",
      "33388 Training Loss: tensor(0.3264)\n",
      "33389 Training Loss: tensor(0.3259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33390 Training Loss: tensor(0.3265)\n",
      "33391 Training Loss: tensor(0.3261)\n",
      "33392 Training Loss: tensor(0.3251)\n",
      "33393 Training Loss: tensor(0.3267)\n",
      "33394 Training Loss: tensor(0.3256)\n",
      "33395 Training Loss: tensor(0.3253)\n",
      "33396 Training Loss: tensor(0.3255)\n",
      "33397 Training Loss: tensor(0.3254)\n",
      "33398 Training Loss: tensor(0.3252)\n",
      "33399 Training Loss: tensor(0.3261)\n",
      "33400 Training Loss: tensor(0.3259)\n",
      "33401 Training Loss: tensor(0.3257)\n",
      "33402 Training Loss: tensor(0.3252)\n",
      "33403 Training Loss: tensor(0.3259)\n",
      "33404 Training Loss: tensor(0.3259)\n",
      "33405 Training Loss: tensor(0.3252)\n",
      "33406 Training Loss: tensor(0.3257)\n",
      "33407 Training Loss: tensor(0.3257)\n",
      "33408 Training Loss: tensor(0.3253)\n",
      "33409 Training Loss: tensor(0.3255)\n",
      "33410 Training Loss: tensor(0.3266)\n",
      "33411 Training Loss: tensor(0.3262)\n",
      "33412 Training Loss: tensor(0.3256)\n",
      "33413 Training Loss: tensor(0.3260)\n",
      "33414 Training Loss: tensor(0.3255)\n",
      "33415 Training Loss: tensor(0.3257)\n",
      "33416 Training Loss: tensor(0.3260)\n",
      "33417 Training Loss: tensor(0.3253)\n",
      "33418 Training Loss: tensor(0.3253)\n",
      "33419 Training Loss: tensor(0.3258)\n",
      "33420 Training Loss: tensor(0.3254)\n",
      "33421 Training Loss: tensor(0.3252)\n",
      "33422 Training Loss: tensor(0.3259)\n",
      "33423 Training Loss: tensor(0.3252)\n",
      "33424 Training Loss: tensor(0.3257)\n",
      "33425 Training Loss: tensor(0.3252)\n",
      "33426 Training Loss: tensor(0.3257)\n",
      "33427 Training Loss: tensor(0.3252)\n",
      "33428 Training Loss: tensor(0.3256)\n",
      "33429 Training Loss: tensor(0.3260)\n",
      "33430 Training Loss: tensor(0.3257)\n",
      "33431 Training Loss: tensor(0.3252)\n",
      "33432 Training Loss: tensor(0.3253)\n",
      "33433 Training Loss: tensor(0.3271)\n",
      "33434 Training Loss: tensor(0.3251)\n",
      "33435 Training Loss: tensor(0.3257)\n",
      "33436 Training Loss: tensor(0.3259)\n",
      "33437 Training Loss: tensor(0.3262)\n",
      "33438 Training Loss: tensor(0.3252)\n",
      "33439 Training Loss: tensor(0.3254)\n",
      "33440 Training Loss: tensor(0.3256)\n",
      "33441 Training Loss: tensor(0.3256)\n",
      "33442 Training Loss: tensor(0.3254)\n",
      "33443 Training Loss: tensor(0.3258)\n",
      "33444 Training Loss: tensor(0.3251)\n",
      "33445 Training Loss: tensor(0.3257)\n",
      "33446 Training Loss: tensor(0.3256)\n",
      "33447 Training Loss: tensor(0.3254)\n",
      "33448 Training Loss: tensor(0.3252)\n",
      "33449 Training Loss: tensor(0.3252)\n",
      "33450 Training Loss: tensor(0.3253)\n",
      "33451 Training Loss: tensor(0.3251)\n",
      "33452 Training Loss: tensor(0.3252)\n",
      "33453 Training Loss: tensor(0.3253)\n",
      "33454 Training Loss: tensor(0.3253)\n",
      "33455 Training Loss: tensor(0.3250)\n",
      "33456 Training Loss: tensor(0.3252)\n",
      "33457 Training Loss: tensor(0.3252)\n",
      "33458 Training Loss: tensor(0.3255)\n",
      "33459 Training Loss: tensor(0.3252)\n",
      "33460 Training Loss: tensor(0.3258)\n",
      "33461 Training Loss: tensor(0.3258)\n",
      "33462 Training Loss: tensor(0.3261)\n",
      "33463 Training Loss: tensor(0.3256)\n",
      "33464 Training Loss: tensor(0.3256)\n",
      "33465 Training Loss: tensor(0.3256)\n",
      "33466 Training Loss: tensor(0.3253)\n",
      "33467 Training Loss: tensor(0.3252)\n",
      "33468 Training Loss: tensor(0.3253)\n",
      "33469 Training Loss: tensor(0.3251)\n",
      "33470 Training Loss: tensor(0.3253)\n",
      "33471 Training Loss: tensor(0.3252)\n",
      "33472 Training Loss: tensor(0.3252)\n",
      "33473 Training Loss: tensor(0.3253)\n",
      "33474 Training Loss: tensor(0.3250)\n",
      "33475 Training Loss: tensor(0.3251)\n",
      "33476 Training Loss: tensor(0.3261)\n",
      "33477 Training Loss: tensor(0.3255)\n",
      "33478 Training Loss: tensor(0.3255)\n",
      "33479 Training Loss: tensor(0.3256)\n",
      "33480 Training Loss: tensor(0.3255)\n",
      "33481 Training Loss: tensor(0.3251)\n",
      "33482 Training Loss: tensor(0.3253)\n",
      "33483 Training Loss: tensor(0.3267)\n",
      "33484 Training Loss: tensor(0.3252)\n",
      "33485 Training Loss: tensor(0.3258)\n",
      "33486 Training Loss: tensor(0.3255)\n",
      "33487 Training Loss: tensor(0.3260)\n",
      "33488 Training Loss: tensor(0.3254)\n",
      "33489 Training Loss: tensor(0.3266)\n",
      "33490 Training Loss: tensor(0.3251)\n",
      "33491 Training Loss: tensor(0.3259)\n",
      "33492 Training Loss: tensor(0.3255)\n",
      "33493 Training Loss: tensor(0.3258)\n",
      "33494 Training Loss: tensor(0.3253)\n",
      "33495 Training Loss: tensor(0.3253)\n",
      "33496 Training Loss: tensor(0.3253)\n",
      "33497 Training Loss: tensor(0.3255)\n",
      "33498 Training Loss: tensor(0.3261)\n",
      "33499 Training Loss: tensor(0.3255)\n",
      "33500 Training Loss: tensor(0.3260)\n",
      "33501 Training Loss: tensor(0.3257)\n",
      "33502 Training Loss: tensor(0.3253)\n",
      "33503 Training Loss: tensor(0.3257)\n",
      "33504 Training Loss: tensor(0.3259)\n",
      "33505 Training Loss: tensor(0.3257)\n",
      "33506 Training Loss: tensor(0.3259)\n",
      "33507 Training Loss: tensor(0.3256)\n",
      "33508 Training Loss: tensor(0.3253)\n",
      "33509 Training Loss: tensor(0.3256)\n",
      "33510 Training Loss: tensor(0.3260)\n",
      "33511 Training Loss: tensor(0.3256)\n",
      "33512 Training Loss: tensor(0.3254)\n",
      "33513 Training Loss: tensor(0.3255)\n",
      "33514 Training Loss: tensor(0.3258)\n",
      "33515 Training Loss: tensor(0.3262)\n",
      "33516 Training Loss: tensor(0.3265)\n",
      "33517 Training Loss: tensor(0.3260)\n",
      "33518 Training Loss: tensor(0.3253)\n",
      "33519 Training Loss: tensor(0.3255)\n",
      "33520 Training Loss: tensor(0.3254)\n",
      "33521 Training Loss: tensor(0.3257)\n",
      "33522 Training Loss: tensor(0.3252)\n",
      "33523 Training Loss: tensor(0.3253)\n",
      "33524 Training Loss: tensor(0.3251)\n",
      "33525 Training Loss: tensor(0.3252)\n",
      "33526 Training Loss: tensor(0.3265)\n",
      "33527 Training Loss: tensor(0.3253)\n",
      "33528 Training Loss: tensor(0.3254)\n",
      "33529 Training Loss: tensor(0.3254)\n",
      "33530 Training Loss: tensor(0.3253)\n",
      "33531 Training Loss: tensor(0.3256)\n",
      "33532 Training Loss: tensor(0.3258)\n",
      "33533 Training Loss: tensor(0.3253)\n",
      "33534 Training Loss: tensor(0.3250)\n",
      "33535 Training Loss: tensor(0.3263)\n",
      "33536 Training Loss: tensor(0.3266)\n",
      "33537 Training Loss: tensor(0.3252)\n",
      "33538 Training Loss: tensor(0.3250)\n",
      "33539 Training Loss: tensor(0.3255)\n",
      "33540 Training Loss: tensor(0.3258)\n",
      "33541 Training Loss: tensor(0.3259)\n",
      "33542 Training Loss: tensor(0.3261)\n",
      "33543 Training Loss: tensor(0.3255)\n",
      "33544 Training Loss: tensor(0.3251)\n",
      "33545 Training Loss: tensor(0.3255)\n",
      "33546 Training Loss: tensor(0.3263)\n",
      "33547 Training Loss: tensor(0.3254)\n",
      "33548 Training Loss: tensor(0.3254)\n",
      "33549 Training Loss: tensor(0.3250)\n",
      "33550 Training Loss: tensor(0.3255)\n",
      "33551 Training Loss: tensor(0.3252)\n",
      "33552 Training Loss: tensor(0.3267)\n",
      "33553 Training Loss: tensor(0.3253)\n",
      "33554 Training Loss: tensor(0.3260)\n",
      "33555 Training Loss: tensor(0.3257)\n",
      "33556 Training Loss: tensor(0.3250)\n",
      "33557 Training Loss: tensor(0.3253)\n",
      "33558 Training Loss: tensor(0.3258)\n",
      "33559 Training Loss: tensor(0.3264)\n",
      "33560 Training Loss: tensor(0.3256)\n",
      "33561 Training Loss: tensor(0.3254)\n",
      "33562 Training Loss: tensor(0.3255)\n",
      "33563 Training Loss: tensor(0.3253)\n",
      "33564 Training Loss: tensor(0.3253)\n",
      "33565 Training Loss: tensor(0.3259)\n",
      "33566 Training Loss: tensor(0.3252)\n",
      "33567 Training Loss: tensor(0.3256)\n",
      "33568 Training Loss: tensor(0.3261)\n",
      "33569 Training Loss: tensor(0.3256)\n",
      "33570 Training Loss: tensor(0.3251)\n",
      "33571 Training Loss: tensor(0.3261)\n",
      "33572 Training Loss: tensor(0.3249)\n",
      "33573 Training Loss: tensor(0.3256)\n",
      "33574 Training Loss: tensor(0.3252)\n",
      "33575 Training Loss: tensor(0.3250)\n",
      "33576 Training Loss: tensor(0.3250)\n",
      "33577 Training Loss: tensor(0.3261)\n",
      "33578 Training Loss: tensor(0.3261)\n",
      "33579 Training Loss: tensor(0.3256)\n",
      "33580 Training Loss: tensor(0.3257)\n",
      "33581 Training Loss: tensor(0.3252)\n",
      "33582 Training Loss: tensor(0.3256)\n",
      "33583 Training Loss: tensor(0.3259)\n",
      "33584 Training Loss: tensor(0.3255)\n",
      "33585 Training Loss: tensor(0.3250)\n",
      "33586 Training Loss: tensor(0.3254)\n",
      "33587 Training Loss: tensor(0.3251)\n",
      "33588 Training Loss: tensor(0.3251)\n",
      "33589 Training Loss: tensor(0.3251)\n",
      "33590 Training Loss: tensor(0.3251)\n",
      "33591 Training Loss: tensor(0.3261)\n",
      "33592 Training Loss: tensor(0.3251)\n",
      "33593 Training Loss: tensor(0.3251)\n",
      "33594 Training Loss: tensor(0.3258)\n",
      "33595 Training Loss: tensor(0.3257)\n",
      "33596 Training Loss: tensor(0.3259)\n",
      "33597 Training Loss: tensor(0.3254)\n",
      "33598 Training Loss: tensor(0.3268)\n",
      "33599 Training Loss: tensor(0.3258)\n",
      "33600 Training Loss: tensor(0.3253)\n",
      "33601 Training Loss: tensor(0.3252)\n",
      "33602 Training Loss: tensor(0.3252)\n",
      "33603 Training Loss: tensor(0.3258)\n",
      "33604 Training Loss: tensor(0.3252)\n",
      "33605 Training Loss: tensor(0.3252)\n",
      "33606 Training Loss: tensor(0.3261)\n",
      "33607 Training Loss: tensor(0.3253)\n",
      "33608 Training Loss: tensor(0.3252)\n",
      "33609 Training Loss: tensor(0.3255)\n",
      "33610 Training Loss: tensor(0.3252)\n",
      "33611 Training Loss: tensor(0.3258)\n",
      "33612 Training Loss: tensor(0.3251)\n",
      "33613 Training Loss: tensor(0.3259)\n",
      "33614 Training Loss: tensor(0.3256)\n",
      "33615 Training Loss: tensor(0.3252)\n",
      "33616 Training Loss: tensor(0.3259)\n",
      "33617 Training Loss: tensor(0.3252)\n",
      "33618 Training Loss: tensor(0.3260)\n",
      "33619 Training Loss: tensor(0.3254)\n",
      "33620 Training Loss: tensor(0.3253)\n",
      "33621 Training Loss: tensor(0.3256)\n",
      "33622 Training Loss: tensor(0.3254)\n",
      "33623 Training Loss: tensor(0.3268)\n",
      "33624 Training Loss: tensor(0.3250)\n",
      "33625 Training Loss: tensor(0.3271)\n",
      "33626 Training Loss: tensor(0.3254)\n",
      "33627 Training Loss: tensor(0.3261)\n",
      "33628 Training Loss: tensor(0.3250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33629 Training Loss: tensor(0.3252)\n",
      "33630 Training Loss: tensor(0.3253)\n",
      "33631 Training Loss: tensor(0.3251)\n",
      "33632 Training Loss: tensor(0.3254)\n",
      "33633 Training Loss: tensor(0.3252)\n",
      "33634 Training Loss: tensor(0.3252)\n",
      "33635 Training Loss: tensor(0.3250)\n",
      "33636 Training Loss: tensor(0.3252)\n",
      "33637 Training Loss: tensor(0.3257)\n",
      "33638 Training Loss: tensor(0.3263)\n",
      "33639 Training Loss: tensor(0.3254)\n",
      "33640 Training Loss: tensor(0.3258)\n",
      "33641 Training Loss: tensor(0.3252)\n",
      "33642 Training Loss: tensor(0.3253)\n",
      "33643 Training Loss: tensor(0.3256)\n",
      "33644 Training Loss: tensor(0.3251)\n",
      "33645 Training Loss: tensor(0.3255)\n",
      "33646 Training Loss: tensor(0.3260)\n",
      "33647 Training Loss: tensor(0.3262)\n",
      "33648 Training Loss: tensor(0.3258)\n",
      "33649 Training Loss: tensor(0.3250)\n",
      "33650 Training Loss: tensor(0.3251)\n",
      "33651 Training Loss: tensor(0.3254)\n",
      "33652 Training Loss: tensor(0.3251)\n",
      "33653 Training Loss: tensor(0.3263)\n",
      "33654 Training Loss: tensor(0.3251)\n",
      "33655 Training Loss: tensor(0.3252)\n",
      "33656 Training Loss: tensor(0.3252)\n",
      "33657 Training Loss: tensor(0.3254)\n",
      "33658 Training Loss: tensor(0.3251)\n",
      "33659 Training Loss: tensor(0.3251)\n",
      "33660 Training Loss: tensor(0.3252)\n",
      "33661 Training Loss: tensor(0.3254)\n",
      "33662 Training Loss: tensor(0.3269)\n",
      "33663 Training Loss: tensor(0.3257)\n",
      "33664 Training Loss: tensor(0.3256)\n",
      "33665 Training Loss: tensor(0.3261)\n",
      "33666 Training Loss: tensor(0.3263)\n",
      "33667 Training Loss: tensor(0.3256)\n",
      "33668 Training Loss: tensor(0.3258)\n",
      "33669 Training Loss: tensor(0.3256)\n",
      "33670 Training Loss: tensor(0.3264)\n",
      "33671 Training Loss: tensor(0.3253)\n",
      "33672 Training Loss: tensor(0.3258)\n",
      "33673 Training Loss: tensor(0.3256)\n",
      "33674 Training Loss: tensor(0.3254)\n",
      "33675 Training Loss: tensor(0.3252)\n",
      "33676 Training Loss: tensor(0.3255)\n",
      "33677 Training Loss: tensor(0.3252)\n",
      "33678 Training Loss: tensor(0.3262)\n",
      "33679 Training Loss: tensor(0.3263)\n",
      "33680 Training Loss: tensor(0.3253)\n",
      "33681 Training Loss: tensor(0.3257)\n",
      "33682 Training Loss: tensor(0.3257)\n",
      "33683 Training Loss: tensor(0.3253)\n",
      "33684 Training Loss: tensor(0.3260)\n",
      "33685 Training Loss: tensor(0.3262)\n",
      "33686 Training Loss: tensor(0.3252)\n",
      "33687 Training Loss: tensor(0.3252)\n",
      "33688 Training Loss: tensor(0.3259)\n",
      "33689 Training Loss: tensor(0.3263)\n",
      "33690 Training Loss: tensor(0.3252)\n",
      "33691 Training Loss: tensor(0.3257)\n",
      "33692 Training Loss: tensor(0.3259)\n",
      "33693 Training Loss: tensor(0.3253)\n",
      "33694 Training Loss: tensor(0.3256)\n",
      "33695 Training Loss: tensor(0.3260)\n",
      "33696 Training Loss: tensor(0.3254)\n",
      "33697 Training Loss: tensor(0.3257)\n",
      "33698 Training Loss: tensor(0.3256)\n",
      "33699 Training Loss: tensor(0.3253)\n",
      "33700 Training Loss: tensor(0.3257)\n",
      "33701 Training Loss: tensor(0.3255)\n",
      "33702 Training Loss: tensor(0.3260)\n",
      "33703 Training Loss: tensor(0.3264)\n",
      "33704 Training Loss: tensor(0.3264)\n",
      "33705 Training Loss: tensor(0.3252)\n",
      "33706 Training Loss: tensor(0.3251)\n",
      "33707 Training Loss: tensor(0.3275)\n",
      "33708 Training Loss: tensor(0.3267)\n",
      "33709 Training Loss: tensor(0.3253)\n",
      "33710 Training Loss: tensor(0.3254)\n",
      "33711 Training Loss: tensor(0.3254)\n",
      "33712 Training Loss: tensor(0.3260)\n",
      "33713 Training Loss: tensor(0.3261)\n",
      "33714 Training Loss: tensor(0.3254)\n",
      "33715 Training Loss: tensor(0.3251)\n",
      "33716 Training Loss: tensor(0.3252)\n",
      "33717 Training Loss: tensor(0.3258)\n",
      "33718 Training Loss: tensor(0.3258)\n",
      "33719 Training Loss: tensor(0.3251)\n",
      "33720 Training Loss: tensor(0.3252)\n",
      "33721 Training Loss: tensor(0.3259)\n",
      "33722 Training Loss: tensor(0.3259)\n",
      "33723 Training Loss: tensor(0.3255)\n",
      "33724 Training Loss: tensor(0.3257)\n",
      "33725 Training Loss: tensor(0.3259)\n",
      "33726 Training Loss: tensor(0.3262)\n",
      "33727 Training Loss: tensor(0.3261)\n",
      "33728 Training Loss: tensor(0.3252)\n",
      "33729 Training Loss: tensor(0.3256)\n",
      "33730 Training Loss: tensor(0.3262)\n",
      "33731 Training Loss: tensor(0.3253)\n",
      "33732 Training Loss: tensor(0.3255)\n",
      "33733 Training Loss: tensor(0.3254)\n",
      "33734 Training Loss: tensor(0.3251)\n",
      "33735 Training Loss: tensor(0.3254)\n",
      "33736 Training Loss: tensor(0.3256)\n",
      "33737 Training Loss: tensor(0.3254)\n",
      "33738 Training Loss: tensor(0.3255)\n",
      "33739 Training Loss: tensor(0.3256)\n",
      "33740 Training Loss: tensor(0.3281)\n",
      "33741 Training Loss: tensor(0.3252)\n",
      "33742 Training Loss: tensor(0.3252)\n",
      "33743 Training Loss: tensor(0.3257)\n",
      "33744 Training Loss: tensor(0.3270)\n",
      "33745 Training Loss: tensor(0.3263)\n",
      "33746 Training Loss: tensor(0.3250)\n",
      "33747 Training Loss: tensor(0.3252)\n",
      "33748 Training Loss: tensor(0.3255)\n",
      "33749 Training Loss: tensor(0.3273)\n",
      "33750 Training Loss: tensor(0.3256)\n",
      "33751 Training Loss: tensor(0.3255)\n",
      "33752 Training Loss: tensor(0.3253)\n",
      "33753 Training Loss: tensor(0.3257)\n",
      "33754 Training Loss: tensor(0.3259)\n",
      "33755 Training Loss: tensor(0.3254)\n",
      "33756 Training Loss: tensor(0.3251)\n",
      "33757 Training Loss: tensor(0.3262)\n",
      "33758 Training Loss: tensor(0.3251)\n",
      "33759 Training Loss: tensor(0.3255)\n",
      "33760 Training Loss: tensor(0.3264)\n",
      "33761 Training Loss: tensor(0.3266)\n",
      "33762 Training Loss: tensor(0.3250)\n",
      "33763 Training Loss: tensor(0.3256)\n",
      "33764 Training Loss: tensor(0.3257)\n",
      "33765 Training Loss: tensor(0.3252)\n",
      "33766 Training Loss: tensor(0.3254)\n",
      "33767 Training Loss: tensor(0.3251)\n",
      "33768 Training Loss: tensor(0.3254)\n",
      "33769 Training Loss: tensor(0.3251)\n",
      "33770 Training Loss: tensor(0.3253)\n",
      "33771 Training Loss: tensor(0.3250)\n",
      "33772 Training Loss: tensor(0.3259)\n",
      "33773 Training Loss: tensor(0.3252)\n",
      "33774 Training Loss: tensor(0.3254)\n",
      "33775 Training Loss: tensor(0.3252)\n",
      "33776 Training Loss: tensor(0.3250)\n",
      "33777 Training Loss: tensor(0.3255)\n",
      "33778 Training Loss: tensor(0.3262)\n",
      "33779 Training Loss: tensor(0.3261)\n",
      "33780 Training Loss: tensor(0.3252)\n",
      "33781 Training Loss: tensor(0.3256)\n",
      "33782 Training Loss: tensor(0.3263)\n",
      "33783 Training Loss: tensor(0.3272)\n",
      "33784 Training Loss: tensor(0.3250)\n",
      "33785 Training Loss: tensor(0.3254)\n",
      "33786 Training Loss: tensor(0.3257)\n",
      "33787 Training Loss: tensor(0.3269)\n",
      "33788 Training Loss: tensor(0.3253)\n",
      "33789 Training Loss: tensor(0.3252)\n",
      "33790 Training Loss: tensor(0.3254)\n",
      "33791 Training Loss: tensor(0.3261)\n",
      "33792 Training Loss: tensor(0.3251)\n",
      "33793 Training Loss: tensor(0.3250)\n",
      "33794 Training Loss: tensor(0.3252)\n",
      "33795 Training Loss: tensor(0.3256)\n",
      "33796 Training Loss: tensor(0.3251)\n",
      "33797 Training Loss: tensor(0.3259)\n",
      "33798 Training Loss: tensor(0.3254)\n",
      "33799 Training Loss: tensor(0.3252)\n",
      "33800 Training Loss: tensor(0.3276)\n",
      "33801 Training Loss: tensor(0.3258)\n",
      "33802 Training Loss: tensor(0.3253)\n",
      "33803 Training Loss: tensor(0.3261)\n",
      "33804 Training Loss: tensor(0.3255)\n",
      "33805 Training Loss: tensor(0.3263)\n",
      "33806 Training Loss: tensor(0.3257)\n",
      "33807 Training Loss: tensor(0.3258)\n",
      "33808 Training Loss: tensor(0.3252)\n",
      "33809 Training Loss: tensor(0.3259)\n",
      "33810 Training Loss: tensor(0.3257)\n",
      "33811 Training Loss: tensor(0.3256)\n",
      "33812 Training Loss: tensor(0.3252)\n",
      "33813 Training Loss: tensor(0.3252)\n",
      "33814 Training Loss: tensor(0.3252)\n",
      "33815 Training Loss: tensor(0.3255)\n",
      "33816 Training Loss: tensor(0.3258)\n",
      "33817 Training Loss: tensor(0.3257)\n",
      "33818 Training Loss: tensor(0.3255)\n",
      "33819 Training Loss: tensor(0.3257)\n",
      "33820 Training Loss: tensor(0.3252)\n",
      "33821 Training Loss: tensor(0.3256)\n",
      "33822 Training Loss: tensor(0.3253)\n",
      "33823 Training Loss: tensor(0.3256)\n",
      "33824 Training Loss: tensor(0.3256)\n",
      "33825 Training Loss: tensor(0.3251)\n",
      "33826 Training Loss: tensor(0.3264)\n",
      "33827 Training Loss: tensor(0.3265)\n",
      "33828 Training Loss: tensor(0.3251)\n",
      "33829 Training Loss: tensor(0.3252)\n",
      "33830 Training Loss: tensor(0.3264)\n",
      "33831 Training Loss: tensor(0.3259)\n",
      "33832 Training Loss: tensor(0.3253)\n",
      "33833 Training Loss: tensor(0.3256)\n",
      "33834 Training Loss: tensor(0.3252)\n",
      "33835 Training Loss: tensor(0.3257)\n",
      "33836 Training Loss: tensor(0.3251)\n",
      "33837 Training Loss: tensor(0.3261)\n",
      "33838 Training Loss: tensor(0.3254)\n",
      "33839 Training Loss: tensor(0.3257)\n",
      "33840 Training Loss: tensor(0.3262)\n",
      "33841 Training Loss: tensor(0.3256)\n",
      "33842 Training Loss: tensor(0.3258)\n",
      "33843 Training Loss: tensor(0.3249)\n",
      "33844 Training Loss: tensor(0.3255)\n",
      "33845 Training Loss: tensor(0.3267)\n",
      "33846 Training Loss: tensor(0.3250)\n",
      "33847 Training Loss: tensor(0.3255)\n",
      "33848 Training Loss: tensor(0.3254)\n",
      "33849 Training Loss: tensor(0.3252)\n",
      "33850 Training Loss: tensor(0.3269)\n",
      "33851 Training Loss: tensor(0.3254)\n",
      "33852 Training Loss: tensor(0.3253)\n",
      "33853 Training Loss: tensor(0.3262)\n",
      "33854 Training Loss: tensor(0.3254)\n",
      "33855 Training Loss: tensor(0.3254)\n",
      "33856 Training Loss: tensor(0.3252)\n",
      "33857 Training Loss: tensor(0.3271)\n",
      "33858 Training Loss: tensor(0.3251)\n",
      "33859 Training Loss: tensor(0.3254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33860 Training Loss: tensor(0.3252)\n",
      "33861 Training Loss: tensor(0.3258)\n",
      "33862 Training Loss: tensor(0.3258)\n",
      "33863 Training Loss: tensor(0.3256)\n",
      "33864 Training Loss: tensor(0.3253)\n",
      "33865 Training Loss: tensor(0.3253)\n",
      "33866 Training Loss: tensor(0.3250)\n",
      "33867 Training Loss: tensor(0.3269)\n",
      "33868 Training Loss: tensor(0.3255)\n",
      "33869 Training Loss: tensor(0.3255)\n",
      "33870 Training Loss: tensor(0.3258)\n",
      "33871 Training Loss: tensor(0.3252)\n",
      "33872 Training Loss: tensor(0.3258)\n",
      "33873 Training Loss: tensor(0.3253)\n",
      "33874 Training Loss: tensor(0.3256)\n",
      "33875 Training Loss: tensor(0.3254)\n",
      "33876 Training Loss: tensor(0.3255)\n",
      "33877 Training Loss: tensor(0.3254)\n",
      "33878 Training Loss: tensor(0.3263)\n",
      "33879 Training Loss: tensor(0.3255)\n",
      "33880 Training Loss: tensor(0.3254)\n",
      "33881 Training Loss: tensor(0.3256)\n",
      "33882 Training Loss: tensor(0.3253)\n",
      "33883 Training Loss: tensor(0.3252)\n",
      "33884 Training Loss: tensor(0.3254)\n",
      "33885 Training Loss: tensor(0.3251)\n",
      "33886 Training Loss: tensor(0.3250)\n",
      "33887 Training Loss: tensor(0.3252)\n",
      "33888 Training Loss: tensor(0.3256)\n",
      "33889 Training Loss: tensor(0.3249)\n",
      "33890 Training Loss: tensor(0.3255)\n",
      "33891 Training Loss: tensor(0.3253)\n",
      "33892 Training Loss: tensor(0.3254)\n",
      "33893 Training Loss: tensor(0.3258)\n",
      "33894 Training Loss: tensor(0.3252)\n",
      "33895 Training Loss: tensor(0.3258)\n",
      "33896 Training Loss: tensor(0.3252)\n",
      "33897 Training Loss: tensor(0.3257)\n",
      "33898 Training Loss: tensor(0.3252)\n",
      "33899 Training Loss: tensor(0.3252)\n",
      "33900 Training Loss: tensor(0.3257)\n",
      "33901 Training Loss: tensor(0.3255)\n",
      "33902 Training Loss: tensor(0.3256)\n",
      "33903 Training Loss: tensor(0.3250)\n",
      "33904 Training Loss: tensor(0.3256)\n",
      "33905 Training Loss: tensor(0.3251)\n",
      "33906 Training Loss: tensor(0.3259)\n",
      "33907 Training Loss: tensor(0.3250)\n",
      "33908 Training Loss: tensor(0.3252)\n",
      "33909 Training Loss: tensor(0.3248)\n",
      "33910 Training Loss: tensor(0.3250)\n",
      "33911 Training Loss: tensor(0.3261)\n",
      "33912 Training Loss: tensor(0.3253)\n",
      "33913 Training Loss: tensor(0.3255)\n",
      "33914 Training Loss: tensor(0.3264)\n",
      "33915 Training Loss: tensor(0.3251)\n",
      "33916 Training Loss: tensor(0.3250)\n",
      "33917 Training Loss: tensor(0.3261)\n",
      "33918 Training Loss: tensor(0.3254)\n",
      "33919 Training Loss: tensor(0.3257)\n",
      "33920 Training Loss: tensor(0.3267)\n",
      "33921 Training Loss: tensor(0.3249)\n",
      "33922 Training Loss: tensor(0.3252)\n",
      "33923 Training Loss: tensor(0.3255)\n",
      "33924 Training Loss: tensor(0.3254)\n",
      "33925 Training Loss: tensor(0.3251)\n",
      "33926 Training Loss: tensor(0.3262)\n",
      "33927 Training Loss: tensor(0.3250)\n",
      "33928 Training Loss: tensor(0.3255)\n",
      "33929 Training Loss: tensor(0.3253)\n",
      "33930 Training Loss: tensor(0.3254)\n",
      "33931 Training Loss: tensor(0.3252)\n",
      "33932 Training Loss: tensor(0.3251)\n",
      "33933 Training Loss: tensor(0.3260)\n",
      "33934 Training Loss: tensor(0.3250)\n",
      "33935 Training Loss: tensor(0.3264)\n",
      "33936 Training Loss: tensor(0.3254)\n",
      "33937 Training Loss: tensor(0.3260)\n",
      "33938 Training Loss: tensor(0.3253)\n",
      "33939 Training Loss: tensor(0.3253)\n",
      "33940 Training Loss: tensor(0.3259)\n",
      "33941 Training Loss: tensor(0.3263)\n",
      "33942 Training Loss: tensor(0.3251)\n",
      "33943 Training Loss: tensor(0.3256)\n",
      "33944 Training Loss: tensor(0.3255)\n",
      "33945 Training Loss: tensor(0.3254)\n",
      "33946 Training Loss: tensor(0.3256)\n",
      "33947 Training Loss: tensor(0.3254)\n",
      "33948 Training Loss: tensor(0.3249)\n",
      "33949 Training Loss: tensor(0.3255)\n",
      "33950 Training Loss: tensor(0.3255)\n",
      "33951 Training Loss: tensor(0.3252)\n",
      "33952 Training Loss: tensor(0.3255)\n",
      "33953 Training Loss: tensor(0.3249)\n",
      "33954 Training Loss: tensor(0.3250)\n",
      "33955 Training Loss: tensor(0.3258)\n",
      "33956 Training Loss: tensor(0.3251)\n",
      "33957 Training Loss: tensor(0.3256)\n",
      "33958 Training Loss: tensor(0.3257)\n",
      "33959 Training Loss: tensor(0.3250)\n",
      "33960 Training Loss: tensor(0.3252)\n",
      "33961 Training Loss: tensor(0.3256)\n",
      "33962 Training Loss: tensor(0.3254)\n",
      "33963 Training Loss: tensor(0.3255)\n",
      "33964 Training Loss: tensor(0.3254)\n",
      "33965 Training Loss: tensor(0.3254)\n",
      "33966 Training Loss: tensor(0.3250)\n",
      "33967 Training Loss: tensor(0.3250)\n",
      "33968 Training Loss: tensor(0.3259)\n",
      "33969 Training Loss: tensor(0.3249)\n",
      "33970 Training Loss: tensor(0.3261)\n",
      "33971 Training Loss: tensor(0.3258)\n",
      "33972 Training Loss: tensor(0.3253)\n",
      "33973 Training Loss: tensor(0.3252)\n",
      "33974 Training Loss: tensor(0.3250)\n",
      "33975 Training Loss: tensor(0.3254)\n",
      "33976 Training Loss: tensor(0.3257)\n",
      "33977 Training Loss: tensor(0.3275)\n",
      "33978 Training Loss: tensor(0.3251)\n",
      "33979 Training Loss: tensor(0.3254)\n",
      "33980 Training Loss: tensor(0.3254)\n",
      "33981 Training Loss: tensor(0.3261)\n",
      "33982 Training Loss: tensor(0.3252)\n",
      "33983 Training Loss: tensor(0.3264)\n",
      "33984 Training Loss: tensor(0.3250)\n",
      "33985 Training Loss: tensor(0.3262)\n",
      "33986 Training Loss: tensor(0.3256)\n",
      "33987 Training Loss: tensor(0.3251)\n",
      "33988 Training Loss: tensor(0.3260)\n",
      "33989 Training Loss: tensor(0.3252)\n",
      "33990 Training Loss: tensor(0.3251)\n",
      "33991 Training Loss: tensor(0.3251)\n",
      "33992 Training Loss: tensor(0.3254)\n",
      "33993 Training Loss: tensor(0.3252)\n",
      "33994 Training Loss: tensor(0.3254)\n",
      "33995 Training Loss: tensor(0.3252)\n",
      "33996 Training Loss: tensor(0.3251)\n",
      "33997 Training Loss: tensor(0.3266)\n",
      "33998 Training Loss: tensor(0.3250)\n",
      "33999 Training Loss: tensor(0.3263)\n",
      "34000 Training Loss: tensor(0.3253)\n",
      "34001 Training Loss: tensor(0.3254)\n",
      "34002 Training Loss: tensor(0.3255)\n",
      "34003 Training Loss: tensor(0.3260)\n",
      "34004 Training Loss: tensor(0.3255)\n",
      "34005 Training Loss: tensor(0.3257)\n",
      "34006 Training Loss: tensor(0.3253)\n",
      "34007 Training Loss: tensor(0.3253)\n",
      "34008 Training Loss: tensor(0.3251)\n",
      "34009 Training Loss: tensor(0.3256)\n",
      "34010 Training Loss: tensor(0.3271)\n",
      "34011 Training Loss: tensor(0.3251)\n",
      "34012 Training Loss: tensor(0.3252)\n",
      "34013 Training Loss: tensor(0.3251)\n",
      "34014 Training Loss: tensor(0.3253)\n",
      "34015 Training Loss: tensor(0.3250)\n",
      "34016 Training Loss: tensor(0.3250)\n",
      "34017 Training Loss: tensor(0.3270)\n",
      "34018 Training Loss: tensor(0.3261)\n",
      "34019 Training Loss: tensor(0.3261)\n",
      "34020 Training Loss: tensor(0.3255)\n",
      "34021 Training Loss: tensor(0.3249)\n",
      "34022 Training Loss: tensor(0.3251)\n",
      "34023 Training Loss: tensor(0.3251)\n",
      "34024 Training Loss: tensor(0.3250)\n",
      "34025 Training Loss: tensor(0.3254)\n",
      "34026 Training Loss: tensor(0.3260)\n",
      "34027 Training Loss: tensor(0.3264)\n",
      "34028 Training Loss: tensor(0.3255)\n",
      "34029 Training Loss: tensor(0.3253)\n",
      "34030 Training Loss: tensor(0.3262)\n",
      "34031 Training Loss: tensor(0.3255)\n",
      "34032 Training Loss: tensor(0.3262)\n",
      "34033 Training Loss: tensor(0.3253)\n",
      "34034 Training Loss: tensor(0.3254)\n",
      "34035 Training Loss: tensor(0.3263)\n",
      "34036 Training Loss: tensor(0.3263)\n",
      "34037 Training Loss: tensor(0.3255)\n",
      "34038 Training Loss: tensor(0.3265)\n",
      "34039 Training Loss: tensor(0.3256)\n",
      "34040 Training Loss: tensor(0.3262)\n",
      "34041 Training Loss: tensor(0.3269)\n",
      "34042 Training Loss: tensor(0.3273)\n",
      "34043 Training Loss: tensor(0.3254)\n",
      "34044 Training Loss: tensor(0.3262)\n",
      "34045 Training Loss: tensor(0.3259)\n",
      "34046 Training Loss: tensor(0.3266)\n",
      "34047 Training Loss: tensor(0.3260)\n",
      "34048 Training Loss: tensor(0.3254)\n",
      "34049 Training Loss: tensor(0.3254)\n",
      "34050 Training Loss: tensor(0.3254)\n",
      "34051 Training Loss: tensor(0.3255)\n",
      "34052 Training Loss: tensor(0.3279)\n",
      "34053 Training Loss: tensor(0.3267)\n",
      "34054 Training Loss: tensor(0.3250)\n",
      "34055 Training Loss: tensor(0.3261)\n",
      "34056 Training Loss: tensor(0.3254)\n",
      "34057 Training Loss: tensor(0.3259)\n",
      "34058 Training Loss: tensor(0.3260)\n",
      "34059 Training Loss: tensor(0.3256)\n",
      "34060 Training Loss: tensor(0.3258)\n",
      "34061 Training Loss: tensor(0.3258)\n",
      "34062 Training Loss: tensor(0.3251)\n",
      "34063 Training Loss: tensor(0.3254)\n",
      "34064 Training Loss: tensor(0.3255)\n",
      "34065 Training Loss: tensor(0.3252)\n",
      "34066 Training Loss: tensor(0.3254)\n",
      "34067 Training Loss: tensor(0.3258)\n",
      "34068 Training Loss: tensor(0.3252)\n",
      "34069 Training Loss: tensor(0.3255)\n",
      "34070 Training Loss: tensor(0.3250)\n",
      "34071 Training Loss: tensor(0.3251)\n",
      "34072 Training Loss: tensor(0.3254)\n",
      "34073 Training Loss: tensor(0.3250)\n",
      "34074 Training Loss: tensor(0.3251)\n",
      "34075 Training Loss: tensor(0.3253)\n",
      "34076 Training Loss: tensor(0.3257)\n",
      "34077 Training Loss: tensor(0.3249)\n",
      "34078 Training Loss: tensor(0.3254)\n",
      "34079 Training Loss: tensor(0.3252)\n",
      "34080 Training Loss: tensor(0.3253)\n",
      "34081 Training Loss: tensor(0.3257)\n",
      "34082 Training Loss: tensor(0.3263)\n",
      "34083 Training Loss: tensor(0.3257)\n",
      "34084 Training Loss: tensor(0.3259)\n",
      "34085 Training Loss: tensor(0.3255)\n",
      "34086 Training Loss: tensor(0.3266)\n",
      "34087 Training Loss: tensor(0.3263)\n",
      "34088 Training Loss: tensor(0.3254)\n",
      "34089 Training Loss: tensor(0.3251)\n",
      "34090 Training Loss: tensor(0.3252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34091 Training Loss: tensor(0.3270)\n",
      "34092 Training Loss: tensor(0.3253)\n",
      "34093 Training Loss: tensor(0.3253)\n",
      "34094 Training Loss: tensor(0.3273)\n",
      "34095 Training Loss: tensor(0.3252)\n",
      "34096 Training Loss: tensor(0.3259)\n",
      "34097 Training Loss: tensor(0.3249)\n",
      "34098 Training Loss: tensor(0.3250)\n",
      "34099 Training Loss: tensor(0.3252)\n",
      "34100 Training Loss: tensor(0.3264)\n",
      "34101 Training Loss: tensor(0.3259)\n",
      "34102 Training Loss: tensor(0.3253)\n",
      "34103 Training Loss: tensor(0.3260)\n",
      "34104 Training Loss: tensor(0.3252)\n",
      "34105 Training Loss: tensor(0.3258)\n",
      "34106 Training Loss: tensor(0.3255)\n",
      "34107 Training Loss: tensor(0.3262)\n",
      "34108 Training Loss: tensor(0.3255)\n",
      "34109 Training Loss: tensor(0.3251)\n",
      "34110 Training Loss: tensor(0.3254)\n",
      "34111 Training Loss: tensor(0.3248)\n",
      "34112 Training Loss: tensor(0.3252)\n",
      "34113 Training Loss: tensor(0.3257)\n",
      "34114 Training Loss: tensor(0.3251)\n",
      "34115 Training Loss: tensor(0.3256)\n",
      "34116 Training Loss: tensor(0.3271)\n",
      "34117 Training Loss: tensor(0.3272)\n",
      "34118 Training Loss: tensor(0.3257)\n",
      "34119 Training Loss: tensor(0.3253)\n",
      "34120 Training Loss: tensor(0.3250)\n",
      "34121 Training Loss: tensor(0.3252)\n",
      "34122 Training Loss: tensor(0.3252)\n",
      "34123 Training Loss: tensor(0.3253)\n",
      "34124 Training Loss: tensor(0.3251)\n",
      "34125 Training Loss: tensor(0.3250)\n",
      "34126 Training Loss: tensor(0.3249)\n",
      "34127 Training Loss: tensor(0.3252)\n",
      "34128 Training Loss: tensor(0.3254)\n",
      "34129 Training Loss: tensor(0.3255)\n",
      "34130 Training Loss: tensor(0.3256)\n",
      "34131 Training Loss: tensor(0.3251)\n",
      "34132 Training Loss: tensor(0.3256)\n",
      "34133 Training Loss: tensor(0.3270)\n",
      "34134 Training Loss: tensor(0.3257)\n",
      "34135 Training Loss: tensor(0.3250)\n",
      "34136 Training Loss: tensor(0.3250)\n",
      "34137 Training Loss: tensor(0.3251)\n",
      "34138 Training Loss: tensor(0.3260)\n",
      "34139 Training Loss: tensor(0.3252)\n",
      "34140 Training Loss: tensor(0.3258)\n",
      "34141 Training Loss: tensor(0.3251)\n",
      "34142 Training Loss: tensor(0.3261)\n",
      "34143 Training Loss: tensor(0.3252)\n",
      "34144 Training Loss: tensor(0.3252)\n",
      "34145 Training Loss: tensor(0.3251)\n",
      "34146 Training Loss: tensor(0.3252)\n",
      "34147 Training Loss: tensor(0.3250)\n",
      "34148 Training Loss: tensor(0.3252)\n",
      "34149 Training Loss: tensor(0.3261)\n",
      "34150 Training Loss: tensor(0.3253)\n",
      "34151 Training Loss: tensor(0.3253)\n",
      "34152 Training Loss: tensor(0.3253)\n",
      "34153 Training Loss: tensor(0.3262)\n",
      "34154 Training Loss: tensor(0.3264)\n",
      "34155 Training Loss: tensor(0.3258)\n",
      "34156 Training Loss: tensor(0.3264)\n",
      "34157 Training Loss: tensor(0.3250)\n",
      "34158 Training Loss: tensor(0.3253)\n",
      "34159 Training Loss: tensor(0.3250)\n",
      "34160 Training Loss: tensor(0.3252)\n",
      "34161 Training Loss: tensor(0.3255)\n",
      "34162 Training Loss: tensor(0.3250)\n",
      "34163 Training Loss: tensor(0.3252)\n",
      "34164 Training Loss: tensor(0.3252)\n",
      "34165 Training Loss: tensor(0.3253)\n",
      "34166 Training Loss: tensor(0.3261)\n",
      "34167 Training Loss: tensor(0.3251)\n",
      "34168 Training Loss: tensor(0.3249)\n",
      "34169 Training Loss: tensor(0.3260)\n",
      "34170 Training Loss: tensor(0.3250)\n",
      "34171 Training Loss: tensor(0.3254)\n",
      "34172 Training Loss: tensor(0.3260)\n",
      "34173 Training Loss: tensor(0.3250)\n",
      "34174 Training Loss: tensor(0.3248)\n",
      "34175 Training Loss: tensor(0.3254)\n",
      "34176 Training Loss: tensor(0.3251)\n",
      "34177 Training Loss: tensor(0.3252)\n",
      "34178 Training Loss: tensor(0.3254)\n",
      "34179 Training Loss: tensor(0.3250)\n",
      "34180 Training Loss: tensor(0.3251)\n",
      "34181 Training Loss: tensor(0.3259)\n",
      "34182 Training Loss: tensor(0.3256)\n",
      "34183 Training Loss: tensor(0.3257)\n",
      "34184 Training Loss: tensor(0.3251)\n",
      "34185 Training Loss: tensor(0.3251)\n",
      "34186 Training Loss: tensor(0.3256)\n",
      "34187 Training Loss: tensor(0.3253)\n",
      "34188 Training Loss: tensor(0.3252)\n",
      "34189 Training Loss: tensor(0.3257)\n",
      "34190 Training Loss: tensor(0.3254)\n",
      "34191 Training Loss: tensor(0.3254)\n",
      "34192 Training Loss: tensor(0.3256)\n",
      "34193 Training Loss: tensor(0.3262)\n",
      "34194 Training Loss: tensor(0.3251)\n",
      "34195 Training Loss: tensor(0.3250)\n",
      "34196 Training Loss: tensor(0.3261)\n",
      "34197 Training Loss: tensor(0.3250)\n",
      "34198 Training Loss: tensor(0.3254)\n",
      "34199 Training Loss: tensor(0.3258)\n",
      "34200 Training Loss: tensor(0.3258)\n",
      "34201 Training Loss: tensor(0.3251)\n",
      "34202 Training Loss: tensor(0.3251)\n",
      "34203 Training Loss: tensor(0.3257)\n",
      "34204 Training Loss: tensor(0.3249)\n",
      "34205 Training Loss: tensor(0.3264)\n",
      "34206 Training Loss: tensor(0.3255)\n",
      "34207 Training Loss: tensor(0.3254)\n",
      "34208 Training Loss: tensor(0.3254)\n",
      "34209 Training Loss: tensor(0.3254)\n",
      "34210 Training Loss: tensor(0.3251)\n",
      "34211 Training Loss: tensor(0.3253)\n",
      "34212 Training Loss: tensor(0.3256)\n",
      "34213 Training Loss: tensor(0.3257)\n",
      "34214 Training Loss: tensor(0.3256)\n",
      "34215 Training Loss: tensor(0.3252)\n",
      "34216 Training Loss: tensor(0.3263)\n",
      "34217 Training Loss: tensor(0.3256)\n",
      "34218 Training Loss: tensor(0.3252)\n",
      "34219 Training Loss: tensor(0.3254)\n",
      "34220 Training Loss: tensor(0.3253)\n",
      "34221 Training Loss: tensor(0.3254)\n",
      "34222 Training Loss: tensor(0.3254)\n",
      "34223 Training Loss: tensor(0.3250)\n",
      "34224 Training Loss: tensor(0.3250)\n",
      "34225 Training Loss: tensor(0.3258)\n",
      "34226 Training Loss: tensor(0.3257)\n",
      "34227 Training Loss: tensor(0.3254)\n",
      "34228 Training Loss: tensor(0.3256)\n",
      "34229 Training Loss: tensor(0.3256)\n",
      "34230 Training Loss: tensor(0.3257)\n",
      "34231 Training Loss: tensor(0.3252)\n",
      "34232 Training Loss: tensor(0.3251)\n",
      "34233 Training Loss: tensor(0.3261)\n",
      "34234 Training Loss: tensor(0.3250)\n",
      "34235 Training Loss: tensor(0.3256)\n",
      "34236 Training Loss: tensor(0.3252)\n",
      "34237 Training Loss: tensor(0.3249)\n",
      "34238 Training Loss: tensor(0.3250)\n",
      "34239 Training Loss: tensor(0.3251)\n",
      "34240 Training Loss: tensor(0.3251)\n",
      "34241 Training Loss: tensor(0.3253)\n",
      "34242 Training Loss: tensor(0.3251)\n",
      "34243 Training Loss: tensor(0.3260)\n",
      "34244 Training Loss: tensor(0.3250)\n",
      "34245 Training Loss: tensor(0.3255)\n",
      "34246 Training Loss: tensor(0.3251)\n",
      "34247 Training Loss: tensor(0.3250)\n",
      "34248 Training Loss: tensor(0.3250)\n",
      "34249 Training Loss: tensor(0.3258)\n",
      "34250 Training Loss: tensor(0.3252)\n",
      "34251 Training Loss: tensor(0.3252)\n",
      "34252 Training Loss: tensor(0.3261)\n",
      "34253 Training Loss: tensor(0.3252)\n",
      "34254 Training Loss: tensor(0.3247)\n",
      "34255 Training Loss: tensor(0.3250)\n",
      "34256 Training Loss: tensor(0.3250)\n",
      "34257 Training Loss: tensor(0.3249)\n",
      "34258 Training Loss: tensor(0.3252)\n",
      "34259 Training Loss: tensor(0.3248)\n",
      "34260 Training Loss: tensor(0.3264)\n",
      "34261 Training Loss: tensor(0.3251)\n",
      "34262 Training Loss: tensor(0.3254)\n",
      "34263 Training Loss: tensor(0.3252)\n",
      "34264 Training Loss: tensor(0.3255)\n",
      "34265 Training Loss: tensor(0.3251)\n",
      "34266 Training Loss: tensor(0.3259)\n",
      "34267 Training Loss: tensor(0.3255)\n",
      "34268 Training Loss: tensor(0.3263)\n",
      "34269 Training Loss: tensor(0.3254)\n",
      "34270 Training Loss: tensor(0.3254)\n",
      "34271 Training Loss: tensor(0.3251)\n",
      "34272 Training Loss: tensor(0.3252)\n",
      "34273 Training Loss: tensor(0.3258)\n",
      "34274 Training Loss: tensor(0.3256)\n",
      "34275 Training Loss: tensor(0.3254)\n",
      "34276 Training Loss: tensor(0.3260)\n",
      "34277 Training Loss: tensor(0.3253)\n",
      "34278 Training Loss: tensor(0.3256)\n",
      "34279 Training Loss: tensor(0.3260)\n",
      "34280 Training Loss: tensor(0.3251)\n",
      "34281 Training Loss: tensor(0.3249)\n",
      "34282 Training Loss: tensor(0.3251)\n",
      "34283 Training Loss: tensor(0.3250)\n",
      "34284 Training Loss: tensor(0.3254)\n",
      "34285 Training Loss: tensor(0.3263)\n",
      "34286 Training Loss: tensor(0.3250)\n",
      "34287 Training Loss: tensor(0.3259)\n",
      "34288 Training Loss: tensor(0.3256)\n",
      "34289 Training Loss: tensor(0.3250)\n",
      "34290 Training Loss: tensor(0.3253)\n",
      "34291 Training Loss: tensor(0.3253)\n",
      "34292 Training Loss: tensor(0.3254)\n",
      "34293 Training Loss: tensor(0.3257)\n",
      "34294 Training Loss: tensor(0.3249)\n",
      "34295 Training Loss: tensor(0.3251)\n",
      "34296 Training Loss: tensor(0.3253)\n",
      "34297 Training Loss: tensor(0.3249)\n",
      "34298 Training Loss: tensor(0.3256)\n",
      "34299 Training Loss: tensor(0.3256)\n",
      "34300 Training Loss: tensor(0.3256)\n",
      "34301 Training Loss: tensor(0.3255)\n",
      "34302 Training Loss: tensor(0.3259)\n",
      "34303 Training Loss: tensor(0.3258)\n",
      "34304 Training Loss: tensor(0.3256)\n",
      "34305 Training Loss: tensor(0.3260)\n",
      "34306 Training Loss: tensor(0.3256)\n",
      "34307 Training Loss: tensor(0.3249)\n",
      "34308 Training Loss: tensor(0.3255)\n",
      "34309 Training Loss: tensor(0.3268)\n",
      "34310 Training Loss: tensor(0.3250)\n",
      "34311 Training Loss: tensor(0.3252)\n",
      "34312 Training Loss: tensor(0.3254)\n",
      "34313 Training Loss: tensor(0.3252)\n",
      "34314 Training Loss: tensor(0.3251)\n",
      "34315 Training Loss: tensor(0.3250)\n",
      "34316 Training Loss: tensor(0.3257)\n",
      "34317 Training Loss: tensor(0.3251)\n",
      "34318 Training Loss: tensor(0.3251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34319 Training Loss: tensor(0.3256)\n",
      "34320 Training Loss: tensor(0.3259)\n",
      "34321 Training Loss: tensor(0.3256)\n",
      "34322 Training Loss: tensor(0.3255)\n",
      "34323 Training Loss: tensor(0.3254)\n",
      "34324 Training Loss: tensor(0.3262)\n",
      "34325 Training Loss: tensor(0.3253)\n",
      "34326 Training Loss: tensor(0.3256)\n",
      "34327 Training Loss: tensor(0.3254)\n",
      "34328 Training Loss: tensor(0.3252)\n",
      "34329 Training Loss: tensor(0.3257)\n",
      "34330 Training Loss: tensor(0.3256)\n",
      "34331 Training Loss: tensor(0.3258)\n",
      "34332 Training Loss: tensor(0.3250)\n",
      "34333 Training Loss: tensor(0.3265)\n",
      "34334 Training Loss: tensor(0.3270)\n",
      "34335 Training Loss: tensor(0.3255)\n",
      "34336 Training Loss: tensor(0.3257)\n",
      "34337 Training Loss: tensor(0.3255)\n",
      "34338 Training Loss: tensor(0.3251)\n",
      "34339 Training Loss: tensor(0.3259)\n",
      "34340 Training Loss: tensor(0.3249)\n",
      "34341 Training Loss: tensor(0.3259)\n",
      "34342 Training Loss: tensor(0.3254)\n",
      "34343 Training Loss: tensor(0.3251)\n",
      "34344 Training Loss: tensor(0.3253)\n",
      "34345 Training Loss: tensor(0.3251)\n",
      "34346 Training Loss: tensor(0.3251)\n",
      "34347 Training Loss: tensor(0.3251)\n",
      "34348 Training Loss: tensor(0.3253)\n",
      "34349 Training Loss: tensor(0.3252)\n",
      "34350 Training Loss: tensor(0.3251)\n",
      "34351 Training Loss: tensor(0.3259)\n",
      "34352 Training Loss: tensor(0.3252)\n",
      "34353 Training Loss: tensor(0.3261)\n",
      "34354 Training Loss: tensor(0.3253)\n",
      "34355 Training Loss: tensor(0.3265)\n",
      "34356 Training Loss: tensor(0.3258)\n",
      "34357 Training Loss: tensor(0.3257)\n",
      "34358 Training Loss: tensor(0.3249)\n",
      "34359 Training Loss: tensor(0.3260)\n",
      "34360 Training Loss: tensor(0.3253)\n",
      "34361 Training Loss: tensor(0.3259)\n",
      "34362 Training Loss: tensor(0.3254)\n",
      "34363 Training Loss: tensor(0.3256)\n",
      "34364 Training Loss: tensor(0.3257)\n",
      "34365 Training Loss: tensor(0.3254)\n",
      "34366 Training Loss: tensor(0.3255)\n",
      "34367 Training Loss: tensor(0.3257)\n",
      "34368 Training Loss: tensor(0.3271)\n",
      "34369 Training Loss: tensor(0.3257)\n",
      "34370 Training Loss: tensor(0.3256)\n",
      "34371 Training Loss: tensor(0.3256)\n",
      "34372 Training Loss: tensor(0.3251)\n",
      "34373 Training Loss: tensor(0.3252)\n",
      "34374 Training Loss: tensor(0.3253)\n",
      "34375 Training Loss: tensor(0.3252)\n",
      "34376 Training Loss: tensor(0.3262)\n",
      "34377 Training Loss: tensor(0.3255)\n",
      "34378 Training Loss: tensor(0.3257)\n",
      "34379 Training Loss: tensor(0.3257)\n",
      "34380 Training Loss: tensor(0.3261)\n",
      "34381 Training Loss: tensor(0.3253)\n",
      "34382 Training Loss: tensor(0.3250)\n",
      "34383 Training Loss: tensor(0.3257)\n",
      "34384 Training Loss: tensor(0.3255)\n",
      "34385 Training Loss: tensor(0.3259)\n",
      "34386 Training Loss: tensor(0.3250)\n",
      "34387 Training Loss: tensor(0.3255)\n",
      "34388 Training Loss: tensor(0.3255)\n",
      "34389 Training Loss: tensor(0.3254)\n",
      "34390 Training Loss: tensor(0.3255)\n",
      "34391 Training Loss: tensor(0.3259)\n",
      "34392 Training Loss: tensor(0.3251)\n",
      "34393 Training Loss: tensor(0.3261)\n",
      "34394 Training Loss: tensor(0.3254)\n",
      "34395 Training Loss: tensor(0.3258)\n",
      "34396 Training Loss: tensor(0.3247)\n",
      "34397 Training Loss: tensor(0.3255)\n",
      "34398 Training Loss: tensor(0.3249)\n",
      "34399 Training Loss: tensor(0.3263)\n",
      "34400 Training Loss: tensor(0.3253)\n",
      "34401 Training Loss: tensor(0.3257)\n",
      "34402 Training Loss: tensor(0.3260)\n",
      "34403 Training Loss: tensor(0.3250)\n",
      "34404 Training Loss: tensor(0.3267)\n",
      "34405 Training Loss: tensor(0.3253)\n",
      "34406 Training Loss: tensor(0.3254)\n",
      "34407 Training Loss: tensor(0.3252)\n",
      "34408 Training Loss: tensor(0.3252)\n",
      "34409 Training Loss: tensor(0.3264)\n",
      "34410 Training Loss: tensor(0.3251)\n",
      "34411 Training Loss: tensor(0.3261)\n",
      "34412 Training Loss: tensor(0.3256)\n",
      "34413 Training Loss: tensor(0.3258)\n",
      "34414 Training Loss: tensor(0.3249)\n",
      "34415 Training Loss: tensor(0.3250)\n",
      "34416 Training Loss: tensor(0.3251)\n",
      "34417 Training Loss: tensor(0.3252)\n",
      "34418 Training Loss: tensor(0.3252)\n",
      "34419 Training Loss: tensor(0.3253)\n",
      "34420 Training Loss: tensor(0.3254)\n",
      "34421 Training Loss: tensor(0.3253)\n",
      "34422 Training Loss: tensor(0.3252)\n",
      "34423 Training Loss: tensor(0.3261)\n",
      "34424 Training Loss: tensor(0.3253)\n",
      "34425 Training Loss: tensor(0.3252)\n",
      "34426 Training Loss: tensor(0.3250)\n",
      "34427 Training Loss: tensor(0.3258)\n",
      "34428 Training Loss: tensor(0.3262)\n",
      "34429 Training Loss: tensor(0.3251)\n",
      "34430 Training Loss: tensor(0.3255)\n",
      "34431 Training Loss: tensor(0.3256)\n",
      "34432 Training Loss: tensor(0.3255)\n",
      "34433 Training Loss: tensor(0.3256)\n",
      "34434 Training Loss: tensor(0.3256)\n",
      "34435 Training Loss: tensor(0.3254)\n",
      "34436 Training Loss: tensor(0.3251)\n",
      "34437 Training Loss: tensor(0.3250)\n",
      "34438 Training Loss: tensor(0.3252)\n",
      "34439 Training Loss: tensor(0.3254)\n",
      "34440 Training Loss: tensor(0.3249)\n",
      "34441 Training Loss: tensor(0.3262)\n",
      "34442 Training Loss: tensor(0.3248)\n",
      "34443 Training Loss: tensor(0.3249)\n",
      "34444 Training Loss: tensor(0.3254)\n",
      "34445 Training Loss: tensor(0.3257)\n",
      "34446 Training Loss: tensor(0.3253)\n",
      "34447 Training Loss: tensor(0.3260)\n",
      "34448 Training Loss: tensor(0.3251)\n",
      "34449 Training Loss: tensor(0.3260)\n",
      "34450 Training Loss: tensor(0.3252)\n",
      "34451 Training Loss: tensor(0.3252)\n",
      "34452 Training Loss: tensor(0.3251)\n",
      "34453 Training Loss: tensor(0.3248)\n",
      "34454 Training Loss: tensor(0.3249)\n",
      "34455 Training Loss: tensor(0.3253)\n",
      "34456 Training Loss: tensor(0.3250)\n",
      "34457 Training Loss: tensor(0.3250)\n",
      "34458 Training Loss: tensor(0.3250)\n",
      "34459 Training Loss: tensor(0.3251)\n",
      "34460 Training Loss: tensor(0.3262)\n",
      "34461 Training Loss: tensor(0.3251)\n",
      "34462 Training Loss: tensor(0.3254)\n",
      "34463 Training Loss: tensor(0.3258)\n",
      "34464 Training Loss: tensor(0.3250)\n",
      "34465 Training Loss: tensor(0.3249)\n",
      "34466 Training Loss: tensor(0.3252)\n",
      "34467 Training Loss: tensor(0.3251)\n",
      "34468 Training Loss: tensor(0.3252)\n",
      "34469 Training Loss: tensor(0.3250)\n",
      "34470 Training Loss: tensor(0.3264)\n",
      "34471 Training Loss: tensor(0.3261)\n",
      "34472 Training Loss: tensor(0.3257)\n",
      "34473 Training Loss: tensor(0.3247)\n",
      "34474 Training Loss: tensor(0.3265)\n",
      "34475 Training Loss: tensor(0.3260)\n",
      "34476 Training Loss: tensor(0.3254)\n",
      "34477 Training Loss: tensor(0.3250)\n",
      "34478 Training Loss: tensor(0.3254)\n",
      "34479 Training Loss: tensor(0.3254)\n",
      "34480 Training Loss: tensor(0.3254)\n",
      "34481 Training Loss: tensor(0.3254)\n",
      "34482 Training Loss: tensor(0.3248)\n",
      "34483 Training Loss: tensor(0.3253)\n",
      "34484 Training Loss: tensor(0.3252)\n",
      "34485 Training Loss: tensor(0.3248)\n",
      "34486 Training Loss: tensor(0.3251)\n",
      "34487 Training Loss: tensor(0.3252)\n",
      "34488 Training Loss: tensor(0.3253)\n",
      "34489 Training Loss: tensor(0.3250)\n",
      "34490 Training Loss: tensor(0.3255)\n",
      "34491 Training Loss: tensor(0.3252)\n",
      "34492 Training Loss: tensor(0.3254)\n",
      "34493 Training Loss: tensor(0.3257)\n",
      "34494 Training Loss: tensor(0.3260)\n",
      "34495 Training Loss: tensor(0.3261)\n",
      "34496 Training Loss: tensor(0.3256)\n",
      "34497 Training Loss: tensor(0.3257)\n",
      "34498 Training Loss: tensor(0.3256)\n",
      "34499 Training Loss: tensor(0.3248)\n",
      "34500 Training Loss: tensor(0.3249)\n",
      "34501 Training Loss: tensor(0.3259)\n",
      "34502 Training Loss: tensor(0.3252)\n",
      "34503 Training Loss: tensor(0.3253)\n",
      "34504 Training Loss: tensor(0.3252)\n",
      "34505 Training Loss: tensor(0.3255)\n",
      "34506 Training Loss: tensor(0.3260)\n",
      "34507 Training Loss: tensor(0.3261)\n",
      "34508 Training Loss: tensor(0.3255)\n",
      "34509 Training Loss: tensor(0.3255)\n",
      "34510 Training Loss: tensor(0.3253)\n",
      "34511 Training Loss: tensor(0.3248)\n",
      "34512 Training Loss: tensor(0.3248)\n",
      "34513 Training Loss: tensor(0.3251)\n",
      "34514 Training Loss: tensor(0.3249)\n",
      "34515 Training Loss: tensor(0.3250)\n",
      "34516 Training Loss: tensor(0.3252)\n",
      "34517 Training Loss: tensor(0.3251)\n",
      "34518 Training Loss: tensor(0.3253)\n",
      "34519 Training Loss: tensor(0.3260)\n",
      "34520 Training Loss: tensor(0.3257)\n",
      "34521 Training Loss: tensor(0.3252)\n",
      "34522 Training Loss: tensor(0.3255)\n",
      "34523 Training Loss: tensor(0.3255)\n",
      "34524 Training Loss: tensor(0.3259)\n",
      "34525 Training Loss: tensor(0.3253)\n",
      "34526 Training Loss: tensor(0.3251)\n",
      "34527 Training Loss: tensor(0.3251)\n",
      "34528 Training Loss: tensor(0.3253)\n",
      "34529 Training Loss: tensor(0.3257)\n",
      "34530 Training Loss: tensor(0.3255)\n",
      "34531 Training Loss: tensor(0.3257)\n",
      "34532 Training Loss: tensor(0.3256)\n",
      "34533 Training Loss: tensor(0.3252)\n",
      "34534 Training Loss: tensor(0.3255)\n",
      "34535 Training Loss: tensor(0.3255)\n",
      "34536 Training Loss: tensor(0.3256)\n",
      "34537 Training Loss: tensor(0.3251)\n",
      "34538 Training Loss: tensor(0.3251)\n",
      "34539 Training Loss: tensor(0.3256)\n",
      "34540 Training Loss: tensor(0.3252)\n",
      "34541 Training Loss: tensor(0.3254)\n",
      "34542 Training Loss: tensor(0.3252)\n",
      "34543 Training Loss: tensor(0.3249)\n",
      "34544 Training Loss: tensor(0.3254)\n",
      "34545 Training Loss: tensor(0.3252)\n",
      "34546 Training Loss: tensor(0.3252)\n",
      "34547 Training Loss: tensor(0.3251)\n",
      "34548 Training Loss: tensor(0.3252)\n",
      "34549 Training Loss: tensor(0.3250)\n",
      "34550 Training Loss: tensor(0.3252)\n",
      "34551 Training Loss: tensor(0.3255)\n",
      "34552 Training Loss: tensor(0.3259)\n",
      "34553 Training Loss: tensor(0.3261)\n",
      "34554 Training Loss: tensor(0.3252)\n",
      "34555 Training Loss: tensor(0.3257)\n",
      "34556 Training Loss: tensor(0.3263)\n",
      "34557 Training Loss: tensor(0.3253)\n",
      "34558 Training Loss: tensor(0.3251)\n",
      "34559 Training Loss: tensor(0.3252)\n",
      "34560 Training Loss: tensor(0.3251)\n",
      "34561 Training Loss: tensor(0.3256)\n",
      "34562 Training Loss: tensor(0.3253)\n",
      "34563 Training Loss: tensor(0.3258)\n",
      "34564 Training Loss: tensor(0.3251)\n",
      "34565 Training Loss: tensor(0.3255)\n",
      "34566 Training Loss: tensor(0.3252)\n",
      "34567 Training Loss: tensor(0.3249)\n",
      "34568 Training Loss: tensor(0.3251)\n",
      "34569 Training Loss: tensor(0.3253)\n",
      "34570 Training Loss: tensor(0.3252)\n",
      "34571 Training Loss: tensor(0.3267)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34572 Training Loss: tensor(0.3251)\n",
      "34573 Training Loss: tensor(0.3250)\n",
      "34574 Training Loss: tensor(0.3250)\n",
      "34575 Training Loss: tensor(0.3252)\n",
      "34576 Training Loss: tensor(0.3261)\n",
      "34577 Training Loss: tensor(0.3253)\n",
      "34578 Training Loss: tensor(0.3250)\n",
      "34579 Training Loss: tensor(0.3257)\n",
      "34580 Training Loss: tensor(0.3249)\n",
      "34581 Training Loss: tensor(0.3255)\n",
      "34582 Training Loss: tensor(0.3258)\n",
      "34583 Training Loss: tensor(0.3251)\n",
      "34584 Training Loss: tensor(0.3250)\n",
      "34585 Training Loss: tensor(0.3258)\n",
      "34586 Training Loss: tensor(0.3250)\n",
      "34587 Training Loss: tensor(0.3252)\n",
      "34588 Training Loss: tensor(0.3251)\n",
      "34589 Training Loss: tensor(0.3254)\n",
      "34590 Training Loss: tensor(0.3254)\n",
      "34591 Training Loss: tensor(0.3252)\n",
      "34592 Training Loss: tensor(0.3253)\n",
      "34593 Training Loss: tensor(0.3250)\n",
      "34594 Training Loss: tensor(0.3259)\n",
      "34595 Training Loss: tensor(0.3251)\n",
      "34596 Training Loss: tensor(0.3254)\n",
      "34597 Training Loss: tensor(0.3257)\n",
      "34598 Training Loss: tensor(0.3253)\n",
      "34599 Training Loss: tensor(0.3256)\n",
      "34600 Training Loss: tensor(0.3263)\n",
      "34601 Training Loss: tensor(0.3263)\n",
      "34602 Training Loss: tensor(0.3253)\n",
      "34603 Training Loss: tensor(0.3262)\n",
      "34604 Training Loss: tensor(0.3250)\n",
      "34605 Training Loss: tensor(0.3255)\n",
      "34606 Training Loss: tensor(0.3255)\n",
      "34607 Training Loss: tensor(0.3251)\n",
      "34608 Training Loss: tensor(0.3249)\n",
      "34609 Training Loss: tensor(0.3255)\n",
      "34610 Training Loss: tensor(0.3253)\n",
      "34611 Training Loss: tensor(0.3255)\n",
      "34612 Training Loss: tensor(0.3258)\n",
      "34613 Training Loss: tensor(0.3254)\n",
      "34614 Training Loss: tensor(0.3253)\n",
      "34615 Training Loss: tensor(0.3256)\n",
      "34616 Training Loss: tensor(0.3251)\n",
      "34617 Training Loss: tensor(0.3253)\n",
      "34618 Training Loss: tensor(0.3249)\n",
      "34619 Training Loss: tensor(0.3249)\n",
      "34620 Training Loss: tensor(0.3247)\n",
      "34621 Training Loss: tensor(0.3257)\n",
      "34622 Training Loss: tensor(0.3262)\n",
      "34623 Training Loss: tensor(0.3251)\n",
      "34624 Training Loss: tensor(0.3263)\n",
      "34625 Training Loss: tensor(0.3250)\n",
      "34626 Training Loss: tensor(0.3250)\n",
      "34627 Training Loss: tensor(0.3251)\n",
      "34628 Training Loss: tensor(0.3251)\n",
      "34629 Training Loss: tensor(0.3252)\n",
      "34630 Training Loss: tensor(0.3251)\n",
      "34631 Training Loss: tensor(0.3256)\n",
      "34632 Training Loss: tensor(0.3253)\n",
      "34633 Training Loss: tensor(0.3250)\n",
      "34634 Training Loss: tensor(0.3252)\n",
      "34635 Training Loss: tensor(0.3252)\n",
      "34636 Training Loss: tensor(0.3265)\n",
      "34637 Training Loss: tensor(0.3250)\n",
      "34638 Training Loss: tensor(0.3252)\n",
      "34639 Training Loss: tensor(0.3252)\n",
      "34640 Training Loss: tensor(0.3260)\n",
      "34641 Training Loss: tensor(0.3252)\n",
      "34642 Training Loss: tensor(0.3253)\n",
      "34643 Training Loss: tensor(0.3253)\n",
      "34644 Training Loss: tensor(0.3248)\n",
      "34645 Training Loss: tensor(0.3249)\n",
      "34646 Training Loss: tensor(0.3257)\n",
      "34647 Training Loss: tensor(0.3248)\n",
      "34648 Training Loss: tensor(0.3251)\n",
      "34649 Training Loss: tensor(0.3256)\n",
      "34650 Training Loss: tensor(0.3250)\n",
      "34651 Training Loss: tensor(0.3248)\n",
      "34652 Training Loss: tensor(0.3250)\n",
      "34653 Training Loss: tensor(0.3251)\n",
      "34654 Training Loss: tensor(0.3253)\n",
      "34655 Training Loss: tensor(0.3256)\n",
      "34656 Training Loss: tensor(0.3254)\n",
      "34657 Training Loss: tensor(0.3253)\n",
      "34658 Training Loss: tensor(0.3250)\n",
      "34659 Training Loss: tensor(0.3255)\n",
      "34660 Training Loss: tensor(0.3261)\n",
      "34661 Training Loss: tensor(0.3252)\n",
      "34662 Training Loss: tensor(0.3260)\n",
      "34663 Training Loss: tensor(0.3249)\n",
      "34664 Training Loss: tensor(0.3259)\n",
      "34665 Training Loss: tensor(0.3253)\n",
      "34666 Training Loss: tensor(0.3260)\n",
      "34667 Training Loss: tensor(0.3256)\n",
      "34668 Training Loss: tensor(0.3260)\n",
      "34669 Training Loss: tensor(0.3264)\n",
      "34670 Training Loss: tensor(0.3250)\n",
      "34671 Training Loss: tensor(0.3255)\n",
      "34672 Training Loss: tensor(0.3250)\n",
      "34673 Training Loss: tensor(0.3262)\n",
      "34674 Training Loss: tensor(0.3251)\n",
      "34675 Training Loss: tensor(0.3253)\n",
      "34676 Training Loss: tensor(0.3264)\n",
      "34677 Training Loss: tensor(0.3250)\n",
      "34678 Training Loss: tensor(0.3261)\n",
      "34679 Training Loss: tensor(0.3250)\n",
      "34680 Training Loss: tensor(0.3249)\n",
      "34681 Training Loss: tensor(0.3251)\n",
      "34682 Training Loss: tensor(0.3263)\n",
      "34683 Training Loss: tensor(0.3251)\n",
      "34684 Training Loss: tensor(0.3251)\n",
      "34685 Training Loss: tensor(0.3250)\n",
      "34686 Training Loss: tensor(0.3249)\n",
      "34687 Training Loss: tensor(0.3249)\n",
      "34688 Training Loss: tensor(0.3259)\n",
      "34689 Training Loss: tensor(0.3251)\n",
      "34690 Training Loss: tensor(0.3266)\n",
      "34691 Training Loss: tensor(0.3252)\n",
      "34692 Training Loss: tensor(0.3252)\n",
      "34693 Training Loss: tensor(0.3250)\n",
      "34694 Training Loss: tensor(0.3255)\n",
      "34695 Training Loss: tensor(0.3255)\n",
      "34696 Training Loss: tensor(0.3248)\n",
      "34697 Training Loss: tensor(0.3261)\n",
      "34698 Training Loss: tensor(0.3256)\n",
      "34699 Training Loss: tensor(0.3255)\n",
      "34700 Training Loss: tensor(0.3251)\n",
      "34701 Training Loss: tensor(0.3250)\n",
      "34702 Training Loss: tensor(0.3258)\n",
      "34703 Training Loss: tensor(0.3252)\n",
      "34704 Training Loss: tensor(0.3258)\n",
      "34705 Training Loss: tensor(0.3251)\n",
      "34706 Training Loss: tensor(0.3253)\n",
      "34707 Training Loss: tensor(0.3255)\n",
      "34708 Training Loss: tensor(0.3250)\n",
      "34709 Training Loss: tensor(0.3253)\n",
      "34710 Training Loss: tensor(0.3250)\n",
      "34711 Training Loss: tensor(0.3250)\n",
      "34712 Training Loss: tensor(0.3256)\n",
      "34713 Training Loss: tensor(0.3255)\n",
      "34714 Training Loss: tensor(0.3260)\n",
      "34715 Training Loss: tensor(0.3259)\n",
      "34716 Training Loss: tensor(0.3253)\n",
      "34717 Training Loss: tensor(0.3248)\n",
      "34718 Training Loss: tensor(0.3250)\n",
      "34719 Training Loss: tensor(0.3251)\n",
      "34720 Training Loss: tensor(0.3256)\n",
      "34721 Training Loss: tensor(0.3248)\n",
      "34722 Training Loss: tensor(0.3252)\n",
      "34723 Training Loss: tensor(0.3254)\n",
      "34724 Training Loss: tensor(0.3262)\n",
      "34725 Training Loss: tensor(0.3258)\n",
      "34726 Training Loss: tensor(0.3252)\n",
      "34727 Training Loss: tensor(0.3257)\n",
      "34728 Training Loss: tensor(0.3263)\n",
      "34729 Training Loss: tensor(0.3254)\n",
      "34730 Training Loss: tensor(0.3253)\n",
      "34731 Training Loss: tensor(0.3254)\n",
      "34732 Training Loss: tensor(0.3254)\n",
      "34733 Training Loss: tensor(0.3258)\n",
      "34734 Training Loss: tensor(0.3255)\n",
      "34735 Training Loss: tensor(0.3259)\n",
      "34736 Training Loss: tensor(0.3249)\n",
      "34737 Training Loss: tensor(0.3258)\n",
      "34738 Training Loss: tensor(0.3258)\n",
      "34739 Training Loss: tensor(0.3261)\n",
      "34740 Training Loss: tensor(0.3263)\n",
      "34741 Training Loss: tensor(0.3253)\n",
      "34742 Training Loss: tensor(0.3248)\n",
      "34743 Training Loss: tensor(0.3259)\n",
      "34744 Training Loss: tensor(0.3261)\n",
      "34745 Training Loss: tensor(0.3255)\n",
      "34746 Training Loss: tensor(0.3252)\n",
      "34747 Training Loss: tensor(0.3252)\n",
      "34748 Training Loss: tensor(0.3259)\n",
      "34749 Training Loss: tensor(0.3251)\n",
      "34750 Training Loss: tensor(0.3254)\n",
      "34751 Training Loss: tensor(0.3253)\n",
      "34752 Training Loss: tensor(0.3265)\n",
      "34753 Training Loss: tensor(0.3253)\n",
      "34754 Training Loss: tensor(0.3263)\n",
      "34755 Training Loss: tensor(0.3254)\n",
      "34756 Training Loss: tensor(0.3257)\n",
      "34757 Training Loss: tensor(0.3259)\n",
      "34758 Training Loss: tensor(0.3263)\n",
      "34759 Training Loss: tensor(0.3252)\n",
      "34760 Training Loss: tensor(0.3249)\n",
      "34761 Training Loss: tensor(0.3256)\n",
      "34762 Training Loss: tensor(0.3259)\n",
      "34763 Training Loss: tensor(0.3256)\n",
      "34764 Training Loss: tensor(0.3249)\n",
      "34765 Training Loss: tensor(0.3251)\n",
      "34766 Training Loss: tensor(0.3254)\n",
      "34767 Training Loss: tensor(0.3258)\n",
      "34768 Training Loss: tensor(0.3250)\n",
      "34769 Training Loss: tensor(0.3255)\n",
      "34770 Training Loss: tensor(0.3255)\n",
      "34771 Training Loss: tensor(0.3248)\n",
      "34772 Training Loss: tensor(0.3250)\n",
      "34773 Training Loss: tensor(0.3262)\n",
      "34774 Training Loss: tensor(0.3253)\n",
      "34775 Training Loss: tensor(0.3251)\n",
      "34776 Training Loss: tensor(0.3261)\n",
      "34777 Training Loss: tensor(0.3256)\n",
      "34778 Training Loss: tensor(0.3257)\n",
      "34779 Training Loss: tensor(0.3254)\n",
      "34780 Training Loss: tensor(0.3255)\n",
      "34781 Training Loss: tensor(0.3255)\n",
      "34782 Training Loss: tensor(0.3249)\n",
      "34783 Training Loss: tensor(0.3249)\n",
      "34784 Training Loss: tensor(0.3251)\n",
      "34785 Training Loss: tensor(0.3263)\n",
      "34786 Training Loss: tensor(0.3250)\n",
      "34787 Training Loss: tensor(0.3276)\n",
      "34788 Training Loss: tensor(0.3258)\n",
      "34789 Training Loss: tensor(0.3251)\n",
      "34790 Training Loss: tensor(0.3265)\n",
      "34791 Training Loss: tensor(0.3264)\n",
      "34792 Training Loss: tensor(0.3261)\n",
      "34793 Training Loss: tensor(0.3254)\n",
      "34794 Training Loss: tensor(0.3254)\n",
      "34795 Training Loss: tensor(0.3259)\n",
      "34796 Training Loss: tensor(0.3250)\n",
      "34797 Training Loss: tensor(0.3249)\n",
      "34798 Training Loss: tensor(0.3251)\n",
      "34799 Training Loss: tensor(0.3254)\n",
      "34800 Training Loss: tensor(0.3256)\n",
      "34801 Training Loss: tensor(0.3252)\n",
      "34802 Training Loss: tensor(0.3251)\n",
      "34803 Training Loss: tensor(0.3248)\n",
      "34804 Training Loss: tensor(0.3261)\n",
      "34805 Training Loss: tensor(0.3251)\n",
      "34806 Training Loss: tensor(0.3251)\n",
      "34807 Training Loss: tensor(0.3249)\n",
      "34808 Training Loss: tensor(0.3252)\n",
      "34809 Training Loss: tensor(0.3263)\n",
      "34810 Training Loss: tensor(0.3254)\n",
      "34811 Training Loss: tensor(0.3252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34812 Training Loss: tensor(0.3249)\n",
      "34813 Training Loss: tensor(0.3254)\n",
      "34814 Training Loss: tensor(0.3257)\n",
      "34815 Training Loss: tensor(0.3256)\n",
      "34816 Training Loss: tensor(0.3254)\n",
      "34817 Training Loss: tensor(0.3251)\n",
      "34818 Training Loss: tensor(0.3248)\n",
      "34819 Training Loss: tensor(0.3255)\n",
      "34820 Training Loss: tensor(0.3250)\n",
      "34821 Training Loss: tensor(0.3267)\n",
      "34822 Training Loss: tensor(0.3251)\n",
      "34823 Training Loss: tensor(0.3256)\n",
      "34824 Training Loss: tensor(0.3253)\n",
      "34825 Training Loss: tensor(0.3260)\n",
      "34826 Training Loss: tensor(0.3249)\n",
      "34827 Training Loss: tensor(0.3250)\n",
      "34828 Training Loss: tensor(0.3258)\n",
      "34829 Training Loss: tensor(0.3255)\n",
      "34830 Training Loss: tensor(0.3256)\n",
      "34831 Training Loss: tensor(0.3251)\n",
      "34832 Training Loss: tensor(0.3251)\n",
      "34833 Training Loss: tensor(0.3255)\n",
      "34834 Training Loss: tensor(0.3249)\n",
      "34835 Training Loss: tensor(0.3257)\n",
      "34836 Training Loss: tensor(0.3258)\n",
      "34837 Training Loss: tensor(0.3258)\n",
      "34838 Training Loss: tensor(0.3253)\n",
      "34839 Training Loss: tensor(0.3253)\n",
      "34840 Training Loss: tensor(0.3256)\n",
      "34841 Training Loss: tensor(0.3256)\n",
      "34842 Training Loss: tensor(0.3251)\n",
      "34843 Training Loss: tensor(0.3250)\n",
      "34844 Training Loss: tensor(0.3251)\n",
      "34845 Training Loss: tensor(0.3250)\n",
      "34846 Training Loss: tensor(0.3248)\n",
      "34847 Training Loss: tensor(0.3252)\n",
      "34848 Training Loss: tensor(0.3250)\n",
      "34849 Training Loss: tensor(0.3252)\n",
      "34850 Training Loss: tensor(0.3262)\n",
      "34851 Training Loss: tensor(0.3249)\n",
      "34852 Training Loss: tensor(0.3249)\n",
      "34853 Training Loss: tensor(0.3259)\n",
      "34854 Training Loss: tensor(0.3266)\n",
      "34855 Training Loss: tensor(0.3247)\n",
      "34856 Training Loss: tensor(0.3255)\n",
      "34857 Training Loss: tensor(0.3256)\n",
      "34858 Training Loss: tensor(0.3250)\n",
      "34859 Training Loss: tensor(0.3256)\n",
      "34860 Training Loss: tensor(0.3250)\n",
      "34861 Training Loss: tensor(0.3250)\n",
      "34862 Training Loss: tensor(0.3259)\n",
      "34863 Training Loss: tensor(0.3254)\n",
      "34864 Training Loss: tensor(0.3253)\n",
      "34865 Training Loss: tensor(0.3251)\n",
      "34866 Training Loss: tensor(0.3248)\n",
      "34867 Training Loss: tensor(0.3255)\n",
      "34868 Training Loss: tensor(0.3255)\n",
      "34869 Training Loss: tensor(0.3257)\n",
      "34870 Training Loss: tensor(0.3250)\n",
      "34871 Training Loss: tensor(0.3256)\n",
      "34872 Training Loss: tensor(0.3249)\n",
      "34873 Training Loss: tensor(0.3252)\n",
      "34874 Training Loss: tensor(0.3254)\n",
      "34875 Training Loss: tensor(0.3251)\n",
      "34876 Training Loss: tensor(0.3248)\n",
      "34877 Training Loss: tensor(0.3249)\n",
      "34878 Training Loss: tensor(0.3249)\n",
      "34879 Training Loss: tensor(0.3248)\n",
      "34880 Training Loss: tensor(0.3259)\n",
      "34881 Training Loss: tensor(0.3247)\n",
      "34882 Training Loss: tensor(0.3253)\n",
      "34883 Training Loss: tensor(0.3252)\n",
      "34884 Training Loss: tensor(0.3251)\n",
      "34885 Training Loss: tensor(0.3267)\n",
      "34886 Training Loss: tensor(0.3259)\n",
      "34887 Training Loss: tensor(0.3253)\n",
      "34888 Training Loss: tensor(0.3271)\n",
      "34889 Training Loss: tensor(0.3252)\n",
      "34890 Training Loss: tensor(0.3249)\n",
      "34891 Training Loss: tensor(0.3262)\n",
      "34892 Training Loss: tensor(0.3264)\n",
      "34893 Training Loss: tensor(0.3252)\n",
      "34894 Training Loss: tensor(0.3256)\n",
      "34895 Training Loss: tensor(0.3253)\n",
      "34896 Training Loss: tensor(0.3255)\n",
      "34897 Training Loss: tensor(0.3262)\n",
      "34898 Training Loss: tensor(0.3253)\n",
      "34899 Training Loss: tensor(0.3256)\n",
      "34900 Training Loss: tensor(0.3254)\n",
      "34901 Training Loss: tensor(0.3249)\n",
      "34902 Training Loss: tensor(0.3250)\n",
      "34903 Training Loss: tensor(0.3259)\n",
      "34904 Training Loss: tensor(0.3261)\n",
      "34905 Training Loss: tensor(0.3257)\n",
      "34906 Training Loss: tensor(0.3256)\n",
      "34907 Training Loss: tensor(0.3252)\n",
      "34908 Training Loss: tensor(0.3252)\n",
      "34909 Training Loss: tensor(0.3260)\n",
      "34910 Training Loss: tensor(0.3257)\n",
      "34911 Training Loss: tensor(0.3249)\n",
      "34912 Training Loss: tensor(0.3253)\n",
      "34913 Training Loss: tensor(0.3253)\n",
      "34914 Training Loss: tensor(0.3255)\n",
      "34915 Training Loss: tensor(0.3252)\n",
      "34916 Training Loss: tensor(0.3263)\n",
      "34917 Training Loss: tensor(0.3258)\n",
      "34918 Training Loss: tensor(0.3260)\n",
      "34919 Training Loss: tensor(0.3256)\n",
      "34920 Training Loss: tensor(0.3250)\n",
      "34921 Training Loss: tensor(0.3251)\n",
      "34922 Training Loss: tensor(0.3259)\n",
      "34923 Training Loss: tensor(0.3254)\n",
      "34924 Training Loss: tensor(0.3252)\n",
      "34925 Training Loss: tensor(0.3253)\n",
      "34926 Training Loss: tensor(0.3252)\n",
      "34927 Training Loss: tensor(0.3256)\n",
      "34928 Training Loss: tensor(0.3267)\n",
      "34929 Training Loss: tensor(0.3255)\n",
      "34930 Training Loss: tensor(0.3252)\n",
      "34931 Training Loss: tensor(0.3253)\n",
      "34932 Training Loss: tensor(0.3253)\n",
      "34933 Training Loss: tensor(0.3255)\n",
      "34934 Training Loss: tensor(0.3255)\n",
      "34935 Training Loss: tensor(0.3260)\n",
      "34936 Training Loss: tensor(0.3249)\n",
      "34937 Training Loss: tensor(0.3254)\n",
      "34938 Training Loss: tensor(0.3249)\n",
      "34939 Training Loss: tensor(0.3257)\n",
      "34940 Training Loss: tensor(0.3252)\n",
      "34941 Training Loss: tensor(0.3250)\n",
      "34942 Training Loss: tensor(0.3255)\n",
      "34943 Training Loss: tensor(0.3258)\n",
      "34944 Training Loss: tensor(0.3254)\n",
      "34945 Training Loss: tensor(0.3256)\n",
      "34946 Training Loss: tensor(0.3256)\n",
      "34947 Training Loss: tensor(0.3253)\n",
      "34948 Training Loss: tensor(0.3250)\n",
      "34949 Training Loss: tensor(0.3253)\n",
      "34950 Training Loss: tensor(0.3252)\n",
      "34951 Training Loss: tensor(0.3256)\n",
      "34952 Training Loss: tensor(0.3253)\n",
      "34953 Training Loss: tensor(0.3250)\n",
      "34954 Training Loss: tensor(0.3265)\n",
      "34955 Training Loss: tensor(0.3251)\n",
      "34956 Training Loss: tensor(0.3261)\n",
      "34957 Training Loss: tensor(0.3249)\n",
      "34958 Training Loss: tensor(0.3258)\n",
      "34959 Training Loss: tensor(0.3257)\n",
      "34960 Training Loss: tensor(0.3249)\n",
      "34961 Training Loss: tensor(0.3253)\n",
      "34962 Training Loss: tensor(0.3254)\n",
      "34963 Training Loss: tensor(0.3250)\n",
      "34964 Training Loss: tensor(0.3257)\n",
      "34965 Training Loss: tensor(0.3252)\n",
      "34966 Training Loss: tensor(0.3247)\n",
      "34967 Training Loss: tensor(0.3248)\n",
      "34968 Training Loss: tensor(0.3250)\n",
      "34969 Training Loss: tensor(0.3257)\n",
      "34970 Training Loss: tensor(0.3254)\n",
      "34971 Training Loss: tensor(0.3249)\n",
      "34972 Training Loss: tensor(0.3255)\n",
      "34973 Training Loss: tensor(0.3258)\n",
      "34974 Training Loss: tensor(0.3255)\n",
      "34975 Training Loss: tensor(0.3248)\n",
      "34976 Training Loss: tensor(0.3257)\n",
      "34977 Training Loss: tensor(0.3251)\n",
      "34978 Training Loss: tensor(0.3247)\n",
      "34979 Training Loss: tensor(0.3252)\n",
      "34980 Training Loss: tensor(0.3252)\n",
      "34981 Training Loss: tensor(0.3249)\n",
      "34982 Training Loss: tensor(0.3250)\n",
      "34983 Training Loss: tensor(0.3249)\n",
      "34984 Training Loss: tensor(0.3255)\n",
      "34985 Training Loss: tensor(0.3262)\n",
      "34986 Training Loss: tensor(0.3258)\n",
      "34987 Training Loss: tensor(0.3248)\n",
      "34988 Training Loss: tensor(0.3251)\n",
      "34989 Training Loss: tensor(0.3264)\n",
      "34990 Training Loss: tensor(0.3252)\n",
      "34991 Training Loss: tensor(0.3265)\n",
      "34992 Training Loss: tensor(0.3253)\n",
      "34993 Training Loss: tensor(0.3254)\n",
      "34994 Training Loss: tensor(0.3255)\n",
      "34995 Training Loss: tensor(0.3251)\n",
      "34996 Training Loss: tensor(0.3250)\n",
      "34997 Training Loss: tensor(0.3253)\n",
      "34998 Training Loss: tensor(0.3250)\n",
      "34999 Training Loss: tensor(0.3255)\n",
      "35000 Training Loss: tensor(0.3248)\n",
      "35001 Training Loss: tensor(0.3260)\n",
      "35002 Training Loss: tensor(0.3249)\n",
      "35003 Training Loss: tensor(0.3250)\n",
      "35004 Training Loss: tensor(0.3255)\n",
      "35005 Training Loss: tensor(0.3255)\n",
      "35006 Training Loss: tensor(0.3256)\n",
      "35007 Training Loss: tensor(0.3255)\n",
      "35008 Training Loss: tensor(0.3255)\n",
      "35009 Training Loss: tensor(0.3249)\n",
      "35010 Training Loss: tensor(0.3253)\n",
      "35011 Training Loss: tensor(0.3250)\n",
      "35012 Training Loss: tensor(0.3262)\n",
      "35013 Training Loss: tensor(0.3248)\n",
      "35014 Training Loss: tensor(0.3253)\n",
      "35015 Training Loss: tensor(0.3255)\n",
      "35016 Training Loss: tensor(0.3260)\n",
      "35017 Training Loss: tensor(0.3258)\n",
      "35018 Training Loss: tensor(0.3251)\n",
      "35019 Training Loss: tensor(0.3266)\n",
      "35020 Training Loss: tensor(0.3258)\n",
      "35021 Training Loss: tensor(0.3255)\n",
      "35022 Training Loss: tensor(0.3262)\n",
      "35023 Training Loss: tensor(0.3250)\n",
      "35024 Training Loss: tensor(0.3255)\n",
      "35025 Training Loss: tensor(0.3262)\n",
      "35026 Training Loss: tensor(0.3259)\n",
      "35027 Training Loss: tensor(0.3251)\n",
      "35028 Training Loss: tensor(0.3253)\n",
      "35029 Training Loss: tensor(0.3252)\n",
      "35030 Training Loss: tensor(0.3251)\n",
      "35031 Training Loss: tensor(0.3249)\n",
      "35032 Training Loss: tensor(0.3250)\n",
      "35033 Training Loss: tensor(0.3248)\n",
      "35034 Training Loss: tensor(0.3248)\n",
      "35035 Training Loss: tensor(0.3253)\n",
      "35036 Training Loss: tensor(0.3252)\n",
      "35037 Training Loss: tensor(0.3253)\n",
      "35038 Training Loss: tensor(0.3254)\n",
      "35039 Training Loss: tensor(0.3249)\n",
      "35040 Training Loss: tensor(0.3254)\n",
      "35041 Training Loss: tensor(0.3260)\n",
      "35042 Training Loss: tensor(0.3253)\n",
      "35043 Training Loss: tensor(0.3259)\n",
      "35044 Training Loss: tensor(0.3248)\n",
      "35045 Training Loss: tensor(0.3260)\n",
      "35046 Training Loss: tensor(0.3263)\n",
      "35047 Training Loss: tensor(0.3259)\n",
      "35048 Training Loss: tensor(0.3271)\n",
      "35049 Training Loss: tensor(0.3254)\n",
      "35050 Training Loss: tensor(0.3259)\n",
      "35051 Training Loss: tensor(0.3248)\n",
      "35052 Training Loss: tensor(0.3265)\n",
      "35053 Training Loss: tensor(0.3262)\n",
      "35054 Training Loss: tensor(0.3270)\n",
      "35055 Training Loss: tensor(0.3266)\n",
      "35056 Training Loss: tensor(0.3255)\n",
      "35057 Training Loss: tensor(0.3255)\n",
      "35058 Training Loss: tensor(0.3254)\n",
      "35059 Training Loss: tensor(0.3266)\n",
      "35060 Training Loss: tensor(0.3255)\n",
      "35061 Training Loss: tensor(0.3252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35062 Training Loss: tensor(0.3253)\n",
      "35063 Training Loss: tensor(0.3256)\n",
      "35064 Training Loss: tensor(0.3253)\n",
      "35065 Training Loss: tensor(0.3268)\n",
      "35066 Training Loss: tensor(0.3252)\n",
      "35067 Training Loss: tensor(0.3259)\n",
      "35068 Training Loss: tensor(0.3250)\n",
      "35069 Training Loss: tensor(0.3256)\n",
      "35070 Training Loss: tensor(0.3252)\n",
      "35071 Training Loss: tensor(0.3260)\n",
      "35072 Training Loss: tensor(0.3252)\n",
      "35073 Training Loss: tensor(0.3253)\n",
      "35074 Training Loss: tensor(0.3255)\n",
      "35075 Training Loss: tensor(0.3256)\n",
      "35076 Training Loss: tensor(0.3259)\n",
      "35077 Training Loss: tensor(0.3256)\n",
      "35078 Training Loss: tensor(0.3256)\n",
      "35079 Training Loss: tensor(0.3256)\n",
      "35080 Training Loss: tensor(0.3252)\n",
      "35081 Training Loss: tensor(0.3253)\n",
      "35082 Training Loss: tensor(0.3251)\n",
      "35083 Training Loss: tensor(0.3254)\n",
      "35084 Training Loss: tensor(0.3250)\n",
      "35085 Training Loss: tensor(0.3254)\n",
      "35086 Training Loss: tensor(0.3250)\n",
      "35087 Training Loss: tensor(0.3252)\n",
      "35088 Training Loss: tensor(0.3255)\n",
      "35089 Training Loss: tensor(0.3254)\n",
      "35090 Training Loss: tensor(0.3249)\n",
      "35091 Training Loss: tensor(0.3260)\n",
      "35092 Training Loss: tensor(0.3247)\n",
      "35093 Training Loss: tensor(0.3257)\n",
      "35094 Training Loss: tensor(0.3253)\n",
      "35095 Training Loss: tensor(0.3269)\n",
      "35096 Training Loss: tensor(0.3254)\n",
      "35097 Training Loss: tensor(0.3252)\n",
      "35098 Training Loss: tensor(0.3251)\n",
      "35099 Training Loss: tensor(0.3253)\n",
      "35100 Training Loss: tensor(0.3266)\n",
      "35101 Training Loss: tensor(0.3250)\n",
      "35102 Training Loss: tensor(0.3263)\n",
      "35103 Training Loss: tensor(0.3253)\n",
      "35104 Training Loss: tensor(0.3257)\n",
      "35105 Training Loss: tensor(0.3256)\n",
      "35106 Training Loss: tensor(0.3251)\n",
      "35107 Training Loss: tensor(0.3251)\n",
      "35108 Training Loss: tensor(0.3254)\n",
      "35109 Training Loss: tensor(0.3262)\n",
      "35110 Training Loss: tensor(0.3250)\n",
      "35111 Training Loss: tensor(0.3261)\n",
      "35112 Training Loss: tensor(0.3248)\n",
      "35113 Training Loss: tensor(0.3249)\n",
      "35114 Training Loss: tensor(0.3253)\n",
      "35115 Training Loss: tensor(0.3255)\n",
      "35116 Training Loss: tensor(0.3255)\n",
      "35117 Training Loss: tensor(0.3262)\n",
      "35118 Training Loss: tensor(0.3249)\n",
      "35119 Training Loss: tensor(0.3247)\n",
      "35120 Training Loss: tensor(0.3254)\n",
      "35121 Training Loss: tensor(0.3252)\n",
      "35122 Training Loss: tensor(0.3252)\n",
      "35123 Training Loss: tensor(0.3258)\n",
      "35124 Training Loss: tensor(0.3249)\n",
      "35125 Training Loss: tensor(0.3261)\n",
      "35126 Training Loss: tensor(0.3248)\n",
      "35127 Training Loss: tensor(0.3254)\n",
      "35128 Training Loss: tensor(0.3249)\n",
      "35129 Training Loss: tensor(0.3254)\n",
      "35130 Training Loss: tensor(0.3250)\n",
      "35131 Training Loss: tensor(0.3259)\n",
      "35132 Training Loss: tensor(0.3246)\n",
      "35133 Training Loss: tensor(0.3250)\n",
      "35134 Training Loss: tensor(0.3255)\n",
      "35135 Training Loss: tensor(0.3255)\n",
      "35136 Training Loss: tensor(0.3252)\n",
      "35137 Training Loss: tensor(0.3256)\n",
      "35138 Training Loss: tensor(0.3255)\n",
      "35139 Training Loss: tensor(0.3250)\n",
      "35140 Training Loss: tensor(0.3250)\n",
      "35141 Training Loss: tensor(0.3253)\n",
      "35142 Training Loss: tensor(0.3249)\n",
      "35143 Training Loss: tensor(0.3252)\n",
      "35144 Training Loss: tensor(0.3253)\n",
      "35145 Training Loss: tensor(0.3254)\n",
      "35146 Training Loss: tensor(0.3249)\n",
      "35147 Training Loss: tensor(0.3248)\n",
      "35148 Training Loss: tensor(0.3254)\n",
      "35149 Training Loss: tensor(0.3249)\n",
      "35150 Training Loss: tensor(0.3253)\n",
      "35151 Training Loss: tensor(0.3263)\n",
      "35152 Training Loss: tensor(0.3260)\n",
      "35153 Training Loss: tensor(0.3249)\n",
      "35154 Training Loss: tensor(0.3260)\n",
      "35155 Training Loss: tensor(0.3250)\n",
      "35156 Training Loss: tensor(0.3249)\n",
      "35157 Training Loss: tensor(0.3252)\n",
      "35158 Training Loss: tensor(0.3247)\n",
      "35159 Training Loss: tensor(0.3249)\n",
      "35160 Training Loss: tensor(0.3253)\n",
      "35161 Training Loss: tensor(0.3248)\n",
      "35162 Training Loss: tensor(0.3252)\n",
      "35163 Training Loss: tensor(0.3260)\n",
      "35164 Training Loss: tensor(0.3247)\n",
      "35165 Training Loss: tensor(0.3247)\n",
      "35166 Training Loss: tensor(0.3253)\n",
      "35167 Training Loss: tensor(0.3250)\n",
      "35168 Training Loss: tensor(0.3250)\n",
      "35169 Training Loss: tensor(0.3248)\n",
      "35170 Training Loss: tensor(0.3248)\n",
      "35171 Training Loss: tensor(0.3253)\n",
      "35172 Training Loss: tensor(0.3257)\n",
      "35173 Training Loss: tensor(0.3251)\n",
      "35174 Training Loss: tensor(0.3256)\n",
      "35175 Training Loss: tensor(0.3252)\n",
      "35176 Training Loss: tensor(0.3246)\n",
      "35177 Training Loss: tensor(0.3253)\n",
      "35178 Training Loss: tensor(0.3246)\n",
      "35179 Training Loss: tensor(0.3255)\n",
      "35180 Training Loss: tensor(0.3249)\n",
      "35181 Training Loss: tensor(0.3253)\n",
      "35182 Training Loss: tensor(0.3251)\n",
      "35183 Training Loss: tensor(0.3255)\n",
      "35184 Training Loss: tensor(0.3250)\n",
      "35185 Training Loss: tensor(0.3248)\n",
      "35186 Training Loss: tensor(0.3253)\n",
      "35187 Training Loss: tensor(0.3266)\n",
      "35188 Training Loss: tensor(0.3247)\n",
      "35189 Training Loss: tensor(0.3250)\n",
      "35190 Training Loss: tensor(0.3252)\n",
      "35191 Training Loss: tensor(0.3247)\n",
      "35192 Training Loss: tensor(0.3250)\n",
      "35193 Training Loss: tensor(0.3250)\n",
      "35194 Training Loss: tensor(0.3253)\n",
      "35195 Training Loss: tensor(0.3252)\n",
      "35196 Training Loss: tensor(0.3253)\n",
      "35197 Training Loss: tensor(0.3264)\n",
      "35198 Training Loss: tensor(0.3247)\n",
      "35199 Training Loss: tensor(0.3250)\n",
      "35200 Training Loss: tensor(0.3248)\n",
      "35201 Training Loss: tensor(0.3263)\n",
      "35202 Training Loss: tensor(0.3265)\n",
      "35203 Training Loss: tensor(0.3249)\n",
      "35204 Training Loss: tensor(0.3254)\n",
      "35205 Training Loss: tensor(0.3267)\n",
      "35206 Training Loss: tensor(0.3251)\n",
      "35207 Training Loss: tensor(0.3268)\n",
      "35208 Training Loss: tensor(0.3258)\n",
      "35209 Training Loss: tensor(0.3246)\n",
      "35210 Training Loss: tensor(0.3252)\n",
      "35211 Training Loss: tensor(0.3249)\n",
      "35212 Training Loss: tensor(0.3248)\n",
      "35213 Training Loss: tensor(0.3253)\n",
      "35214 Training Loss: tensor(0.3247)\n",
      "35215 Training Loss: tensor(0.3250)\n",
      "35216 Training Loss: tensor(0.3249)\n",
      "35217 Training Loss: tensor(0.3260)\n",
      "35218 Training Loss: tensor(0.3247)\n",
      "35219 Training Loss: tensor(0.3248)\n",
      "35220 Training Loss: tensor(0.3256)\n",
      "35221 Training Loss: tensor(0.3247)\n",
      "35222 Training Loss: tensor(0.3253)\n",
      "35223 Training Loss: tensor(0.3250)\n",
      "35224 Training Loss: tensor(0.3247)\n",
      "35225 Training Loss: tensor(0.3247)\n",
      "35226 Training Loss: tensor(0.3261)\n",
      "35227 Training Loss: tensor(0.3251)\n",
      "35228 Training Loss: tensor(0.3255)\n",
      "35229 Training Loss: tensor(0.3250)\n",
      "35230 Training Loss: tensor(0.3256)\n",
      "35231 Training Loss: tensor(0.3252)\n",
      "35232 Training Loss: tensor(0.3255)\n",
      "35233 Training Loss: tensor(0.3266)\n",
      "35234 Training Loss: tensor(0.3252)\n",
      "35235 Training Loss: tensor(0.3259)\n",
      "35236 Training Loss: tensor(0.3251)\n",
      "35237 Training Loss: tensor(0.3249)\n",
      "35238 Training Loss: tensor(0.3259)\n",
      "35239 Training Loss: tensor(0.3254)\n",
      "35240 Training Loss: tensor(0.3255)\n",
      "35241 Training Loss: tensor(0.3250)\n",
      "35242 Training Loss: tensor(0.3252)\n",
      "35243 Training Loss: tensor(0.3250)\n",
      "35244 Training Loss: tensor(0.3254)\n",
      "35245 Training Loss: tensor(0.3252)\n",
      "35246 Training Loss: tensor(0.3256)\n",
      "35247 Training Loss: tensor(0.3257)\n",
      "35248 Training Loss: tensor(0.3249)\n",
      "35249 Training Loss: tensor(0.3257)\n",
      "35250 Training Loss: tensor(0.3249)\n",
      "35251 Training Loss: tensor(0.3250)\n",
      "35252 Training Loss: tensor(0.3255)\n",
      "35253 Training Loss: tensor(0.3253)\n",
      "35254 Training Loss: tensor(0.3249)\n",
      "35255 Training Loss: tensor(0.3256)\n",
      "35256 Training Loss: tensor(0.3253)\n",
      "35257 Training Loss: tensor(0.3249)\n",
      "35258 Training Loss: tensor(0.3257)\n",
      "35259 Training Loss: tensor(0.3248)\n",
      "35260 Training Loss: tensor(0.3248)\n",
      "35261 Training Loss: tensor(0.3258)\n",
      "35262 Training Loss: tensor(0.3263)\n",
      "35263 Training Loss: tensor(0.3250)\n",
      "35264 Training Loss: tensor(0.3247)\n",
      "35265 Training Loss: tensor(0.3257)\n",
      "35266 Training Loss: tensor(0.3263)\n",
      "35267 Training Loss: tensor(0.3252)\n",
      "35268 Training Loss: tensor(0.3250)\n",
      "35269 Training Loss: tensor(0.3255)\n",
      "35270 Training Loss: tensor(0.3257)\n",
      "35271 Training Loss: tensor(0.3252)\n",
      "35272 Training Loss: tensor(0.3252)\n",
      "35273 Training Loss: tensor(0.3249)\n",
      "35274 Training Loss: tensor(0.3250)\n",
      "35275 Training Loss: tensor(0.3253)\n",
      "35276 Training Loss: tensor(0.3252)\n",
      "35277 Training Loss: tensor(0.3248)\n",
      "35278 Training Loss: tensor(0.3250)\n",
      "35279 Training Loss: tensor(0.3252)\n",
      "35280 Training Loss: tensor(0.3246)\n",
      "35281 Training Loss: tensor(0.3251)\n",
      "35282 Training Loss: tensor(0.3253)\n",
      "35283 Training Loss: tensor(0.3252)\n",
      "35284 Training Loss: tensor(0.3250)\n",
      "35285 Training Loss: tensor(0.3249)\n",
      "35286 Training Loss: tensor(0.3262)\n",
      "35287 Training Loss: tensor(0.3255)\n",
      "35288 Training Loss: tensor(0.3253)\n",
      "35289 Training Loss: tensor(0.3250)\n",
      "35290 Training Loss: tensor(0.3255)\n",
      "35291 Training Loss: tensor(0.3251)\n",
      "35292 Training Loss: tensor(0.3247)\n",
      "35293 Training Loss: tensor(0.3253)\n",
      "35294 Training Loss: tensor(0.3263)\n",
      "35295 Training Loss: tensor(0.3256)\n",
      "35296 Training Loss: tensor(0.3249)\n",
      "35297 Training Loss: tensor(0.3248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35298 Training Loss: tensor(0.3262)\n",
      "35299 Training Loss: tensor(0.3252)\n",
      "35300 Training Loss: tensor(0.3250)\n",
      "35301 Training Loss: tensor(0.3250)\n",
      "35302 Training Loss: tensor(0.3262)\n",
      "35303 Training Loss: tensor(0.3248)\n",
      "35304 Training Loss: tensor(0.3249)\n",
      "35305 Training Loss: tensor(0.3251)\n",
      "35306 Training Loss: tensor(0.3258)\n",
      "35307 Training Loss: tensor(0.3254)\n",
      "35308 Training Loss: tensor(0.3250)\n",
      "35309 Training Loss: tensor(0.3262)\n",
      "35310 Training Loss: tensor(0.3250)\n",
      "35311 Training Loss: tensor(0.3260)\n",
      "35312 Training Loss: tensor(0.3251)\n",
      "35313 Training Loss: tensor(0.3247)\n",
      "35314 Training Loss: tensor(0.3255)\n",
      "35315 Training Loss: tensor(0.3248)\n",
      "35316 Training Loss: tensor(0.3259)\n",
      "35317 Training Loss: tensor(0.3249)\n",
      "35318 Training Loss: tensor(0.3248)\n",
      "35319 Training Loss: tensor(0.3249)\n",
      "35320 Training Loss: tensor(0.3257)\n",
      "35321 Training Loss: tensor(0.3250)\n",
      "35322 Training Loss: tensor(0.3251)\n",
      "35323 Training Loss: tensor(0.3253)\n",
      "35324 Training Loss: tensor(0.3254)\n",
      "35325 Training Loss: tensor(0.3256)\n",
      "35326 Training Loss: tensor(0.3247)\n",
      "35327 Training Loss: tensor(0.3251)\n",
      "35328 Training Loss: tensor(0.3261)\n",
      "35329 Training Loss: tensor(0.3255)\n",
      "35330 Training Loss: tensor(0.3249)\n",
      "35331 Training Loss: tensor(0.3249)\n",
      "35332 Training Loss: tensor(0.3251)\n",
      "35333 Training Loss: tensor(0.3253)\n",
      "35334 Training Loss: tensor(0.3251)\n",
      "35335 Training Loss: tensor(0.3248)\n",
      "35336 Training Loss: tensor(0.3251)\n",
      "35337 Training Loss: tensor(0.3248)\n",
      "35338 Training Loss: tensor(0.3247)\n",
      "35339 Training Loss: tensor(0.3250)\n",
      "35340 Training Loss: tensor(0.3258)\n",
      "35341 Training Loss: tensor(0.3254)\n",
      "35342 Training Loss: tensor(0.3254)\n",
      "35343 Training Loss: tensor(0.3263)\n",
      "35344 Training Loss: tensor(0.3249)\n",
      "35345 Training Loss: tensor(0.3247)\n",
      "35346 Training Loss: tensor(0.3247)\n",
      "35347 Training Loss: tensor(0.3256)\n",
      "35348 Training Loss: tensor(0.3252)\n",
      "35349 Training Loss: tensor(0.3251)\n",
      "35350 Training Loss: tensor(0.3250)\n",
      "35351 Training Loss: tensor(0.3252)\n",
      "35352 Training Loss: tensor(0.3251)\n",
      "35353 Training Loss: tensor(0.3250)\n",
      "35354 Training Loss: tensor(0.3250)\n",
      "35355 Training Loss: tensor(0.3258)\n",
      "35356 Training Loss: tensor(0.3255)\n",
      "35357 Training Loss: tensor(0.3250)\n",
      "35358 Training Loss: tensor(0.3247)\n",
      "35359 Training Loss: tensor(0.3249)\n",
      "35360 Training Loss: tensor(0.3261)\n",
      "35361 Training Loss: tensor(0.3253)\n",
      "35362 Training Loss: tensor(0.3256)\n",
      "35363 Training Loss: tensor(0.3255)\n",
      "35364 Training Loss: tensor(0.3266)\n",
      "35365 Training Loss: tensor(0.3263)\n",
      "35366 Training Loss: tensor(0.3253)\n",
      "35367 Training Loss: tensor(0.3252)\n",
      "35368 Training Loss: tensor(0.3259)\n",
      "35369 Training Loss: tensor(0.3256)\n",
      "35370 Training Loss: tensor(0.3253)\n",
      "35371 Training Loss: tensor(0.3257)\n",
      "35372 Training Loss: tensor(0.3249)\n",
      "35373 Training Loss: tensor(0.3249)\n",
      "35374 Training Loss: tensor(0.3252)\n",
      "35375 Training Loss: tensor(0.3254)\n",
      "35376 Training Loss: tensor(0.3267)\n",
      "35377 Training Loss: tensor(0.3253)\n",
      "35378 Training Loss: tensor(0.3250)\n",
      "35379 Training Loss: tensor(0.3259)\n",
      "35380 Training Loss: tensor(0.3252)\n",
      "35381 Training Loss: tensor(0.3250)\n",
      "35382 Training Loss: tensor(0.3250)\n",
      "35383 Training Loss: tensor(0.3249)\n",
      "35384 Training Loss: tensor(0.3253)\n",
      "35385 Training Loss: tensor(0.3255)\n",
      "35386 Training Loss: tensor(0.3251)\n",
      "35387 Training Loss: tensor(0.3265)\n",
      "35388 Training Loss: tensor(0.3253)\n",
      "35389 Training Loss: tensor(0.3249)\n",
      "35390 Training Loss: tensor(0.3254)\n",
      "35391 Training Loss: tensor(0.3250)\n",
      "35392 Training Loss: tensor(0.3259)\n",
      "35393 Training Loss: tensor(0.3251)\n",
      "35394 Training Loss: tensor(0.3247)\n",
      "35395 Training Loss: tensor(0.3248)\n",
      "35396 Training Loss: tensor(0.3248)\n",
      "35397 Training Loss: tensor(0.3258)\n",
      "35398 Training Loss: tensor(0.3254)\n",
      "35399 Training Loss: tensor(0.3255)\n",
      "35400 Training Loss: tensor(0.3261)\n",
      "35401 Training Loss: tensor(0.3248)\n",
      "35402 Training Loss: tensor(0.3249)\n",
      "35403 Training Loss: tensor(0.3249)\n",
      "35404 Training Loss: tensor(0.3250)\n",
      "35405 Training Loss: tensor(0.3253)\n",
      "35406 Training Loss: tensor(0.3255)\n",
      "35407 Training Loss: tensor(0.3258)\n",
      "35408 Training Loss: tensor(0.3255)\n",
      "35409 Training Loss: tensor(0.3256)\n",
      "35410 Training Loss: tensor(0.3265)\n",
      "35411 Training Loss: tensor(0.3254)\n",
      "35412 Training Loss: tensor(0.3251)\n",
      "35413 Training Loss: tensor(0.3253)\n",
      "35414 Training Loss: tensor(0.3257)\n",
      "35415 Training Loss: tensor(0.3256)\n",
      "35416 Training Loss: tensor(0.3246)\n",
      "35417 Training Loss: tensor(0.3254)\n",
      "35418 Training Loss: tensor(0.3249)\n",
      "35419 Training Loss: tensor(0.3259)\n",
      "35420 Training Loss: tensor(0.3267)\n",
      "35421 Training Loss: tensor(0.3253)\n",
      "35422 Training Loss: tensor(0.3256)\n",
      "35423 Training Loss: tensor(0.3258)\n",
      "35424 Training Loss: tensor(0.3260)\n",
      "35425 Training Loss: tensor(0.3252)\n",
      "35426 Training Loss: tensor(0.3250)\n",
      "35427 Training Loss: tensor(0.3253)\n",
      "35428 Training Loss: tensor(0.3258)\n",
      "35429 Training Loss: tensor(0.3256)\n",
      "35430 Training Loss: tensor(0.3251)\n",
      "35431 Training Loss: tensor(0.3260)\n",
      "35432 Training Loss: tensor(0.3248)\n",
      "35433 Training Loss: tensor(0.3251)\n",
      "35434 Training Loss: tensor(0.3250)\n",
      "35435 Training Loss: tensor(0.3254)\n",
      "35436 Training Loss: tensor(0.3252)\n",
      "35437 Training Loss: tensor(0.3249)\n",
      "35438 Training Loss: tensor(0.3250)\n",
      "35439 Training Loss: tensor(0.3250)\n",
      "35440 Training Loss: tensor(0.3250)\n",
      "35441 Training Loss: tensor(0.3252)\n",
      "35442 Training Loss: tensor(0.3256)\n",
      "35443 Training Loss: tensor(0.3247)\n",
      "35444 Training Loss: tensor(0.3249)\n",
      "35445 Training Loss: tensor(0.3250)\n",
      "35446 Training Loss: tensor(0.3248)\n",
      "35447 Training Loss: tensor(0.3250)\n",
      "35448 Training Loss: tensor(0.3267)\n",
      "35449 Training Loss: tensor(0.3249)\n",
      "35450 Training Loss: tensor(0.3257)\n",
      "35451 Training Loss: tensor(0.3250)\n",
      "35452 Training Loss: tensor(0.3248)\n",
      "35453 Training Loss: tensor(0.3259)\n",
      "35454 Training Loss: tensor(0.3255)\n",
      "35455 Training Loss: tensor(0.3246)\n",
      "35456 Training Loss: tensor(0.3248)\n",
      "35457 Training Loss: tensor(0.3251)\n",
      "35458 Training Loss: tensor(0.3248)\n",
      "35459 Training Loss: tensor(0.3248)\n",
      "35460 Training Loss: tensor(0.3246)\n",
      "35461 Training Loss: tensor(0.3255)\n",
      "35462 Training Loss: tensor(0.3249)\n",
      "35463 Training Loss: tensor(0.3255)\n",
      "35464 Training Loss: tensor(0.3248)\n",
      "35465 Training Loss: tensor(0.3251)\n",
      "35466 Training Loss: tensor(0.3249)\n",
      "35467 Training Loss: tensor(0.3248)\n",
      "35468 Training Loss: tensor(0.3259)\n",
      "35469 Training Loss: tensor(0.3254)\n",
      "35470 Training Loss: tensor(0.3250)\n",
      "35471 Training Loss: tensor(0.3251)\n",
      "35472 Training Loss: tensor(0.3253)\n",
      "35473 Training Loss: tensor(0.3253)\n",
      "35474 Training Loss: tensor(0.3258)\n",
      "35475 Training Loss: tensor(0.3255)\n",
      "35476 Training Loss: tensor(0.3252)\n",
      "35477 Training Loss: tensor(0.3251)\n",
      "35478 Training Loss: tensor(0.3251)\n",
      "35479 Training Loss: tensor(0.3251)\n",
      "35480 Training Loss: tensor(0.3252)\n",
      "35481 Training Loss: tensor(0.3254)\n",
      "35482 Training Loss: tensor(0.3254)\n",
      "35483 Training Loss: tensor(0.3255)\n",
      "35484 Training Loss: tensor(0.3246)\n",
      "35485 Training Loss: tensor(0.3255)\n",
      "35486 Training Loss: tensor(0.3250)\n",
      "35487 Training Loss: tensor(0.3250)\n",
      "35488 Training Loss: tensor(0.3256)\n",
      "35489 Training Loss: tensor(0.3254)\n",
      "35490 Training Loss: tensor(0.3256)\n",
      "35491 Training Loss: tensor(0.3253)\n",
      "35492 Training Loss: tensor(0.3247)\n",
      "35493 Training Loss: tensor(0.3247)\n",
      "35494 Training Loss: tensor(0.3251)\n",
      "35495 Training Loss: tensor(0.3248)\n",
      "35496 Training Loss: tensor(0.3249)\n",
      "35497 Training Loss: tensor(0.3250)\n",
      "35498 Training Loss: tensor(0.3251)\n",
      "35499 Training Loss: tensor(0.3251)\n",
      "35500 Training Loss: tensor(0.3258)\n",
      "35501 Training Loss: tensor(0.3252)\n",
      "35502 Training Loss: tensor(0.3250)\n",
      "35503 Training Loss: tensor(0.3254)\n",
      "35504 Training Loss: tensor(0.3252)\n",
      "35505 Training Loss: tensor(0.3252)\n",
      "35506 Training Loss: tensor(0.3254)\n",
      "35507 Training Loss: tensor(0.3250)\n",
      "35508 Training Loss: tensor(0.3249)\n",
      "35509 Training Loss: tensor(0.3253)\n",
      "35510 Training Loss: tensor(0.3247)\n",
      "35511 Training Loss: tensor(0.3249)\n",
      "35512 Training Loss: tensor(0.3251)\n",
      "35513 Training Loss: tensor(0.3247)\n",
      "35514 Training Loss: tensor(0.3255)\n",
      "35515 Training Loss: tensor(0.3257)\n",
      "35516 Training Loss: tensor(0.3270)\n",
      "35517 Training Loss: tensor(0.3260)\n",
      "35518 Training Loss: tensor(0.3249)\n",
      "35519 Training Loss: tensor(0.3247)\n",
      "35520 Training Loss: tensor(0.3250)\n",
      "35521 Training Loss: tensor(0.3246)\n",
      "35522 Training Loss: tensor(0.3254)\n",
      "35523 Training Loss: tensor(0.3249)\n",
      "35524 Training Loss: tensor(0.3254)\n",
      "35525 Training Loss: tensor(0.3263)\n",
      "35526 Training Loss: tensor(0.3251)\n",
      "35527 Training Loss: tensor(0.3250)\n",
      "35528 Training Loss: tensor(0.3258)\n",
      "35529 Training Loss: tensor(0.3250)\n",
      "35530 Training Loss: tensor(0.3248)\n",
      "35531 Training Loss: tensor(0.3252)\n",
      "35532 Training Loss: tensor(0.3247)\n",
      "35533 Training Loss: tensor(0.3247)\n",
      "35534 Training Loss: tensor(0.3253)\n",
      "35535 Training Loss: tensor(0.3250)\n",
      "35536 Training Loss: tensor(0.3254)\n",
      "35537 Training Loss: tensor(0.3250)\n",
      "35538 Training Loss: tensor(0.3247)\n",
      "35539 Training Loss: tensor(0.3253)\n",
      "35540 Training Loss: tensor(0.3264)\n",
      "35541 Training Loss: tensor(0.3256)\n",
      "35542 Training Loss: tensor(0.3256)\n",
      "35543 Training Loss: tensor(0.3253)\n",
      "35544 Training Loss: tensor(0.3248)\n",
      "35545 Training Loss: tensor(0.3249)\n",
      "35546 Training Loss: tensor(0.3254)\n",
      "35547 Training Loss: tensor(0.3252)\n",
      "35548 Training Loss: tensor(0.3249)\n",
      "35549 Training Loss: tensor(0.3253)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35550 Training Loss: tensor(0.3256)\n",
      "35551 Training Loss: tensor(0.3256)\n",
      "35552 Training Loss: tensor(0.3249)\n",
      "35553 Training Loss: tensor(0.3250)\n",
      "35554 Training Loss: tensor(0.3253)\n",
      "35555 Training Loss: tensor(0.3252)\n",
      "35556 Training Loss: tensor(0.3254)\n",
      "35557 Training Loss: tensor(0.3248)\n",
      "35558 Training Loss: tensor(0.3250)\n",
      "35559 Training Loss: tensor(0.3251)\n",
      "35560 Training Loss: tensor(0.3267)\n",
      "35561 Training Loss: tensor(0.3249)\n",
      "35562 Training Loss: tensor(0.3247)\n",
      "35563 Training Loss: tensor(0.3253)\n",
      "35564 Training Loss: tensor(0.3252)\n",
      "35565 Training Loss: tensor(0.3251)\n",
      "35566 Training Loss: tensor(0.3249)\n",
      "35567 Training Loss: tensor(0.3250)\n",
      "35568 Training Loss: tensor(0.3252)\n",
      "35569 Training Loss: tensor(0.3250)\n",
      "35570 Training Loss: tensor(0.3254)\n",
      "35571 Training Loss: tensor(0.3255)\n",
      "35572 Training Loss: tensor(0.3256)\n",
      "35573 Training Loss: tensor(0.3252)\n",
      "35574 Training Loss: tensor(0.3258)\n",
      "35575 Training Loss: tensor(0.3262)\n",
      "35576 Training Loss: tensor(0.3257)\n",
      "35577 Training Loss: tensor(0.3253)\n",
      "35578 Training Loss: tensor(0.3251)\n",
      "35579 Training Loss: tensor(0.3257)\n",
      "35580 Training Loss: tensor(0.3253)\n",
      "35581 Training Loss: tensor(0.3249)\n",
      "35582 Training Loss: tensor(0.3255)\n",
      "35583 Training Loss: tensor(0.3252)\n",
      "35584 Training Loss: tensor(0.3253)\n",
      "35585 Training Loss: tensor(0.3249)\n",
      "35586 Training Loss: tensor(0.3253)\n",
      "35587 Training Loss: tensor(0.3251)\n",
      "35588 Training Loss: tensor(0.3251)\n",
      "35589 Training Loss: tensor(0.3246)\n",
      "35590 Training Loss: tensor(0.3249)\n",
      "35591 Training Loss: tensor(0.3251)\n",
      "35592 Training Loss: tensor(0.3254)\n",
      "35593 Training Loss: tensor(0.3256)\n",
      "35594 Training Loss: tensor(0.3248)\n",
      "35595 Training Loss: tensor(0.3255)\n",
      "35596 Training Loss: tensor(0.3255)\n",
      "35597 Training Loss: tensor(0.3258)\n",
      "35598 Training Loss: tensor(0.3251)\n",
      "35599 Training Loss: tensor(0.3255)\n",
      "35600 Training Loss: tensor(0.3248)\n",
      "35601 Training Loss: tensor(0.3251)\n",
      "35602 Training Loss: tensor(0.3258)\n",
      "35603 Training Loss: tensor(0.3251)\n",
      "35604 Training Loss: tensor(0.3257)\n",
      "35605 Training Loss: tensor(0.3254)\n",
      "35606 Training Loss: tensor(0.3247)\n",
      "35607 Training Loss: tensor(0.3248)\n",
      "35608 Training Loss: tensor(0.3252)\n",
      "35609 Training Loss: tensor(0.3250)\n",
      "35610 Training Loss: tensor(0.3252)\n",
      "35611 Training Loss: tensor(0.3247)\n",
      "35612 Training Loss: tensor(0.3254)\n",
      "35613 Training Loss: tensor(0.3250)\n",
      "35614 Training Loss: tensor(0.3253)\n",
      "35615 Training Loss: tensor(0.3252)\n",
      "35616 Training Loss: tensor(0.3254)\n",
      "35617 Training Loss: tensor(0.3265)\n",
      "35618 Training Loss: tensor(0.3248)\n",
      "35619 Training Loss: tensor(0.3259)\n",
      "35620 Training Loss: tensor(0.3249)\n",
      "35621 Training Loss: tensor(0.3256)\n",
      "35622 Training Loss: tensor(0.3251)\n",
      "35623 Training Loss: tensor(0.3251)\n",
      "35624 Training Loss: tensor(0.3253)\n",
      "35625 Training Loss: tensor(0.3251)\n",
      "35626 Training Loss: tensor(0.3251)\n",
      "35627 Training Loss: tensor(0.3253)\n",
      "35628 Training Loss: tensor(0.3252)\n",
      "35629 Training Loss: tensor(0.3249)\n",
      "35630 Training Loss: tensor(0.3250)\n",
      "35631 Training Loss: tensor(0.3250)\n",
      "35632 Training Loss: tensor(0.3254)\n",
      "35633 Training Loss: tensor(0.3254)\n",
      "35634 Training Loss: tensor(0.3253)\n",
      "35635 Training Loss: tensor(0.3258)\n",
      "35636 Training Loss: tensor(0.3248)\n",
      "35637 Training Loss: tensor(0.3247)\n",
      "35638 Training Loss: tensor(0.3255)\n",
      "35639 Training Loss: tensor(0.3263)\n",
      "35640 Training Loss: tensor(0.3247)\n",
      "35641 Training Loss: tensor(0.3254)\n",
      "35642 Training Loss: tensor(0.3250)\n",
      "35643 Training Loss: tensor(0.3248)\n",
      "35644 Training Loss: tensor(0.3266)\n",
      "35645 Training Loss: tensor(0.3250)\n",
      "35646 Training Loss: tensor(0.3250)\n",
      "35647 Training Loss: tensor(0.3271)\n",
      "35648 Training Loss: tensor(0.3251)\n",
      "35649 Training Loss: tensor(0.3250)\n",
      "35650 Training Loss: tensor(0.3250)\n",
      "35651 Training Loss: tensor(0.3254)\n",
      "35652 Training Loss: tensor(0.3246)\n",
      "35653 Training Loss: tensor(0.3252)\n",
      "35654 Training Loss: tensor(0.3252)\n",
      "35655 Training Loss: tensor(0.3247)\n",
      "35656 Training Loss: tensor(0.3246)\n",
      "35657 Training Loss: tensor(0.3253)\n",
      "35658 Training Loss: tensor(0.3254)\n",
      "35659 Training Loss: tensor(0.3247)\n",
      "35660 Training Loss: tensor(0.3246)\n",
      "35661 Training Loss: tensor(0.3250)\n",
      "35662 Training Loss: tensor(0.3246)\n",
      "35663 Training Loss: tensor(0.3249)\n",
      "35664 Training Loss: tensor(0.3249)\n",
      "35665 Training Loss: tensor(0.3252)\n",
      "35666 Training Loss: tensor(0.3265)\n",
      "35667 Training Loss: tensor(0.3258)\n",
      "35668 Training Loss: tensor(0.3257)\n",
      "35669 Training Loss: tensor(0.3255)\n",
      "35670 Training Loss: tensor(0.3253)\n",
      "35671 Training Loss: tensor(0.3254)\n",
      "35672 Training Loss: tensor(0.3260)\n",
      "35673 Training Loss: tensor(0.3254)\n",
      "35674 Training Loss: tensor(0.3253)\n",
      "35675 Training Loss: tensor(0.3256)\n",
      "35676 Training Loss: tensor(0.3261)\n",
      "35677 Training Loss: tensor(0.3261)\n",
      "35678 Training Loss: tensor(0.3247)\n",
      "35679 Training Loss: tensor(0.3252)\n",
      "35680 Training Loss: tensor(0.3256)\n",
      "35681 Training Loss: tensor(0.3258)\n",
      "35682 Training Loss: tensor(0.3250)\n",
      "35683 Training Loss: tensor(0.3259)\n",
      "35684 Training Loss: tensor(0.3257)\n",
      "35685 Training Loss: tensor(0.3263)\n",
      "35686 Training Loss: tensor(0.3253)\n",
      "35687 Training Loss: tensor(0.3248)\n",
      "35688 Training Loss: tensor(0.3249)\n",
      "35689 Training Loss: tensor(0.3253)\n",
      "35690 Training Loss: tensor(0.3251)\n",
      "35691 Training Loss: tensor(0.3250)\n",
      "35692 Training Loss: tensor(0.3247)\n",
      "35693 Training Loss: tensor(0.3249)\n",
      "35694 Training Loss: tensor(0.3249)\n",
      "35695 Training Loss: tensor(0.3250)\n",
      "35696 Training Loss: tensor(0.3249)\n",
      "35697 Training Loss: tensor(0.3247)\n",
      "35698 Training Loss: tensor(0.3251)\n",
      "35699 Training Loss: tensor(0.3250)\n",
      "35700 Training Loss: tensor(0.3251)\n",
      "35701 Training Loss: tensor(0.3251)\n",
      "35702 Training Loss: tensor(0.3253)\n",
      "35703 Training Loss: tensor(0.3252)\n",
      "35704 Training Loss: tensor(0.3248)\n",
      "35705 Training Loss: tensor(0.3246)\n",
      "35706 Training Loss: tensor(0.3252)\n",
      "35707 Training Loss: tensor(0.3255)\n",
      "35708 Training Loss: tensor(0.3246)\n",
      "35709 Training Loss: tensor(0.3249)\n",
      "35710 Training Loss: tensor(0.3257)\n",
      "35711 Training Loss: tensor(0.3254)\n",
      "35712 Training Loss: tensor(0.3253)\n",
      "35713 Training Loss: tensor(0.3257)\n",
      "35714 Training Loss: tensor(0.3251)\n",
      "35715 Training Loss: tensor(0.3263)\n",
      "35716 Training Loss: tensor(0.3256)\n",
      "35717 Training Loss: tensor(0.3250)\n",
      "35718 Training Loss: tensor(0.3253)\n",
      "35719 Training Loss: tensor(0.3251)\n",
      "35720 Training Loss: tensor(0.3250)\n",
      "35721 Training Loss: tensor(0.3247)\n",
      "35722 Training Loss: tensor(0.3249)\n",
      "35723 Training Loss: tensor(0.3257)\n",
      "35724 Training Loss: tensor(0.3249)\n",
      "35725 Training Loss: tensor(0.3245)\n",
      "35726 Training Loss: tensor(0.3251)\n",
      "35727 Training Loss: tensor(0.3250)\n",
      "35728 Training Loss: tensor(0.3249)\n",
      "35729 Training Loss: tensor(0.3254)\n",
      "35730 Training Loss: tensor(0.3258)\n",
      "35731 Training Loss: tensor(0.3252)\n",
      "35732 Training Loss: tensor(0.3258)\n",
      "35733 Training Loss: tensor(0.3248)\n",
      "35734 Training Loss: tensor(0.3247)\n",
      "35735 Training Loss: tensor(0.3251)\n",
      "35736 Training Loss: tensor(0.3249)\n",
      "35737 Training Loss: tensor(0.3250)\n",
      "35738 Training Loss: tensor(0.3257)\n",
      "35739 Training Loss: tensor(0.3253)\n",
      "35740 Training Loss: tensor(0.3248)\n",
      "35741 Training Loss: tensor(0.3247)\n",
      "35742 Training Loss: tensor(0.3248)\n",
      "35743 Training Loss: tensor(0.3258)\n",
      "35744 Training Loss: tensor(0.3249)\n",
      "35745 Training Loss: tensor(0.3248)\n",
      "35746 Training Loss: tensor(0.3249)\n",
      "35747 Training Loss: tensor(0.3254)\n",
      "35748 Training Loss: tensor(0.3249)\n",
      "35749 Training Loss: tensor(0.3256)\n",
      "35750 Training Loss: tensor(0.3254)\n",
      "35751 Training Loss: tensor(0.3250)\n",
      "35752 Training Loss: tensor(0.3246)\n",
      "35753 Training Loss: tensor(0.3253)\n",
      "35754 Training Loss: tensor(0.3247)\n",
      "35755 Training Loss: tensor(0.3250)\n",
      "35756 Training Loss: tensor(0.3259)\n",
      "35757 Training Loss: tensor(0.3249)\n",
      "35758 Training Loss: tensor(0.3264)\n",
      "35759 Training Loss: tensor(0.3255)\n",
      "35760 Training Loss: tensor(0.3249)\n",
      "35761 Training Loss: tensor(0.3253)\n",
      "35762 Training Loss: tensor(0.3252)\n",
      "35763 Training Loss: tensor(0.3250)\n",
      "35764 Training Loss: tensor(0.3265)\n",
      "35765 Training Loss: tensor(0.3252)\n",
      "35766 Training Loss: tensor(0.3251)\n",
      "35767 Training Loss: tensor(0.3253)\n",
      "35768 Training Loss: tensor(0.3253)\n",
      "35769 Training Loss: tensor(0.3253)\n",
      "35770 Training Loss: tensor(0.3249)\n",
      "35771 Training Loss: tensor(0.3258)\n",
      "35772 Training Loss: tensor(0.3251)\n",
      "35773 Training Loss: tensor(0.3248)\n",
      "35774 Training Loss: tensor(0.3253)\n",
      "35775 Training Loss: tensor(0.3256)\n",
      "35776 Training Loss: tensor(0.3249)\n",
      "35777 Training Loss: tensor(0.3256)\n",
      "35778 Training Loss: tensor(0.3259)\n",
      "35779 Training Loss: tensor(0.3257)\n",
      "35780 Training Loss: tensor(0.3258)\n",
      "35781 Training Loss: tensor(0.3250)\n",
      "35782 Training Loss: tensor(0.3248)\n",
      "35783 Training Loss: tensor(0.3250)\n",
      "35784 Training Loss: tensor(0.3252)\n",
      "35785 Training Loss: tensor(0.3257)\n",
      "35786 Training Loss: tensor(0.3249)\n",
      "35787 Training Loss: tensor(0.3258)\n",
      "35788 Training Loss: tensor(0.3248)\n",
      "35789 Training Loss: tensor(0.3254)\n",
      "35790 Training Loss: tensor(0.3249)\n",
      "35791 Training Loss: tensor(0.3246)\n",
      "35792 Training Loss: tensor(0.3255)\n",
      "35793 Training Loss: tensor(0.3254)\n",
      "35794 Training Loss: tensor(0.3251)\n",
      "35795 Training Loss: tensor(0.3247)\n",
      "35796 Training Loss: tensor(0.3249)\n",
      "35797 Training Loss: tensor(0.3248)\n",
      "35798 Training Loss: tensor(0.3249)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35799 Training Loss: tensor(0.3246)\n",
      "35800 Training Loss: tensor(0.3254)\n",
      "35801 Training Loss: tensor(0.3253)\n",
      "35802 Training Loss: tensor(0.3261)\n",
      "35803 Training Loss: tensor(0.3248)\n",
      "35804 Training Loss: tensor(0.3250)\n",
      "35805 Training Loss: tensor(0.3253)\n",
      "35806 Training Loss: tensor(0.3253)\n",
      "35807 Training Loss: tensor(0.3248)\n",
      "35808 Training Loss: tensor(0.3255)\n",
      "35809 Training Loss: tensor(0.3260)\n",
      "35810 Training Loss: tensor(0.3254)\n",
      "35811 Training Loss: tensor(0.3248)\n",
      "35812 Training Loss: tensor(0.3253)\n",
      "35813 Training Loss: tensor(0.3255)\n",
      "35814 Training Loss: tensor(0.3250)\n",
      "35815 Training Loss: tensor(0.3261)\n",
      "35816 Training Loss: tensor(0.3251)\n",
      "35817 Training Loss: tensor(0.3264)\n",
      "35818 Training Loss: tensor(0.3250)\n",
      "35819 Training Loss: tensor(0.3249)\n",
      "35820 Training Loss: tensor(0.3251)\n",
      "35821 Training Loss: tensor(0.3252)\n",
      "35822 Training Loss: tensor(0.3248)\n",
      "35823 Training Loss: tensor(0.3250)\n",
      "35824 Training Loss: tensor(0.3247)\n",
      "35825 Training Loss: tensor(0.3257)\n",
      "35826 Training Loss: tensor(0.3251)\n",
      "35827 Training Loss: tensor(0.3272)\n",
      "35828 Training Loss: tensor(0.3250)\n",
      "35829 Training Loss: tensor(0.3248)\n",
      "35830 Training Loss: tensor(0.3260)\n",
      "35831 Training Loss: tensor(0.3250)\n",
      "35832 Training Loss: tensor(0.3253)\n",
      "35833 Training Loss: tensor(0.3255)\n",
      "35834 Training Loss: tensor(0.3256)\n",
      "35835 Training Loss: tensor(0.3251)\n",
      "35836 Training Loss: tensor(0.3250)\n",
      "35837 Training Loss: tensor(0.3248)\n",
      "35838 Training Loss: tensor(0.3257)\n",
      "35839 Training Loss: tensor(0.3252)\n",
      "35840 Training Loss: tensor(0.3255)\n",
      "35841 Training Loss: tensor(0.3256)\n",
      "35842 Training Loss: tensor(0.3252)\n",
      "35843 Training Loss: tensor(0.3255)\n",
      "35844 Training Loss: tensor(0.3254)\n",
      "35845 Training Loss: tensor(0.3252)\n",
      "35846 Training Loss: tensor(0.3247)\n",
      "35847 Training Loss: tensor(0.3249)\n",
      "35848 Training Loss: tensor(0.3249)\n",
      "35849 Training Loss: tensor(0.3255)\n",
      "35850 Training Loss: tensor(0.3251)\n",
      "35851 Training Loss: tensor(0.3246)\n",
      "35852 Training Loss: tensor(0.3252)\n",
      "35853 Training Loss: tensor(0.3248)\n",
      "35854 Training Loss: tensor(0.3251)\n",
      "35855 Training Loss: tensor(0.3249)\n",
      "35856 Training Loss: tensor(0.3248)\n",
      "35857 Training Loss: tensor(0.3254)\n",
      "35858 Training Loss: tensor(0.3263)\n",
      "35859 Training Loss: tensor(0.3248)\n",
      "35860 Training Loss: tensor(0.3250)\n",
      "35861 Training Loss: tensor(0.3258)\n",
      "35862 Training Loss: tensor(0.3252)\n",
      "35863 Training Loss: tensor(0.3259)\n",
      "35864 Training Loss: tensor(0.3254)\n",
      "35865 Training Loss: tensor(0.3251)\n",
      "35866 Training Loss: tensor(0.3260)\n",
      "35867 Training Loss: tensor(0.3252)\n",
      "35868 Training Loss: tensor(0.3248)\n",
      "35869 Training Loss: tensor(0.3254)\n",
      "35870 Training Loss: tensor(0.3259)\n",
      "35871 Training Loss: tensor(0.3248)\n",
      "35872 Training Loss: tensor(0.3258)\n",
      "35873 Training Loss: tensor(0.3254)\n",
      "35874 Training Loss: tensor(0.3255)\n",
      "35875 Training Loss: tensor(0.3251)\n",
      "35876 Training Loss: tensor(0.3254)\n",
      "35877 Training Loss: tensor(0.3248)\n",
      "35878 Training Loss: tensor(0.3264)\n",
      "35879 Training Loss: tensor(0.3262)\n",
      "35880 Training Loss: tensor(0.3253)\n",
      "35881 Training Loss: tensor(0.3250)\n",
      "35882 Training Loss: tensor(0.3250)\n",
      "35883 Training Loss: tensor(0.3249)\n",
      "35884 Training Loss: tensor(0.3247)\n",
      "35885 Training Loss: tensor(0.3251)\n",
      "35886 Training Loss: tensor(0.3254)\n",
      "35887 Training Loss: tensor(0.3253)\n",
      "35888 Training Loss: tensor(0.3250)\n",
      "35889 Training Loss: tensor(0.3245)\n",
      "35890 Training Loss: tensor(0.3248)\n",
      "35891 Training Loss: tensor(0.3248)\n",
      "35892 Training Loss: tensor(0.3251)\n",
      "35893 Training Loss: tensor(0.3253)\n",
      "35894 Training Loss: tensor(0.3251)\n",
      "35895 Training Loss: tensor(0.3247)\n",
      "35896 Training Loss: tensor(0.3249)\n",
      "35897 Training Loss: tensor(0.3261)\n",
      "35898 Training Loss: tensor(0.3251)\n",
      "35899 Training Loss: tensor(0.3247)\n",
      "35900 Training Loss: tensor(0.3251)\n",
      "35901 Training Loss: tensor(0.3259)\n",
      "35902 Training Loss: tensor(0.3246)\n",
      "35903 Training Loss: tensor(0.3254)\n",
      "35904 Training Loss: tensor(0.3251)\n",
      "35905 Training Loss: tensor(0.3257)\n",
      "35906 Training Loss: tensor(0.3253)\n",
      "35907 Training Loss: tensor(0.3250)\n",
      "35908 Training Loss: tensor(0.3248)\n",
      "35909 Training Loss: tensor(0.3255)\n",
      "35910 Training Loss: tensor(0.3261)\n",
      "35911 Training Loss: tensor(0.3252)\n",
      "35912 Training Loss: tensor(0.3255)\n",
      "35913 Training Loss: tensor(0.3252)\n",
      "35914 Training Loss: tensor(0.3249)\n",
      "35915 Training Loss: tensor(0.3253)\n",
      "35916 Training Loss: tensor(0.3251)\n",
      "35917 Training Loss: tensor(0.3251)\n",
      "35918 Training Loss: tensor(0.3249)\n",
      "35919 Training Loss: tensor(0.3250)\n",
      "35920 Training Loss: tensor(0.3251)\n",
      "35921 Training Loss: tensor(0.3258)\n",
      "35922 Training Loss: tensor(0.3254)\n",
      "35923 Training Loss: tensor(0.3246)\n",
      "35924 Training Loss: tensor(0.3246)\n",
      "35925 Training Loss: tensor(0.3250)\n",
      "35926 Training Loss: tensor(0.3255)\n",
      "35927 Training Loss: tensor(0.3249)\n",
      "35928 Training Loss: tensor(0.3255)\n",
      "35929 Training Loss: tensor(0.3255)\n",
      "35930 Training Loss: tensor(0.3248)\n",
      "35931 Training Loss: tensor(0.3245)\n",
      "35932 Training Loss: tensor(0.3250)\n",
      "35933 Training Loss: tensor(0.3247)\n",
      "35934 Training Loss: tensor(0.3251)\n",
      "35935 Training Loss: tensor(0.3251)\n",
      "35936 Training Loss: tensor(0.3251)\n",
      "35937 Training Loss: tensor(0.3247)\n",
      "35938 Training Loss: tensor(0.3248)\n",
      "35939 Training Loss: tensor(0.3249)\n",
      "35940 Training Loss: tensor(0.3247)\n",
      "35941 Training Loss: tensor(0.3257)\n",
      "35942 Training Loss: tensor(0.3244)\n",
      "35943 Training Loss: tensor(0.3260)\n",
      "35944 Training Loss: tensor(0.3252)\n",
      "35945 Training Loss: tensor(0.3250)\n",
      "35946 Training Loss: tensor(0.3250)\n",
      "35947 Training Loss: tensor(0.3251)\n",
      "35948 Training Loss: tensor(0.3247)\n",
      "35949 Training Loss: tensor(0.3249)\n",
      "35950 Training Loss: tensor(0.3261)\n",
      "35951 Training Loss: tensor(0.3259)\n",
      "35952 Training Loss: tensor(0.3244)\n",
      "35953 Training Loss: tensor(0.3250)\n",
      "35954 Training Loss: tensor(0.3246)\n",
      "35955 Training Loss: tensor(0.3250)\n",
      "35956 Training Loss: tensor(0.3247)\n",
      "35957 Training Loss: tensor(0.3250)\n",
      "35958 Training Loss: tensor(0.3250)\n",
      "35959 Training Loss: tensor(0.3254)\n",
      "35960 Training Loss: tensor(0.3250)\n",
      "35961 Training Loss: tensor(0.3253)\n",
      "35962 Training Loss: tensor(0.3250)\n",
      "35963 Training Loss: tensor(0.3248)\n",
      "35964 Training Loss: tensor(0.3251)\n",
      "35965 Training Loss: tensor(0.3256)\n",
      "35966 Training Loss: tensor(0.3256)\n",
      "35967 Training Loss: tensor(0.3257)\n",
      "35968 Training Loss: tensor(0.3264)\n",
      "35969 Training Loss: tensor(0.3253)\n",
      "35970 Training Loss: tensor(0.3251)\n",
      "35971 Training Loss: tensor(0.3255)\n",
      "35972 Training Loss: tensor(0.3252)\n",
      "35973 Training Loss: tensor(0.3257)\n",
      "35974 Training Loss: tensor(0.3252)\n",
      "35975 Training Loss: tensor(0.3263)\n",
      "35976 Training Loss: tensor(0.3246)\n",
      "35977 Training Loss: tensor(0.3252)\n",
      "35978 Training Loss: tensor(0.3256)\n",
      "35979 Training Loss: tensor(0.3252)\n",
      "35980 Training Loss: tensor(0.3250)\n",
      "35981 Training Loss: tensor(0.3259)\n",
      "35982 Training Loss: tensor(0.3249)\n",
      "35983 Training Loss: tensor(0.3248)\n",
      "35984 Training Loss: tensor(0.3256)\n",
      "35985 Training Loss: tensor(0.3253)\n",
      "35986 Training Loss: tensor(0.3259)\n",
      "35987 Training Loss: tensor(0.3256)\n",
      "35988 Training Loss: tensor(0.3248)\n",
      "35989 Training Loss: tensor(0.3255)\n",
      "35990 Training Loss: tensor(0.3253)\n",
      "35991 Training Loss: tensor(0.3254)\n",
      "35992 Training Loss: tensor(0.3249)\n",
      "35993 Training Loss: tensor(0.3248)\n",
      "35994 Training Loss: tensor(0.3252)\n",
      "35995 Training Loss: tensor(0.3253)\n",
      "35996 Training Loss: tensor(0.3249)\n",
      "35997 Training Loss: tensor(0.3253)\n",
      "35998 Training Loss: tensor(0.3257)\n",
      "35999 Training Loss: tensor(0.3254)\n",
      "36000 Training Loss: tensor(0.3255)\n",
      "36001 Training Loss: tensor(0.3251)\n",
      "36002 Training Loss: tensor(0.3254)\n",
      "36003 Training Loss: tensor(0.3255)\n",
      "36004 Training Loss: tensor(0.3250)\n",
      "36005 Training Loss: tensor(0.3249)\n",
      "36006 Training Loss: tensor(0.3250)\n",
      "36007 Training Loss: tensor(0.3263)\n",
      "36008 Training Loss: tensor(0.3261)\n",
      "36009 Training Loss: tensor(0.3250)\n",
      "36010 Training Loss: tensor(0.3247)\n",
      "36011 Training Loss: tensor(0.3252)\n",
      "36012 Training Loss: tensor(0.3248)\n",
      "36013 Training Loss: tensor(0.3257)\n",
      "36014 Training Loss: tensor(0.3254)\n",
      "36015 Training Loss: tensor(0.3252)\n",
      "36016 Training Loss: tensor(0.3257)\n",
      "36017 Training Loss: tensor(0.3263)\n",
      "36018 Training Loss: tensor(0.3249)\n",
      "36019 Training Loss: tensor(0.3249)\n",
      "36020 Training Loss: tensor(0.3256)\n",
      "36021 Training Loss: tensor(0.3254)\n",
      "36022 Training Loss: tensor(0.3256)\n",
      "36023 Training Loss: tensor(0.3249)\n",
      "36024 Training Loss: tensor(0.3252)\n",
      "36025 Training Loss: tensor(0.3251)\n",
      "36026 Training Loss: tensor(0.3250)\n",
      "36027 Training Loss: tensor(0.3262)\n",
      "36028 Training Loss: tensor(0.3251)\n",
      "36029 Training Loss: tensor(0.3260)\n",
      "36030 Training Loss: tensor(0.3252)\n",
      "36031 Training Loss: tensor(0.3255)\n",
      "36032 Training Loss: tensor(0.3254)\n",
      "36033 Training Loss: tensor(0.3261)\n",
      "36034 Training Loss: tensor(0.3250)\n",
      "36035 Training Loss: tensor(0.3252)\n",
      "36036 Training Loss: tensor(0.3260)\n",
      "36037 Training Loss: tensor(0.3250)\n",
      "36038 Training Loss: tensor(0.3252)\n",
      "36039 Training Loss: tensor(0.3249)\n",
      "36040 Training Loss: tensor(0.3249)\n",
      "36041 Training Loss: tensor(0.3256)\n",
      "36042 Training Loss: tensor(0.3254)\n",
      "36043 Training Loss: tensor(0.3255)\n",
      "36044 Training Loss: tensor(0.3251)\n",
      "36045 Training Loss: tensor(0.3248)\n",
      "36046 Training Loss: tensor(0.3249)\n",
      "36047 Training Loss: tensor(0.3254)\n",
      "36048 Training Loss: tensor(0.3259)\n",
      "36049 Training Loss: tensor(0.3249)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36050 Training Loss: tensor(0.3250)\n",
      "36051 Training Loss: tensor(0.3247)\n",
      "36052 Training Loss: tensor(0.3253)\n",
      "36053 Training Loss: tensor(0.3253)\n",
      "36054 Training Loss: tensor(0.3250)\n",
      "36055 Training Loss: tensor(0.3248)\n",
      "36056 Training Loss: tensor(0.3252)\n",
      "36057 Training Loss: tensor(0.3256)\n",
      "36058 Training Loss: tensor(0.3253)\n",
      "36059 Training Loss: tensor(0.3252)\n",
      "36060 Training Loss: tensor(0.3253)\n",
      "36061 Training Loss: tensor(0.3249)\n",
      "36062 Training Loss: tensor(0.3252)\n",
      "36063 Training Loss: tensor(0.3254)\n",
      "36064 Training Loss: tensor(0.3251)\n",
      "36065 Training Loss: tensor(0.3248)\n",
      "36066 Training Loss: tensor(0.3249)\n",
      "36067 Training Loss: tensor(0.3248)\n",
      "36068 Training Loss: tensor(0.3250)\n",
      "36069 Training Loss: tensor(0.3248)\n",
      "36070 Training Loss: tensor(0.3247)\n",
      "36071 Training Loss: tensor(0.3250)\n",
      "36072 Training Loss: tensor(0.3248)\n",
      "36073 Training Loss: tensor(0.3254)\n",
      "36074 Training Loss: tensor(0.3255)\n",
      "36075 Training Loss: tensor(0.3248)\n",
      "36076 Training Loss: tensor(0.3249)\n",
      "36077 Training Loss: tensor(0.3244)\n",
      "36078 Training Loss: tensor(0.3245)\n",
      "36079 Training Loss: tensor(0.3248)\n",
      "36080 Training Loss: tensor(0.3254)\n",
      "36081 Training Loss: tensor(0.3253)\n",
      "36082 Training Loss: tensor(0.3249)\n",
      "36083 Training Loss: tensor(0.3252)\n",
      "36084 Training Loss: tensor(0.3253)\n",
      "36085 Training Loss: tensor(0.3255)\n",
      "36086 Training Loss: tensor(0.3248)\n",
      "36087 Training Loss: tensor(0.3245)\n",
      "36088 Training Loss: tensor(0.3254)\n",
      "36089 Training Loss: tensor(0.3246)\n",
      "36090 Training Loss: tensor(0.3258)\n",
      "36091 Training Loss: tensor(0.3262)\n",
      "36092 Training Loss: tensor(0.3248)\n",
      "36093 Training Loss: tensor(0.3252)\n",
      "36094 Training Loss: tensor(0.3250)\n",
      "36095 Training Loss: tensor(0.3253)\n",
      "36096 Training Loss: tensor(0.3251)\n",
      "36097 Training Loss: tensor(0.3254)\n",
      "36098 Training Loss: tensor(0.3249)\n",
      "36099 Training Loss: tensor(0.3247)\n",
      "36100 Training Loss: tensor(0.3260)\n",
      "36101 Training Loss: tensor(0.3249)\n",
      "36102 Training Loss: tensor(0.3251)\n",
      "36103 Training Loss: tensor(0.3265)\n",
      "36104 Training Loss: tensor(0.3254)\n",
      "36105 Training Loss: tensor(0.3259)\n",
      "36106 Training Loss: tensor(0.3254)\n",
      "36107 Training Loss: tensor(0.3255)\n",
      "36108 Training Loss: tensor(0.3250)\n",
      "36109 Training Loss: tensor(0.3251)\n",
      "36110 Training Loss: tensor(0.3261)\n",
      "36111 Training Loss: tensor(0.3248)\n",
      "36112 Training Loss: tensor(0.3250)\n",
      "36113 Training Loss: tensor(0.3251)\n",
      "36114 Training Loss: tensor(0.3253)\n",
      "36115 Training Loss: tensor(0.3246)\n",
      "36116 Training Loss: tensor(0.3256)\n",
      "36117 Training Loss: tensor(0.3257)\n",
      "36118 Training Loss: tensor(0.3253)\n",
      "36119 Training Loss: tensor(0.3249)\n",
      "36120 Training Loss: tensor(0.3252)\n",
      "36121 Training Loss: tensor(0.3248)\n",
      "36122 Training Loss: tensor(0.3252)\n",
      "36123 Training Loss: tensor(0.3249)\n",
      "36124 Training Loss: tensor(0.3251)\n",
      "36125 Training Loss: tensor(0.3253)\n",
      "36126 Training Loss: tensor(0.3255)\n",
      "36127 Training Loss: tensor(0.3247)\n",
      "36128 Training Loss: tensor(0.3250)\n",
      "36129 Training Loss: tensor(0.3249)\n",
      "36130 Training Loss: tensor(0.3252)\n",
      "36131 Training Loss: tensor(0.3263)\n",
      "36132 Training Loss: tensor(0.3251)\n",
      "36133 Training Loss: tensor(0.3254)\n",
      "36134 Training Loss: tensor(0.3243)\n",
      "36135 Training Loss: tensor(0.3249)\n",
      "36136 Training Loss: tensor(0.3249)\n",
      "36137 Training Loss: tensor(0.3261)\n",
      "36138 Training Loss: tensor(0.3247)\n",
      "36139 Training Loss: tensor(0.3249)\n",
      "36140 Training Loss: tensor(0.3254)\n",
      "36141 Training Loss: tensor(0.3251)\n",
      "36142 Training Loss: tensor(0.3247)\n",
      "36143 Training Loss: tensor(0.3248)\n",
      "36144 Training Loss: tensor(0.3251)\n",
      "36145 Training Loss: tensor(0.3250)\n",
      "36146 Training Loss: tensor(0.3246)\n",
      "36147 Training Loss: tensor(0.3250)\n",
      "36148 Training Loss: tensor(0.3250)\n",
      "36149 Training Loss: tensor(0.3252)\n",
      "36150 Training Loss: tensor(0.3243)\n",
      "36151 Training Loss: tensor(0.3248)\n",
      "36152 Training Loss: tensor(0.3250)\n",
      "36153 Training Loss: tensor(0.3254)\n",
      "36154 Training Loss: tensor(0.3248)\n",
      "36155 Training Loss: tensor(0.3252)\n",
      "36156 Training Loss: tensor(0.3245)\n",
      "36157 Training Loss: tensor(0.3246)\n",
      "36158 Training Loss: tensor(0.3247)\n",
      "36159 Training Loss: tensor(0.3247)\n",
      "36160 Training Loss: tensor(0.3248)\n",
      "36161 Training Loss: tensor(0.3252)\n",
      "36162 Training Loss: tensor(0.3245)\n",
      "36163 Training Loss: tensor(0.3251)\n",
      "36164 Training Loss: tensor(0.3251)\n",
      "36165 Training Loss: tensor(0.3243)\n",
      "36166 Training Loss: tensor(0.3247)\n",
      "36167 Training Loss: tensor(0.3247)\n",
      "36168 Training Loss: tensor(0.3245)\n",
      "36169 Training Loss: tensor(0.3250)\n",
      "36170 Training Loss: tensor(0.3245)\n",
      "36171 Training Loss: tensor(0.3252)\n",
      "36172 Training Loss: tensor(0.3261)\n",
      "36173 Training Loss: tensor(0.3246)\n",
      "36174 Training Loss: tensor(0.3257)\n",
      "36175 Training Loss: tensor(0.3257)\n",
      "36176 Training Loss: tensor(0.3247)\n",
      "36177 Training Loss: tensor(0.3249)\n",
      "36178 Training Loss: tensor(0.3249)\n",
      "36179 Training Loss: tensor(0.3264)\n",
      "36180 Training Loss: tensor(0.3247)\n",
      "36181 Training Loss: tensor(0.3254)\n",
      "36182 Training Loss: tensor(0.3247)\n",
      "36183 Training Loss: tensor(0.3257)\n",
      "36184 Training Loss: tensor(0.3253)\n",
      "36185 Training Loss: tensor(0.3251)\n",
      "36186 Training Loss: tensor(0.3253)\n",
      "36187 Training Loss: tensor(0.3249)\n",
      "36188 Training Loss: tensor(0.3258)\n",
      "36189 Training Loss: tensor(0.3254)\n",
      "36190 Training Loss: tensor(0.3247)\n",
      "36191 Training Loss: tensor(0.3246)\n",
      "36192 Training Loss: tensor(0.3245)\n",
      "36193 Training Loss: tensor(0.3254)\n",
      "36194 Training Loss: tensor(0.3257)\n",
      "36195 Training Loss: tensor(0.3244)\n",
      "36196 Training Loss: tensor(0.3254)\n",
      "36197 Training Loss: tensor(0.3244)\n",
      "36198 Training Loss: tensor(0.3252)\n",
      "36199 Training Loss: tensor(0.3246)\n",
      "36200 Training Loss: tensor(0.3248)\n",
      "36201 Training Loss: tensor(0.3247)\n",
      "36202 Training Loss: tensor(0.3249)\n",
      "36203 Training Loss: tensor(0.3247)\n",
      "36204 Training Loss: tensor(0.3249)\n",
      "36205 Training Loss: tensor(0.3253)\n",
      "36206 Training Loss: tensor(0.3246)\n",
      "36207 Training Loss: tensor(0.3247)\n",
      "36208 Training Loss: tensor(0.3243)\n",
      "36209 Training Loss: tensor(0.3252)\n",
      "36210 Training Loss: tensor(0.3256)\n",
      "36211 Training Loss: tensor(0.3246)\n",
      "36212 Training Loss: tensor(0.3248)\n",
      "36213 Training Loss: tensor(0.3245)\n",
      "36214 Training Loss: tensor(0.3244)\n",
      "36215 Training Loss: tensor(0.3255)\n",
      "36216 Training Loss: tensor(0.3247)\n",
      "36217 Training Loss: tensor(0.3247)\n",
      "36218 Training Loss: tensor(0.3254)\n",
      "36219 Training Loss: tensor(0.3255)\n",
      "36220 Training Loss: tensor(0.3246)\n",
      "36221 Training Loss: tensor(0.3249)\n",
      "36222 Training Loss: tensor(0.3250)\n",
      "36223 Training Loss: tensor(0.3254)\n",
      "36224 Training Loss: tensor(0.3250)\n",
      "36225 Training Loss: tensor(0.3255)\n",
      "36226 Training Loss: tensor(0.3254)\n",
      "36227 Training Loss: tensor(0.3248)\n",
      "36228 Training Loss: tensor(0.3246)\n",
      "36229 Training Loss: tensor(0.3251)\n",
      "36230 Training Loss: tensor(0.3245)\n",
      "36231 Training Loss: tensor(0.3251)\n",
      "36232 Training Loss: tensor(0.3246)\n",
      "36233 Training Loss: tensor(0.3253)\n",
      "36234 Training Loss: tensor(0.3247)\n",
      "36235 Training Loss: tensor(0.3243)\n",
      "36236 Training Loss: tensor(0.3251)\n",
      "36237 Training Loss: tensor(0.3247)\n",
      "36238 Training Loss: tensor(0.3245)\n",
      "36239 Training Loss: tensor(0.3248)\n",
      "36240 Training Loss: tensor(0.3256)\n",
      "36241 Training Loss: tensor(0.3253)\n",
      "36242 Training Loss: tensor(0.3252)\n",
      "36243 Training Loss: tensor(0.3246)\n",
      "36244 Training Loss: tensor(0.3253)\n",
      "36245 Training Loss: tensor(0.3248)\n",
      "36246 Training Loss: tensor(0.3249)\n",
      "36247 Training Loss: tensor(0.3249)\n",
      "36248 Training Loss: tensor(0.3245)\n",
      "36249 Training Loss: tensor(0.3248)\n",
      "36250 Training Loss: tensor(0.3248)\n",
      "36251 Training Loss: tensor(0.3248)\n",
      "36252 Training Loss: tensor(0.3251)\n",
      "36253 Training Loss: tensor(0.3254)\n",
      "36254 Training Loss: tensor(0.3251)\n",
      "36255 Training Loss: tensor(0.3247)\n",
      "36256 Training Loss: tensor(0.3243)\n",
      "36257 Training Loss: tensor(0.3251)\n",
      "36258 Training Loss: tensor(0.3256)\n",
      "36259 Training Loss: tensor(0.3249)\n",
      "36260 Training Loss: tensor(0.3252)\n",
      "36261 Training Loss: tensor(0.3247)\n",
      "36262 Training Loss: tensor(0.3254)\n",
      "36263 Training Loss: tensor(0.3253)\n",
      "36264 Training Loss: tensor(0.3257)\n",
      "36265 Training Loss: tensor(0.3252)\n",
      "36266 Training Loss: tensor(0.3251)\n",
      "36267 Training Loss: tensor(0.3249)\n",
      "36268 Training Loss: tensor(0.3261)\n",
      "36269 Training Loss: tensor(0.3251)\n",
      "36270 Training Loss: tensor(0.3250)\n",
      "36271 Training Loss: tensor(0.3245)\n",
      "36272 Training Loss: tensor(0.3245)\n",
      "36273 Training Loss: tensor(0.3248)\n",
      "36274 Training Loss: tensor(0.3254)\n",
      "36275 Training Loss: tensor(0.3250)\n",
      "36276 Training Loss: tensor(0.3250)\n",
      "36277 Training Loss: tensor(0.3250)\n",
      "36278 Training Loss: tensor(0.3248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36279 Training Loss: tensor(0.3249)\n",
      "36280 Training Loss: tensor(0.3249)\n",
      "36281 Training Loss: tensor(0.3254)\n",
      "36282 Training Loss: tensor(0.3250)\n",
      "36283 Training Loss: tensor(0.3253)\n",
      "36284 Training Loss: tensor(0.3256)\n",
      "36285 Training Loss: tensor(0.3247)\n",
      "36286 Training Loss: tensor(0.3251)\n",
      "36287 Training Loss: tensor(0.3249)\n",
      "36288 Training Loss: tensor(0.3249)\n",
      "36289 Training Loss: tensor(0.3249)\n",
      "36290 Training Loss: tensor(0.3247)\n",
      "36291 Training Loss: tensor(0.3250)\n",
      "36292 Training Loss: tensor(0.3245)\n",
      "36293 Training Loss: tensor(0.3254)\n",
      "36294 Training Loss: tensor(0.3245)\n",
      "36295 Training Loss: tensor(0.3247)\n",
      "36296 Training Loss: tensor(0.3253)\n",
      "36297 Training Loss: tensor(0.3251)\n",
      "36298 Training Loss: tensor(0.3249)\n",
      "36299 Training Loss: tensor(0.3262)\n",
      "36300 Training Loss: tensor(0.3243)\n",
      "36301 Training Loss: tensor(0.3248)\n",
      "36302 Training Loss: tensor(0.3253)\n",
      "36303 Training Loss: tensor(0.3248)\n",
      "36304 Training Loss: tensor(0.3257)\n",
      "36305 Training Loss: tensor(0.3250)\n",
      "36306 Training Loss: tensor(0.3259)\n",
      "36307 Training Loss: tensor(0.3254)\n",
      "36308 Training Loss: tensor(0.3245)\n",
      "36309 Training Loss: tensor(0.3248)\n",
      "36310 Training Loss: tensor(0.3245)\n",
      "36311 Training Loss: tensor(0.3246)\n",
      "36312 Training Loss: tensor(0.3244)\n",
      "36313 Training Loss: tensor(0.3258)\n",
      "36314 Training Loss: tensor(0.3251)\n",
      "36315 Training Loss: tensor(0.3256)\n",
      "36316 Training Loss: tensor(0.3255)\n",
      "36317 Training Loss: tensor(0.3249)\n",
      "36318 Training Loss: tensor(0.3249)\n",
      "36319 Training Loss: tensor(0.3248)\n",
      "36320 Training Loss: tensor(0.3254)\n",
      "36321 Training Loss: tensor(0.3254)\n",
      "36322 Training Loss: tensor(0.3249)\n",
      "36323 Training Loss: tensor(0.3250)\n",
      "36324 Training Loss: tensor(0.3250)\n",
      "36325 Training Loss: tensor(0.3257)\n",
      "36326 Training Loss: tensor(0.3259)\n",
      "36327 Training Loss: tensor(0.3250)\n",
      "36328 Training Loss: tensor(0.3247)\n",
      "36329 Training Loss: tensor(0.3248)\n",
      "36330 Training Loss: tensor(0.3254)\n",
      "36331 Training Loss: tensor(0.3245)\n",
      "36332 Training Loss: tensor(0.3250)\n",
      "36333 Training Loss: tensor(0.3243)\n",
      "36334 Training Loss: tensor(0.3247)\n",
      "36335 Training Loss: tensor(0.3249)\n",
      "36336 Training Loss: tensor(0.3250)\n",
      "36337 Training Loss: tensor(0.3252)\n",
      "36338 Training Loss: tensor(0.3257)\n",
      "36339 Training Loss: tensor(0.3247)\n",
      "36340 Training Loss: tensor(0.3249)\n",
      "36341 Training Loss: tensor(0.3245)\n",
      "36342 Training Loss: tensor(0.3249)\n",
      "36343 Training Loss: tensor(0.3253)\n",
      "36344 Training Loss: tensor(0.3248)\n",
      "36345 Training Loss: tensor(0.3261)\n",
      "36346 Training Loss: tensor(0.3251)\n",
      "36347 Training Loss: tensor(0.3246)\n",
      "36348 Training Loss: tensor(0.3256)\n",
      "36349 Training Loss: tensor(0.3255)\n",
      "36350 Training Loss: tensor(0.3251)\n",
      "36351 Training Loss: tensor(0.3248)\n",
      "36352 Training Loss: tensor(0.3256)\n",
      "36353 Training Loss: tensor(0.3261)\n",
      "36354 Training Loss: tensor(0.3244)\n",
      "36355 Training Loss: tensor(0.3243)\n",
      "36356 Training Loss: tensor(0.3252)\n",
      "36357 Training Loss: tensor(0.3260)\n",
      "36358 Training Loss: tensor(0.3245)\n",
      "36359 Training Loss: tensor(0.3256)\n",
      "36360 Training Loss: tensor(0.3259)\n",
      "36361 Training Loss: tensor(0.3251)\n",
      "36362 Training Loss: tensor(0.3254)\n",
      "36363 Training Loss: tensor(0.3260)\n",
      "36364 Training Loss: tensor(0.3259)\n",
      "36365 Training Loss: tensor(0.3254)\n",
      "36366 Training Loss: tensor(0.3254)\n",
      "36367 Training Loss: tensor(0.3254)\n",
      "36368 Training Loss: tensor(0.3254)\n",
      "36369 Training Loss: tensor(0.3251)\n",
      "36370 Training Loss: tensor(0.3248)\n",
      "36371 Training Loss: tensor(0.3249)\n",
      "36372 Training Loss: tensor(0.3250)\n",
      "36373 Training Loss: tensor(0.3247)\n",
      "36374 Training Loss: tensor(0.3252)\n",
      "36375 Training Loss: tensor(0.3252)\n",
      "36376 Training Loss: tensor(0.3247)\n",
      "36377 Training Loss: tensor(0.3251)\n",
      "36378 Training Loss: tensor(0.3251)\n",
      "36379 Training Loss: tensor(0.3253)\n",
      "36380 Training Loss: tensor(0.3254)\n",
      "36381 Training Loss: tensor(0.3252)\n",
      "36382 Training Loss: tensor(0.3246)\n",
      "36383 Training Loss: tensor(0.3246)\n",
      "36384 Training Loss: tensor(0.3246)\n",
      "36385 Training Loss: tensor(0.3251)\n",
      "36386 Training Loss: tensor(0.3249)\n",
      "36387 Training Loss: tensor(0.3246)\n",
      "36388 Training Loss: tensor(0.3251)\n",
      "36389 Training Loss: tensor(0.3250)\n",
      "36390 Training Loss: tensor(0.3247)\n",
      "36391 Training Loss: tensor(0.3247)\n",
      "36392 Training Loss: tensor(0.3249)\n",
      "36393 Training Loss: tensor(0.3257)\n",
      "36394 Training Loss: tensor(0.3249)\n",
      "36395 Training Loss: tensor(0.3250)\n",
      "36396 Training Loss: tensor(0.3252)\n",
      "36397 Training Loss: tensor(0.3244)\n",
      "36398 Training Loss: tensor(0.3251)\n",
      "36399 Training Loss: tensor(0.3247)\n",
      "36400 Training Loss: tensor(0.3242)\n",
      "36401 Training Loss: tensor(0.3262)\n",
      "36402 Training Loss: tensor(0.3247)\n",
      "36403 Training Loss: tensor(0.3269)\n",
      "36404 Training Loss: tensor(0.3266)\n",
      "36405 Training Loss: tensor(0.3256)\n",
      "36406 Training Loss: tensor(0.3249)\n",
      "36407 Training Loss: tensor(0.3250)\n",
      "36408 Training Loss: tensor(0.3250)\n",
      "36409 Training Loss: tensor(0.3248)\n",
      "36410 Training Loss: tensor(0.3248)\n",
      "36411 Training Loss: tensor(0.3251)\n",
      "36412 Training Loss: tensor(0.3250)\n",
      "36413 Training Loss: tensor(0.3249)\n",
      "36414 Training Loss: tensor(0.3251)\n",
      "36415 Training Loss: tensor(0.3253)\n",
      "36416 Training Loss: tensor(0.3247)\n",
      "36417 Training Loss: tensor(0.3244)\n",
      "36418 Training Loss: tensor(0.3257)\n",
      "36419 Training Loss: tensor(0.3248)\n",
      "36420 Training Loss: tensor(0.3258)\n",
      "36421 Training Loss: tensor(0.3250)\n",
      "36422 Training Loss: tensor(0.3255)\n",
      "36423 Training Loss: tensor(0.3254)\n",
      "36424 Training Loss: tensor(0.3247)\n",
      "36425 Training Loss: tensor(0.3258)\n",
      "36426 Training Loss: tensor(0.3248)\n",
      "36427 Training Loss: tensor(0.3251)\n",
      "36428 Training Loss: tensor(0.3244)\n",
      "36429 Training Loss: tensor(0.3252)\n",
      "36430 Training Loss: tensor(0.3250)\n",
      "36431 Training Loss: tensor(0.3249)\n",
      "36432 Training Loss: tensor(0.3254)\n",
      "36433 Training Loss: tensor(0.3250)\n",
      "36434 Training Loss: tensor(0.3249)\n",
      "36435 Training Loss: tensor(0.3260)\n",
      "36436 Training Loss: tensor(0.3248)\n",
      "36437 Training Loss: tensor(0.3249)\n",
      "36438 Training Loss: tensor(0.3260)\n",
      "36439 Training Loss: tensor(0.3252)\n",
      "36440 Training Loss: tensor(0.3247)\n",
      "36441 Training Loss: tensor(0.3248)\n",
      "36442 Training Loss: tensor(0.3250)\n",
      "36443 Training Loss: tensor(0.3257)\n",
      "36444 Training Loss: tensor(0.3246)\n",
      "36445 Training Loss: tensor(0.3254)\n",
      "36446 Training Loss: tensor(0.3242)\n",
      "36447 Training Loss: tensor(0.3251)\n",
      "36448 Training Loss: tensor(0.3245)\n",
      "36449 Training Loss: tensor(0.3245)\n",
      "36450 Training Loss: tensor(0.3252)\n",
      "36451 Training Loss: tensor(0.3250)\n",
      "36452 Training Loss: tensor(0.3249)\n",
      "36453 Training Loss: tensor(0.3258)\n",
      "36454 Training Loss: tensor(0.3257)\n",
      "36455 Training Loss: tensor(0.3252)\n",
      "36456 Training Loss: tensor(0.3246)\n",
      "36457 Training Loss: tensor(0.3245)\n",
      "36458 Training Loss: tensor(0.3246)\n",
      "36459 Training Loss: tensor(0.3258)\n",
      "36460 Training Loss: tensor(0.3256)\n",
      "36461 Training Loss: tensor(0.3248)\n",
      "36462 Training Loss: tensor(0.3248)\n",
      "36463 Training Loss: tensor(0.3250)\n",
      "36464 Training Loss: tensor(0.3250)\n",
      "36465 Training Loss: tensor(0.3248)\n",
      "36466 Training Loss: tensor(0.3246)\n",
      "36467 Training Loss: tensor(0.3250)\n",
      "36468 Training Loss: tensor(0.3251)\n",
      "36469 Training Loss: tensor(0.3246)\n",
      "36470 Training Loss: tensor(0.3252)\n",
      "36471 Training Loss: tensor(0.3258)\n",
      "36472 Training Loss: tensor(0.3248)\n",
      "36473 Training Loss: tensor(0.3244)\n",
      "36474 Training Loss: tensor(0.3256)\n",
      "36475 Training Loss: tensor(0.3254)\n",
      "36476 Training Loss: tensor(0.3243)\n",
      "36477 Training Loss: tensor(0.3251)\n",
      "36478 Training Loss: tensor(0.3251)\n",
      "36479 Training Loss: tensor(0.3242)\n",
      "36480 Training Loss: tensor(0.3256)\n",
      "36481 Training Loss: tensor(0.3255)\n",
      "36482 Training Loss: tensor(0.3260)\n",
      "36483 Training Loss: tensor(0.3249)\n",
      "36484 Training Loss: tensor(0.3250)\n",
      "36485 Training Loss: tensor(0.3247)\n",
      "36486 Training Loss: tensor(0.3251)\n",
      "36487 Training Loss: tensor(0.3244)\n",
      "36488 Training Loss: tensor(0.3246)\n",
      "36489 Training Loss: tensor(0.3245)\n",
      "36490 Training Loss: tensor(0.3245)\n",
      "36491 Training Loss: tensor(0.3251)\n",
      "36492 Training Loss: tensor(0.3245)\n",
      "36493 Training Loss: tensor(0.3251)\n",
      "36494 Training Loss: tensor(0.3262)\n",
      "36495 Training Loss: tensor(0.3250)\n",
      "36496 Training Loss: tensor(0.3245)\n",
      "36497 Training Loss: tensor(0.3249)\n",
      "36498 Training Loss: tensor(0.3249)\n",
      "36499 Training Loss: tensor(0.3259)\n",
      "36500 Training Loss: tensor(0.3248)\n",
      "36501 Training Loss: tensor(0.3248)\n",
      "36502 Training Loss: tensor(0.3249)\n",
      "36503 Training Loss: tensor(0.3250)\n",
      "36504 Training Loss: tensor(0.3254)\n",
      "36505 Training Loss: tensor(0.3243)\n",
      "36506 Training Loss: tensor(0.3250)\n",
      "36507 Training Loss: tensor(0.3248)\n",
      "36508 Training Loss: tensor(0.3249)\n",
      "36509 Training Loss: tensor(0.3260)\n",
      "36510 Training Loss: tensor(0.3250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36511 Training Loss: tensor(0.3248)\n",
      "36512 Training Loss: tensor(0.3243)\n",
      "36513 Training Loss: tensor(0.3254)\n",
      "36514 Training Loss: tensor(0.3246)\n",
      "36515 Training Loss: tensor(0.3257)\n",
      "36516 Training Loss: tensor(0.3260)\n",
      "36517 Training Loss: tensor(0.3253)\n",
      "36518 Training Loss: tensor(0.3253)\n",
      "36519 Training Loss: tensor(0.3250)\n",
      "36520 Training Loss: tensor(0.3247)\n",
      "36521 Training Loss: tensor(0.3249)\n",
      "36522 Training Loss: tensor(0.3254)\n",
      "36523 Training Loss: tensor(0.3253)\n",
      "36524 Training Loss: tensor(0.3246)\n",
      "36525 Training Loss: tensor(0.3246)\n",
      "36526 Training Loss: tensor(0.3250)\n",
      "36527 Training Loss: tensor(0.3249)\n",
      "36528 Training Loss: tensor(0.3250)\n",
      "36529 Training Loss: tensor(0.3250)\n",
      "36530 Training Loss: tensor(0.3249)\n",
      "36531 Training Loss: tensor(0.3246)\n",
      "36532 Training Loss: tensor(0.3257)\n",
      "36533 Training Loss: tensor(0.3252)\n",
      "36534 Training Loss: tensor(0.3253)\n",
      "36535 Training Loss: tensor(0.3260)\n",
      "36536 Training Loss: tensor(0.3250)\n",
      "36537 Training Loss: tensor(0.3259)\n",
      "36538 Training Loss: tensor(0.3254)\n",
      "36539 Training Loss: tensor(0.3258)\n",
      "36540 Training Loss: tensor(0.3246)\n",
      "36541 Training Loss: tensor(0.3245)\n",
      "36542 Training Loss: tensor(0.3248)\n",
      "36543 Training Loss: tensor(0.3249)\n",
      "36544 Training Loss: tensor(0.3271)\n",
      "36545 Training Loss: tensor(0.3252)\n",
      "36546 Training Loss: tensor(0.3249)\n",
      "36547 Training Loss: tensor(0.3247)\n",
      "36548 Training Loss: tensor(0.3251)\n",
      "36549 Training Loss: tensor(0.3249)\n",
      "36550 Training Loss: tensor(0.3245)\n",
      "36551 Training Loss: tensor(0.3246)\n",
      "36552 Training Loss: tensor(0.3255)\n",
      "36553 Training Loss: tensor(0.3255)\n",
      "36554 Training Loss: tensor(0.3254)\n",
      "36555 Training Loss: tensor(0.3248)\n",
      "36556 Training Loss: tensor(0.3252)\n",
      "36557 Training Loss: tensor(0.3244)\n",
      "36558 Training Loss: tensor(0.3244)\n",
      "36559 Training Loss: tensor(0.3258)\n",
      "36560 Training Loss: tensor(0.3246)\n",
      "36561 Training Loss: tensor(0.3248)\n",
      "36562 Training Loss: tensor(0.3251)\n",
      "36563 Training Loss: tensor(0.3255)\n",
      "36564 Training Loss: tensor(0.3248)\n",
      "36565 Training Loss: tensor(0.3244)\n",
      "36566 Training Loss: tensor(0.3266)\n",
      "36567 Training Loss: tensor(0.3258)\n",
      "36568 Training Loss: tensor(0.3244)\n",
      "36569 Training Loss: tensor(0.3252)\n",
      "36570 Training Loss: tensor(0.3248)\n",
      "36571 Training Loss: tensor(0.3248)\n",
      "36572 Training Loss: tensor(0.3248)\n",
      "36573 Training Loss: tensor(0.3245)\n",
      "36574 Training Loss: tensor(0.3248)\n",
      "36575 Training Loss: tensor(0.3249)\n",
      "36576 Training Loss: tensor(0.3248)\n",
      "36577 Training Loss: tensor(0.3253)\n",
      "36578 Training Loss: tensor(0.3257)\n",
      "36579 Training Loss: tensor(0.3243)\n",
      "36580 Training Loss: tensor(0.3252)\n",
      "36581 Training Loss: tensor(0.3250)\n",
      "36582 Training Loss: tensor(0.3265)\n",
      "36583 Training Loss: tensor(0.3249)\n",
      "36584 Training Loss: tensor(0.3263)\n",
      "36585 Training Loss: tensor(0.3244)\n",
      "36586 Training Loss: tensor(0.3257)\n",
      "36587 Training Loss: tensor(0.3250)\n",
      "36588 Training Loss: tensor(0.3247)\n",
      "36589 Training Loss: tensor(0.3250)\n",
      "36590 Training Loss: tensor(0.3256)\n",
      "36591 Training Loss: tensor(0.3259)\n",
      "36592 Training Loss: tensor(0.3251)\n",
      "36593 Training Loss: tensor(0.3256)\n",
      "36594 Training Loss: tensor(0.3255)\n",
      "36595 Training Loss: tensor(0.3253)\n",
      "36596 Training Loss: tensor(0.3255)\n",
      "36597 Training Loss: tensor(0.3256)\n",
      "36598 Training Loss: tensor(0.3249)\n",
      "36599 Training Loss: tensor(0.3247)\n",
      "36600 Training Loss: tensor(0.3255)\n",
      "36601 Training Loss: tensor(0.3247)\n",
      "36602 Training Loss: tensor(0.3251)\n",
      "36603 Training Loss: tensor(0.3246)\n",
      "36604 Training Loss: tensor(0.3249)\n",
      "36605 Training Loss: tensor(0.3254)\n",
      "36606 Training Loss: tensor(0.3248)\n",
      "36607 Training Loss: tensor(0.3249)\n",
      "36608 Training Loss: tensor(0.3247)\n",
      "36609 Training Loss: tensor(0.3255)\n",
      "36610 Training Loss: tensor(0.3252)\n",
      "36611 Training Loss: tensor(0.3244)\n",
      "36612 Training Loss: tensor(0.3243)\n",
      "36613 Training Loss: tensor(0.3250)\n",
      "36614 Training Loss: tensor(0.3247)\n",
      "36615 Training Loss: tensor(0.3245)\n",
      "36616 Training Loss: tensor(0.3246)\n",
      "36617 Training Loss: tensor(0.3249)\n",
      "36618 Training Loss: tensor(0.3247)\n",
      "36619 Training Loss: tensor(0.3249)\n",
      "36620 Training Loss: tensor(0.3253)\n",
      "36621 Training Loss: tensor(0.3256)\n",
      "36622 Training Loss: tensor(0.3249)\n",
      "36623 Training Loss: tensor(0.3253)\n",
      "36624 Training Loss: tensor(0.3246)\n",
      "36625 Training Loss: tensor(0.3263)\n",
      "36626 Training Loss: tensor(0.3251)\n",
      "36627 Training Loss: tensor(0.3251)\n",
      "36628 Training Loss: tensor(0.3249)\n",
      "36629 Training Loss: tensor(0.3252)\n",
      "36630 Training Loss: tensor(0.3249)\n",
      "36631 Training Loss: tensor(0.3245)\n",
      "36632 Training Loss: tensor(0.3247)\n",
      "36633 Training Loss: tensor(0.3253)\n",
      "36634 Training Loss: tensor(0.3255)\n",
      "36635 Training Loss: tensor(0.3253)\n",
      "36636 Training Loss: tensor(0.3246)\n",
      "36637 Training Loss: tensor(0.3260)\n",
      "36638 Training Loss: tensor(0.3248)\n",
      "36639 Training Loss: tensor(0.3256)\n",
      "36640 Training Loss: tensor(0.3246)\n",
      "36641 Training Loss: tensor(0.3254)\n",
      "36642 Training Loss: tensor(0.3256)\n",
      "36643 Training Loss: tensor(0.3252)\n",
      "36644 Training Loss: tensor(0.3248)\n",
      "36645 Training Loss: tensor(0.3252)\n",
      "36646 Training Loss: tensor(0.3246)\n",
      "36647 Training Loss: tensor(0.3249)\n",
      "36648 Training Loss: tensor(0.3244)\n",
      "36649 Training Loss: tensor(0.3249)\n",
      "36650 Training Loss: tensor(0.3248)\n",
      "36651 Training Loss: tensor(0.3250)\n",
      "36652 Training Loss: tensor(0.3257)\n",
      "36653 Training Loss: tensor(0.3246)\n",
      "36654 Training Loss: tensor(0.3247)\n",
      "36655 Training Loss: tensor(0.3250)\n",
      "36656 Training Loss: tensor(0.3249)\n",
      "36657 Training Loss: tensor(0.3246)\n",
      "36658 Training Loss: tensor(0.3250)\n",
      "36659 Training Loss: tensor(0.3269)\n",
      "36660 Training Loss: tensor(0.3243)\n",
      "36661 Training Loss: tensor(0.3247)\n",
      "36662 Training Loss: tensor(0.3247)\n",
      "36663 Training Loss: tensor(0.3259)\n",
      "36664 Training Loss: tensor(0.3255)\n",
      "36665 Training Loss: tensor(0.3252)\n",
      "36666 Training Loss: tensor(0.3248)\n",
      "36667 Training Loss: tensor(0.3250)\n",
      "36668 Training Loss: tensor(0.3248)\n",
      "36669 Training Loss: tensor(0.3255)\n",
      "36670 Training Loss: tensor(0.3250)\n",
      "36671 Training Loss: tensor(0.3247)\n",
      "36672 Training Loss: tensor(0.3253)\n",
      "36673 Training Loss: tensor(0.3249)\n",
      "36674 Training Loss: tensor(0.3257)\n",
      "36675 Training Loss: tensor(0.3254)\n",
      "36676 Training Loss: tensor(0.3249)\n",
      "36677 Training Loss: tensor(0.3252)\n",
      "36678 Training Loss: tensor(0.3248)\n",
      "36679 Training Loss: tensor(0.3244)\n",
      "36680 Training Loss: tensor(0.3248)\n",
      "36681 Training Loss: tensor(0.3244)\n",
      "36682 Training Loss: tensor(0.3247)\n",
      "36683 Training Loss: tensor(0.3248)\n",
      "36684 Training Loss: tensor(0.3247)\n",
      "36685 Training Loss: tensor(0.3251)\n",
      "36686 Training Loss: tensor(0.3256)\n",
      "36687 Training Loss: tensor(0.3248)\n",
      "36688 Training Loss: tensor(0.3256)\n",
      "36689 Training Loss: tensor(0.3248)\n",
      "36690 Training Loss: tensor(0.3248)\n",
      "36691 Training Loss: tensor(0.3262)\n",
      "36692 Training Loss: tensor(0.3249)\n",
      "36693 Training Loss: tensor(0.3253)\n",
      "36694 Training Loss: tensor(0.3252)\n",
      "36695 Training Loss: tensor(0.3247)\n",
      "36696 Training Loss: tensor(0.3247)\n",
      "36697 Training Loss: tensor(0.3248)\n",
      "36698 Training Loss: tensor(0.3261)\n",
      "36699 Training Loss: tensor(0.3257)\n",
      "36700 Training Loss: tensor(0.3246)\n",
      "36701 Training Loss: tensor(0.3250)\n",
      "36702 Training Loss: tensor(0.3262)\n",
      "36703 Training Loss: tensor(0.3253)\n",
      "36704 Training Loss: tensor(0.3246)\n",
      "36705 Training Loss: tensor(0.3247)\n",
      "36706 Training Loss: tensor(0.3247)\n",
      "36707 Training Loss: tensor(0.3265)\n",
      "36708 Training Loss: tensor(0.3260)\n",
      "36709 Training Loss: tensor(0.3246)\n",
      "36710 Training Loss: tensor(0.3247)\n",
      "36711 Training Loss: tensor(0.3244)\n",
      "36712 Training Loss: tensor(0.3247)\n",
      "36713 Training Loss: tensor(0.3246)\n",
      "36714 Training Loss: tensor(0.3256)\n",
      "36715 Training Loss: tensor(0.3249)\n",
      "36716 Training Loss: tensor(0.3254)\n",
      "36717 Training Loss: tensor(0.3259)\n",
      "36718 Training Loss: tensor(0.3247)\n",
      "36719 Training Loss: tensor(0.3249)\n",
      "36720 Training Loss: tensor(0.3251)\n",
      "36721 Training Loss: tensor(0.3247)\n",
      "36722 Training Loss: tensor(0.3253)\n",
      "36723 Training Loss: tensor(0.3250)\n",
      "36724 Training Loss: tensor(0.3255)\n",
      "36725 Training Loss: tensor(0.3244)\n",
      "36726 Training Loss: tensor(0.3257)\n",
      "36727 Training Loss: tensor(0.3256)\n",
      "36728 Training Loss: tensor(0.3249)\n",
      "36729 Training Loss: tensor(0.3250)\n",
      "36730 Training Loss: tensor(0.3258)\n",
      "36731 Training Loss: tensor(0.3260)\n",
      "36732 Training Loss: tensor(0.3251)\n",
      "36733 Training Loss: tensor(0.3255)\n",
      "36734 Training Loss: tensor(0.3251)\n",
      "36735 Training Loss: tensor(0.3246)\n",
      "36736 Training Loss: tensor(0.3259)\n",
      "36737 Training Loss: tensor(0.3249)\n",
      "36738 Training Loss: tensor(0.3253)\n",
      "36739 Training Loss: tensor(0.3251)\n",
      "36740 Training Loss: tensor(0.3258)\n",
      "36741 Training Loss: tensor(0.3250)\n",
      "36742 Training Loss: tensor(0.3254)\n",
      "36743 Training Loss: tensor(0.3256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36744 Training Loss: tensor(0.3245)\n",
      "36745 Training Loss: tensor(0.3249)\n",
      "36746 Training Loss: tensor(0.3262)\n",
      "36747 Training Loss: tensor(0.3250)\n",
      "36748 Training Loss: tensor(0.3249)\n",
      "36749 Training Loss: tensor(0.3246)\n",
      "36750 Training Loss: tensor(0.3252)\n",
      "36751 Training Loss: tensor(0.3248)\n",
      "36752 Training Loss: tensor(0.3250)\n",
      "36753 Training Loss: tensor(0.3250)\n",
      "36754 Training Loss: tensor(0.3247)\n",
      "36755 Training Loss: tensor(0.3260)\n",
      "36756 Training Loss: tensor(0.3248)\n",
      "36757 Training Loss: tensor(0.3248)\n",
      "36758 Training Loss: tensor(0.3258)\n",
      "36759 Training Loss: tensor(0.3252)\n",
      "36760 Training Loss: tensor(0.3247)\n",
      "36761 Training Loss: tensor(0.3247)\n",
      "36762 Training Loss: tensor(0.3244)\n",
      "36763 Training Loss: tensor(0.3254)\n",
      "36764 Training Loss: tensor(0.3260)\n",
      "36765 Training Loss: tensor(0.3247)\n",
      "36766 Training Loss: tensor(0.3259)\n",
      "36767 Training Loss: tensor(0.3252)\n",
      "36768 Training Loss: tensor(0.3246)\n",
      "36769 Training Loss: tensor(0.3245)\n",
      "36770 Training Loss: tensor(0.3249)\n",
      "36771 Training Loss: tensor(0.3251)\n",
      "36772 Training Loss: tensor(0.3258)\n",
      "36773 Training Loss: tensor(0.3249)\n",
      "36774 Training Loss: tensor(0.3250)\n",
      "36775 Training Loss: tensor(0.3262)\n",
      "36776 Training Loss: tensor(0.3258)\n",
      "36777 Training Loss: tensor(0.3257)\n",
      "36778 Training Loss: tensor(0.3248)\n",
      "36779 Training Loss: tensor(0.3258)\n",
      "36780 Training Loss: tensor(0.3251)\n",
      "36781 Training Loss: tensor(0.3244)\n",
      "36782 Training Loss: tensor(0.3247)\n",
      "36783 Training Loss: tensor(0.3255)\n",
      "36784 Training Loss: tensor(0.3246)\n",
      "36785 Training Loss: tensor(0.3254)\n",
      "36786 Training Loss: tensor(0.3255)\n",
      "36787 Training Loss: tensor(0.3254)\n",
      "36788 Training Loss: tensor(0.3248)\n",
      "36789 Training Loss: tensor(0.3247)\n",
      "36790 Training Loss: tensor(0.3245)\n",
      "36791 Training Loss: tensor(0.3244)\n",
      "36792 Training Loss: tensor(0.3243)\n",
      "36793 Training Loss: tensor(0.3255)\n",
      "36794 Training Loss: tensor(0.3247)\n",
      "36795 Training Loss: tensor(0.3253)\n",
      "36796 Training Loss: tensor(0.3252)\n",
      "36797 Training Loss: tensor(0.3243)\n",
      "36798 Training Loss: tensor(0.3245)\n",
      "36799 Training Loss: tensor(0.3247)\n",
      "36800 Training Loss: tensor(0.3250)\n",
      "36801 Training Loss: tensor(0.3250)\n",
      "36802 Training Loss: tensor(0.3252)\n",
      "36803 Training Loss: tensor(0.3258)\n",
      "36804 Training Loss: tensor(0.3247)\n",
      "36805 Training Loss: tensor(0.3253)\n",
      "36806 Training Loss: tensor(0.3257)\n",
      "36807 Training Loss: tensor(0.3250)\n",
      "36808 Training Loss: tensor(0.3249)\n",
      "36809 Training Loss: tensor(0.3257)\n",
      "36810 Training Loss: tensor(0.3249)\n",
      "36811 Training Loss: tensor(0.3254)\n",
      "36812 Training Loss: tensor(0.3252)\n",
      "36813 Training Loss: tensor(0.3249)\n",
      "36814 Training Loss: tensor(0.3250)\n",
      "36815 Training Loss: tensor(0.3246)\n",
      "36816 Training Loss: tensor(0.3252)\n",
      "36817 Training Loss: tensor(0.3248)\n",
      "36818 Training Loss: tensor(0.3258)\n",
      "36819 Training Loss: tensor(0.3250)\n",
      "36820 Training Loss: tensor(0.3251)\n",
      "36821 Training Loss: tensor(0.3245)\n",
      "36822 Training Loss: tensor(0.3257)\n",
      "36823 Training Loss: tensor(0.3245)\n",
      "36824 Training Loss: tensor(0.3246)\n",
      "36825 Training Loss: tensor(0.3245)\n",
      "36826 Training Loss: tensor(0.3251)\n",
      "36827 Training Loss: tensor(0.3251)\n",
      "36828 Training Loss: tensor(0.3249)\n",
      "36829 Training Loss: tensor(0.3253)\n",
      "36830 Training Loss: tensor(0.3250)\n",
      "36831 Training Loss: tensor(0.3250)\n",
      "36832 Training Loss: tensor(0.3249)\n",
      "36833 Training Loss: tensor(0.3254)\n",
      "36834 Training Loss: tensor(0.3247)\n",
      "36835 Training Loss: tensor(0.3249)\n",
      "36836 Training Loss: tensor(0.3247)\n",
      "36837 Training Loss: tensor(0.3249)\n",
      "36838 Training Loss: tensor(0.3248)\n",
      "36839 Training Loss: tensor(0.3243)\n",
      "36840 Training Loss: tensor(0.3256)\n",
      "36841 Training Loss: tensor(0.3251)\n",
      "36842 Training Loss: tensor(0.3254)\n",
      "36843 Training Loss: tensor(0.3245)\n",
      "36844 Training Loss: tensor(0.3253)\n",
      "36845 Training Loss: tensor(0.3250)\n",
      "36846 Training Loss: tensor(0.3242)\n",
      "36847 Training Loss: tensor(0.3245)\n",
      "36848 Training Loss: tensor(0.3241)\n",
      "36849 Training Loss: tensor(0.3252)\n",
      "36850 Training Loss: tensor(0.3249)\n",
      "36851 Training Loss: tensor(0.3244)\n",
      "36852 Training Loss: tensor(0.3247)\n",
      "36853 Training Loss: tensor(0.3258)\n",
      "36854 Training Loss: tensor(0.3249)\n",
      "36855 Training Loss: tensor(0.3250)\n",
      "36856 Training Loss: tensor(0.3249)\n",
      "36857 Training Loss: tensor(0.3245)\n",
      "36858 Training Loss: tensor(0.3253)\n",
      "36859 Training Loss: tensor(0.3244)\n",
      "36860 Training Loss: tensor(0.3263)\n",
      "36861 Training Loss: tensor(0.3250)\n",
      "36862 Training Loss: tensor(0.3253)\n",
      "36863 Training Loss: tensor(0.3259)\n",
      "36864 Training Loss: tensor(0.3247)\n",
      "36865 Training Loss: tensor(0.3256)\n",
      "36866 Training Loss: tensor(0.3248)\n",
      "36867 Training Loss: tensor(0.3252)\n",
      "36868 Training Loss: tensor(0.3245)\n",
      "36869 Training Loss: tensor(0.3258)\n",
      "36870 Training Loss: tensor(0.3247)\n",
      "36871 Training Loss: tensor(0.3244)\n",
      "36872 Training Loss: tensor(0.3245)\n",
      "36873 Training Loss: tensor(0.3246)\n",
      "36874 Training Loss: tensor(0.3255)\n",
      "36875 Training Loss: tensor(0.3244)\n",
      "36876 Training Loss: tensor(0.3254)\n",
      "36877 Training Loss: tensor(0.3246)\n",
      "36878 Training Loss: tensor(0.3255)\n",
      "36879 Training Loss: tensor(0.3248)\n",
      "36880 Training Loss: tensor(0.3254)\n",
      "36881 Training Loss: tensor(0.3246)\n",
      "36882 Training Loss: tensor(0.3246)\n",
      "36883 Training Loss: tensor(0.3247)\n",
      "36884 Training Loss: tensor(0.3247)\n",
      "36885 Training Loss: tensor(0.3248)\n",
      "36886 Training Loss: tensor(0.3245)\n",
      "36887 Training Loss: tensor(0.3250)\n",
      "36888 Training Loss: tensor(0.3249)\n",
      "36889 Training Loss: tensor(0.3247)\n",
      "36890 Training Loss: tensor(0.3243)\n",
      "36891 Training Loss: tensor(0.3247)\n",
      "36892 Training Loss: tensor(0.3251)\n",
      "36893 Training Loss: tensor(0.3257)\n",
      "36894 Training Loss: tensor(0.3255)\n",
      "36895 Training Loss: tensor(0.3244)\n",
      "36896 Training Loss: tensor(0.3246)\n",
      "36897 Training Loss: tensor(0.3243)\n",
      "36898 Training Loss: tensor(0.3251)\n",
      "36899 Training Loss: tensor(0.3252)\n",
      "36900 Training Loss: tensor(0.3249)\n",
      "36901 Training Loss: tensor(0.3246)\n",
      "36902 Training Loss: tensor(0.3254)\n",
      "36903 Training Loss: tensor(0.3246)\n",
      "36904 Training Loss: tensor(0.3249)\n",
      "36905 Training Loss: tensor(0.3245)\n",
      "36906 Training Loss: tensor(0.3246)\n",
      "36907 Training Loss: tensor(0.3249)\n",
      "36908 Training Loss: tensor(0.3254)\n",
      "36909 Training Loss: tensor(0.3254)\n",
      "36910 Training Loss: tensor(0.3245)\n",
      "36911 Training Loss: tensor(0.3247)\n",
      "36912 Training Loss: tensor(0.3245)\n",
      "36913 Training Loss: tensor(0.3253)\n",
      "36914 Training Loss: tensor(0.3252)\n",
      "36915 Training Loss: tensor(0.3247)\n",
      "36916 Training Loss: tensor(0.3252)\n",
      "36917 Training Loss: tensor(0.3248)\n",
      "36918 Training Loss: tensor(0.3246)\n",
      "36919 Training Loss: tensor(0.3248)\n",
      "36920 Training Loss: tensor(0.3244)\n",
      "36921 Training Loss: tensor(0.3244)\n",
      "36922 Training Loss: tensor(0.3251)\n",
      "36923 Training Loss: tensor(0.3257)\n",
      "36924 Training Loss: tensor(0.3251)\n",
      "36925 Training Loss: tensor(0.3251)\n",
      "36926 Training Loss: tensor(0.3246)\n",
      "36927 Training Loss: tensor(0.3246)\n",
      "36928 Training Loss: tensor(0.3253)\n",
      "36929 Training Loss: tensor(0.3248)\n",
      "36930 Training Loss: tensor(0.3245)\n",
      "36931 Training Loss: tensor(0.3259)\n",
      "36932 Training Loss: tensor(0.3252)\n",
      "36933 Training Loss: tensor(0.3251)\n",
      "36934 Training Loss: tensor(0.3263)\n",
      "36935 Training Loss: tensor(0.3250)\n",
      "36936 Training Loss: tensor(0.3246)\n",
      "36937 Training Loss: tensor(0.3255)\n",
      "36938 Training Loss: tensor(0.3251)\n",
      "36939 Training Loss: tensor(0.3245)\n",
      "36940 Training Loss: tensor(0.3244)\n",
      "36941 Training Loss: tensor(0.3244)\n",
      "36942 Training Loss: tensor(0.3255)\n",
      "36943 Training Loss: tensor(0.3250)\n",
      "36944 Training Loss: tensor(0.3253)\n",
      "36945 Training Loss: tensor(0.3253)\n",
      "36946 Training Loss: tensor(0.3251)\n",
      "36947 Training Loss: tensor(0.3250)\n",
      "36948 Training Loss: tensor(0.3253)\n",
      "36949 Training Loss: tensor(0.3249)\n",
      "36950 Training Loss: tensor(0.3252)\n",
      "36951 Training Loss: tensor(0.3245)\n",
      "36952 Training Loss: tensor(0.3249)\n",
      "36953 Training Loss: tensor(0.3255)\n",
      "36954 Training Loss: tensor(0.3256)\n",
      "36955 Training Loss: tensor(0.3255)\n",
      "36956 Training Loss: tensor(0.3245)\n",
      "36957 Training Loss: tensor(0.3245)\n",
      "36958 Training Loss: tensor(0.3247)\n",
      "36959 Training Loss: tensor(0.3258)\n",
      "36960 Training Loss: tensor(0.3253)\n",
      "36961 Training Loss: tensor(0.3250)\n",
      "36962 Training Loss: tensor(0.3248)\n",
      "36963 Training Loss: tensor(0.3246)\n",
      "36964 Training Loss: tensor(0.3246)\n",
      "36965 Training Loss: tensor(0.3250)\n",
      "36966 Training Loss: tensor(0.3247)\n",
      "36967 Training Loss: tensor(0.3252)\n",
      "36968 Training Loss: tensor(0.3254)\n",
      "36969 Training Loss: tensor(0.3246)\n",
      "36970 Training Loss: tensor(0.3262)\n",
      "36971 Training Loss: tensor(0.3257)\n",
      "36972 Training Loss: tensor(0.3249)\n",
      "36973 Training Loss: tensor(0.3249)\n",
      "36974 Training Loss: tensor(0.3253)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36975 Training Loss: tensor(0.3250)\n",
      "36976 Training Loss: tensor(0.3246)\n",
      "36977 Training Loss: tensor(0.3246)\n",
      "36978 Training Loss: tensor(0.3245)\n",
      "36979 Training Loss: tensor(0.3248)\n",
      "36980 Training Loss: tensor(0.3255)\n",
      "36981 Training Loss: tensor(0.3249)\n",
      "36982 Training Loss: tensor(0.3252)\n",
      "36983 Training Loss: tensor(0.3244)\n",
      "36984 Training Loss: tensor(0.3249)\n",
      "36985 Training Loss: tensor(0.3246)\n",
      "36986 Training Loss: tensor(0.3250)\n",
      "36987 Training Loss: tensor(0.3259)\n",
      "36988 Training Loss: tensor(0.3251)\n",
      "36989 Training Loss: tensor(0.3247)\n",
      "36990 Training Loss: tensor(0.3243)\n",
      "36991 Training Loss: tensor(0.3245)\n",
      "36992 Training Loss: tensor(0.3247)\n",
      "36993 Training Loss: tensor(0.3244)\n",
      "36994 Training Loss: tensor(0.3244)\n",
      "36995 Training Loss: tensor(0.3242)\n",
      "36996 Training Loss: tensor(0.3258)\n",
      "36997 Training Loss: tensor(0.3249)\n",
      "36998 Training Loss: tensor(0.3246)\n",
      "36999 Training Loss: tensor(0.3262)\n",
      "37000 Training Loss: tensor(0.3244)\n",
      "37001 Training Loss: tensor(0.3246)\n",
      "37002 Training Loss: tensor(0.3264)\n",
      "37003 Training Loss: tensor(0.3252)\n",
      "37004 Training Loss: tensor(0.3247)\n",
      "37005 Training Loss: tensor(0.3253)\n",
      "37006 Training Loss: tensor(0.3255)\n",
      "37007 Training Loss: tensor(0.3247)\n",
      "37008 Training Loss: tensor(0.3261)\n",
      "37009 Training Loss: tensor(0.3255)\n",
      "37010 Training Loss: tensor(0.3253)\n",
      "37011 Training Loss: tensor(0.3248)\n",
      "37012 Training Loss: tensor(0.3253)\n",
      "37013 Training Loss: tensor(0.3245)\n",
      "37014 Training Loss: tensor(0.3253)\n",
      "37015 Training Loss: tensor(0.3260)\n",
      "37016 Training Loss: tensor(0.3253)\n",
      "37017 Training Loss: tensor(0.3245)\n",
      "37018 Training Loss: tensor(0.3249)\n",
      "37019 Training Loss: tensor(0.3244)\n",
      "37020 Training Loss: tensor(0.3245)\n",
      "37021 Training Loss: tensor(0.3246)\n",
      "37022 Training Loss: tensor(0.3256)\n",
      "37023 Training Loss: tensor(0.3250)\n",
      "37024 Training Loss: tensor(0.3248)\n",
      "37025 Training Loss: tensor(0.3249)\n",
      "37026 Training Loss: tensor(0.3243)\n",
      "37027 Training Loss: tensor(0.3246)\n",
      "37028 Training Loss: tensor(0.3259)\n",
      "37029 Training Loss: tensor(0.3247)\n",
      "37030 Training Loss: tensor(0.3253)\n",
      "37031 Training Loss: tensor(0.3242)\n",
      "37032 Training Loss: tensor(0.3251)\n",
      "37033 Training Loss: tensor(0.3249)\n",
      "37034 Training Loss: tensor(0.3248)\n",
      "37035 Training Loss: tensor(0.3245)\n",
      "37036 Training Loss: tensor(0.3259)\n",
      "37037 Training Loss: tensor(0.3251)\n",
      "37038 Training Loss: tensor(0.3260)\n",
      "37039 Training Loss: tensor(0.3247)\n",
      "37040 Training Loss: tensor(0.3247)\n",
      "37041 Training Loss: tensor(0.3241)\n",
      "37042 Training Loss: tensor(0.3247)\n",
      "37043 Training Loss: tensor(0.3249)\n",
      "37044 Training Loss: tensor(0.3244)\n",
      "37045 Training Loss: tensor(0.3245)\n",
      "37046 Training Loss: tensor(0.3243)\n",
      "37047 Training Loss: tensor(0.3247)\n",
      "37048 Training Loss: tensor(0.3250)\n",
      "37049 Training Loss: tensor(0.3255)\n",
      "37050 Training Loss: tensor(0.3253)\n",
      "37051 Training Loss: tensor(0.3252)\n",
      "37052 Training Loss: tensor(0.3241)\n",
      "37053 Training Loss: tensor(0.3257)\n",
      "37054 Training Loss: tensor(0.3251)\n",
      "37055 Training Loss: tensor(0.3260)\n",
      "37056 Training Loss: tensor(0.3251)\n",
      "37057 Training Loss: tensor(0.3245)\n",
      "37058 Training Loss: tensor(0.3249)\n",
      "37059 Training Loss: tensor(0.3251)\n",
      "37060 Training Loss: tensor(0.3250)\n",
      "37061 Training Loss: tensor(0.3248)\n",
      "37062 Training Loss: tensor(0.3246)\n",
      "37063 Training Loss: tensor(0.3250)\n",
      "37064 Training Loss: tensor(0.3263)\n",
      "37065 Training Loss: tensor(0.3244)\n",
      "37066 Training Loss: tensor(0.3252)\n",
      "37067 Training Loss: tensor(0.3245)\n",
      "37068 Training Loss: tensor(0.3249)\n",
      "37069 Training Loss: tensor(0.3246)\n",
      "37070 Training Loss: tensor(0.3255)\n",
      "37071 Training Loss: tensor(0.3256)\n",
      "37072 Training Loss: tensor(0.3252)\n",
      "37073 Training Loss: tensor(0.3245)\n",
      "37074 Training Loss: tensor(0.3252)\n",
      "37075 Training Loss: tensor(0.3247)\n",
      "37076 Training Loss: tensor(0.3252)\n",
      "37077 Training Loss: tensor(0.3247)\n",
      "37078 Training Loss: tensor(0.3241)\n",
      "37079 Training Loss: tensor(0.3244)\n",
      "37080 Training Loss: tensor(0.3248)\n",
      "37081 Training Loss: tensor(0.3255)\n",
      "37082 Training Loss: tensor(0.3247)\n",
      "37083 Training Loss: tensor(0.3251)\n",
      "37084 Training Loss: tensor(0.3246)\n",
      "37085 Training Loss: tensor(0.3248)\n",
      "37086 Training Loss: tensor(0.3248)\n",
      "37087 Training Loss: tensor(0.3247)\n",
      "37088 Training Loss: tensor(0.3246)\n",
      "37089 Training Loss: tensor(0.3249)\n",
      "37090 Training Loss: tensor(0.3259)\n",
      "37091 Training Loss: tensor(0.3251)\n",
      "37092 Training Loss: tensor(0.3244)\n",
      "37093 Training Loss: tensor(0.3251)\n",
      "37094 Training Loss: tensor(0.3257)\n",
      "37095 Training Loss: tensor(0.3264)\n",
      "37096 Training Loss: tensor(0.3252)\n",
      "37097 Training Loss: tensor(0.3246)\n",
      "37098 Training Loss: tensor(0.3246)\n",
      "37099 Training Loss: tensor(0.3250)\n",
      "37100 Training Loss: tensor(0.3243)\n",
      "37101 Training Loss: tensor(0.3241)\n",
      "37102 Training Loss: tensor(0.3249)\n",
      "37103 Training Loss: tensor(0.3253)\n",
      "37104 Training Loss: tensor(0.3254)\n",
      "37105 Training Loss: tensor(0.3256)\n",
      "37106 Training Loss: tensor(0.3254)\n",
      "37107 Training Loss: tensor(0.3247)\n",
      "37108 Training Loss: tensor(0.3252)\n",
      "37109 Training Loss: tensor(0.3258)\n",
      "37110 Training Loss: tensor(0.3246)\n",
      "37111 Training Loss: tensor(0.3242)\n",
      "37112 Training Loss: tensor(0.3255)\n",
      "37113 Training Loss: tensor(0.3248)\n",
      "37114 Training Loss: tensor(0.3254)\n",
      "37115 Training Loss: tensor(0.3251)\n",
      "37116 Training Loss: tensor(0.3253)\n",
      "37117 Training Loss: tensor(0.3257)\n",
      "37118 Training Loss: tensor(0.3257)\n",
      "37119 Training Loss: tensor(0.3264)\n",
      "37120 Training Loss: tensor(0.3257)\n",
      "37121 Training Loss: tensor(0.3250)\n",
      "37122 Training Loss: tensor(0.3256)\n",
      "37123 Training Loss: tensor(0.3251)\n",
      "37124 Training Loss: tensor(0.3245)\n",
      "37125 Training Loss: tensor(0.3266)\n",
      "37126 Training Loss: tensor(0.3249)\n",
      "37127 Training Loss: tensor(0.3246)\n",
      "37128 Training Loss: tensor(0.3256)\n",
      "37129 Training Loss: tensor(0.3250)\n",
      "37130 Training Loss: tensor(0.3242)\n",
      "37131 Training Loss: tensor(0.3249)\n",
      "37132 Training Loss: tensor(0.3257)\n",
      "37133 Training Loss: tensor(0.3256)\n",
      "37134 Training Loss: tensor(0.3246)\n",
      "37135 Training Loss: tensor(0.3243)\n",
      "37136 Training Loss: tensor(0.3246)\n",
      "37137 Training Loss: tensor(0.3244)\n",
      "37138 Training Loss: tensor(0.3249)\n",
      "37139 Training Loss: tensor(0.3250)\n",
      "37140 Training Loss: tensor(0.3248)\n",
      "37141 Training Loss: tensor(0.3249)\n",
      "37142 Training Loss: tensor(0.3244)\n",
      "37143 Training Loss: tensor(0.3256)\n",
      "37144 Training Loss: tensor(0.3249)\n",
      "37145 Training Loss: tensor(0.3248)\n",
      "37146 Training Loss: tensor(0.3244)\n",
      "37147 Training Loss: tensor(0.3252)\n",
      "37148 Training Loss: tensor(0.3253)\n",
      "37149 Training Loss: tensor(0.3242)\n",
      "37150 Training Loss: tensor(0.3249)\n",
      "37151 Training Loss: tensor(0.3248)\n",
      "37152 Training Loss: tensor(0.3248)\n",
      "37153 Training Loss: tensor(0.3246)\n",
      "37154 Training Loss: tensor(0.3255)\n",
      "37155 Training Loss: tensor(0.3248)\n",
      "37156 Training Loss: tensor(0.3246)\n",
      "37157 Training Loss: tensor(0.3255)\n",
      "37158 Training Loss: tensor(0.3245)\n",
      "37159 Training Loss: tensor(0.3246)\n",
      "37160 Training Loss: tensor(0.3247)\n",
      "37161 Training Loss: tensor(0.3246)\n",
      "37162 Training Loss: tensor(0.3247)\n",
      "37163 Training Loss: tensor(0.3247)\n",
      "37164 Training Loss: tensor(0.3251)\n",
      "37165 Training Loss: tensor(0.3244)\n",
      "37166 Training Loss: tensor(0.3245)\n",
      "37167 Training Loss: tensor(0.3252)\n",
      "37168 Training Loss: tensor(0.3243)\n",
      "37169 Training Loss: tensor(0.3257)\n",
      "37170 Training Loss: tensor(0.3244)\n",
      "37171 Training Loss: tensor(0.3251)\n",
      "37172 Training Loss: tensor(0.3252)\n",
      "37173 Training Loss: tensor(0.3241)\n",
      "37174 Training Loss: tensor(0.3243)\n",
      "37175 Training Loss: tensor(0.3258)\n",
      "37176 Training Loss: tensor(0.3254)\n",
      "37177 Training Loss: tensor(0.3248)\n",
      "37178 Training Loss: tensor(0.3249)\n",
      "37179 Training Loss: tensor(0.3245)\n",
      "37180 Training Loss: tensor(0.3248)\n",
      "37181 Training Loss: tensor(0.3248)\n",
      "37182 Training Loss: tensor(0.3246)\n",
      "37183 Training Loss: tensor(0.3249)\n",
      "37184 Training Loss: tensor(0.3247)\n",
      "37185 Training Loss: tensor(0.3243)\n",
      "37186 Training Loss: tensor(0.3253)\n",
      "37187 Training Loss: tensor(0.3249)\n",
      "37188 Training Loss: tensor(0.3256)\n",
      "37189 Training Loss: tensor(0.3248)\n",
      "37190 Training Loss: tensor(0.3248)\n",
      "37191 Training Loss: tensor(0.3245)\n",
      "37192 Training Loss: tensor(0.3245)\n",
      "37193 Training Loss: tensor(0.3248)\n",
      "37194 Training Loss: tensor(0.3250)\n",
      "37195 Training Loss: tensor(0.3250)\n",
      "37196 Training Loss: tensor(0.3254)\n",
      "37197 Training Loss: tensor(0.3247)\n",
      "37198 Training Loss: tensor(0.3248)\n",
      "37199 Training Loss: tensor(0.3244)\n",
      "37200 Training Loss: tensor(0.3252)\n",
      "37201 Training Loss: tensor(0.3251)\n",
      "37202 Training Loss: tensor(0.3247)\n",
      "37203 Training Loss: tensor(0.3245)\n",
      "37204 Training Loss: tensor(0.3262)\n",
      "37205 Training Loss: tensor(0.3256)\n",
      "37206 Training Loss: tensor(0.3253)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37207 Training Loss: tensor(0.3244)\n",
      "37208 Training Loss: tensor(0.3243)\n",
      "37209 Training Loss: tensor(0.3244)\n",
      "37210 Training Loss: tensor(0.3264)\n",
      "37211 Training Loss: tensor(0.3253)\n",
      "37212 Training Loss: tensor(0.3251)\n",
      "37213 Training Loss: tensor(0.3248)\n",
      "37214 Training Loss: tensor(0.3247)\n",
      "37215 Training Loss: tensor(0.3248)\n",
      "37216 Training Loss: tensor(0.3243)\n",
      "37217 Training Loss: tensor(0.3251)\n",
      "37218 Training Loss: tensor(0.3254)\n",
      "37219 Training Loss: tensor(0.3254)\n",
      "37220 Training Loss: tensor(0.3245)\n",
      "37221 Training Loss: tensor(0.3245)\n",
      "37222 Training Loss: tensor(0.3248)\n",
      "37223 Training Loss: tensor(0.3247)\n",
      "37224 Training Loss: tensor(0.3250)\n",
      "37225 Training Loss: tensor(0.3241)\n",
      "37226 Training Loss: tensor(0.3244)\n",
      "37227 Training Loss: tensor(0.3253)\n",
      "37228 Training Loss: tensor(0.3246)\n",
      "37229 Training Loss: tensor(0.3245)\n",
      "37230 Training Loss: tensor(0.3246)\n",
      "37231 Training Loss: tensor(0.3256)\n",
      "37232 Training Loss: tensor(0.3242)\n",
      "37233 Training Loss: tensor(0.3257)\n",
      "37234 Training Loss: tensor(0.3247)\n",
      "37235 Training Loss: tensor(0.3254)\n",
      "37236 Training Loss: tensor(0.3240)\n",
      "37237 Training Loss: tensor(0.3245)\n",
      "37238 Training Loss: tensor(0.3251)\n",
      "37239 Training Loss: tensor(0.3255)\n",
      "37240 Training Loss: tensor(0.3246)\n",
      "37241 Training Loss: tensor(0.3241)\n",
      "37242 Training Loss: tensor(0.3256)\n",
      "37243 Training Loss: tensor(0.3248)\n",
      "37244 Training Loss: tensor(0.3252)\n",
      "37245 Training Loss: tensor(0.3243)\n",
      "37246 Training Loss: tensor(0.3247)\n",
      "37247 Training Loss: tensor(0.3257)\n",
      "37248 Training Loss: tensor(0.3241)\n",
      "37249 Training Loss: tensor(0.3243)\n",
      "37250 Training Loss: tensor(0.3261)\n",
      "37251 Training Loss: tensor(0.3252)\n",
      "37252 Training Loss: tensor(0.3246)\n",
      "37253 Training Loss: tensor(0.3252)\n",
      "37254 Training Loss: tensor(0.3252)\n",
      "37255 Training Loss: tensor(0.3246)\n",
      "37256 Training Loss: tensor(0.3249)\n",
      "37257 Training Loss: tensor(0.3247)\n",
      "37258 Training Loss: tensor(0.3250)\n",
      "37259 Training Loss: tensor(0.3244)\n",
      "37260 Training Loss: tensor(0.3247)\n",
      "37261 Training Loss: tensor(0.3256)\n",
      "37262 Training Loss: tensor(0.3243)\n",
      "37263 Training Loss: tensor(0.3247)\n",
      "37264 Training Loss: tensor(0.3253)\n",
      "37265 Training Loss: tensor(0.3247)\n",
      "37266 Training Loss: tensor(0.3245)\n",
      "37267 Training Loss: tensor(0.3246)\n",
      "37268 Training Loss: tensor(0.3255)\n",
      "37269 Training Loss: tensor(0.3244)\n",
      "37270 Training Loss: tensor(0.3251)\n",
      "37271 Training Loss: tensor(0.3247)\n",
      "37272 Training Loss: tensor(0.3241)\n",
      "37273 Training Loss: tensor(0.3255)\n",
      "37274 Training Loss: tensor(0.3250)\n",
      "37275 Training Loss: tensor(0.3253)\n",
      "37276 Training Loss: tensor(0.3249)\n",
      "37277 Training Loss: tensor(0.3252)\n",
      "37278 Training Loss: tensor(0.3250)\n",
      "37279 Training Loss: tensor(0.3247)\n",
      "37280 Training Loss: tensor(0.3260)\n",
      "37281 Training Loss: tensor(0.3247)\n",
      "37282 Training Loss: tensor(0.3250)\n",
      "37283 Training Loss: tensor(0.3248)\n",
      "37284 Training Loss: tensor(0.3246)\n",
      "37285 Training Loss: tensor(0.3255)\n",
      "37286 Training Loss: tensor(0.3248)\n",
      "37287 Training Loss: tensor(0.3254)\n",
      "37288 Training Loss: tensor(0.3247)\n",
      "37289 Training Loss: tensor(0.3253)\n",
      "37290 Training Loss: tensor(0.3252)\n",
      "37291 Training Loss: tensor(0.3253)\n",
      "37292 Training Loss: tensor(0.3245)\n",
      "37293 Training Loss: tensor(0.3251)\n",
      "37294 Training Loss: tensor(0.3256)\n",
      "37295 Training Loss: tensor(0.3247)\n",
      "37296 Training Loss: tensor(0.3246)\n",
      "37297 Training Loss: tensor(0.3252)\n",
      "37298 Training Loss: tensor(0.3258)\n",
      "37299 Training Loss: tensor(0.3246)\n",
      "37300 Training Loss: tensor(0.3253)\n",
      "37301 Training Loss: tensor(0.3248)\n",
      "37302 Training Loss: tensor(0.3249)\n",
      "37303 Training Loss: tensor(0.3243)\n",
      "37304 Training Loss: tensor(0.3248)\n",
      "37305 Training Loss: tensor(0.3245)\n",
      "37306 Training Loss: tensor(0.3259)\n",
      "37307 Training Loss: tensor(0.3244)\n",
      "37308 Training Loss: tensor(0.3245)\n",
      "37309 Training Loss: tensor(0.3261)\n",
      "37310 Training Loss: tensor(0.3248)\n",
      "37311 Training Loss: tensor(0.3240)\n",
      "37312 Training Loss: tensor(0.3243)\n",
      "37313 Training Loss: tensor(0.3247)\n",
      "37314 Training Loss: tensor(0.3246)\n",
      "37315 Training Loss: tensor(0.3253)\n",
      "37316 Training Loss: tensor(0.3241)\n",
      "37317 Training Loss: tensor(0.3249)\n",
      "37318 Training Loss: tensor(0.3245)\n",
      "37319 Training Loss: tensor(0.3248)\n",
      "37320 Training Loss: tensor(0.3261)\n",
      "37321 Training Loss: tensor(0.3242)\n",
      "37322 Training Loss: tensor(0.3253)\n",
      "37323 Training Loss: tensor(0.3242)\n",
      "37324 Training Loss: tensor(0.3246)\n",
      "37325 Training Loss: tensor(0.3252)\n",
      "37326 Training Loss: tensor(0.3252)\n",
      "37327 Training Loss: tensor(0.3249)\n",
      "37328 Training Loss: tensor(0.3256)\n",
      "37329 Training Loss: tensor(0.3255)\n",
      "37330 Training Loss: tensor(0.3253)\n",
      "37331 Training Loss: tensor(0.3249)\n",
      "37332 Training Loss: tensor(0.3250)\n",
      "37333 Training Loss: tensor(0.3245)\n",
      "37334 Training Loss: tensor(0.3250)\n",
      "37335 Training Loss: tensor(0.3250)\n",
      "37336 Training Loss: tensor(0.3247)\n",
      "37337 Training Loss: tensor(0.3249)\n",
      "37338 Training Loss: tensor(0.3246)\n",
      "37339 Training Loss: tensor(0.3251)\n",
      "37340 Training Loss: tensor(0.3248)\n",
      "37341 Training Loss: tensor(0.3253)\n",
      "37342 Training Loss: tensor(0.3255)\n",
      "37343 Training Loss: tensor(0.3252)\n",
      "37344 Training Loss: tensor(0.3252)\n",
      "37345 Training Loss: tensor(0.3252)\n",
      "37346 Training Loss: tensor(0.3249)\n",
      "37347 Training Loss: tensor(0.3249)\n",
      "37348 Training Loss: tensor(0.3256)\n",
      "37349 Training Loss: tensor(0.3246)\n",
      "37350 Training Loss: tensor(0.3248)\n",
      "37351 Training Loss: tensor(0.3249)\n",
      "37352 Training Loss: tensor(0.3259)\n",
      "37353 Training Loss: tensor(0.3245)\n",
      "37354 Training Loss: tensor(0.3248)\n",
      "37355 Training Loss: tensor(0.3244)\n",
      "37356 Training Loss: tensor(0.3250)\n",
      "37357 Training Loss: tensor(0.3246)\n",
      "37358 Training Loss: tensor(0.3252)\n",
      "37359 Training Loss: tensor(0.3243)\n",
      "37360 Training Loss: tensor(0.3251)\n",
      "37361 Training Loss: tensor(0.3253)\n",
      "37362 Training Loss: tensor(0.3246)\n",
      "37363 Training Loss: tensor(0.3249)\n",
      "37364 Training Loss: tensor(0.3246)\n",
      "37365 Training Loss: tensor(0.3255)\n",
      "37366 Training Loss: tensor(0.3255)\n",
      "37367 Training Loss: tensor(0.3254)\n",
      "37368 Training Loss: tensor(0.3244)\n",
      "37369 Training Loss: tensor(0.3260)\n",
      "37370 Training Loss: tensor(0.3244)\n",
      "37371 Training Loss: tensor(0.3245)\n",
      "37372 Training Loss: tensor(0.3252)\n",
      "37373 Training Loss: tensor(0.3245)\n",
      "37374 Training Loss: tensor(0.3246)\n",
      "37375 Training Loss: tensor(0.3252)\n",
      "37376 Training Loss: tensor(0.3244)\n",
      "37377 Training Loss: tensor(0.3253)\n",
      "37378 Training Loss: tensor(0.3254)\n",
      "37379 Training Loss: tensor(0.3252)\n",
      "37380 Training Loss: tensor(0.3243)\n",
      "37381 Training Loss: tensor(0.3247)\n",
      "37382 Training Loss: tensor(0.3246)\n",
      "37383 Training Loss: tensor(0.3248)\n",
      "37384 Training Loss: tensor(0.3242)\n",
      "37385 Training Loss: tensor(0.3247)\n",
      "37386 Training Loss: tensor(0.3260)\n",
      "37387 Training Loss: tensor(0.3251)\n",
      "37388 Training Loss: tensor(0.3246)\n",
      "37389 Training Loss: tensor(0.3245)\n",
      "37390 Training Loss: tensor(0.3243)\n",
      "37391 Training Loss: tensor(0.3258)\n",
      "37392 Training Loss: tensor(0.3252)\n",
      "37393 Training Loss: tensor(0.3249)\n",
      "37394 Training Loss: tensor(0.3255)\n",
      "37395 Training Loss: tensor(0.3263)\n",
      "37396 Training Loss: tensor(0.3250)\n",
      "37397 Training Loss: tensor(0.3246)\n",
      "37398 Training Loss: tensor(0.3253)\n",
      "37399 Training Loss: tensor(0.3254)\n",
      "37400 Training Loss: tensor(0.3245)\n",
      "37401 Training Loss: tensor(0.3258)\n",
      "37402 Training Loss: tensor(0.3252)\n",
      "37403 Training Loss: tensor(0.3258)\n",
      "37404 Training Loss: tensor(0.3258)\n",
      "37405 Training Loss: tensor(0.3271)\n",
      "37406 Training Loss: tensor(0.3241)\n",
      "37407 Training Loss: tensor(0.3242)\n",
      "37408 Training Loss: tensor(0.3253)\n",
      "37409 Training Loss: tensor(0.3252)\n",
      "37410 Training Loss: tensor(0.3246)\n",
      "37411 Training Loss: tensor(0.3256)\n",
      "37412 Training Loss: tensor(0.3247)\n",
      "37413 Training Loss: tensor(0.3252)\n",
      "37414 Training Loss: tensor(0.3243)\n",
      "37415 Training Loss: tensor(0.3248)\n",
      "37416 Training Loss: tensor(0.3250)\n",
      "37417 Training Loss: tensor(0.3259)\n",
      "37418 Training Loss: tensor(0.3246)\n",
      "37419 Training Loss: tensor(0.3249)\n",
      "37420 Training Loss: tensor(0.3256)\n",
      "37421 Training Loss: tensor(0.3246)\n",
      "37422 Training Loss: tensor(0.3248)\n",
      "37423 Training Loss: tensor(0.3253)\n",
      "37424 Training Loss: tensor(0.3254)\n",
      "37425 Training Loss: tensor(0.3258)\n",
      "37426 Training Loss: tensor(0.3249)\n",
      "37427 Training Loss: tensor(0.3249)\n",
      "37428 Training Loss: tensor(0.3249)\n",
      "37429 Training Loss: tensor(0.3260)\n",
      "37430 Training Loss: tensor(0.3252)\n",
      "37431 Training Loss: tensor(0.3244)\n",
      "37432 Training Loss: tensor(0.3245)\n",
      "37433 Training Loss: tensor(0.3254)\n",
      "37434 Training Loss: tensor(0.3253)\n",
      "37435 Training Loss: tensor(0.3253)\n",
      "37436 Training Loss: tensor(0.3246)\n",
      "37437 Training Loss: tensor(0.3249)\n",
      "37438 Training Loss: tensor(0.3250)\n",
      "37439 Training Loss: tensor(0.3244)\n",
      "37440 Training Loss: tensor(0.3243)\n",
      "37441 Training Loss: tensor(0.3242)\n",
      "37442 Training Loss: tensor(0.3249)\n",
      "37443 Training Loss: tensor(0.3245)\n",
      "37444 Training Loss: tensor(0.3255)\n",
      "37445 Training Loss: tensor(0.3249)\n",
      "37446 Training Loss: tensor(0.3240)\n",
      "37447 Training Loss: tensor(0.3250)\n",
      "37448 Training Loss: tensor(0.3253)\n",
      "37449 Training Loss: tensor(0.3244)\n",
      "37450 Training Loss: tensor(0.3253)\n",
      "37451 Training Loss: tensor(0.3255)\n",
      "37452 Training Loss: tensor(0.3252)\n",
      "37453 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37454 Training Loss: tensor(0.3250)\n",
      "37455 Training Loss: tensor(0.3248)\n",
      "37456 Training Loss: tensor(0.3246)\n",
      "37457 Training Loss: tensor(0.3246)\n",
      "37458 Training Loss: tensor(0.3244)\n",
      "37459 Training Loss: tensor(0.3243)\n",
      "37460 Training Loss: tensor(0.3241)\n",
      "37461 Training Loss: tensor(0.3252)\n",
      "37462 Training Loss: tensor(0.3243)\n",
      "37463 Training Loss: tensor(0.3255)\n",
      "37464 Training Loss: tensor(0.3265)\n",
      "37465 Training Loss: tensor(0.3248)\n",
      "37466 Training Loss: tensor(0.3251)\n",
      "37467 Training Loss: tensor(0.3246)\n",
      "37468 Training Loss: tensor(0.3238)\n",
      "37469 Training Loss: tensor(0.3243)\n",
      "37470 Training Loss: tensor(0.3246)\n",
      "37471 Training Loss: tensor(0.3252)\n",
      "37472 Training Loss: tensor(0.3248)\n",
      "37473 Training Loss: tensor(0.3253)\n",
      "37474 Training Loss: tensor(0.3254)\n",
      "37475 Training Loss: tensor(0.3249)\n",
      "37476 Training Loss: tensor(0.3245)\n",
      "37477 Training Loss: tensor(0.3244)\n",
      "37478 Training Loss: tensor(0.3244)\n",
      "37479 Training Loss: tensor(0.3251)\n",
      "37480 Training Loss: tensor(0.3252)\n",
      "37481 Training Loss: tensor(0.3246)\n",
      "37482 Training Loss: tensor(0.3248)\n",
      "37483 Training Loss: tensor(0.3253)\n",
      "37484 Training Loss: tensor(0.3248)\n",
      "37485 Training Loss: tensor(0.3249)\n",
      "37486 Training Loss: tensor(0.3243)\n",
      "37487 Training Loss: tensor(0.3251)\n",
      "37488 Training Loss: tensor(0.3251)\n",
      "37489 Training Loss: tensor(0.3240)\n",
      "37490 Training Loss: tensor(0.3250)\n",
      "37491 Training Loss: tensor(0.3249)\n",
      "37492 Training Loss: tensor(0.3245)\n",
      "37493 Training Loss: tensor(0.3250)\n",
      "37494 Training Loss: tensor(0.3246)\n",
      "37495 Training Loss: tensor(0.3256)\n",
      "37496 Training Loss: tensor(0.3252)\n",
      "37497 Training Loss: tensor(0.3248)\n",
      "37498 Training Loss: tensor(0.3248)\n",
      "37499 Training Loss: tensor(0.3248)\n",
      "37500 Training Loss: tensor(0.3252)\n",
      "37501 Training Loss: tensor(0.3249)\n",
      "37502 Training Loss: tensor(0.3250)\n",
      "37503 Training Loss: tensor(0.3259)\n",
      "37504 Training Loss: tensor(0.3255)\n",
      "37505 Training Loss: tensor(0.3250)\n",
      "37506 Training Loss: tensor(0.3243)\n",
      "37507 Training Loss: tensor(0.3253)\n",
      "37508 Training Loss: tensor(0.3257)\n",
      "37509 Training Loss: tensor(0.3254)\n",
      "37510 Training Loss: tensor(0.3252)\n",
      "37511 Training Loss: tensor(0.3241)\n",
      "37512 Training Loss: tensor(0.3245)\n",
      "37513 Training Loss: tensor(0.3246)\n",
      "37514 Training Loss: tensor(0.3246)\n",
      "37515 Training Loss: tensor(0.3243)\n",
      "37516 Training Loss: tensor(0.3248)\n",
      "37517 Training Loss: tensor(0.3249)\n",
      "37518 Training Loss: tensor(0.3243)\n",
      "37519 Training Loss: tensor(0.3245)\n",
      "37520 Training Loss: tensor(0.3248)\n",
      "37521 Training Loss: tensor(0.3263)\n",
      "37522 Training Loss: tensor(0.3244)\n",
      "37523 Training Loss: tensor(0.3254)\n",
      "37524 Training Loss: tensor(0.3256)\n",
      "37525 Training Loss: tensor(0.3248)\n",
      "37526 Training Loss: tensor(0.3247)\n",
      "37527 Training Loss: tensor(0.3249)\n",
      "37528 Training Loss: tensor(0.3245)\n",
      "37529 Training Loss: tensor(0.3252)\n",
      "37530 Training Loss: tensor(0.3244)\n",
      "37531 Training Loss: tensor(0.3246)\n",
      "37532 Training Loss: tensor(0.3253)\n",
      "37533 Training Loss: tensor(0.3246)\n",
      "37534 Training Loss: tensor(0.3247)\n",
      "37535 Training Loss: tensor(0.3247)\n",
      "37536 Training Loss: tensor(0.3247)\n",
      "37537 Training Loss: tensor(0.3251)\n",
      "37538 Training Loss: tensor(0.3244)\n",
      "37539 Training Loss: tensor(0.3245)\n",
      "37540 Training Loss: tensor(0.3241)\n",
      "37541 Training Loss: tensor(0.3249)\n",
      "37542 Training Loss: tensor(0.3248)\n",
      "37543 Training Loss: tensor(0.3242)\n",
      "37544 Training Loss: tensor(0.3254)\n",
      "37545 Training Loss: tensor(0.3247)\n",
      "37546 Training Loss: tensor(0.3240)\n",
      "37547 Training Loss: tensor(0.3250)\n",
      "37548 Training Loss: tensor(0.3244)\n",
      "37549 Training Loss: tensor(0.3245)\n",
      "37550 Training Loss: tensor(0.3246)\n",
      "37551 Training Loss: tensor(0.3247)\n",
      "37552 Training Loss: tensor(0.3252)\n",
      "37553 Training Loss: tensor(0.3248)\n",
      "37554 Training Loss: tensor(0.3247)\n",
      "37555 Training Loss: tensor(0.3244)\n",
      "37556 Training Loss: tensor(0.3243)\n",
      "37557 Training Loss: tensor(0.3246)\n",
      "37558 Training Loss: tensor(0.3249)\n",
      "37559 Training Loss: tensor(0.3242)\n",
      "37560 Training Loss: tensor(0.3243)\n",
      "37561 Training Loss: tensor(0.3244)\n",
      "37562 Training Loss: tensor(0.3263)\n",
      "37563 Training Loss: tensor(0.3247)\n",
      "37564 Training Loss: tensor(0.3253)\n",
      "37565 Training Loss: tensor(0.3256)\n",
      "37566 Training Loss: tensor(0.3258)\n",
      "37567 Training Loss: tensor(0.3242)\n",
      "37568 Training Loss: tensor(0.3251)\n",
      "37569 Training Loss: tensor(0.3243)\n",
      "37570 Training Loss: tensor(0.3247)\n",
      "37571 Training Loss: tensor(0.3250)\n",
      "37572 Training Loss: tensor(0.3247)\n",
      "37573 Training Loss: tensor(0.3244)\n",
      "37574 Training Loss: tensor(0.3245)\n",
      "37575 Training Loss: tensor(0.3250)\n",
      "37576 Training Loss: tensor(0.3247)\n",
      "37577 Training Loss: tensor(0.3242)\n",
      "37578 Training Loss: tensor(0.3250)\n",
      "37579 Training Loss: tensor(0.3256)\n",
      "37580 Training Loss: tensor(0.3255)\n",
      "37581 Training Loss: tensor(0.3268)\n",
      "37582 Training Loss: tensor(0.3247)\n",
      "37583 Training Loss: tensor(0.3257)\n",
      "37584 Training Loss: tensor(0.3246)\n",
      "37585 Training Loss: tensor(0.3242)\n",
      "37586 Training Loss: tensor(0.3246)\n",
      "37587 Training Loss: tensor(0.3244)\n",
      "37588 Training Loss: tensor(0.3247)\n",
      "37589 Training Loss: tensor(0.3244)\n",
      "37590 Training Loss: tensor(0.3245)\n",
      "37591 Training Loss: tensor(0.3242)\n",
      "37592 Training Loss: tensor(0.3253)\n",
      "37593 Training Loss: tensor(0.3254)\n",
      "37594 Training Loss: tensor(0.3241)\n",
      "37595 Training Loss: tensor(0.3245)\n",
      "37596 Training Loss: tensor(0.3250)\n",
      "37597 Training Loss: tensor(0.3250)\n",
      "37598 Training Loss: tensor(0.3247)\n",
      "37599 Training Loss: tensor(0.3252)\n",
      "37600 Training Loss: tensor(0.3248)\n",
      "37601 Training Loss: tensor(0.3248)\n",
      "37602 Training Loss: tensor(0.3261)\n",
      "37603 Training Loss: tensor(0.3252)\n",
      "37604 Training Loss: tensor(0.3249)\n",
      "37605 Training Loss: tensor(0.3260)\n",
      "37606 Training Loss: tensor(0.3252)\n",
      "37607 Training Loss: tensor(0.3251)\n",
      "37608 Training Loss: tensor(0.3253)\n",
      "37609 Training Loss: tensor(0.3243)\n",
      "37610 Training Loss: tensor(0.3246)\n",
      "37611 Training Loss: tensor(0.3258)\n",
      "37612 Training Loss: tensor(0.3247)\n",
      "37613 Training Loss: tensor(0.3252)\n",
      "37614 Training Loss: tensor(0.3240)\n",
      "37615 Training Loss: tensor(0.3242)\n",
      "37616 Training Loss: tensor(0.3251)\n",
      "37617 Training Loss: tensor(0.3257)\n",
      "37618 Training Loss: tensor(0.3245)\n",
      "37619 Training Loss: tensor(0.3253)\n",
      "37620 Training Loss: tensor(0.3247)\n",
      "37621 Training Loss: tensor(0.3249)\n",
      "37622 Training Loss: tensor(0.3250)\n",
      "37623 Training Loss: tensor(0.3249)\n",
      "37624 Training Loss: tensor(0.3248)\n",
      "37625 Training Loss: tensor(0.3244)\n",
      "37626 Training Loss: tensor(0.3246)\n",
      "37627 Training Loss: tensor(0.3246)\n",
      "37628 Training Loss: tensor(0.3248)\n",
      "37629 Training Loss: tensor(0.3245)\n",
      "37630 Training Loss: tensor(0.3249)\n",
      "37631 Training Loss: tensor(0.3243)\n",
      "37632 Training Loss: tensor(0.3242)\n",
      "37633 Training Loss: tensor(0.3257)\n",
      "37634 Training Loss: tensor(0.3252)\n",
      "37635 Training Loss: tensor(0.3243)\n",
      "37636 Training Loss: tensor(0.3257)\n",
      "37637 Training Loss: tensor(0.3247)\n",
      "37638 Training Loss: tensor(0.3249)\n",
      "37639 Training Loss: tensor(0.3247)\n",
      "37640 Training Loss: tensor(0.3254)\n",
      "37641 Training Loss: tensor(0.3255)\n",
      "37642 Training Loss: tensor(0.3253)\n",
      "37643 Training Loss: tensor(0.3243)\n",
      "37644 Training Loss: tensor(0.3243)\n",
      "37645 Training Loss: tensor(0.3249)\n",
      "37646 Training Loss: tensor(0.3249)\n",
      "37647 Training Loss: tensor(0.3254)\n",
      "37648 Training Loss: tensor(0.3255)\n",
      "37649 Training Loss: tensor(0.3250)\n",
      "37650 Training Loss: tensor(0.3248)\n",
      "37651 Training Loss: tensor(0.3246)\n",
      "37652 Training Loss: tensor(0.3246)\n",
      "37653 Training Loss: tensor(0.3243)\n",
      "37654 Training Loss: tensor(0.3247)\n",
      "37655 Training Loss: tensor(0.3243)\n",
      "37656 Training Loss: tensor(0.3249)\n",
      "37657 Training Loss: tensor(0.3249)\n",
      "37658 Training Loss: tensor(0.3245)\n",
      "37659 Training Loss: tensor(0.3246)\n",
      "37660 Training Loss: tensor(0.3241)\n",
      "37661 Training Loss: tensor(0.3248)\n",
      "37662 Training Loss: tensor(0.3241)\n",
      "37663 Training Loss: tensor(0.3260)\n",
      "37664 Training Loss: tensor(0.3248)\n",
      "37665 Training Loss: tensor(0.3245)\n",
      "37666 Training Loss: tensor(0.3245)\n",
      "37667 Training Loss: tensor(0.3246)\n",
      "37668 Training Loss: tensor(0.3253)\n",
      "37669 Training Loss: tensor(0.3244)\n",
      "37670 Training Loss: tensor(0.3248)\n",
      "37671 Training Loss: tensor(0.3241)\n",
      "37672 Training Loss: tensor(0.3250)\n",
      "37673 Training Loss: tensor(0.3243)\n",
      "37674 Training Loss: tensor(0.3248)\n",
      "37675 Training Loss: tensor(0.3253)\n",
      "37676 Training Loss: tensor(0.3250)\n",
      "37677 Training Loss: tensor(0.3242)\n",
      "37678 Training Loss: tensor(0.3248)\n",
      "37679 Training Loss: tensor(0.3243)\n",
      "37680 Training Loss: tensor(0.3245)\n",
      "37681 Training Loss: tensor(0.3246)\n",
      "37682 Training Loss: tensor(0.3241)\n",
      "37683 Training Loss: tensor(0.3251)\n",
      "37684 Training Loss: tensor(0.3242)\n",
      "37685 Training Loss: tensor(0.3241)\n",
      "37686 Training Loss: tensor(0.3242)\n",
      "37687 Training Loss: tensor(0.3242)\n",
      "37688 Training Loss: tensor(0.3246)\n",
      "37689 Training Loss: tensor(0.3246)\n",
      "37690 Training Loss: tensor(0.3249)\n",
      "37691 Training Loss: tensor(0.3242)\n",
      "37692 Training Loss: tensor(0.3254)\n",
      "37693 Training Loss: tensor(0.3243)\n",
      "37694 Training Loss: tensor(0.3249)\n",
      "37695 Training Loss: tensor(0.3249)\n",
      "37696 Training Loss: tensor(0.3249)\n",
      "37697 Training Loss: tensor(0.3243)\n",
      "37698 Training Loss: tensor(0.3259)\n",
      "37699 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37700 Training Loss: tensor(0.3248)\n",
      "37701 Training Loss: tensor(0.3258)\n",
      "37702 Training Loss: tensor(0.3258)\n",
      "37703 Training Loss: tensor(0.3245)\n",
      "37704 Training Loss: tensor(0.3247)\n",
      "37705 Training Loss: tensor(0.3244)\n",
      "37706 Training Loss: tensor(0.3245)\n",
      "37707 Training Loss: tensor(0.3243)\n",
      "37708 Training Loss: tensor(0.3246)\n",
      "37709 Training Loss: tensor(0.3249)\n",
      "37710 Training Loss: tensor(0.3245)\n",
      "37711 Training Loss: tensor(0.3247)\n",
      "37712 Training Loss: tensor(0.3250)\n",
      "37713 Training Loss: tensor(0.3241)\n",
      "37714 Training Loss: tensor(0.3245)\n",
      "37715 Training Loss: tensor(0.3244)\n",
      "37716 Training Loss: tensor(0.3245)\n",
      "37717 Training Loss: tensor(0.3249)\n",
      "37718 Training Loss: tensor(0.3243)\n",
      "37719 Training Loss: tensor(0.3249)\n",
      "37720 Training Loss: tensor(0.3243)\n",
      "37721 Training Loss: tensor(0.3247)\n",
      "37722 Training Loss: tensor(0.3260)\n",
      "37723 Training Loss: tensor(0.3246)\n",
      "37724 Training Loss: tensor(0.3257)\n",
      "37725 Training Loss: tensor(0.3267)\n",
      "37726 Training Loss: tensor(0.3244)\n",
      "37727 Training Loss: tensor(0.3245)\n",
      "37728 Training Loss: tensor(0.3256)\n",
      "37729 Training Loss: tensor(0.3255)\n",
      "37730 Training Loss: tensor(0.3244)\n",
      "37731 Training Loss: tensor(0.3249)\n",
      "37732 Training Loss: tensor(0.3243)\n",
      "37733 Training Loss: tensor(0.3250)\n",
      "37734 Training Loss: tensor(0.3257)\n",
      "37735 Training Loss: tensor(0.3257)\n",
      "37736 Training Loss: tensor(0.3241)\n",
      "37737 Training Loss: tensor(0.3248)\n",
      "37738 Training Loss: tensor(0.3251)\n",
      "37739 Training Loss: tensor(0.3242)\n",
      "37740 Training Loss: tensor(0.3247)\n",
      "37741 Training Loss: tensor(0.3245)\n",
      "37742 Training Loss: tensor(0.3241)\n",
      "37743 Training Loss: tensor(0.3254)\n",
      "37744 Training Loss: tensor(0.3252)\n",
      "37745 Training Loss: tensor(0.3260)\n",
      "37746 Training Loss: tensor(0.3242)\n",
      "37747 Training Loss: tensor(0.3242)\n",
      "37748 Training Loss: tensor(0.3253)\n",
      "37749 Training Loss: tensor(0.3250)\n",
      "37750 Training Loss: tensor(0.3248)\n",
      "37751 Training Loss: tensor(0.3247)\n",
      "37752 Training Loss: tensor(0.3261)\n",
      "37753 Training Loss: tensor(0.3245)\n",
      "37754 Training Loss: tensor(0.3248)\n",
      "37755 Training Loss: tensor(0.3251)\n",
      "37756 Training Loss: tensor(0.3247)\n",
      "37757 Training Loss: tensor(0.3248)\n",
      "37758 Training Loss: tensor(0.3247)\n",
      "37759 Training Loss: tensor(0.3251)\n",
      "37760 Training Loss: tensor(0.3255)\n",
      "37761 Training Loss: tensor(0.3245)\n",
      "37762 Training Loss: tensor(0.3241)\n",
      "37763 Training Loss: tensor(0.3239)\n",
      "37764 Training Loss: tensor(0.3249)\n",
      "37765 Training Loss: tensor(0.3260)\n",
      "37766 Training Loss: tensor(0.3250)\n",
      "37767 Training Loss: tensor(0.3255)\n",
      "37768 Training Loss: tensor(0.3250)\n",
      "37769 Training Loss: tensor(0.3249)\n",
      "37770 Training Loss: tensor(0.3259)\n",
      "37771 Training Loss: tensor(0.3241)\n",
      "37772 Training Loss: tensor(0.3246)\n",
      "37773 Training Loss: tensor(0.3254)\n",
      "37774 Training Loss: tensor(0.3254)\n",
      "37775 Training Loss: tensor(0.3257)\n",
      "37776 Training Loss: tensor(0.3245)\n",
      "37777 Training Loss: tensor(0.3250)\n",
      "37778 Training Loss: tensor(0.3247)\n",
      "37779 Training Loss: tensor(0.3249)\n",
      "37780 Training Loss: tensor(0.3259)\n",
      "37781 Training Loss: tensor(0.3256)\n",
      "37782 Training Loss: tensor(0.3250)\n",
      "37783 Training Loss: tensor(0.3242)\n",
      "37784 Training Loss: tensor(0.3250)\n",
      "37785 Training Loss: tensor(0.3253)\n",
      "37786 Training Loss: tensor(0.3248)\n",
      "37787 Training Loss: tensor(0.3249)\n",
      "37788 Training Loss: tensor(0.3249)\n",
      "37789 Training Loss: tensor(0.3254)\n",
      "37790 Training Loss: tensor(0.3255)\n",
      "37791 Training Loss: tensor(0.3241)\n",
      "37792 Training Loss: tensor(0.3249)\n",
      "37793 Training Loss: tensor(0.3247)\n",
      "37794 Training Loss: tensor(0.3250)\n",
      "37795 Training Loss: tensor(0.3244)\n",
      "37796 Training Loss: tensor(0.3270)\n",
      "37797 Training Loss: tensor(0.3248)\n",
      "37798 Training Loss: tensor(0.3245)\n",
      "37799 Training Loss: tensor(0.3244)\n",
      "37800 Training Loss: tensor(0.3242)\n",
      "37801 Training Loss: tensor(0.3247)\n",
      "37802 Training Loss: tensor(0.3254)\n",
      "37803 Training Loss: tensor(0.3244)\n",
      "37804 Training Loss: tensor(0.3248)\n",
      "37805 Training Loss: tensor(0.3245)\n",
      "37806 Training Loss: tensor(0.3247)\n",
      "37807 Training Loss: tensor(0.3248)\n",
      "37808 Training Loss: tensor(0.3242)\n",
      "37809 Training Loss: tensor(0.3250)\n",
      "37810 Training Loss: tensor(0.3251)\n",
      "37811 Training Loss: tensor(0.3255)\n",
      "37812 Training Loss: tensor(0.3251)\n",
      "37813 Training Loss: tensor(0.3245)\n",
      "37814 Training Loss: tensor(0.3246)\n",
      "37815 Training Loss: tensor(0.3244)\n",
      "37816 Training Loss: tensor(0.3245)\n",
      "37817 Training Loss: tensor(0.3252)\n",
      "37818 Training Loss: tensor(0.3246)\n",
      "37819 Training Loss: tensor(0.3250)\n",
      "37820 Training Loss: tensor(0.3243)\n",
      "37821 Training Loss: tensor(0.3249)\n",
      "37822 Training Loss: tensor(0.3249)\n",
      "37823 Training Loss: tensor(0.3246)\n",
      "37824 Training Loss: tensor(0.3248)\n",
      "37825 Training Loss: tensor(0.3243)\n",
      "37826 Training Loss: tensor(0.3241)\n",
      "37827 Training Loss: tensor(0.3248)\n",
      "37828 Training Loss: tensor(0.3261)\n",
      "37829 Training Loss: tensor(0.3246)\n",
      "37830 Training Loss: tensor(0.3247)\n",
      "37831 Training Loss: tensor(0.3247)\n",
      "37832 Training Loss: tensor(0.3246)\n",
      "37833 Training Loss: tensor(0.3250)\n",
      "37834 Training Loss: tensor(0.3253)\n",
      "37835 Training Loss: tensor(0.3240)\n",
      "37836 Training Loss: tensor(0.3242)\n",
      "37837 Training Loss: tensor(0.3250)\n",
      "37838 Training Loss: tensor(0.3245)\n",
      "37839 Training Loss: tensor(0.3246)\n",
      "37840 Training Loss: tensor(0.3249)\n",
      "37841 Training Loss: tensor(0.3249)\n",
      "37842 Training Loss: tensor(0.3247)\n",
      "37843 Training Loss: tensor(0.3241)\n",
      "37844 Training Loss: tensor(0.3255)\n",
      "37845 Training Loss: tensor(0.3250)\n",
      "37846 Training Loss: tensor(0.3245)\n",
      "37847 Training Loss: tensor(0.3243)\n",
      "37848 Training Loss: tensor(0.3250)\n",
      "37849 Training Loss: tensor(0.3255)\n",
      "37850 Training Loss: tensor(0.3251)\n",
      "37851 Training Loss: tensor(0.3247)\n",
      "37852 Training Loss: tensor(0.3248)\n",
      "37853 Training Loss: tensor(0.3247)\n",
      "37854 Training Loss: tensor(0.3246)\n",
      "37855 Training Loss: tensor(0.3247)\n",
      "37856 Training Loss: tensor(0.3242)\n",
      "37857 Training Loss: tensor(0.3250)\n",
      "37858 Training Loss: tensor(0.3262)\n",
      "37859 Training Loss: tensor(0.3245)\n",
      "37860 Training Loss: tensor(0.3242)\n",
      "37861 Training Loss: tensor(0.3243)\n",
      "37862 Training Loss: tensor(0.3245)\n",
      "37863 Training Loss: tensor(0.3254)\n",
      "37864 Training Loss: tensor(0.3246)\n",
      "37865 Training Loss: tensor(0.3247)\n",
      "37866 Training Loss: tensor(0.3250)\n",
      "37867 Training Loss: tensor(0.3254)\n",
      "37868 Training Loss: tensor(0.3248)\n",
      "37869 Training Loss: tensor(0.3236)\n",
      "37870 Training Loss: tensor(0.3252)\n",
      "37871 Training Loss: tensor(0.3248)\n",
      "37872 Training Loss: tensor(0.3243)\n",
      "37873 Training Loss: tensor(0.3243)\n",
      "37874 Training Loss: tensor(0.3241)\n",
      "37875 Training Loss: tensor(0.3244)\n",
      "37876 Training Loss: tensor(0.3256)\n",
      "37877 Training Loss: tensor(0.3249)\n",
      "37878 Training Loss: tensor(0.3252)\n",
      "37879 Training Loss: tensor(0.3248)\n",
      "37880 Training Loss: tensor(0.3258)\n",
      "37881 Training Loss: tensor(0.3253)\n",
      "37882 Training Loss: tensor(0.3249)\n",
      "37883 Training Loss: tensor(0.3244)\n",
      "37884 Training Loss: tensor(0.3240)\n",
      "37885 Training Loss: tensor(0.3261)\n",
      "37886 Training Loss: tensor(0.3252)\n",
      "37887 Training Loss: tensor(0.3257)\n",
      "37888 Training Loss: tensor(0.3245)\n",
      "37889 Training Loss: tensor(0.3245)\n",
      "37890 Training Loss: tensor(0.3254)\n",
      "37891 Training Loss: tensor(0.3249)\n",
      "37892 Training Loss: tensor(0.3246)\n",
      "37893 Training Loss: tensor(0.3253)\n",
      "37894 Training Loss: tensor(0.3251)\n",
      "37895 Training Loss: tensor(0.3249)\n",
      "37896 Training Loss: tensor(0.3257)\n",
      "37897 Training Loss: tensor(0.3244)\n",
      "37898 Training Loss: tensor(0.3242)\n",
      "37899 Training Loss: tensor(0.3262)\n",
      "37900 Training Loss: tensor(0.3243)\n",
      "37901 Training Loss: tensor(0.3259)\n",
      "37902 Training Loss: tensor(0.3241)\n",
      "37903 Training Loss: tensor(0.3254)\n",
      "37904 Training Loss: tensor(0.3243)\n",
      "37905 Training Loss: tensor(0.3252)\n",
      "37906 Training Loss: tensor(0.3249)\n",
      "37907 Training Loss: tensor(0.3247)\n",
      "37908 Training Loss: tensor(0.3251)\n",
      "37909 Training Loss: tensor(0.3249)\n",
      "37910 Training Loss: tensor(0.3242)\n",
      "37911 Training Loss: tensor(0.3255)\n",
      "37912 Training Loss: tensor(0.3242)\n",
      "37913 Training Loss: tensor(0.3246)\n",
      "37914 Training Loss: tensor(0.3246)\n",
      "37915 Training Loss: tensor(0.3249)\n",
      "37916 Training Loss: tensor(0.3244)\n",
      "37917 Training Loss: tensor(0.3246)\n",
      "37918 Training Loss: tensor(0.3254)\n",
      "37919 Training Loss: tensor(0.3248)\n",
      "37920 Training Loss: tensor(0.3247)\n",
      "37921 Training Loss: tensor(0.3241)\n",
      "37922 Training Loss: tensor(0.3242)\n",
      "37923 Training Loss: tensor(0.3243)\n",
      "37924 Training Loss: tensor(0.3258)\n",
      "37925 Training Loss: tensor(0.3253)\n",
      "37926 Training Loss: tensor(0.3253)\n",
      "37927 Training Loss: tensor(0.3247)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37928 Training Loss: tensor(0.3243)\n",
      "37929 Training Loss: tensor(0.3249)\n",
      "37930 Training Loss: tensor(0.3255)\n",
      "37931 Training Loss: tensor(0.3241)\n",
      "37932 Training Loss: tensor(0.3245)\n",
      "37933 Training Loss: tensor(0.3245)\n",
      "37934 Training Loss: tensor(0.3244)\n",
      "37935 Training Loss: tensor(0.3243)\n",
      "37936 Training Loss: tensor(0.3247)\n",
      "37937 Training Loss: tensor(0.3254)\n",
      "37938 Training Loss: tensor(0.3244)\n",
      "37939 Training Loss: tensor(0.3252)\n",
      "37940 Training Loss: tensor(0.3244)\n",
      "37941 Training Loss: tensor(0.3271)\n",
      "37942 Training Loss: tensor(0.3244)\n",
      "37943 Training Loss: tensor(0.3244)\n",
      "37944 Training Loss: tensor(0.3246)\n",
      "37945 Training Loss: tensor(0.3247)\n",
      "37946 Training Loss: tensor(0.3252)\n",
      "37947 Training Loss: tensor(0.3265)\n",
      "37948 Training Loss: tensor(0.3244)\n",
      "37949 Training Loss: tensor(0.3252)\n",
      "37950 Training Loss: tensor(0.3247)\n",
      "37951 Training Loss: tensor(0.3245)\n",
      "37952 Training Loss: tensor(0.3242)\n",
      "37953 Training Loss: tensor(0.3243)\n",
      "37954 Training Loss: tensor(0.3251)\n",
      "37955 Training Loss: tensor(0.3247)\n",
      "37956 Training Loss: tensor(0.3252)\n",
      "37957 Training Loss: tensor(0.3248)\n",
      "37958 Training Loss: tensor(0.3261)\n",
      "37959 Training Loss: tensor(0.3261)\n",
      "37960 Training Loss: tensor(0.3242)\n",
      "37961 Training Loss: tensor(0.3247)\n",
      "37962 Training Loss: tensor(0.3249)\n",
      "37963 Training Loss: tensor(0.3239)\n",
      "37964 Training Loss: tensor(0.3249)\n",
      "37965 Training Loss: tensor(0.3244)\n",
      "37966 Training Loss: tensor(0.3249)\n",
      "37967 Training Loss: tensor(0.3249)\n",
      "37968 Training Loss: tensor(0.3244)\n",
      "37969 Training Loss: tensor(0.3247)\n",
      "37970 Training Loss: tensor(0.3252)\n",
      "37971 Training Loss: tensor(0.3250)\n",
      "37972 Training Loss: tensor(0.3247)\n",
      "37973 Training Loss: tensor(0.3245)\n",
      "37974 Training Loss: tensor(0.3252)\n",
      "37975 Training Loss: tensor(0.3254)\n",
      "37976 Training Loss: tensor(0.3239)\n",
      "37977 Training Loss: tensor(0.3248)\n",
      "37978 Training Loss: tensor(0.3247)\n",
      "37979 Training Loss: tensor(0.3264)\n",
      "37980 Training Loss: tensor(0.3246)\n",
      "37981 Training Loss: tensor(0.3250)\n",
      "37982 Training Loss: tensor(0.3248)\n",
      "37983 Training Loss: tensor(0.3247)\n",
      "37984 Training Loss: tensor(0.3249)\n",
      "37985 Training Loss: tensor(0.3273)\n",
      "37986 Training Loss: tensor(0.3245)\n",
      "37987 Training Loss: tensor(0.3242)\n",
      "37988 Training Loss: tensor(0.3253)\n",
      "37989 Training Loss: tensor(0.3259)\n",
      "37990 Training Loss: tensor(0.3252)\n",
      "37991 Training Loss: tensor(0.3248)\n",
      "37992 Training Loss: tensor(0.3255)\n",
      "37993 Training Loss: tensor(0.3260)\n",
      "37994 Training Loss: tensor(0.3254)\n",
      "37995 Training Loss: tensor(0.3253)\n",
      "37996 Training Loss: tensor(0.3247)\n",
      "37997 Training Loss: tensor(0.3244)\n",
      "37998 Training Loss: tensor(0.3256)\n",
      "37999 Training Loss: tensor(0.3246)\n",
      "38000 Training Loss: tensor(0.3246)\n",
      "38001 Training Loss: tensor(0.3247)\n",
      "38002 Training Loss: tensor(0.3244)\n",
      "38003 Training Loss: tensor(0.3245)\n",
      "38004 Training Loss: tensor(0.3247)\n",
      "38005 Training Loss: tensor(0.3250)\n",
      "38006 Training Loss: tensor(0.3249)\n",
      "38007 Training Loss: tensor(0.3245)\n",
      "38008 Training Loss: tensor(0.3242)\n",
      "38009 Training Loss: tensor(0.3241)\n",
      "38010 Training Loss: tensor(0.3250)\n",
      "38011 Training Loss: tensor(0.3247)\n",
      "38012 Training Loss: tensor(0.3246)\n",
      "38013 Training Loss: tensor(0.3243)\n",
      "38014 Training Loss: tensor(0.3253)\n",
      "38015 Training Loss: tensor(0.3247)\n",
      "38016 Training Loss: tensor(0.3249)\n",
      "38017 Training Loss: tensor(0.3244)\n",
      "38018 Training Loss: tensor(0.3244)\n",
      "38019 Training Loss: tensor(0.3249)\n",
      "38020 Training Loss: tensor(0.3261)\n",
      "38021 Training Loss: tensor(0.3240)\n",
      "38022 Training Loss: tensor(0.3250)\n",
      "38023 Training Loss: tensor(0.3243)\n",
      "38024 Training Loss: tensor(0.3253)\n",
      "38025 Training Loss: tensor(0.3250)\n",
      "38026 Training Loss: tensor(0.3247)\n",
      "38027 Training Loss: tensor(0.3245)\n",
      "38028 Training Loss: tensor(0.3245)\n",
      "38029 Training Loss: tensor(0.3252)\n",
      "38030 Training Loss: tensor(0.3239)\n",
      "38031 Training Loss: tensor(0.3242)\n",
      "38032 Training Loss: tensor(0.3250)\n",
      "38033 Training Loss: tensor(0.3253)\n",
      "38034 Training Loss: tensor(0.3244)\n",
      "38035 Training Loss: tensor(0.3257)\n",
      "38036 Training Loss: tensor(0.3247)\n",
      "38037 Training Loss: tensor(0.3252)\n",
      "38038 Training Loss: tensor(0.3247)\n",
      "38039 Training Loss: tensor(0.3243)\n",
      "38040 Training Loss: tensor(0.3254)\n",
      "38041 Training Loss: tensor(0.3255)\n",
      "38042 Training Loss: tensor(0.3254)\n",
      "38043 Training Loss: tensor(0.3251)\n",
      "38044 Training Loss: tensor(0.3253)\n",
      "38045 Training Loss: tensor(0.3246)\n",
      "38046 Training Loss: tensor(0.3252)\n",
      "38047 Training Loss: tensor(0.3244)\n",
      "38048 Training Loss: tensor(0.3243)\n",
      "38049 Training Loss: tensor(0.3246)\n",
      "38050 Training Loss: tensor(0.3247)\n",
      "38051 Training Loss: tensor(0.3257)\n",
      "38052 Training Loss: tensor(0.3243)\n",
      "38053 Training Loss: tensor(0.3249)\n",
      "38054 Training Loss: tensor(0.3253)\n",
      "38055 Training Loss: tensor(0.3248)\n",
      "38056 Training Loss: tensor(0.3247)\n",
      "38057 Training Loss: tensor(0.3253)\n",
      "38058 Training Loss: tensor(0.3247)\n",
      "38059 Training Loss: tensor(0.3256)\n",
      "38060 Training Loss: tensor(0.3244)\n",
      "38061 Training Loss: tensor(0.3248)\n",
      "38062 Training Loss: tensor(0.3245)\n",
      "38063 Training Loss: tensor(0.3247)\n",
      "38064 Training Loss: tensor(0.3249)\n",
      "38065 Training Loss: tensor(0.3249)\n",
      "38066 Training Loss: tensor(0.3239)\n",
      "38067 Training Loss: tensor(0.3249)\n",
      "38068 Training Loss: tensor(0.3246)\n",
      "38069 Training Loss: tensor(0.3252)\n",
      "38070 Training Loss: tensor(0.3249)\n",
      "38071 Training Loss: tensor(0.3243)\n",
      "38072 Training Loss: tensor(0.3242)\n",
      "38073 Training Loss: tensor(0.3251)\n",
      "38074 Training Loss: tensor(0.3245)\n",
      "38075 Training Loss: tensor(0.3251)\n",
      "38076 Training Loss: tensor(0.3244)\n",
      "38077 Training Loss: tensor(0.3249)\n",
      "38078 Training Loss: tensor(0.3242)\n",
      "38079 Training Loss: tensor(0.3246)\n",
      "38080 Training Loss: tensor(0.3246)\n",
      "38081 Training Loss: tensor(0.3241)\n",
      "38082 Training Loss: tensor(0.3244)\n",
      "38083 Training Loss: tensor(0.3244)\n",
      "38084 Training Loss: tensor(0.3247)\n",
      "38085 Training Loss: tensor(0.3241)\n",
      "38086 Training Loss: tensor(0.3254)\n",
      "38087 Training Loss: tensor(0.3249)\n",
      "38088 Training Loss: tensor(0.3254)\n",
      "38089 Training Loss: tensor(0.3251)\n",
      "38090 Training Loss: tensor(0.3243)\n",
      "38091 Training Loss: tensor(0.3250)\n",
      "38092 Training Loss: tensor(0.3241)\n",
      "38093 Training Loss: tensor(0.3243)\n",
      "38094 Training Loss: tensor(0.3245)\n",
      "38095 Training Loss: tensor(0.3252)\n",
      "38096 Training Loss: tensor(0.3244)\n",
      "38097 Training Loss: tensor(0.3251)\n",
      "38098 Training Loss: tensor(0.3253)\n",
      "38099 Training Loss: tensor(0.3250)\n",
      "38100 Training Loss: tensor(0.3244)\n",
      "38101 Training Loss: tensor(0.3256)\n",
      "38102 Training Loss: tensor(0.3254)\n",
      "38103 Training Loss: tensor(0.3252)\n",
      "38104 Training Loss: tensor(0.3244)\n",
      "38105 Training Loss: tensor(0.3248)\n",
      "38106 Training Loss: tensor(0.3245)\n",
      "38107 Training Loss: tensor(0.3245)\n",
      "38108 Training Loss: tensor(0.3250)\n",
      "38109 Training Loss: tensor(0.3249)\n",
      "38110 Training Loss: tensor(0.3245)\n",
      "38111 Training Loss: tensor(0.3243)\n",
      "38112 Training Loss: tensor(0.3248)\n",
      "38113 Training Loss: tensor(0.3249)\n",
      "38114 Training Loss: tensor(0.3241)\n",
      "38115 Training Loss: tensor(0.3248)\n",
      "38116 Training Loss: tensor(0.3246)\n",
      "38117 Training Loss: tensor(0.3250)\n",
      "38118 Training Loss: tensor(0.3247)\n",
      "38119 Training Loss: tensor(0.3257)\n",
      "38120 Training Loss: tensor(0.3245)\n",
      "38121 Training Loss: tensor(0.3256)\n",
      "38122 Training Loss: tensor(0.3251)\n",
      "38123 Training Loss: tensor(0.3242)\n",
      "38124 Training Loss: tensor(0.3245)\n",
      "38125 Training Loss: tensor(0.3248)\n",
      "38126 Training Loss: tensor(0.3249)\n",
      "38127 Training Loss: tensor(0.3253)\n",
      "38128 Training Loss: tensor(0.3253)\n",
      "38129 Training Loss: tensor(0.3250)\n",
      "38130 Training Loss: tensor(0.3247)\n",
      "38131 Training Loss: tensor(0.3247)\n",
      "38132 Training Loss: tensor(0.3254)\n",
      "38133 Training Loss: tensor(0.3254)\n",
      "38134 Training Loss: tensor(0.3251)\n",
      "38135 Training Loss: tensor(0.3249)\n",
      "38136 Training Loss: tensor(0.3247)\n",
      "38137 Training Loss: tensor(0.3246)\n",
      "38138 Training Loss: tensor(0.3255)\n",
      "38139 Training Loss: tensor(0.3254)\n",
      "38140 Training Loss: tensor(0.3253)\n",
      "38141 Training Loss: tensor(0.3240)\n",
      "38142 Training Loss: tensor(0.3250)\n",
      "38143 Training Loss: tensor(0.3255)\n",
      "38144 Training Loss: tensor(0.3248)\n",
      "38145 Training Loss: tensor(0.3249)\n",
      "38146 Training Loss: tensor(0.3246)\n",
      "38147 Training Loss: tensor(0.3250)\n",
      "38148 Training Loss: tensor(0.3253)\n",
      "38149 Training Loss: tensor(0.3253)\n",
      "38150 Training Loss: tensor(0.3249)\n",
      "38151 Training Loss: tensor(0.3251)\n",
      "38152 Training Loss: tensor(0.3245)\n",
      "38153 Training Loss: tensor(0.3255)\n",
      "38154 Training Loss: tensor(0.3241)\n",
      "38155 Training Loss: tensor(0.3244)\n",
      "38156 Training Loss: tensor(0.3248)\n",
      "38157 Training Loss: tensor(0.3243)\n",
      "38158 Training Loss: tensor(0.3248)\n",
      "38159 Training Loss: tensor(0.3245)\n",
      "38160 Training Loss: tensor(0.3251)\n",
      "38161 Training Loss: tensor(0.3249)\n",
      "38162 Training Loss: tensor(0.3243)\n",
      "38163 Training Loss: tensor(0.3249)\n",
      "38164 Training Loss: tensor(0.3240)\n",
      "38165 Training Loss: tensor(0.3269)\n",
      "38166 Training Loss: tensor(0.3242)\n",
      "38167 Training Loss: tensor(0.3243)\n",
      "38168 Training Loss: tensor(0.3237)\n",
      "38169 Training Loss: tensor(0.3252)\n",
      "38170 Training Loss: tensor(0.3245)\n",
      "38171 Training Loss: tensor(0.3247)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38172 Training Loss: tensor(0.3247)\n",
      "38173 Training Loss: tensor(0.3244)\n",
      "38174 Training Loss: tensor(0.3244)\n",
      "38175 Training Loss: tensor(0.3240)\n",
      "38176 Training Loss: tensor(0.3245)\n",
      "38177 Training Loss: tensor(0.3243)\n",
      "38178 Training Loss: tensor(0.3254)\n",
      "38179 Training Loss: tensor(0.3243)\n",
      "38180 Training Loss: tensor(0.3260)\n",
      "38181 Training Loss: tensor(0.3244)\n",
      "38182 Training Loss: tensor(0.3246)\n",
      "38183 Training Loss: tensor(0.3239)\n",
      "38184 Training Loss: tensor(0.3245)\n",
      "38185 Training Loss: tensor(0.3239)\n",
      "38186 Training Loss: tensor(0.3242)\n",
      "38187 Training Loss: tensor(0.3240)\n",
      "38188 Training Loss: tensor(0.3250)\n",
      "38189 Training Loss: tensor(0.3244)\n",
      "38190 Training Loss: tensor(0.3265)\n",
      "38191 Training Loss: tensor(0.3247)\n",
      "38192 Training Loss: tensor(0.3254)\n",
      "38193 Training Loss: tensor(0.3250)\n",
      "38194 Training Loss: tensor(0.3250)\n",
      "38195 Training Loss: tensor(0.3251)\n",
      "38196 Training Loss: tensor(0.3253)\n",
      "38197 Training Loss: tensor(0.3242)\n",
      "38198 Training Loss: tensor(0.3256)\n",
      "38199 Training Loss: tensor(0.3245)\n",
      "38200 Training Loss: tensor(0.3243)\n",
      "38201 Training Loss: tensor(0.3245)\n",
      "38202 Training Loss: tensor(0.3239)\n",
      "38203 Training Loss: tensor(0.3253)\n",
      "38204 Training Loss: tensor(0.3241)\n",
      "38205 Training Loss: tensor(0.3242)\n",
      "38206 Training Loss: tensor(0.3250)\n",
      "38207 Training Loss: tensor(0.3248)\n",
      "38208 Training Loss: tensor(0.3247)\n",
      "38209 Training Loss: tensor(0.3245)\n",
      "38210 Training Loss: tensor(0.3254)\n",
      "38211 Training Loss: tensor(0.3247)\n",
      "38212 Training Loss: tensor(0.3248)\n",
      "38213 Training Loss: tensor(0.3244)\n",
      "38214 Training Loss: tensor(0.3257)\n",
      "38215 Training Loss: tensor(0.3246)\n",
      "38216 Training Loss: tensor(0.3256)\n",
      "38217 Training Loss: tensor(0.3256)\n",
      "38218 Training Loss: tensor(0.3243)\n",
      "38219 Training Loss: tensor(0.3255)\n",
      "38220 Training Loss: tensor(0.3246)\n",
      "38221 Training Loss: tensor(0.3247)\n",
      "38222 Training Loss: tensor(0.3245)\n",
      "38223 Training Loss: tensor(0.3248)\n",
      "38224 Training Loss: tensor(0.3250)\n",
      "38225 Training Loss: tensor(0.3258)\n",
      "38226 Training Loss: tensor(0.3245)\n",
      "38227 Training Loss: tensor(0.3257)\n",
      "38228 Training Loss: tensor(0.3243)\n",
      "38229 Training Loss: tensor(0.3242)\n",
      "38230 Training Loss: tensor(0.3252)\n",
      "38231 Training Loss: tensor(0.3246)\n",
      "38232 Training Loss: tensor(0.3247)\n",
      "38233 Training Loss: tensor(0.3242)\n",
      "38234 Training Loss: tensor(0.3254)\n",
      "38235 Training Loss: tensor(0.3248)\n",
      "38236 Training Loss: tensor(0.3252)\n",
      "38237 Training Loss: tensor(0.3240)\n",
      "38238 Training Loss: tensor(0.3266)\n",
      "38239 Training Loss: tensor(0.3240)\n",
      "38240 Training Loss: tensor(0.3254)\n",
      "38241 Training Loss: tensor(0.3245)\n",
      "38242 Training Loss: tensor(0.3247)\n",
      "38243 Training Loss: tensor(0.3249)\n",
      "38244 Training Loss: tensor(0.3250)\n",
      "38245 Training Loss: tensor(0.3256)\n",
      "38246 Training Loss: tensor(0.3251)\n",
      "38247 Training Loss: tensor(0.3252)\n",
      "38248 Training Loss: tensor(0.3249)\n",
      "38249 Training Loss: tensor(0.3242)\n",
      "38250 Training Loss: tensor(0.3249)\n",
      "38251 Training Loss: tensor(0.3257)\n",
      "38252 Training Loss: tensor(0.3251)\n",
      "38253 Training Loss: tensor(0.3252)\n",
      "38254 Training Loss: tensor(0.3255)\n",
      "38255 Training Loss: tensor(0.3247)\n",
      "38256 Training Loss: tensor(0.3241)\n",
      "38257 Training Loss: tensor(0.3249)\n",
      "38258 Training Loss: tensor(0.3250)\n",
      "38259 Training Loss: tensor(0.3252)\n",
      "38260 Training Loss: tensor(0.3247)\n",
      "38261 Training Loss: tensor(0.3244)\n",
      "38262 Training Loss: tensor(0.3244)\n",
      "38263 Training Loss: tensor(0.3240)\n",
      "38264 Training Loss: tensor(0.3253)\n",
      "38265 Training Loss: tensor(0.3249)\n",
      "38266 Training Loss: tensor(0.3250)\n",
      "38267 Training Loss: tensor(0.3247)\n",
      "38268 Training Loss: tensor(0.3252)\n",
      "38269 Training Loss: tensor(0.3251)\n",
      "38270 Training Loss: tensor(0.3248)\n",
      "38271 Training Loss: tensor(0.3254)\n",
      "38272 Training Loss: tensor(0.3251)\n",
      "38273 Training Loss: tensor(0.3251)\n",
      "38274 Training Loss: tensor(0.3247)\n",
      "38275 Training Loss: tensor(0.3252)\n",
      "38276 Training Loss: tensor(0.3259)\n",
      "38277 Training Loss: tensor(0.3247)\n",
      "38278 Training Loss: tensor(0.3242)\n",
      "38279 Training Loss: tensor(0.3240)\n",
      "38280 Training Loss: tensor(0.3260)\n",
      "38281 Training Loss: tensor(0.3245)\n",
      "38282 Training Loss: tensor(0.3250)\n",
      "38283 Training Loss: tensor(0.3242)\n",
      "38284 Training Loss: tensor(0.3245)\n",
      "38285 Training Loss: tensor(0.3244)\n",
      "38286 Training Loss: tensor(0.3244)\n",
      "38287 Training Loss: tensor(0.3249)\n",
      "38288 Training Loss: tensor(0.3245)\n",
      "38289 Training Loss: tensor(0.3243)\n",
      "38290 Training Loss: tensor(0.3247)\n",
      "38291 Training Loss: tensor(0.3250)\n",
      "38292 Training Loss: tensor(0.3246)\n",
      "38293 Training Loss: tensor(0.3243)\n",
      "38294 Training Loss: tensor(0.3240)\n",
      "38295 Training Loss: tensor(0.3244)\n",
      "38296 Training Loss: tensor(0.3247)\n",
      "38297 Training Loss: tensor(0.3243)\n",
      "38298 Training Loss: tensor(0.3245)\n",
      "38299 Training Loss: tensor(0.3240)\n",
      "38300 Training Loss: tensor(0.3248)\n",
      "38301 Training Loss: tensor(0.3243)\n",
      "38302 Training Loss: tensor(0.3242)\n",
      "38303 Training Loss: tensor(0.3251)\n",
      "38304 Training Loss: tensor(0.3243)\n",
      "38305 Training Loss: tensor(0.3261)\n",
      "38306 Training Loss: tensor(0.3249)\n",
      "38307 Training Loss: tensor(0.3250)\n",
      "38308 Training Loss: tensor(0.3242)\n",
      "38309 Training Loss: tensor(0.3241)\n",
      "38310 Training Loss: tensor(0.3259)\n",
      "38311 Training Loss: tensor(0.3243)\n",
      "38312 Training Loss: tensor(0.3249)\n",
      "38313 Training Loss: tensor(0.3252)\n",
      "38314 Training Loss: tensor(0.3245)\n",
      "38315 Training Loss: tensor(0.3252)\n",
      "38316 Training Loss: tensor(0.3243)\n",
      "38317 Training Loss: tensor(0.3257)\n",
      "38318 Training Loss: tensor(0.3240)\n",
      "38319 Training Loss: tensor(0.3240)\n",
      "38320 Training Loss: tensor(0.3256)\n",
      "38321 Training Loss: tensor(0.3259)\n",
      "38322 Training Loss: tensor(0.3244)\n",
      "38323 Training Loss: tensor(0.3248)\n",
      "38324 Training Loss: tensor(0.3247)\n",
      "38325 Training Loss: tensor(0.3246)\n",
      "38326 Training Loss: tensor(0.3241)\n",
      "38327 Training Loss: tensor(0.3252)\n",
      "38328 Training Loss: tensor(0.3253)\n",
      "38329 Training Loss: tensor(0.3246)\n",
      "38330 Training Loss: tensor(0.3253)\n",
      "38331 Training Loss: tensor(0.3238)\n",
      "38332 Training Loss: tensor(0.3250)\n",
      "38333 Training Loss: tensor(0.3259)\n",
      "38334 Training Loss: tensor(0.3263)\n",
      "38335 Training Loss: tensor(0.3246)\n",
      "38336 Training Loss: tensor(0.3245)\n",
      "38337 Training Loss: tensor(0.3259)\n",
      "38338 Training Loss: tensor(0.3261)\n",
      "38339 Training Loss: tensor(0.3242)\n",
      "38340 Training Loss: tensor(0.3250)\n",
      "38341 Training Loss: tensor(0.3249)\n",
      "38342 Training Loss: tensor(0.3249)\n",
      "38343 Training Loss: tensor(0.3244)\n",
      "38344 Training Loss: tensor(0.3245)\n",
      "38345 Training Loss: tensor(0.3247)\n",
      "38346 Training Loss: tensor(0.3249)\n",
      "38347 Training Loss: tensor(0.3249)\n",
      "38348 Training Loss: tensor(0.3251)\n",
      "38349 Training Loss: tensor(0.3249)\n",
      "38350 Training Loss: tensor(0.3249)\n",
      "38351 Training Loss: tensor(0.3261)\n",
      "38352 Training Loss: tensor(0.3245)\n",
      "38353 Training Loss: tensor(0.3247)\n",
      "38354 Training Loss: tensor(0.3244)\n",
      "38355 Training Loss: tensor(0.3247)\n",
      "38356 Training Loss: tensor(0.3242)\n",
      "38357 Training Loss: tensor(0.3249)\n",
      "38358 Training Loss: tensor(0.3241)\n",
      "38359 Training Loss: tensor(0.3247)\n",
      "38360 Training Loss: tensor(0.3256)\n",
      "38361 Training Loss: tensor(0.3251)\n",
      "38362 Training Loss: tensor(0.3255)\n",
      "38363 Training Loss: tensor(0.3242)\n",
      "38364 Training Loss: tensor(0.3253)\n",
      "38365 Training Loss: tensor(0.3256)\n",
      "38366 Training Loss: tensor(0.3246)\n",
      "38367 Training Loss: tensor(0.3253)\n",
      "38368 Training Loss: tensor(0.3243)\n",
      "38369 Training Loss: tensor(0.3247)\n",
      "38370 Training Loss: tensor(0.3241)\n",
      "38371 Training Loss: tensor(0.3247)\n",
      "38372 Training Loss: tensor(0.3239)\n",
      "38373 Training Loss: tensor(0.3238)\n",
      "38374 Training Loss: tensor(0.3243)\n",
      "38375 Training Loss: tensor(0.3254)\n",
      "38376 Training Loss: tensor(0.3243)\n",
      "38377 Training Loss: tensor(0.3248)\n",
      "38378 Training Loss: tensor(0.3246)\n",
      "38379 Training Loss: tensor(0.3251)\n",
      "38380 Training Loss: tensor(0.3241)\n",
      "38381 Training Loss: tensor(0.3244)\n",
      "38382 Training Loss: tensor(0.3248)\n",
      "38383 Training Loss: tensor(0.3241)\n",
      "38384 Training Loss: tensor(0.3248)\n",
      "38385 Training Loss: tensor(0.3235)\n",
      "38386 Training Loss: tensor(0.3248)\n",
      "38387 Training Loss: tensor(0.3247)\n",
      "38388 Training Loss: tensor(0.3242)\n",
      "38389 Training Loss: tensor(0.3252)\n",
      "38390 Training Loss: tensor(0.3254)\n",
      "38391 Training Loss: tensor(0.3241)\n",
      "38392 Training Loss: tensor(0.3250)\n",
      "38393 Training Loss: tensor(0.3240)\n",
      "38394 Training Loss: tensor(0.3250)\n",
      "38395 Training Loss: tensor(0.3252)\n",
      "38396 Training Loss: tensor(0.3247)\n",
      "38397 Training Loss: tensor(0.3243)\n",
      "38398 Training Loss: tensor(0.3249)\n",
      "38399 Training Loss: tensor(0.3252)\n",
      "38400 Training Loss: tensor(0.3257)\n",
      "38401 Training Loss: tensor(0.3248)\n",
      "38402 Training Loss: tensor(0.3250)\n",
      "38403 Training Loss: tensor(0.3246)\n",
      "38404 Training Loss: tensor(0.3244)\n",
      "38405 Training Loss: tensor(0.3253)\n",
      "38406 Training Loss: tensor(0.3251)\n",
      "38407 Training Loss: tensor(0.3242)\n",
      "38408 Training Loss: tensor(0.3256)\n",
      "38409 Training Loss: tensor(0.3248)\n",
      "38410 Training Loss: tensor(0.3246)\n",
      "38411 Training Loss: tensor(0.3247)\n",
      "38412 Training Loss: tensor(0.3259)\n",
      "38413 Training Loss: tensor(0.3243)\n",
      "38414 Training Loss: tensor(0.3254)\n",
      "38415 Training Loss: tensor(0.3253)\n",
      "38416 Training Loss: tensor(0.3247)\n",
      "38417 Training Loss: tensor(0.3250)\n",
      "38418 Training Loss: tensor(0.3243)\n",
      "38419 Training Loss: tensor(0.3258)\n",
      "38420 Training Loss: tensor(0.3247)\n",
      "38421 Training Loss: tensor(0.3251)\n",
      "38422 Training Loss: tensor(0.3246)\n",
      "38423 Training Loss: tensor(0.3242)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38424 Training Loss: tensor(0.3243)\n",
      "38425 Training Loss: tensor(0.3240)\n",
      "38426 Training Loss: tensor(0.3249)\n",
      "38427 Training Loss: tensor(0.3270)\n",
      "38428 Training Loss: tensor(0.3240)\n",
      "38429 Training Loss: tensor(0.3254)\n",
      "38430 Training Loss: tensor(0.3252)\n",
      "38431 Training Loss: tensor(0.3254)\n",
      "38432 Training Loss: tensor(0.3249)\n",
      "38433 Training Loss: tensor(0.3240)\n",
      "38434 Training Loss: tensor(0.3242)\n",
      "38435 Training Loss: tensor(0.3258)\n",
      "38436 Training Loss: tensor(0.3242)\n",
      "38437 Training Loss: tensor(0.3249)\n",
      "38438 Training Loss: tensor(0.3248)\n",
      "38439 Training Loss: tensor(0.3256)\n",
      "38440 Training Loss: tensor(0.3247)\n",
      "38441 Training Loss: tensor(0.3253)\n",
      "38442 Training Loss: tensor(0.3248)\n",
      "38443 Training Loss: tensor(0.3249)\n",
      "38444 Training Loss: tensor(0.3248)\n",
      "38445 Training Loss: tensor(0.3244)\n",
      "38446 Training Loss: tensor(0.3243)\n",
      "38447 Training Loss: tensor(0.3252)\n",
      "38448 Training Loss: tensor(0.3242)\n",
      "38449 Training Loss: tensor(0.3252)\n",
      "38450 Training Loss: tensor(0.3246)\n",
      "38451 Training Loss: tensor(0.3247)\n",
      "38452 Training Loss: tensor(0.3243)\n",
      "38453 Training Loss: tensor(0.3245)\n",
      "38454 Training Loss: tensor(0.3247)\n",
      "38455 Training Loss: tensor(0.3239)\n",
      "38456 Training Loss: tensor(0.3249)\n",
      "38457 Training Loss: tensor(0.3258)\n",
      "38458 Training Loss: tensor(0.3247)\n",
      "38459 Training Loss: tensor(0.3242)\n",
      "38460 Training Loss: tensor(0.3247)\n",
      "38461 Training Loss: tensor(0.3245)\n",
      "38462 Training Loss: tensor(0.3249)\n",
      "38463 Training Loss: tensor(0.3254)\n",
      "38464 Training Loss: tensor(0.3242)\n",
      "38465 Training Loss: tensor(0.3246)\n",
      "38466 Training Loss: tensor(0.3250)\n",
      "38467 Training Loss: tensor(0.3244)\n",
      "38468 Training Loss: tensor(0.3257)\n",
      "38469 Training Loss: tensor(0.3248)\n",
      "38470 Training Loss: tensor(0.3243)\n",
      "38471 Training Loss: tensor(0.3240)\n",
      "38472 Training Loss: tensor(0.3245)\n",
      "38473 Training Loss: tensor(0.3247)\n",
      "38474 Training Loss: tensor(0.3246)\n",
      "38475 Training Loss: tensor(0.3246)\n",
      "38476 Training Loss: tensor(0.3250)\n",
      "38477 Training Loss: tensor(0.3244)\n",
      "38478 Training Loss: tensor(0.3253)\n",
      "38479 Training Loss: tensor(0.3252)\n",
      "38480 Training Loss: tensor(0.3253)\n",
      "38481 Training Loss: tensor(0.3251)\n",
      "38482 Training Loss: tensor(0.3247)\n",
      "38483 Training Loss: tensor(0.3243)\n",
      "38484 Training Loss: tensor(0.3249)\n",
      "38485 Training Loss: tensor(0.3245)\n",
      "38486 Training Loss: tensor(0.3256)\n",
      "38487 Training Loss: tensor(0.3248)\n",
      "38488 Training Loss: tensor(0.3244)\n",
      "38489 Training Loss: tensor(0.3248)\n",
      "38490 Training Loss: tensor(0.3246)\n",
      "38491 Training Loss: tensor(0.3248)\n",
      "38492 Training Loss: tensor(0.3245)\n",
      "38493 Training Loss: tensor(0.3246)\n",
      "38494 Training Loss: tensor(0.3246)\n",
      "38495 Training Loss: tensor(0.3246)\n",
      "38496 Training Loss: tensor(0.3243)\n",
      "38497 Training Loss: tensor(0.3245)\n",
      "38498 Training Loss: tensor(0.3247)\n",
      "38499 Training Loss: tensor(0.3248)\n",
      "38500 Training Loss: tensor(0.3260)\n",
      "38501 Training Loss: tensor(0.3251)\n",
      "38502 Training Loss: tensor(0.3246)\n",
      "38503 Training Loss: tensor(0.3254)\n",
      "38504 Training Loss: tensor(0.3239)\n",
      "38505 Training Loss: tensor(0.3256)\n",
      "38506 Training Loss: tensor(0.3260)\n",
      "38507 Training Loss: tensor(0.3245)\n",
      "38508 Training Loss: tensor(0.3251)\n",
      "38509 Training Loss: tensor(0.3250)\n",
      "38510 Training Loss: tensor(0.3247)\n",
      "38511 Training Loss: tensor(0.3243)\n",
      "38512 Training Loss: tensor(0.3242)\n",
      "38513 Training Loss: tensor(0.3246)\n",
      "38514 Training Loss: tensor(0.3249)\n",
      "38515 Training Loss: tensor(0.3257)\n",
      "38516 Training Loss: tensor(0.3257)\n",
      "38517 Training Loss: tensor(0.3254)\n",
      "38518 Training Loss: tensor(0.3248)\n",
      "38519 Training Loss: tensor(0.3247)\n",
      "38520 Training Loss: tensor(0.3251)\n",
      "38521 Training Loss: tensor(0.3244)\n",
      "38522 Training Loss: tensor(0.3248)\n",
      "38523 Training Loss: tensor(0.3249)\n",
      "38524 Training Loss: tensor(0.3244)\n",
      "38525 Training Loss: tensor(0.3247)\n",
      "38526 Training Loss: tensor(0.3256)\n",
      "38527 Training Loss: tensor(0.3252)\n",
      "38528 Training Loss: tensor(0.3242)\n",
      "38529 Training Loss: tensor(0.3244)\n",
      "38530 Training Loss: tensor(0.3250)\n",
      "38531 Training Loss: tensor(0.3249)\n",
      "38532 Training Loss: tensor(0.3250)\n",
      "38533 Training Loss: tensor(0.3261)\n",
      "38534 Training Loss: tensor(0.3241)\n",
      "38535 Training Loss: tensor(0.3246)\n",
      "38536 Training Loss: tensor(0.3241)\n",
      "38537 Training Loss: tensor(0.3255)\n",
      "38538 Training Loss: tensor(0.3257)\n",
      "38539 Training Loss: tensor(0.3248)\n",
      "38540 Training Loss: tensor(0.3245)\n",
      "38541 Training Loss: tensor(0.3246)\n",
      "38542 Training Loss: tensor(0.3249)\n",
      "38543 Training Loss: tensor(0.3248)\n",
      "38544 Training Loss: tensor(0.3244)\n",
      "38545 Training Loss: tensor(0.3250)\n",
      "38546 Training Loss: tensor(0.3245)\n",
      "38547 Training Loss: tensor(0.3244)\n",
      "38548 Training Loss: tensor(0.3256)\n",
      "38549 Training Loss: tensor(0.3249)\n",
      "38550 Training Loss: tensor(0.3246)\n",
      "38551 Training Loss: tensor(0.3245)\n",
      "38552 Training Loss: tensor(0.3254)\n",
      "38553 Training Loss: tensor(0.3252)\n",
      "38554 Training Loss: tensor(0.3252)\n",
      "38555 Training Loss: tensor(0.3251)\n",
      "38556 Training Loss: tensor(0.3251)\n",
      "38557 Training Loss: tensor(0.3250)\n",
      "38558 Training Loss: tensor(0.3242)\n",
      "38559 Training Loss: tensor(0.3249)\n",
      "38560 Training Loss: tensor(0.3247)\n",
      "38561 Training Loss: tensor(0.3251)\n",
      "38562 Training Loss: tensor(0.3246)\n",
      "38563 Training Loss: tensor(0.3256)\n",
      "38564 Training Loss: tensor(0.3247)\n",
      "38565 Training Loss: tensor(0.3247)\n",
      "38566 Training Loss: tensor(0.3241)\n",
      "38567 Training Loss: tensor(0.3245)\n",
      "38568 Training Loss: tensor(0.3252)\n",
      "38569 Training Loss: tensor(0.3241)\n",
      "38570 Training Loss: tensor(0.3248)\n",
      "38571 Training Loss: tensor(0.3246)\n",
      "38572 Training Loss: tensor(0.3259)\n",
      "38573 Training Loss: tensor(0.3249)\n",
      "38574 Training Loss: tensor(0.3251)\n",
      "38575 Training Loss: tensor(0.3244)\n",
      "38576 Training Loss: tensor(0.3242)\n",
      "38577 Training Loss: tensor(0.3247)\n",
      "38578 Training Loss: tensor(0.3252)\n",
      "38579 Training Loss: tensor(0.3255)\n",
      "38580 Training Loss: tensor(0.3254)\n",
      "38581 Training Loss: tensor(0.3246)\n",
      "38582 Training Loss: tensor(0.3243)\n",
      "38583 Training Loss: tensor(0.3247)\n",
      "38584 Training Loss: tensor(0.3243)\n",
      "38585 Training Loss: tensor(0.3248)\n",
      "38586 Training Loss: tensor(0.3252)\n",
      "38587 Training Loss: tensor(0.3256)\n",
      "38588 Training Loss: tensor(0.3246)\n",
      "38589 Training Loss: tensor(0.3244)\n",
      "38590 Training Loss: tensor(0.3254)\n",
      "38591 Training Loss: tensor(0.3251)\n",
      "38592 Training Loss: tensor(0.3255)\n",
      "38593 Training Loss: tensor(0.3254)\n",
      "38594 Training Loss: tensor(0.3247)\n",
      "38595 Training Loss: tensor(0.3243)\n",
      "38596 Training Loss: tensor(0.3244)\n",
      "38597 Training Loss: tensor(0.3247)\n",
      "38598 Training Loss: tensor(0.3241)\n",
      "38599 Training Loss: tensor(0.3247)\n",
      "38600 Training Loss: tensor(0.3239)\n",
      "38601 Training Loss: tensor(0.3243)\n",
      "38602 Training Loss: tensor(0.3251)\n",
      "38603 Training Loss: tensor(0.3244)\n",
      "38604 Training Loss: tensor(0.3255)\n",
      "38605 Training Loss: tensor(0.3248)\n",
      "38606 Training Loss: tensor(0.3251)\n",
      "38607 Training Loss: tensor(0.3244)\n",
      "38608 Training Loss: tensor(0.3245)\n",
      "38609 Training Loss: tensor(0.3245)\n",
      "38610 Training Loss: tensor(0.3243)\n",
      "38611 Training Loss: tensor(0.3241)\n",
      "38612 Training Loss: tensor(0.3252)\n",
      "38613 Training Loss: tensor(0.3248)\n",
      "38614 Training Loss: tensor(0.3249)\n",
      "38615 Training Loss: tensor(0.3244)\n",
      "38616 Training Loss: tensor(0.3251)\n",
      "38617 Training Loss: tensor(0.3245)\n",
      "38618 Training Loss: tensor(0.3251)\n",
      "38619 Training Loss: tensor(0.3251)\n",
      "38620 Training Loss: tensor(0.3254)\n",
      "38621 Training Loss: tensor(0.3249)\n",
      "38622 Training Loss: tensor(0.3247)\n",
      "38623 Training Loss: tensor(0.3243)\n",
      "38624 Training Loss: tensor(0.3243)\n",
      "38625 Training Loss: tensor(0.3254)\n",
      "38626 Training Loss: tensor(0.3247)\n",
      "38627 Training Loss: tensor(0.3248)\n",
      "38628 Training Loss: tensor(0.3252)\n",
      "38629 Training Loss: tensor(0.3247)\n",
      "38630 Training Loss: tensor(0.3247)\n",
      "38631 Training Loss: tensor(0.3248)\n",
      "38632 Training Loss: tensor(0.3242)\n",
      "38633 Training Loss: tensor(0.3246)\n",
      "38634 Training Loss: tensor(0.3248)\n",
      "38635 Training Loss: tensor(0.3246)\n",
      "38636 Training Loss: tensor(0.3251)\n",
      "38637 Training Loss: tensor(0.3242)\n",
      "38638 Training Loss: tensor(0.3244)\n",
      "38639 Training Loss: tensor(0.3260)\n",
      "38640 Training Loss: tensor(0.3243)\n",
      "38641 Training Loss: tensor(0.3241)\n",
      "38642 Training Loss: tensor(0.3252)\n",
      "38643 Training Loss: tensor(0.3247)\n",
      "38644 Training Loss: tensor(0.3250)\n",
      "38645 Training Loss: tensor(0.3244)\n",
      "38646 Training Loss: tensor(0.3254)\n",
      "38647 Training Loss: tensor(0.3259)\n",
      "38648 Training Loss: tensor(0.3256)\n",
      "38649 Training Loss: tensor(0.3244)\n",
      "38650 Training Loss: tensor(0.3247)\n",
      "38651 Training Loss: tensor(0.3245)\n",
      "38652 Training Loss: tensor(0.3263)\n",
      "38653 Training Loss: tensor(0.3249)\n",
      "38654 Training Loss: tensor(0.3247)\n",
      "38655 Training Loss: tensor(0.3246)\n",
      "38656 Training Loss: tensor(0.3239)\n",
      "38657 Training Loss: tensor(0.3255)\n",
      "38658 Training Loss: tensor(0.3240)\n",
      "38659 Training Loss: tensor(0.3243)\n",
      "38660 Training Loss: tensor(0.3249)\n",
      "38661 Training Loss: tensor(0.3241)\n",
      "38662 Training Loss: tensor(0.3250)\n",
      "38663 Training Loss: tensor(0.3245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38664 Training Loss: tensor(0.3248)\n",
      "38665 Training Loss: tensor(0.3249)\n",
      "38666 Training Loss: tensor(0.3247)\n",
      "38667 Training Loss: tensor(0.3272)\n",
      "38668 Training Loss: tensor(0.3244)\n",
      "38669 Training Loss: tensor(0.3252)\n",
      "38670 Training Loss: tensor(0.3246)\n",
      "38671 Training Loss: tensor(0.3251)\n",
      "38672 Training Loss: tensor(0.3242)\n",
      "38673 Training Loss: tensor(0.3256)\n",
      "38674 Training Loss: tensor(0.3245)\n",
      "38675 Training Loss: tensor(0.3249)\n",
      "38676 Training Loss: tensor(0.3251)\n",
      "38677 Training Loss: tensor(0.3244)\n",
      "38678 Training Loss: tensor(0.3244)\n",
      "38679 Training Loss: tensor(0.3241)\n",
      "38680 Training Loss: tensor(0.3250)\n",
      "38681 Training Loss: tensor(0.3250)\n",
      "38682 Training Loss: tensor(0.3249)\n",
      "38683 Training Loss: tensor(0.3254)\n",
      "38684 Training Loss: tensor(0.3244)\n",
      "38685 Training Loss: tensor(0.3252)\n",
      "38686 Training Loss: tensor(0.3239)\n",
      "38687 Training Loss: tensor(0.3250)\n",
      "38688 Training Loss: tensor(0.3251)\n",
      "38689 Training Loss: tensor(0.3259)\n",
      "38690 Training Loss: tensor(0.3247)\n",
      "38691 Training Loss: tensor(0.3247)\n",
      "38692 Training Loss: tensor(0.3246)\n",
      "38693 Training Loss: tensor(0.3253)\n",
      "38694 Training Loss: tensor(0.3252)\n",
      "38695 Training Loss: tensor(0.3244)\n",
      "38696 Training Loss: tensor(0.3239)\n",
      "38697 Training Loss: tensor(0.3254)\n",
      "38698 Training Loss: tensor(0.3251)\n",
      "38699 Training Loss: tensor(0.3249)\n",
      "38700 Training Loss: tensor(0.3246)\n",
      "38701 Training Loss: tensor(0.3247)\n",
      "38702 Training Loss: tensor(0.3247)\n",
      "38703 Training Loss: tensor(0.3245)\n",
      "38704 Training Loss: tensor(0.3257)\n",
      "38705 Training Loss: tensor(0.3242)\n",
      "38706 Training Loss: tensor(0.3243)\n",
      "38707 Training Loss: tensor(0.3246)\n",
      "38708 Training Loss: tensor(0.3246)\n",
      "38709 Training Loss: tensor(0.3249)\n",
      "38710 Training Loss: tensor(0.3247)\n",
      "38711 Training Loss: tensor(0.3261)\n",
      "38712 Training Loss: tensor(0.3245)\n",
      "38713 Training Loss: tensor(0.3263)\n",
      "38714 Training Loss: tensor(0.3250)\n",
      "38715 Training Loss: tensor(0.3248)\n",
      "38716 Training Loss: tensor(0.3244)\n",
      "38717 Training Loss: tensor(0.3240)\n",
      "38718 Training Loss: tensor(0.3244)\n",
      "38719 Training Loss: tensor(0.3243)\n",
      "38720 Training Loss: tensor(0.3252)\n",
      "38721 Training Loss: tensor(0.3250)\n",
      "38722 Training Loss: tensor(0.3252)\n",
      "38723 Training Loss: tensor(0.3245)\n",
      "38724 Training Loss: tensor(0.3242)\n",
      "38725 Training Loss: tensor(0.3249)\n",
      "38726 Training Loss: tensor(0.3255)\n",
      "38727 Training Loss: tensor(0.3247)\n",
      "38728 Training Loss: tensor(0.3247)\n",
      "38729 Training Loss: tensor(0.3240)\n",
      "38730 Training Loss: tensor(0.3244)\n",
      "38731 Training Loss: tensor(0.3255)\n",
      "38732 Training Loss: tensor(0.3239)\n",
      "38733 Training Loss: tensor(0.3246)\n",
      "38734 Training Loss: tensor(0.3246)\n",
      "38735 Training Loss: tensor(0.3248)\n",
      "38736 Training Loss: tensor(0.3249)\n",
      "38737 Training Loss: tensor(0.3242)\n",
      "38738 Training Loss: tensor(0.3240)\n",
      "38739 Training Loss: tensor(0.3251)\n",
      "38740 Training Loss: tensor(0.3243)\n",
      "38741 Training Loss: tensor(0.3238)\n",
      "38742 Training Loss: tensor(0.3249)\n",
      "38743 Training Loss: tensor(0.3253)\n",
      "38744 Training Loss: tensor(0.3244)\n",
      "38745 Training Loss: tensor(0.3250)\n",
      "38746 Training Loss: tensor(0.3247)\n",
      "38747 Training Loss: tensor(0.3249)\n",
      "38748 Training Loss: tensor(0.3240)\n",
      "38749 Training Loss: tensor(0.3245)\n",
      "38750 Training Loss: tensor(0.3252)\n",
      "38751 Training Loss: tensor(0.3249)\n",
      "38752 Training Loss: tensor(0.3248)\n",
      "38753 Training Loss: tensor(0.3253)\n",
      "38754 Training Loss: tensor(0.3243)\n",
      "38755 Training Loss: tensor(0.3245)\n",
      "38756 Training Loss: tensor(0.3247)\n",
      "38757 Training Loss: tensor(0.3242)\n",
      "38758 Training Loss: tensor(0.3258)\n",
      "38759 Training Loss: tensor(0.3249)\n",
      "38760 Training Loss: tensor(0.3247)\n",
      "38761 Training Loss: tensor(0.3250)\n",
      "38762 Training Loss: tensor(0.3247)\n",
      "38763 Training Loss: tensor(0.3242)\n",
      "38764 Training Loss: tensor(0.3243)\n",
      "38765 Training Loss: tensor(0.3247)\n",
      "38766 Training Loss: tensor(0.3264)\n",
      "38767 Training Loss: tensor(0.3243)\n",
      "38768 Training Loss: tensor(0.3242)\n",
      "38769 Training Loss: tensor(0.3243)\n",
      "38770 Training Loss: tensor(0.3246)\n",
      "38771 Training Loss: tensor(0.3242)\n",
      "38772 Training Loss: tensor(0.3250)\n",
      "38773 Training Loss: tensor(0.3248)\n",
      "38774 Training Loss: tensor(0.3252)\n",
      "38775 Training Loss: tensor(0.3240)\n",
      "38776 Training Loss: tensor(0.3256)\n",
      "38777 Training Loss: tensor(0.3243)\n",
      "38778 Training Loss: tensor(0.3246)\n",
      "38779 Training Loss: tensor(0.3247)\n",
      "38780 Training Loss: tensor(0.3248)\n",
      "38781 Training Loss: tensor(0.3248)\n",
      "38782 Training Loss: tensor(0.3247)\n",
      "38783 Training Loss: tensor(0.3241)\n",
      "38784 Training Loss: tensor(0.3240)\n",
      "38785 Training Loss: tensor(0.3251)\n",
      "38786 Training Loss: tensor(0.3255)\n",
      "38787 Training Loss: tensor(0.3251)\n",
      "38788 Training Loss: tensor(0.3251)\n",
      "38789 Training Loss: tensor(0.3249)\n",
      "38790 Training Loss: tensor(0.3245)\n",
      "38791 Training Loss: tensor(0.3243)\n",
      "38792 Training Loss: tensor(0.3243)\n",
      "38793 Training Loss: tensor(0.3247)\n",
      "38794 Training Loss: tensor(0.3245)\n",
      "38795 Training Loss: tensor(0.3251)\n",
      "38796 Training Loss: tensor(0.3249)\n",
      "38797 Training Loss: tensor(0.3245)\n",
      "38798 Training Loss: tensor(0.3247)\n",
      "38799 Training Loss: tensor(0.3240)\n",
      "38800 Training Loss: tensor(0.3243)\n",
      "38801 Training Loss: tensor(0.3244)\n",
      "38802 Training Loss: tensor(0.3252)\n",
      "38803 Training Loss: tensor(0.3251)\n",
      "38804 Training Loss: tensor(0.3261)\n",
      "38805 Training Loss: tensor(0.3241)\n",
      "38806 Training Loss: tensor(0.3243)\n",
      "38807 Training Loss: tensor(0.3237)\n",
      "38808 Training Loss: tensor(0.3240)\n",
      "38809 Training Loss: tensor(0.3241)\n",
      "38810 Training Loss: tensor(0.3253)\n",
      "38811 Training Loss: tensor(0.3262)\n",
      "38812 Training Loss: tensor(0.3254)\n",
      "38813 Training Loss: tensor(0.3241)\n",
      "38814 Training Loss: tensor(0.3241)\n",
      "38815 Training Loss: tensor(0.3249)\n",
      "38816 Training Loss: tensor(0.3245)\n",
      "38817 Training Loss: tensor(0.3245)\n",
      "38818 Training Loss: tensor(0.3239)\n",
      "38819 Training Loss: tensor(0.3245)\n",
      "38820 Training Loss: tensor(0.3244)\n",
      "38821 Training Loss: tensor(0.3244)\n",
      "38822 Training Loss: tensor(0.3253)\n",
      "38823 Training Loss: tensor(0.3246)\n",
      "38824 Training Loss: tensor(0.3254)\n",
      "38825 Training Loss: tensor(0.3250)\n",
      "38826 Training Loss: tensor(0.3244)\n",
      "38827 Training Loss: tensor(0.3248)\n",
      "38828 Training Loss: tensor(0.3242)\n",
      "38829 Training Loss: tensor(0.3257)\n",
      "38830 Training Loss: tensor(0.3247)\n",
      "38831 Training Loss: tensor(0.3243)\n",
      "38832 Training Loss: tensor(0.3242)\n",
      "38833 Training Loss: tensor(0.3243)\n",
      "38834 Training Loss: tensor(0.3242)\n",
      "38835 Training Loss: tensor(0.3248)\n",
      "38836 Training Loss: tensor(0.3246)\n",
      "38837 Training Loss: tensor(0.3257)\n",
      "38838 Training Loss: tensor(0.3245)\n",
      "38839 Training Loss: tensor(0.3245)\n",
      "38840 Training Loss: tensor(0.3247)\n",
      "38841 Training Loss: tensor(0.3248)\n",
      "38842 Training Loss: tensor(0.3253)\n",
      "38843 Training Loss: tensor(0.3254)\n",
      "38844 Training Loss: tensor(0.3247)\n",
      "38845 Training Loss: tensor(0.3257)\n",
      "38846 Training Loss: tensor(0.3247)\n",
      "38847 Training Loss: tensor(0.3244)\n",
      "38848 Training Loss: tensor(0.3247)\n",
      "38849 Training Loss: tensor(0.3253)\n",
      "38850 Training Loss: tensor(0.3241)\n",
      "38851 Training Loss: tensor(0.3246)\n",
      "38852 Training Loss: tensor(0.3240)\n",
      "38853 Training Loss: tensor(0.3236)\n",
      "38854 Training Loss: tensor(0.3243)\n",
      "38855 Training Loss: tensor(0.3247)\n",
      "38856 Training Loss: tensor(0.3244)\n",
      "38857 Training Loss: tensor(0.3243)\n",
      "38858 Training Loss: tensor(0.3243)\n",
      "38859 Training Loss: tensor(0.3261)\n",
      "38860 Training Loss: tensor(0.3243)\n",
      "38861 Training Loss: tensor(0.3245)\n",
      "38862 Training Loss: tensor(0.3252)\n",
      "38863 Training Loss: tensor(0.3248)\n",
      "38864 Training Loss: tensor(0.3245)\n",
      "38865 Training Loss: tensor(0.3243)\n",
      "38866 Training Loss: tensor(0.3259)\n",
      "38867 Training Loss: tensor(0.3244)\n",
      "38868 Training Loss: tensor(0.3243)\n",
      "38869 Training Loss: tensor(0.3242)\n",
      "38870 Training Loss: tensor(0.3252)\n",
      "38871 Training Loss: tensor(0.3255)\n",
      "38872 Training Loss: tensor(0.3240)\n",
      "38873 Training Loss: tensor(0.3247)\n",
      "38874 Training Loss: tensor(0.3242)\n",
      "38875 Training Loss: tensor(0.3242)\n",
      "38876 Training Loss: tensor(0.3240)\n",
      "38877 Training Loss: tensor(0.3244)\n",
      "38878 Training Loss: tensor(0.3237)\n",
      "38879 Training Loss: tensor(0.3245)\n",
      "38880 Training Loss: tensor(0.3243)\n",
      "38881 Training Loss: tensor(0.3242)\n",
      "38882 Training Loss: tensor(0.3252)\n",
      "38883 Training Loss: tensor(0.3252)\n",
      "38884 Training Loss: tensor(0.3241)\n",
      "38885 Training Loss: tensor(0.3238)\n",
      "38886 Training Loss: tensor(0.3251)\n",
      "38887 Training Loss: tensor(0.3255)\n",
      "38888 Training Loss: tensor(0.3245)\n",
      "38889 Training Loss: tensor(0.3244)\n",
      "38890 Training Loss: tensor(0.3243)\n",
      "38891 Training Loss: tensor(0.3255)\n",
      "38892 Training Loss: tensor(0.3245)\n",
      "38893 Training Loss: tensor(0.3249)\n",
      "38894 Training Loss: tensor(0.3267)\n",
      "38895 Training Loss: tensor(0.3251)\n",
      "38896 Training Loss: tensor(0.3245)\n",
      "38897 Training Loss: tensor(0.3265)\n",
      "38898 Training Loss: tensor(0.3256)\n",
      "38899 Training Loss: tensor(0.3257)\n",
      "38900 Training Loss: tensor(0.3249)\n",
      "38901 Training Loss: tensor(0.3247)\n",
      "38902 Training Loss: tensor(0.3249)\n",
      "38903 Training Loss: tensor(0.3250)\n",
      "38904 Training Loss: tensor(0.3247)\n",
      "38905 Training Loss: tensor(0.3246)\n",
      "38906 Training Loss: tensor(0.3239)\n",
      "38907 Training Loss: tensor(0.3251)\n",
      "38908 Training Loss: tensor(0.3245)\n",
      "38909 Training Loss: tensor(0.3244)\n",
      "38910 Training Loss: tensor(0.3255)\n",
      "38911 Training Loss: tensor(0.3245)\n",
      "38912 Training Loss: tensor(0.3255)\n",
      "38913 Training Loss: tensor(0.3237)\n",
      "38914 Training Loss: tensor(0.3242)\n",
      "38915 Training Loss: tensor(0.3245)\n",
      "38916 Training Loss: tensor(0.3242)\n",
      "38917 Training Loss: tensor(0.3240)\n",
      "38918 Training Loss: tensor(0.3247)\n",
      "38919 Training Loss: tensor(0.3237)\n",
      "38920 Training Loss: tensor(0.3257)\n",
      "38921 Training Loss: tensor(0.3260)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38922 Training Loss: tensor(0.3249)\n",
      "38923 Training Loss: tensor(0.3237)\n",
      "38924 Training Loss: tensor(0.3246)\n",
      "38925 Training Loss: tensor(0.3244)\n",
      "38926 Training Loss: tensor(0.3257)\n",
      "38927 Training Loss: tensor(0.3254)\n",
      "38928 Training Loss: tensor(0.3244)\n",
      "38929 Training Loss: tensor(0.3241)\n",
      "38930 Training Loss: tensor(0.3238)\n",
      "38931 Training Loss: tensor(0.3250)\n",
      "38932 Training Loss: tensor(0.3257)\n",
      "38933 Training Loss: tensor(0.3250)\n",
      "38934 Training Loss: tensor(0.3245)\n",
      "38935 Training Loss: tensor(0.3246)\n",
      "38936 Training Loss: tensor(0.3238)\n",
      "38937 Training Loss: tensor(0.3245)\n",
      "38938 Training Loss: tensor(0.3238)\n",
      "38939 Training Loss: tensor(0.3238)\n",
      "38940 Training Loss: tensor(0.3245)\n",
      "38941 Training Loss: tensor(0.3241)\n",
      "38942 Training Loss: tensor(0.3250)\n",
      "38943 Training Loss: tensor(0.3246)\n",
      "38944 Training Loss: tensor(0.3245)\n",
      "38945 Training Loss: tensor(0.3246)\n",
      "38946 Training Loss: tensor(0.3245)\n",
      "38947 Training Loss: tensor(0.3241)\n",
      "38948 Training Loss: tensor(0.3252)\n",
      "38949 Training Loss: tensor(0.3242)\n",
      "38950 Training Loss: tensor(0.3245)\n",
      "38951 Training Loss: tensor(0.3239)\n",
      "38952 Training Loss: tensor(0.3248)\n",
      "38953 Training Loss: tensor(0.3251)\n",
      "38954 Training Loss: tensor(0.3241)\n",
      "38955 Training Loss: tensor(0.3248)\n",
      "38956 Training Loss: tensor(0.3247)\n",
      "38957 Training Loss: tensor(0.3241)\n",
      "38958 Training Loss: tensor(0.3251)\n",
      "38959 Training Loss: tensor(0.3246)\n",
      "38960 Training Loss: tensor(0.3244)\n",
      "38961 Training Loss: tensor(0.3240)\n",
      "38962 Training Loss: tensor(0.3245)\n",
      "38963 Training Loss: tensor(0.3249)\n",
      "38964 Training Loss: tensor(0.3245)\n",
      "38965 Training Loss: tensor(0.3260)\n",
      "38966 Training Loss: tensor(0.3240)\n",
      "38967 Training Loss: tensor(0.3243)\n",
      "38968 Training Loss: tensor(0.3252)\n",
      "38969 Training Loss: tensor(0.3250)\n",
      "38970 Training Loss: tensor(0.3250)\n",
      "38971 Training Loss: tensor(0.3247)\n",
      "38972 Training Loss: tensor(0.3248)\n",
      "38973 Training Loss: tensor(0.3245)\n",
      "38974 Training Loss: tensor(0.3243)\n",
      "38975 Training Loss: tensor(0.3241)\n",
      "38976 Training Loss: tensor(0.3250)\n",
      "38977 Training Loss: tensor(0.3245)\n",
      "38978 Training Loss: tensor(0.3244)\n",
      "38979 Training Loss: tensor(0.3245)\n",
      "38980 Training Loss: tensor(0.3248)\n",
      "38981 Training Loss: tensor(0.3241)\n",
      "38982 Training Loss: tensor(0.3253)\n",
      "38983 Training Loss: tensor(0.3246)\n",
      "38984 Training Loss: tensor(0.3245)\n",
      "38985 Training Loss: tensor(0.3252)\n",
      "38986 Training Loss: tensor(0.3255)\n",
      "38987 Training Loss: tensor(0.3245)\n",
      "38988 Training Loss: tensor(0.3248)\n",
      "38989 Training Loss: tensor(0.3249)\n",
      "38990 Training Loss: tensor(0.3244)\n",
      "38991 Training Loss: tensor(0.3239)\n",
      "38992 Training Loss: tensor(0.3239)\n",
      "38993 Training Loss: tensor(0.3244)\n",
      "38994 Training Loss: tensor(0.3250)\n",
      "38995 Training Loss: tensor(0.3252)\n",
      "38996 Training Loss: tensor(0.3263)\n",
      "38997 Training Loss: tensor(0.3245)\n",
      "38998 Training Loss: tensor(0.3254)\n",
      "38999 Training Loss: tensor(0.3246)\n",
      "39000 Training Loss: tensor(0.3246)\n",
      "39001 Training Loss: tensor(0.3250)\n",
      "39002 Training Loss: tensor(0.3242)\n",
      "39003 Training Loss: tensor(0.3256)\n",
      "39004 Training Loss: tensor(0.3255)\n",
      "39005 Training Loss: tensor(0.3240)\n",
      "39006 Training Loss: tensor(0.3244)\n",
      "39007 Training Loss: tensor(0.3241)\n",
      "39008 Training Loss: tensor(0.3248)\n",
      "39009 Training Loss: tensor(0.3245)\n",
      "39010 Training Loss: tensor(0.3251)\n",
      "39011 Training Loss: tensor(0.3248)\n",
      "39012 Training Loss: tensor(0.3243)\n",
      "39013 Training Loss: tensor(0.3244)\n",
      "39014 Training Loss: tensor(0.3245)\n",
      "39015 Training Loss: tensor(0.3252)\n",
      "39016 Training Loss: tensor(0.3252)\n",
      "39017 Training Loss: tensor(0.3249)\n",
      "39018 Training Loss: tensor(0.3253)\n",
      "39019 Training Loss: tensor(0.3246)\n",
      "39020 Training Loss: tensor(0.3248)\n",
      "39021 Training Loss: tensor(0.3256)\n",
      "39022 Training Loss: tensor(0.3244)\n",
      "39023 Training Loss: tensor(0.3240)\n",
      "39024 Training Loss: tensor(0.3244)\n",
      "39025 Training Loss: tensor(0.3260)\n",
      "39026 Training Loss: tensor(0.3237)\n",
      "39027 Training Loss: tensor(0.3248)\n",
      "39028 Training Loss: tensor(0.3241)\n",
      "39029 Training Loss: tensor(0.3244)\n",
      "39030 Training Loss: tensor(0.3257)\n",
      "39031 Training Loss: tensor(0.3252)\n",
      "39032 Training Loss: tensor(0.3247)\n",
      "39033 Training Loss: tensor(0.3239)\n",
      "39034 Training Loss: tensor(0.3249)\n",
      "39035 Training Loss: tensor(0.3255)\n",
      "39036 Training Loss: tensor(0.3245)\n",
      "39037 Training Loss: tensor(0.3245)\n",
      "39038 Training Loss: tensor(0.3243)\n",
      "39039 Training Loss: tensor(0.3241)\n",
      "39040 Training Loss: tensor(0.3238)\n",
      "39041 Training Loss: tensor(0.3251)\n",
      "39042 Training Loss: tensor(0.3243)\n",
      "39043 Training Loss: tensor(0.3248)\n",
      "39044 Training Loss: tensor(0.3240)\n",
      "39045 Training Loss: tensor(0.3256)\n",
      "39046 Training Loss: tensor(0.3251)\n",
      "39047 Training Loss: tensor(0.3255)\n",
      "39048 Training Loss: tensor(0.3255)\n",
      "39049 Training Loss: tensor(0.3245)\n",
      "39050 Training Loss: tensor(0.3256)\n",
      "39051 Training Loss: tensor(0.3248)\n",
      "39052 Training Loss: tensor(0.3247)\n",
      "39053 Training Loss: tensor(0.3249)\n",
      "39054 Training Loss: tensor(0.3241)\n",
      "39055 Training Loss: tensor(0.3257)\n",
      "39056 Training Loss: tensor(0.3258)\n",
      "39057 Training Loss: tensor(0.3241)\n",
      "39058 Training Loss: tensor(0.3242)\n",
      "39059 Training Loss: tensor(0.3249)\n",
      "39060 Training Loss: tensor(0.3252)\n",
      "39061 Training Loss: tensor(0.3251)\n",
      "39062 Training Loss: tensor(0.3248)\n",
      "39063 Training Loss: tensor(0.3247)\n",
      "39064 Training Loss: tensor(0.3256)\n",
      "39065 Training Loss: tensor(0.3245)\n",
      "39066 Training Loss: tensor(0.3246)\n",
      "39067 Training Loss: tensor(0.3250)\n",
      "39068 Training Loss: tensor(0.3242)\n",
      "39069 Training Loss: tensor(0.3251)\n",
      "39070 Training Loss: tensor(0.3257)\n",
      "39071 Training Loss: tensor(0.3244)\n",
      "39072 Training Loss: tensor(0.3245)\n",
      "39073 Training Loss: tensor(0.3244)\n",
      "39074 Training Loss: tensor(0.3247)\n",
      "39075 Training Loss: tensor(0.3256)\n",
      "39076 Training Loss: tensor(0.3246)\n",
      "39077 Training Loss: tensor(0.3246)\n",
      "39078 Training Loss: tensor(0.3251)\n",
      "39079 Training Loss: tensor(0.3242)\n",
      "39080 Training Loss: tensor(0.3243)\n",
      "39081 Training Loss: tensor(0.3248)\n",
      "39082 Training Loss: tensor(0.3252)\n",
      "39083 Training Loss: tensor(0.3245)\n",
      "39084 Training Loss: tensor(0.3245)\n",
      "39085 Training Loss: tensor(0.3256)\n",
      "39086 Training Loss: tensor(0.3248)\n",
      "39087 Training Loss: tensor(0.3248)\n",
      "39088 Training Loss: tensor(0.3247)\n",
      "39089 Training Loss: tensor(0.3254)\n",
      "39090 Training Loss: tensor(0.3259)\n",
      "39091 Training Loss: tensor(0.3250)\n",
      "39092 Training Loss: tensor(0.3242)\n",
      "39093 Training Loss: tensor(0.3245)\n",
      "39094 Training Loss: tensor(0.3252)\n",
      "39095 Training Loss: tensor(0.3246)\n",
      "39096 Training Loss: tensor(0.3253)\n",
      "39097 Training Loss: tensor(0.3264)\n",
      "39098 Training Loss: tensor(0.3251)\n",
      "39099 Training Loss: tensor(0.3253)\n",
      "39100 Training Loss: tensor(0.3248)\n",
      "39101 Training Loss: tensor(0.3246)\n",
      "39102 Training Loss: tensor(0.3248)\n",
      "39103 Training Loss: tensor(0.3253)\n",
      "39104 Training Loss: tensor(0.3260)\n",
      "39105 Training Loss: tensor(0.3249)\n",
      "39106 Training Loss: tensor(0.3251)\n",
      "39107 Training Loss: tensor(0.3255)\n",
      "39108 Training Loss: tensor(0.3245)\n",
      "39109 Training Loss: tensor(0.3250)\n",
      "39110 Training Loss: tensor(0.3250)\n",
      "39111 Training Loss: tensor(0.3242)\n",
      "39112 Training Loss: tensor(0.3243)\n",
      "39113 Training Loss: tensor(0.3251)\n",
      "39114 Training Loss: tensor(0.3254)\n",
      "39115 Training Loss: tensor(0.3243)\n",
      "39116 Training Loss: tensor(0.3255)\n",
      "39117 Training Loss: tensor(0.3244)\n",
      "39118 Training Loss: tensor(0.3247)\n",
      "39119 Training Loss: tensor(0.3253)\n",
      "39120 Training Loss: tensor(0.3248)\n",
      "39121 Training Loss: tensor(0.3246)\n",
      "39122 Training Loss: tensor(0.3260)\n",
      "39123 Training Loss: tensor(0.3248)\n",
      "39124 Training Loss: tensor(0.3244)\n",
      "39125 Training Loss: tensor(0.3244)\n",
      "39126 Training Loss: tensor(0.3242)\n",
      "39127 Training Loss: tensor(0.3244)\n",
      "39128 Training Loss: tensor(0.3246)\n",
      "39129 Training Loss: tensor(0.3245)\n",
      "39130 Training Loss: tensor(0.3248)\n",
      "39131 Training Loss: tensor(0.3255)\n",
      "39132 Training Loss: tensor(0.3249)\n",
      "39133 Training Loss: tensor(0.3249)\n",
      "39134 Training Loss: tensor(0.3247)\n",
      "39135 Training Loss: tensor(0.3251)\n",
      "39136 Training Loss: tensor(0.3249)\n",
      "39137 Training Loss: tensor(0.3251)\n",
      "39138 Training Loss: tensor(0.3248)\n",
      "39139 Training Loss: tensor(0.3253)\n",
      "39140 Training Loss: tensor(0.3258)\n",
      "39141 Training Loss: tensor(0.3245)\n",
      "39142 Training Loss: tensor(0.3242)\n",
      "39143 Training Loss: tensor(0.3251)\n",
      "39144 Training Loss: tensor(0.3244)\n",
      "39145 Training Loss: tensor(0.3243)\n",
      "39146 Training Loss: tensor(0.3244)\n",
      "39147 Training Loss: tensor(0.3242)\n",
      "39148 Training Loss: tensor(0.3240)\n",
      "39149 Training Loss: tensor(0.3242)\n",
      "39150 Training Loss: tensor(0.3240)\n",
      "39151 Training Loss: tensor(0.3239)\n",
      "39152 Training Loss: tensor(0.3242)\n",
      "39153 Training Loss: tensor(0.3244)\n",
      "39154 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39155 Training Loss: tensor(0.3253)\n",
      "39156 Training Loss: tensor(0.3256)\n",
      "39157 Training Loss: tensor(0.3246)\n",
      "39158 Training Loss: tensor(0.3238)\n",
      "39159 Training Loss: tensor(0.3245)\n",
      "39160 Training Loss: tensor(0.3248)\n",
      "39161 Training Loss: tensor(0.3243)\n",
      "39162 Training Loss: tensor(0.3252)\n",
      "39163 Training Loss: tensor(0.3244)\n",
      "39164 Training Loss: tensor(0.3244)\n",
      "39165 Training Loss: tensor(0.3244)\n",
      "39166 Training Loss: tensor(0.3254)\n",
      "39167 Training Loss: tensor(0.3245)\n",
      "39168 Training Loss: tensor(0.3245)\n",
      "39169 Training Loss: tensor(0.3240)\n",
      "39170 Training Loss: tensor(0.3248)\n",
      "39171 Training Loss: tensor(0.3248)\n",
      "39172 Training Loss: tensor(0.3244)\n",
      "39173 Training Loss: tensor(0.3259)\n",
      "39174 Training Loss: tensor(0.3240)\n",
      "39175 Training Loss: tensor(0.3257)\n",
      "39176 Training Loss: tensor(0.3247)\n",
      "39177 Training Loss: tensor(0.3249)\n",
      "39178 Training Loss: tensor(0.3246)\n",
      "39179 Training Loss: tensor(0.3246)\n",
      "39180 Training Loss: tensor(0.3242)\n",
      "39181 Training Loss: tensor(0.3242)\n",
      "39182 Training Loss: tensor(0.3246)\n",
      "39183 Training Loss: tensor(0.3239)\n",
      "39184 Training Loss: tensor(0.3246)\n",
      "39185 Training Loss: tensor(0.3248)\n",
      "39186 Training Loss: tensor(0.3245)\n",
      "39187 Training Loss: tensor(0.3246)\n",
      "39188 Training Loss: tensor(0.3248)\n",
      "39189 Training Loss: tensor(0.3242)\n",
      "39190 Training Loss: tensor(0.3243)\n",
      "39191 Training Loss: tensor(0.3241)\n",
      "39192 Training Loss: tensor(0.3243)\n",
      "39193 Training Loss: tensor(0.3259)\n",
      "39194 Training Loss: tensor(0.3242)\n",
      "39195 Training Loss: tensor(0.3245)\n",
      "39196 Training Loss: tensor(0.3237)\n",
      "39197 Training Loss: tensor(0.3246)\n",
      "39198 Training Loss: tensor(0.3251)\n",
      "39199 Training Loss: tensor(0.3239)\n",
      "39200 Training Loss: tensor(0.3252)\n",
      "39201 Training Loss: tensor(0.3243)\n",
      "39202 Training Loss: tensor(0.3242)\n",
      "39203 Training Loss: tensor(0.3240)\n",
      "39204 Training Loss: tensor(0.3236)\n",
      "39205 Training Loss: tensor(0.3249)\n",
      "39206 Training Loss: tensor(0.3247)\n",
      "39207 Training Loss: tensor(0.3250)\n",
      "39208 Training Loss: tensor(0.3251)\n",
      "39209 Training Loss: tensor(0.3252)\n",
      "39210 Training Loss: tensor(0.3243)\n",
      "39211 Training Loss: tensor(0.3249)\n",
      "39212 Training Loss: tensor(0.3247)\n",
      "39213 Training Loss: tensor(0.3248)\n",
      "39214 Training Loss: tensor(0.3243)\n",
      "39215 Training Loss: tensor(0.3250)\n",
      "39216 Training Loss: tensor(0.3236)\n",
      "39217 Training Loss: tensor(0.3240)\n",
      "39218 Training Loss: tensor(0.3247)\n",
      "39219 Training Loss: tensor(0.3256)\n",
      "39220 Training Loss: tensor(0.3254)\n",
      "39221 Training Loss: tensor(0.3251)\n",
      "39222 Training Loss: tensor(0.3251)\n",
      "39223 Training Loss: tensor(0.3240)\n",
      "39224 Training Loss: tensor(0.3238)\n",
      "39225 Training Loss: tensor(0.3252)\n",
      "39226 Training Loss: tensor(0.3251)\n",
      "39227 Training Loss: tensor(0.3242)\n",
      "39228 Training Loss: tensor(0.3249)\n",
      "39229 Training Loss: tensor(0.3244)\n",
      "39230 Training Loss: tensor(0.3255)\n",
      "39231 Training Loss: tensor(0.3249)\n",
      "39232 Training Loss: tensor(0.3246)\n",
      "39233 Training Loss: tensor(0.3246)\n",
      "39234 Training Loss: tensor(0.3243)\n",
      "39235 Training Loss: tensor(0.3250)\n",
      "39236 Training Loss: tensor(0.3246)\n",
      "39237 Training Loss: tensor(0.3242)\n",
      "39238 Training Loss: tensor(0.3248)\n",
      "39239 Training Loss: tensor(0.3252)\n",
      "39240 Training Loss: tensor(0.3262)\n",
      "39241 Training Loss: tensor(0.3246)\n",
      "39242 Training Loss: tensor(0.3254)\n",
      "39243 Training Loss: tensor(0.3246)\n",
      "39244 Training Loss: tensor(0.3240)\n",
      "39245 Training Loss: tensor(0.3251)\n",
      "39246 Training Loss: tensor(0.3250)\n",
      "39247 Training Loss: tensor(0.3244)\n",
      "39248 Training Loss: tensor(0.3242)\n",
      "39249 Training Loss: tensor(0.3251)\n",
      "39250 Training Loss: tensor(0.3246)\n",
      "39251 Training Loss: tensor(0.3239)\n",
      "39252 Training Loss: tensor(0.3247)\n",
      "39253 Training Loss: tensor(0.3242)\n",
      "39254 Training Loss: tensor(0.3246)\n",
      "39255 Training Loss: tensor(0.3241)\n",
      "39256 Training Loss: tensor(0.3265)\n",
      "39257 Training Loss: tensor(0.3254)\n",
      "39258 Training Loss: tensor(0.3241)\n",
      "39259 Training Loss: tensor(0.3243)\n",
      "39260 Training Loss: tensor(0.3243)\n",
      "39261 Training Loss: tensor(0.3247)\n",
      "39262 Training Loss: tensor(0.3249)\n",
      "39263 Training Loss: tensor(0.3256)\n",
      "39264 Training Loss: tensor(0.3253)\n",
      "39265 Training Loss: tensor(0.3243)\n",
      "39266 Training Loss: tensor(0.3244)\n",
      "39267 Training Loss: tensor(0.3243)\n",
      "39268 Training Loss: tensor(0.3255)\n",
      "39269 Training Loss: tensor(0.3244)\n",
      "39270 Training Loss: tensor(0.3245)\n",
      "39271 Training Loss: tensor(0.3243)\n",
      "39272 Training Loss: tensor(0.3251)\n",
      "39273 Training Loss: tensor(0.3243)\n",
      "39274 Training Loss: tensor(0.3247)\n",
      "39275 Training Loss: tensor(0.3245)\n",
      "39276 Training Loss: tensor(0.3240)\n",
      "39277 Training Loss: tensor(0.3252)\n",
      "39278 Training Loss: tensor(0.3242)\n",
      "39279 Training Loss: tensor(0.3250)\n",
      "39280 Training Loss: tensor(0.3255)\n",
      "39281 Training Loss: tensor(0.3244)\n",
      "39282 Training Loss: tensor(0.3254)\n",
      "39283 Training Loss: tensor(0.3246)\n",
      "39284 Training Loss: tensor(0.3240)\n",
      "39285 Training Loss: tensor(0.3242)\n",
      "39286 Training Loss: tensor(0.3243)\n",
      "39287 Training Loss: tensor(0.3243)\n",
      "39288 Training Loss: tensor(0.3253)\n",
      "39289 Training Loss: tensor(0.3246)\n",
      "39290 Training Loss: tensor(0.3243)\n",
      "39291 Training Loss: tensor(0.3241)\n",
      "39292 Training Loss: tensor(0.3249)\n",
      "39293 Training Loss: tensor(0.3242)\n",
      "39294 Training Loss: tensor(0.3251)\n",
      "39295 Training Loss: tensor(0.3257)\n",
      "39296 Training Loss: tensor(0.3246)\n",
      "39297 Training Loss: tensor(0.3245)\n",
      "39298 Training Loss: tensor(0.3244)\n",
      "39299 Training Loss: tensor(0.3241)\n",
      "39300 Training Loss: tensor(0.3247)\n",
      "39301 Training Loss: tensor(0.3244)\n",
      "39302 Training Loss: tensor(0.3255)\n",
      "39303 Training Loss: tensor(0.3250)\n",
      "39304 Training Loss: tensor(0.3247)\n",
      "39305 Training Loss: tensor(0.3249)\n",
      "39306 Training Loss: tensor(0.3251)\n",
      "39307 Training Loss: tensor(0.3239)\n",
      "39308 Training Loss: tensor(0.3255)\n",
      "39309 Training Loss: tensor(0.3239)\n",
      "39310 Training Loss: tensor(0.3243)\n",
      "39311 Training Loss: tensor(0.3248)\n",
      "39312 Training Loss: tensor(0.3241)\n",
      "39313 Training Loss: tensor(0.3236)\n",
      "39314 Training Loss: tensor(0.3254)\n",
      "39315 Training Loss: tensor(0.3248)\n",
      "39316 Training Loss: tensor(0.3260)\n",
      "39317 Training Loss: tensor(0.3248)\n",
      "39318 Training Loss: tensor(0.3240)\n",
      "39319 Training Loss: tensor(0.3248)\n",
      "39320 Training Loss: tensor(0.3258)\n",
      "39321 Training Loss: tensor(0.3260)\n",
      "39322 Training Loss: tensor(0.3241)\n",
      "39323 Training Loss: tensor(0.3248)\n",
      "39324 Training Loss: tensor(0.3251)\n",
      "39325 Training Loss: tensor(0.3253)\n",
      "39326 Training Loss: tensor(0.3248)\n",
      "39327 Training Loss: tensor(0.3251)\n",
      "39328 Training Loss: tensor(0.3249)\n",
      "39329 Training Loss: tensor(0.3247)\n",
      "39330 Training Loss: tensor(0.3256)\n",
      "39331 Training Loss: tensor(0.3250)\n",
      "39332 Training Loss: tensor(0.3254)\n",
      "39333 Training Loss: tensor(0.3252)\n",
      "39334 Training Loss: tensor(0.3247)\n",
      "39335 Training Loss: tensor(0.3243)\n",
      "39336 Training Loss: tensor(0.3251)\n",
      "39337 Training Loss: tensor(0.3243)\n",
      "39338 Training Loss: tensor(0.3252)\n",
      "39339 Training Loss: tensor(0.3247)\n",
      "39340 Training Loss: tensor(0.3250)\n",
      "39341 Training Loss: tensor(0.3247)\n",
      "39342 Training Loss: tensor(0.3242)\n",
      "39343 Training Loss: tensor(0.3242)\n",
      "39344 Training Loss: tensor(0.3243)\n",
      "39345 Training Loss: tensor(0.3241)\n",
      "39346 Training Loss: tensor(0.3249)\n",
      "39347 Training Loss: tensor(0.3241)\n",
      "39348 Training Loss: tensor(0.3240)\n",
      "39349 Training Loss: tensor(0.3246)\n",
      "39350 Training Loss: tensor(0.3242)\n",
      "39351 Training Loss: tensor(0.3245)\n",
      "39352 Training Loss: tensor(0.3250)\n",
      "39353 Training Loss: tensor(0.3250)\n",
      "39354 Training Loss: tensor(0.3250)\n",
      "39355 Training Loss: tensor(0.3248)\n",
      "39356 Training Loss: tensor(0.3247)\n",
      "39357 Training Loss: tensor(0.3252)\n",
      "39358 Training Loss: tensor(0.3253)\n",
      "39359 Training Loss: tensor(0.3248)\n",
      "39360 Training Loss: tensor(0.3242)\n",
      "39361 Training Loss: tensor(0.3244)\n",
      "39362 Training Loss: tensor(0.3247)\n",
      "39363 Training Loss: tensor(0.3240)\n",
      "39364 Training Loss: tensor(0.3243)\n",
      "39365 Training Loss: tensor(0.3244)\n",
      "39366 Training Loss: tensor(0.3250)\n",
      "39367 Training Loss: tensor(0.3256)\n",
      "39368 Training Loss: tensor(0.3245)\n",
      "39369 Training Loss: tensor(0.3242)\n",
      "39370 Training Loss: tensor(0.3241)\n",
      "39371 Training Loss: tensor(0.3264)\n",
      "39372 Training Loss: tensor(0.3241)\n",
      "39373 Training Loss: tensor(0.3239)\n",
      "39374 Training Loss: tensor(0.3249)\n",
      "39375 Training Loss: tensor(0.3255)\n",
      "39376 Training Loss: tensor(0.3246)\n",
      "39377 Training Loss: tensor(0.3249)\n",
      "39378 Training Loss: tensor(0.3242)\n",
      "39379 Training Loss: tensor(0.3243)\n",
      "39380 Training Loss: tensor(0.3240)\n",
      "39381 Training Loss: tensor(0.3250)\n",
      "39382 Training Loss: tensor(0.3240)\n",
      "39383 Training Loss: tensor(0.3242)\n",
      "39384 Training Loss: tensor(0.3236)\n",
      "39385 Training Loss: tensor(0.3265)\n",
      "39386 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39387 Training Loss: tensor(0.3246)\n",
      "39388 Training Loss: tensor(0.3246)\n",
      "39389 Training Loss: tensor(0.3249)\n",
      "39390 Training Loss: tensor(0.3241)\n",
      "39391 Training Loss: tensor(0.3245)\n",
      "39392 Training Loss: tensor(0.3265)\n",
      "39393 Training Loss: tensor(0.3247)\n",
      "39394 Training Loss: tensor(0.3260)\n",
      "39395 Training Loss: tensor(0.3240)\n",
      "39396 Training Loss: tensor(0.3245)\n",
      "39397 Training Loss: tensor(0.3247)\n",
      "39398 Training Loss: tensor(0.3240)\n",
      "39399 Training Loss: tensor(0.3251)\n",
      "39400 Training Loss: tensor(0.3244)\n",
      "39401 Training Loss: tensor(0.3249)\n",
      "39402 Training Loss: tensor(0.3250)\n",
      "39403 Training Loss: tensor(0.3252)\n",
      "39404 Training Loss: tensor(0.3249)\n",
      "39405 Training Loss: tensor(0.3242)\n",
      "39406 Training Loss: tensor(0.3244)\n",
      "39407 Training Loss: tensor(0.3246)\n",
      "39408 Training Loss: tensor(0.3243)\n",
      "39409 Training Loss: tensor(0.3241)\n",
      "39410 Training Loss: tensor(0.3245)\n",
      "39411 Training Loss: tensor(0.3245)\n",
      "39412 Training Loss: tensor(0.3253)\n",
      "39413 Training Loss: tensor(0.3251)\n",
      "39414 Training Loss: tensor(0.3246)\n",
      "39415 Training Loss: tensor(0.3253)\n",
      "39416 Training Loss: tensor(0.3254)\n",
      "39417 Training Loss: tensor(0.3242)\n",
      "39418 Training Loss: tensor(0.3246)\n",
      "39419 Training Loss: tensor(0.3248)\n",
      "39420 Training Loss: tensor(0.3245)\n",
      "39421 Training Loss: tensor(0.3259)\n",
      "39422 Training Loss: tensor(0.3242)\n",
      "39423 Training Loss: tensor(0.3245)\n",
      "39424 Training Loss: tensor(0.3240)\n",
      "39425 Training Loss: tensor(0.3241)\n",
      "39426 Training Loss: tensor(0.3254)\n",
      "39427 Training Loss: tensor(0.3239)\n",
      "39428 Training Loss: tensor(0.3242)\n",
      "39429 Training Loss: tensor(0.3246)\n",
      "39430 Training Loss: tensor(0.3245)\n",
      "39431 Training Loss: tensor(0.3248)\n",
      "39432 Training Loss: tensor(0.3241)\n",
      "39433 Training Loss: tensor(0.3240)\n",
      "39434 Training Loss: tensor(0.3249)\n",
      "39435 Training Loss: tensor(0.3246)\n",
      "39436 Training Loss: tensor(0.3252)\n",
      "39437 Training Loss: tensor(0.3246)\n",
      "39438 Training Loss: tensor(0.3252)\n",
      "39439 Training Loss: tensor(0.3244)\n",
      "39440 Training Loss: tensor(0.3245)\n",
      "39441 Training Loss: tensor(0.3253)\n",
      "39442 Training Loss: tensor(0.3251)\n",
      "39443 Training Loss: tensor(0.3249)\n",
      "39444 Training Loss: tensor(0.3245)\n",
      "39445 Training Loss: tensor(0.3263)\n",
      "39446 Training Loss: tensor(0.3242)\n",
      "39447 Training Loss: tensor(0.3245)\n",
      "39448 Training Loss: tensor(0.3246)\n",
      "39449 Training Loss: tensor(0.3252)\n",
      "39450 Training Loss: tensor(0.3251)\n",
      "39451 Training Loss: tensor(0.3256)\n",
      "39452 Training Loss: tensor(0.3248)\n",
      "39453 Training Loss: tensor(0.3248)\n",
      "39454 Training Loss: tensor(0.3245)\n",
      "39455 Training Loss: tensor(0.3248)\n",
      "39456 Training Loss: tensor(0.3242)\n",
      "39457 Training Loss: tensor(0.3249)\n",
      "39458 Training Loss: tensor(0.3255)\n",
      "39459 Training Loss: tensor(0.3242)\n",
      "39460 Training Loss: tensor(0.3253)\n",
      "39461 Training Loss: tensor(0.3243)\n",
      "39462 Training Loss: tensor(0.3256)\n",
      "39463 Training Loss: tensor(0.3249)\n",
      "39464 Training Loss: tensor(0.3243)\n",
      "39465 Training Loss: tensor(0.3242)\n",
      "39466 Training Loss: tensor(0.3243)\n",
      "39467 Training Loss: tensor(0.3247)\n",
      "39468 Training Loss: tensor(0.3250)\n",
      "39469 Training Loss: tensor(0.3250)\n",
      "39470 Training Loss: tensor(0.3258)\n",
      "39471 Training Loss: tensor(0.3251)\n",
      "39472 Training Loss: tensor(0.3240)\n",
      "39473 Training Loss: tensor(0.3239)\n",
      "39474 Training Loss: tensor(0.3240)\n",
      "39475 Training Loss: tensor(0.3242)\n",
      "39476 Training Loss: tensor(0.3246)\n",
      "39477 Training Loss: tensor(0.3249)\n",
      "39478 Training Loss: tensor(0.3258)\n",
      "39479 Training Loss: tensor(0.3240)\n",
      "39480 Training Loss: tensor(0.3254)\n",
      "39481 Training Loss: tensor(0.3245)\n",
      "39482 Training Loss: tensor(0.3237)\n",
      "39483 Training Loss: tensor(0.3248)\n",
      "39484 Training Loss: tensor(0.3243)\n",
      "39485 Training Loss: tensor(0.3249)\n",
      "39486 Training Loss: tensor(0.3243)\n",
      "39487 Training Loss: tensor(0.3244)\n",
      "39488 Training Loss: tensor(0.3253)\n",
      "39489 Training Loss: tensor(0.3251)\n",
      "39490 Training Loss: tensor(0.3249)\n",
      "39491 Training Loss: tensor(0.3261)\n",
      "39492 Training Loss: tensor(0.3250)\n",
      "39493 Training Loss: tensor(0.3254)\n",
      "39494 Training Loss: tensor(0.3245)\n",
      "39495 Training Loss: tensor(0.3246)\n",
      "39496 Training Loss: tensor(0.3260)\n",
      "39497 Training Loss: tensor(0.3246)\n",
      "39498 Training Loss: tensor(0.3245)\n",
      "39499 Training Loss: tensor(0.3242)\n",
      "39500 Training Loss: tensor(0.3243)\n",
      "39501 Training Loss: tensor(0.3257)\n",
      "39502 Training Loss: tensor(0.3252)\n",
      "39503 Training Loss: tensor(0.3247)\n",
      "39504 Training Loss: tensor(0.3251)\n",
      "39505 Training Loss: tensor(0.3250)\n",
      "39506 Training Loss: tensor(0.3248)\n",
      "39507 Training Loss: tensor(0.3252)\n",
      "39508 Training Loss: tensor(0.3251)\n",
      "39509 Training Loss: tensor(0.3244)\n",
      "39510 Training Loss: tensor(0.3243)\n",
      "39511 Training Loss: tensor(0.3254)\n",
      "39512 Training Loss: tensor(0.3250)\n",
      "39513 Training Loss: tensor(0.3257)\n",
      "39514 Training Loss: tensor(0.3248)\n",
      "39515 Training Loss: tensor(0.3248)\n",
      "39516 Training Loss: tensor(0.3242)\n",
      "39517 Training Loss: tensor(0.3254)\n",
      "39518 Training Loss: tensor(0.3247)\n",
      "39519 Training Loss: tensor(0.3244)\n",
      "39520 Training Loss: tensor(0.3250)\n",
      "39521 Training Loss: tensor(0.3247)\n",
      "39522 Training Loss: tensor(0.3248)\n",
      "39523 Training Loss: tensor(0.3246)\n",
      "39524 Training Loss: tensor(0.3253)\n",
      "39525 Training Loss: tensor(0.3251)\n",
      "39526 Training Loss: tensor(0.3250)\n",
      "39527 Training Loss: tensor(0.3254)\n",
      "39528 Training Loss: tensor(0.3244)\n",
      "39529 Training Loss: tensor(0.3240)\n",
      "39530 Training Loss: tensor(0.3245)\n",
      "39531 Training Loss: tensor(0.3251)\n",
      "39532 Training Loss: tensor(0.3247)\n",
      "39533 Training Loss: tensor(0.3252)\n",
      "39534 Training Loss: tensor(0.3250)\n",
      "39535 Training Loss: tensor(0.3259)\n",
      "39536 Training Loss: tensor(0.3257)\n",
      "39537 Training Loss: tensor(0.3244)\n",
      "39538 Training Loss: tensor(0.3246)\n",
      "39539 Training Loss: tensor(0.3249)\n",
      "39540 Training Loss: tensor(0.3243)\n",
      "39541 Training Loss: tensor(0.3257)\n",
      "39542 Training Loss: tensor(0.3249)\n",
      "39543 Training Loss: tensor(0.3249)\n",
      "39544 Training Loss: tensor(0.3249)\n",
      "39545 Training Loss: tensor(0.3247)\n",
      "39546 Training Loss: tensor(0.3249)\n",
      "39547 Training Loss: tensor(0.3252)\n",
      "39548 Training Loss: tensor(0.3243)\n",
      "39549 Training Loss: tensor(0.3242)\n",
      "39550 Training Loss: tensor(0.3251)\n",
      "39551 Training Loss: tensor(0.3252)\n",
      "39552 Training Loss: tensor(0.3247)\n",
      "39553 Training Loss: tensor(0.3248)\n",
      "39554 Training Loss: tensor(0.3264)\n",
      "39555 Training Loss: tensor(0.3247)\n",
      "39556 Training Loss: tensor(0.3249)\n",
      "39557 Training Loss: tensor(0.3259)\n",
      "39558 Training Loss: tensor(0.3251)\n",
      "39559 Training Loss: tensor(0.3253)\n",
      "39560 Training Loss: tensor(0.3257)\n",
      "39561 Training Loss: tensor(0.3250)\n",
      "39562 Training Loss: tensor(0.3242)\n",
      "39563 Training Loss: tensor(0.3245)\n",
      "39564 Training Loss: tensor(0.3249)\n",
      "39565 Training Loss: tensor(0.3243)\n",
      "39566 Training Loss: tensor(0.3246)\n",
      "39567 Training Loss: tensor(0.3244)\n",
      "39568 Training Loss: tensor(0.3242)\n",
      "39569 Training Loss: tensor(0.3253)\n",
      "39570 Training Loss: tensor(0.3259)\n",
      "39571 Training Loss: tensor(0.3249)\n",
      "39572 Training Loss: tensor(0.3242)\n",
      "39573 Training Loss: tensor(0.3253)\n",
      "39574 Training Loss: tensor(0.3244)\n",
      "39575 Training Loss: tensor(0.3248)\n",
      "39576 Training Loss: tensor(0.3247)\n",
      "39577 Training Loss: tensor(0.3243)\n",
      "39578 Training Loss: tensor(0.3256)\n",
      "39579 Training Loss: tensor(0.3256)\n",
      "39580 Training Loss: tensor(0.3253)\n",
      "39581 Training Loss: tensor(0.3251)\n",
      "39582 Training Loss: tensor(0.3247)\n",
      "39583 Training Loss: tensor(0.3248)\n",
      "39584 Training Loss: tensor(0.3248)\n",
      "39585 Training Loss: tensor(0.3256)\n",
      "39586 Training Loss: tensor(0.3242)\n",
      "39587 Training Loss: tensor(0.3246)\n",
      "39588 Training Loss: tensor(0.3267)\n",
      "39589 Training Loss: tensor(0.3257)\n",
      "39590 Training Loss: tensor(0.3245)\n",
      "39591 Training Loss: tensor(0.3249)\n",
      "39592 Training Loss: tensor(0.3249)\n",
      "39593 Training Loss: tensor(0.3250)\n",
      "39594 Training Loss: tensor(0.3241)\n",
      "39595 Training Loss: tensor(0.3242)\n",
      "39596 Training Loss: tensor(0.3246)\n",
      "39597 Training Loss: tensor(0.3247)\n",
      "39598 Training Loss: tensor(0.3237)\n",
      "39599 Training Loss: tensor(0.3249)\n",
      "39600 Training Loss: tensor(0.3242)\n",
      "39601 Training Loss: tensor(0.3263)\n",
      "39602 Training Loss: tensor(0.3244)\n",
      "39603 Training Loss: tensor(0.3251)\n",
      "39604 Training Loss: tensor(0.3247)\n",
      "39605 Training Loss: tensor(0.3255)\n",
      "39606 Training Loss: tensor(0.3245)\n",
      "39607 Training Loss: tensor(0.3249)\n",
      "39608 Training Loss: tensor(0.3250)\n",
      "39609 Training Loss: tensor(0.3257)\n",
      "39610 Training Loss: tensor(0.3245)\n",
      "39611 Training Loss: tensor(0.3242)\n",
      "39612 Training Loss: tensor(0.3243)\n",
      "39613 Training Loss: tensor(0.3248)\n",
      "39614 Training Loss: tensor(0.3246)\n",
      "39615 Training Loss: tensor(0.3252)\n",
      "39616 Training Loss: tensor(0.3245)\n",
      "39617 Training Loss: tensor(0.3243)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39618 Training Loss: tensor(0.3243)\n",
      "39619 Training Loss: tensor(0.3253)\n",
      "39620 Training Loss: tensor(0.3252)\n",
      "39621 Training Loss: tensor(0.3250)\n",
      "39622 Training Loss: tensor(0.3245)\n",
      "39623 Training Loss: tensor(0.3260)\n",
      "39624 Training Loss: tensor(0.3244)\n",
      "39625 Training Loss: tensor(0.3243)\n",
      "39626 Training Loss: tensor(0.3252)\n",
      "39627 Training Loss: tensor(0.3262)\n",
      "39628 Training Loss: tensor(0.3254)\n",
      "39629 Training Loss: tensor(0.3250)\n",
      "39630 Training Loss: tensor(0.3245)\n",
      "39631 Training Loss: tensor(0.3250)\n",
      "39632 Training Loss: tensor(0.3245)\n",
      "39633 Training Loss: tensor(0.3243)\n",
      "39634 Training Loss: tensor(0.3248)\n",
      "39635 Training Loss: tensor(0.3252)\n",
      "39636 Training Loss: tensor(0.3241)\n",
      "39637 Training Loss: tensor(0.3257)\n",
      "39638 Training Loss: tensor(0.3249)\n",
      "39639 Training Loss: tensor(0.3245)\n",
      "39640 Training Loss: tensor(0.3241)\n",
      "39641 Training Loss: tensor(0.3248)\n",
      "39642 Training Loss: tensor(0.3241)\n",
      "39643 Training Loss: tensor(0.3246)\n",
      "39644 Training Loss: tensor(0.3256)\n",
      "39645 Training Loss: tensor(0.3242)\n",
      "39646 Training Loss: tensor(0.3253)\n",
      "39647 Training Loss: tensor(0.3243)\n",
      "39648 Training Loss: tensor(0.3250)\n",
      "39649 Training Loss: tensor(0.3238)\n",
      "39650 Training Loss: tensor(0.3252)\n",
      "39651 Training Loss: tensor(0.3242)\n",
      "39652 Training Loss: tensor(0.3244)\n",
      "39653 Training Loss: tensor(0.3252)\n",
      "39654 Training Loss: tensor(0.3246)\n",
      "39655 Training Loss: tensor(0.3242)\n",
      "39656 Training Loss: tensor(0.3252)\n",
      "39657 Training Loss: tensor(0.3248)\n",
      "39658 Training Loss: tensor(0.3250)\n",
      "39659 Training Loss: tensor(0.3244)\n",
      "39660 Training Loss: tensor(0.3240)\n",
      "39661 Training Loss: tensor(0.3245)\n",
      "39662 Training Loss: tensor(0.3253)\n",
      "39663 Training Loss: tensor(0.3248)\n",
      "39664 Training Loss: tensor(0.3246)\n",
      "39665 Training Loss: tensor(0.3246)\n",
      "39666 Training Loss: tensor(0.3250)\n",
      "39667 Training Loss: tensor(0.3247)\n",
      "39668 Training Loss: tensor(0.3246)\n",
      "39669 Training Loss: tensor(0.3243)\n",
      "39670 Training Loss: tensor(0.3246)\n",
      "39671 Training Loss: tensor(0.3252)\n",
      "39672 Training Loss: tensor(0.3253)\n",
      "39673 Training Loss: tensor(0.3260)\n",
      "39674 Training Loss: tensor(0.3247)\n",
      "39675 Training Loss: tensor(0.3242)\n",
      "39676 Training Loss: tensor(0.3243)\n",
      "39677 Training Loss: tensor(0.3253)\n",
      "39678 Training Loss: tensor(0.3240)\n",
      "39679 Training Loss: tensor(0.3247)\n",
      "39680 Training Loss: tensor(0.3247)\n",
      "39681 Training Loss: tensor(0.3242)\n",
      "39682 Training Loss: tensor(0.3243)\n",
      "39683 Training Loss: tensor(0.3245)\n",
      "39684 Training Loss: tensor(0.3245)\n",
      "39685 Training Loss: tensor(0.3254)\n",
      "39686 Training Loss: tensor(0.3237)\n",
      "39687 Training Loss: tensor(0.3249)\n",
      "39688 Training Loss: tensor(0.3244)\n",
      "39689 Training Loss: tensor(0.3245)\n",
      "39690 Training Loss: tensor(0.3245)\n",
      "39691 Training Loss: tensor(0.3237)\n",
      "39692 Training Loss: tensor(0.3247)\n",
      "39693 Training Loss: tensor(0.3249)\n",
      "39694 Training Loss: tensor(0.3247)\n",
      "39695 Training Loss: tensor(0.3246)\n",
      "39696 Training Loss: tensor(0.3243)\n",
      "39697 Training Loss: tensor(0.3247)\n",
      "39698 Training Loss: tensor(0.3245)\n",
      "39699 Training Loss: tensor(0.3252)\n",
      "39700 Training Loss: tensor(0.3249)\n",
      "39701 Training Loss: tensor(0.3259)\n",
      "39702 Training Loss: tensor(0.3250)\n",
      "39703 Training Loss: tensor(0.3249)\n",
      "39704 Training Loss: tensor(0.3250)\n",
      "39705 Training Loss: tensor(0.3252)\n",
      "39706 Training Loss: tensor(0.3238)\n",
      "39707 Training Loss: tensor(0.3242)\n",
      "39708 Training Loss: tensor(0.3249)\n",
      "39709 Training Loss: tensor(0.3255)\n",
      "39710 Training Loss: tensor(0.3241)\n",
      "39711 Training Loss: tensor(0.3247)\n",
      "39712 Training Loss: tensor(0.3253)\n",
      "39713 Training Loss: tensor(0.3248)\n",
      "39714 Training Loss: tensor(0.3245)\n",
      "39715 Training Loss: tensor(0.3245)\n",
      "39716 Training Loss: tensor(0.3243)\n",
      "39717 Training Loss: tensor(0.3248)\n",
      "39718 Training Loss: tensor(0.3243)\n",
      "39719 Training Loss: tensor(0.3244)\n",
      "39720 Training Loss: tensor(0.3244)\n",
      "39721 Training Loss: tensor(0.3248)\n",
      "39722 Training Loss: tensor(0.3244)\n",
      "39723 Training Loss: tensor(0.3250)\n",
      "39724 Training Loss: tensor(0.3249)\n",
      "39725 Training Loss: tensor(0.3244)\n",
      "39726 Training Loss: tensor(0.3244)\n",
      "39727 Training Loss: tensor(0.3241)\n",
      "39728 Training Loss: tensor(0.3235)\n",
      "39729 Training Loss: tensor(0.3248)\n",
      "39730 Training Loss: tensor(0.3245)\n",
      "39731 Training Loss: tensor(0.3240)\n",
      "39732 Training Loss: tensor(0.3242)\n",
      "39733 Training Loss: tensor(0.3254)\n",
      "39734 Training Loss: tensor(0.3246)\n",
      "39735 Training Loss: tensor(0.3241)\n",
      "39736 Training Loss: tensor(0.3244)\n",
      "39737 Training Loss: tensor(0.3256)\n",
      "39738 Training Loss: tensor(0.3242)\n",
      "39739 Training Loss: tensor(0.3246)\n",
      "39740 Training Loss: tensor(0.3247)\n",
      "39741 Training Loss: tensor(0.3246)\n",
      "39742 Training Loss: tensor(0.3244)\n",
      "39743 Training Loss: tensor(0.3266)\n",
      "39744 Training Loss: tensor(0.3248)\n",
      "39745 Training Loss: tensor(0.3236)\n",
      "39746 Training Loss: tensor(0.3251)\n",
      "39747 Training Loss: tensor(0.3241)\n",
      "39748 Training Loss: tensor(0.3243)\n",
      "39749 Training Loss: tensor(0.3242)\n",
      "39750 Training Loss: tensor(0.3244)\n",
      "39751 Training Loss: tensor(0.3239)\n",
      "39752 Training Loss: tensor(0.3253)\n",
      "39753 Training Loss: tensor(0.3255)\n",
      "39754 Training Loss: tensor(0.3242)\n",
      "39755 Training Loss: tensor(0.3244)\n",
      "39756 Training Loss: tensor(0.3251)\n",
      "39757 Training Loss: tensor(0.3253)\n",
      "39758 Training Loss: tensor(0.3249)\n",
      "39759 Training Loss: tensor(0.3246)\n",
      "39760 Training Loss: tensor(0.3246)\n",
      "39761 Training Loss: tensor(0.3241)\n",
      "39762 Training Loss: tensor(0.3253)\n",
      "39763 Training Loss: tensor(0.3250)\n",
      "39764 Training Loss: tensor(0.3244)\n",
      "39765 Training Loss: tensor(0.3250)\n",
      "39766 Training Loss: tensor(0.3249)\n",
      "39767 Training Loss: tensor(0.3253)\n",
      "39768 Training Loss: tensor(0.3247)\n",
      "39769 Training Loss: tensor(0.3240)\n",
      "39770 Training Loss: tensor(0.3237)\n",
      "39771 Training Loss: tensor(0.3236)\n",
      "39772 Training Loss: tensor(0.3242)\n",
      "39773 Training Loss: tensor(0.3235)\n",
      "39774 Training Loss: tensor(0.3239)\n",
      "39775 Training Loss: tensor(0.3247)\n",
      "39776 Training Loss: tensor(0.3240)\n",
      "39777 Training Loss: tensor(0.3250)\n",
      "39778 Training Loss: tensor(0.3246)\n",
      "39779 Training Loss: tensor(0.3250)\n",
      "39780 Training Loss: tensor(0.3256)\n",
      "39781 Training Loss: tensor(0.3252)\n",
      "39782 Training Loss: tensor(0.3248)\n",
      "39783 Training Loss: tensor(0.3268)\n",
      "39784 Training Loss: tensor(0.3254)\n",
      "39785 Training Loss: tensor(0.3242)\n",
      "39786 Training Loss: tensor(0.3248)\n",
      "39787 Training Loss: tensor(0.3242)\n",
      "39788 Training Loss: tensor(0.3247)\n",
      "39789 Training Loss: tensor(0.3240)\n",
      "39790 Training Loss: tensor(0.3246)\n",
      "39791 Training Loss: tensor(0.3250)\n",
      "39792 Training Loss: tensor(0.3262)\n",
      "39793 Training Loss: tensor(0.3246)\n",
      "39794 Training Loss: tensor(0.3244)\n",
      "39795 Training Loss: tensor(0.3247)\n",
      "39796 Training Loss: tensor(0.3249)\n",
      "39797 Training Loss: tensor(0.3245)\n",
      "39798 Training Loss: tensor(0.3250)\n",
      "39799 Training Loss: tensor(0.3249)\n",
      "39800 Training Loss: tensor(0.3247)\n",
      "39801 Training Loss: tensor(0.3240)\n",
      "39802 Training Loss: tensor(0.3244)\n",
      "39803 Training Loss: tensor(0.3262)\n",
      "39804 Training Loss: tensor(0.3255)\n",
      "39805 Training Loss: tensor(0.3244)\n",
      "39806 Training Loss: tensor(0.3246)\n",
      "39807 Training Loss: tensor(0.3258)\n",
      "39808 Training Loss: tensor(0.3256)\n",
      "39809 Training Loss: tensor(0.3241)\n",
      "39810 Training Loss: tensor(0.3246)\n",
      "39811 Training Loss: tensor(0.3245)\n",
      "39812 Training Loss: tensor(0.3243)\n",
      "39813 Training Loss: tensor(0.3250)\n",
      "39814 Training Loss: tensor(0.3244)\n",
      "39815 Training Loss: tensor(0.3245)\n",
      "39816 Training Loss: tensor(0.3246)\n",
      "39817 Training Loss: tensor(0.3244)\n",
      "39818 Training Loss: tensor(0.3239)\n",
      "39819 Training Loss: tensor(0.3252)\n",
      "39820 Training Loss: tensor(0.3244)\n",
      "39821 Training Loss: tensor(0.3250)\n",
      "39822 Training Loss: tensor(0.3242)\n",
      "39823 Training Loss: tensor(0.3238)\n",
      "39824 Training Loss: tensor(0.3249)\n",
      "39825 Training Loss: tensor(0.3245)\n",
      "39826 Training Loss: tensor(0.3242)\n",
      "39827 Training Loss: tensor(0.3250)\n",
      "39828 Training Loss: tensor(0.3246)\n",
      "39829 Training Loss: tensor(0.3250)\n",
      "39830 Training Loss: tensor(0.3239)\n",
      "39831 Training Loss: tensor(0.3251)\n",
      "39832 Training Loss: tensor(0.3235)\n",
      "39833 Training Loss: tensor(0.3245)\n",
      "39834 Training Loss: tensor(0.3241)\n",
      "39835 Training Loss: tensor(0.3241)\n",
      "39836 Training Loss: tensor(0.3247)\n",
      "39837 Training Loss: tensor(0.3242)\n",
      "39838 Training Loss: tensor(0.3251)\n",
      "39839 Training Loss: tensor(0.3245)\n",
      "39840 Training Loss: tensor(0.3253)\n",
      "39841 Training Loss: tensor(0.3249)\n",
      "39842 Training Loss: tensor(0.3249)\n",
      "39843 Training Loss: tensor(0.3264)\n",
      "39844 Training Loss: tensor(0.3238)\n",
      "39845 Training Loss: tensor(0.3243)\n",
      "39846 Training Loss: tensor(0.3243)\n",
      "39847 Training Loss: tensor(0.3251)\n",
      "39848 Training Loss: tensor(0.3251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39849 Training Loss: tensor(0.3252)\n",
      "39850 Training Loss: tensor(0.3243)\n",
      "39851 Training Loss: tensor(0.3247)\n",
      "39852 Training Loss: tensor(0.3248)\n",
      "39853 Training Loss: tensor(0.3240)\n",
      "39854 Training Loss: tensor(0.3248)\n",
      "39855 Training Loss: tensor(0.3251)\n",
      "39856 Training Loss: tensor(0.3247)\n",
      "39857 Training Loss: tensor(0.3247)\n",
      "39858 Training Loss: tensor(0.3247)\n",
      "39859 Training Loss: tensor(0.3252)\n",
      "39860 Training Loss: tensor(0.3247)\n",
      "39861 Training Loss: tensor(0.3245)\n",
      "39862 Training Loss: tensor(0.3242)\n",
      "39863 Training Loss: tensor(0.3244)\n",
      "39864 Training Loss: tensor(0.3249)\n",
      "39865 Training Loss: tensor(0.3247)\n",
      "39866 Training Loss: tensor(0.3244)\n",
      "39867 Training Loss: tensor(0.3256)\n",
      "39868 Training Loss: tensor(0.3243)\n",
      "39869 Training Loss: tensor(0.3248)\n",
      "39870 Training Loss: tensor(0.3248)\n",
      "39871 Training Loss: tensor(0.3241)\n",
      "39872 Training Loss: tensor(0.3246)\n",
      "39873 Training Loss: tensor(0.3245)\n",
      "39874 Training Loss: tensor(0.3239)\n",
      "39875 Training Loss: tensor(0.3255)\n",
      "39876 Training Loss: tensor(0.3241)\n",
      "39877 Training Loss: tensor(0.3245)\n",
      "39878 Training Loss: tensor(0.3243)\n",
      "39879 Training Loss: tensor(0.3246)\n",
      "39880 Training Loss: tensor(0.3250)\n",
      "39881 Training Loss: tensor(0.3251)\n",
      "39882 Training Loss: tensor(0.3250)\n",
      "39883 Training Loss: tensor(0.3245)\n",
      "39884 Training Loss: tensor(0.3242)\n",
      "39885 Training Loss: tensor(0.3246)\n",
      "39886 Training Loss: tensor(0.3251)\n",
      "39887 Training Loss: tensor(0.3246)\n",
      "39888 Training Loss: tensor(0.3255)\n",
      "39889 Training Loss: tensor(0.3248)\n",
      "39890 Training Loss: tensor(0.3251)\n",
      "39891 Training Loss: tensor(0.3247)\n",
      "39892 Training Loss: tensor(0.3244)\n",
      "39893 Training Loss: tensor(0.3240)\n",
      "39894 Training Loss: tensor(0.3249)\n",
      "39895 Training Loss: tensor(0.3244)\n",
      "39896 Training Loss: tensor(0.3240)\n",
      "39897 Training Loss: tensor(0.3261)\n",
      "39898 Training Loss: tensor(0.3244)\n",
      "39899 Training Loss: tensor(0.3239)\n",
      "39900 Training Loss: tensor(0.3241)\n",
      "39901 Training Loss: tensor(0.3246)\n",
      "39902 Training Loss: tensor(0.3245)\n",
      "39903 Training Loss: tensor(0.3247)\n",
      "39904 Training Loss: tensor(0.3262)\n",
      "39905 Training Loss: tensor(0.3241)\n",
      "39906 Training Loss: tensor(0.3250)\n",
      "39907 Training Loss: tensor(0.3252)\n",
      "39908 Training Loss: tensor(0.3246)\n",
      "39909 Training Loss: tensor(0.3246)\n",
      "39910 Training Loss: tensor(0.3251)\n",
      "39911 Training Loss: tensor(0.3258)\n",
      "39912 Training Loss: tensor(0.3247)\n",
      "39913 Training Loss: tensor(0.3243)\n",
      "39914 Training Loss: tensor(0.3244)\n",
      "39915 Training Loss: tensor(0.3246)\n",
      "39916 Training Loss: tensor(0.3244)\n",
      "39917 Training Loss: tensor(0.3245)\n",
      "39918 Training Loss: tensor(0.3248)\n",
      "39919 Training Loss: tensor(0.3246)\n",
      "39920 Training Loss: tensor(0.3239)\n",
      "39921 Training Loss: tensor(0.3246)\n",
      "39922 Training Loss: tensor(0.3244)\n",
      "39923 Training Loss: tensor(0.3249)\n",
      "39924 Training Loss: tensor(0.3246)\n",
      "39925 Training Loss: tensor(0.3240)\n",
      "39926 Training Loss: tensor(0.3245)\n",
      "39927 Training Loss: tensor(0.3246)\n",
      "39928 Training Loss: tensor(0.3241)\n",
      "39929 Training Loss: tensor(0.3244)\n",
      "39930 Training Loss: tensor(0.3251)\n",
      "39931 Training Loss: tensor(0.3251)\n",
      "39932 Training Loss: tensor(0.3248)\n",
      "39933 Training Loss: tensor(0.3245)\n",
      "39934 Training Loss: tensor(0.3242)\n",
      "39935 Training Loss: tensor(0.3245)\n",
      "39936 Training Loss: tensor(0.3256)\n",
      "39937 Training Loss: tensor(0.3247)\n",
      "39938 Training Loss: tensor(0.3243)\n",
      "39939 Training Loss: tensor(0.3239)\n",
      "39940 Training Loss: tensor(0.3251)\n",
      "39941 Training Loss: tensor(0.3242)\n",
      "39942 Training Loss: tensor(0.3248)\n",
      "39943 Training Loss: tensor(0.3243)\n",
      "39944 Training Loss: tensor(0.3247)\n",
      "39945 Training Loss: tensor(0.3243)\n",
      "39946 Training Loss: tensor(0.3248)\n",
      "39947 Training Loss: tensor(0.3250)\n",
      "39948 Training Loss: tensor(0.3245)\n",
      "39949 Training Loss: tensor(0.3264)\n",
      "39950 Training Loss: tensor(0.3241)\n",
      "39951 Training Loss: tensor(0.3242)\n",
      "39952 Training Loss: tensor(0.3257)\n",
      "39953 Training Loss: tensor(0.3244)\n",
      "39954 Training Loss: tensor(0.3248)\n",
      "39955 Training Loss: tensor(0.3240)\n",
      "39956 Training Loss: tensor(0.3246)\n",
      "39957 Training Loss: tensor(0.3256)\n",
      "39958 Training Loss: tensor(0.3244)\n",
      "39959 Training Loss: tensor(0.3251)\n",
      "39960 Training Loss: tensor(0.3245)\n",
      "39961 Training Loss: tensor(0.3247)\n",
      "39962 Training Loss: tensor(0.3239)\n",
      "39963 Training Loss: tensor(0.3257)\n",
      "39964 Training Loss: tensor(0.3244)\n",
      "39965 Training Loss: tensor(0.3261)\n",
      "39966 Training Loss: tensor(0.3242)\n",
      "39967 Training Loss: tensor(0.3242)\n",
      "39968 Training Loss: tensor(0.3238)\n",
      "39969 Training Loss: tensor(0.3244)\n",
      "39970 Training Loss: tensor(0.3250)\n",
      "39971 Training Loss: tensor(0.3242)\n",
      "39972 Training Loss: tensor(0.3251)\n",
      "39973 Training Loss: tensor(0.3247)\n",
      "39974 Training Loss: tensor(0.3239)\n",
      "39975 Training Loss: tensor(0.3238)\n",
      "39976 Training Loss: tensor(0.3244)\n",
      "39977 Training Loss: tensor(0.3248)\n",
      "39978 Training Loss: tensor(0.3242)\n",
      "39979 Training Loss: tensor(0.3255)\n",
      "39980 Training Loss: tensor(0.3251)\n",
      "39981 Training Loss: tensor(0.3247)\n",
      "39982 Training Loss: tensor(0.3253)\n",
      "39983 Training Loss: tensor(0.3246)\n",
      "39984 Training Loss: tensor(0.3245)\n",
      "39985 Training Loss: tensor(0.3237)\n",
      "39986 Training Loss: tensor(0.3246)\n",
      "39987 Training Loss: tensor(0.3243)\n",
      "39988 Training Loss: tensor(0.3241)\n",
      "39989 Training Loss: tensor(0.3242)\n",
      "39990 Training Loss: tensor(0.3252)\n",
      "39991 Training Loss: tensor(0.3244)\n",
      "39992 Training Loss: tensor(0.3241)\n",
      "39993 Training Loss: tensor(0.3238)\n",
      "39994 Training Loss: tensor(0.3249)\n",
      "39995 Training Loss: tensor(0.3245)\n",
      "39996 Training Loss: tensor(0.3244)\n",
      "39997 Training Loss: tensor(0.3241)\n",
      "39998 Training Loss: tensor(0.3247)\n",
      "39999 Training Loss: tensor(0.3249)\n",
      "40000 Training Loss: tensor(0.3240)\n",
      "40001 Training Loss: tensor(0.3245)\n",
      "40002 Training Loss: tensor(0.3247)\n",
      "40003 Training Loss: tensor(0.3240)\n",
      "40004 Training Loss: tensor(0.3245)\n",
      "40005 Training Loss: tensor(0.3242)\n",
      "40006 Training Loss: tensor(0.3250)\n",
      "40007 Training Loss: tensor(0.3248)\n",
      "40008 Training Loss: tensor(0.3241)\n",
      "40009 Training Loss: tensor(0.3240)\n",
      "40010 Training Loss: tensor(0.3243)\n",
      "40011 Training Loss: tensor(0.3241)\n",
      "40012 Training Loss: tensor(0.3253)\n",
      "40013 Training Loss: tensor(0.3237)\n",
      "40014 Training Loss: tensor(0.3237)\n",
      "40015 Training Loss: tensor(0.3241)\n",
      "40016 Training Loss: tensor(0.3241)\n",
      "40017 Training Loss: tensor(0.3243)\n",
      "40018 Training Loss: tensor(0.3241)\n",
      "40019 Training Loss: tensor(0.3244)\n",
      "40020 Training Loss: tensor(0.3248)\n",
      "40021 Training Loss: tensor(0.3247)\n",
      "40022 Training Loss: tensor(0.3241)\n",
      "40023 Training Loss: tensor(0.3249)\n",
      "40024 Training Loss: tensor(0.3242)\n",
      "40025 Training Loss: tensor(0.3247)\n",
      "40026 Training Loss: tensor(0.3240)\n",
      "40027 Training Loss: tensor(0.3260)\n",
      "40028 Training Loss: tensor(0.3251)\n",
      "40029 Training Loss: tensor(0.3249)\n",
      "40030 Training Loss: tensor(0.3243)\n",
      "40031 Training Loss: tensor(0.3236)\n",
      "40032 Training Loss: tensor(0.3248)\n",
      "40033 Training Loss: tensor(0.3250)\n",
      "40034 Training Loss: tensor(0.3245)\n",
      "40035 Training Loss: tensor(0.3243)\n",
      "40036 Training Loss: tensor(0.3243)\n",
      "40037 Training Loss: tensor(0.3246)\n",
      "40038 Training Loss: tensor(0.3243)\n",
      "40039 Training Loss: tensor(0.3248)\n",
      "40040 Training Loss: tensor(0.3244)\n",
      "40041 Training Loss: tensor(0.3244)\n",
      "40042 Training Loss: tensor(0.3255)\n",
      "40043 Training Loss: tensor(0.3250)\n",
      "40044 Training Loss: tensor(0.3252)\n",
      "40045 Training Loss: tensor(0.3246)\n",
      "40046 Training Loss: tensor(0.3240)\n",
      "40047 Training Loss: tensor(0.3253)\n",
      "40048 Training Loss: tensor(0.3239)\n",
      "40049 Training Loss: tensor(0.3241)\n",
      "40050 Training Loss: tensor(0.3243)\n",
      "40051 Training Loss: tensor(0.3242)\n",
      "40052 Training Loss: tensor(0.3257)\n",
      "40053 Training Loss: tensor(0.3244)\n",
      "40054 Training Loss: tensor(0.3242)\n",
      "40055 Training Loss: tensor(0.3248)\n",
      "40056 Training Loss: tensor(0.3242)\n",
      "40057 Training Loss: tensor(0.3258)\n",
      "40058 Training Loss: tensor(0.3253)\n",
      "40059 Training Loss: tensor(0.3241)\n",
      "40060 Training Loss: tensor(0.3241)\n",
      "40061 Training Loss: tensor(0.3253)\n",
      "40062 Training Loss: tensor(0.3243)\n",
      "40063 Training Loss: tensor(0.3241)\n",
      "40064 Training Loss: tensor(0.3240)\n",
      "40065 Training Loss: tensor(0.3240)\n",
      "40066 Training Loss: tensor(0.3242)\n",
      "40067 Training Loss: tensor(0.3247)\n",
      "40068 Training Loss: tensor(0.3252)\n",
      "40069 Training Loss: tensor(0.3254)\n",
      "40070 Training Loss: tensor(0.3253)\n",
      "40071 Training Loss: tensor(0.3257)\n",
      "40072 Training Loss: tensor(0.3240)\n",
      "40073 Training Loss: tensor(0.3247)\n",
      "40074 Training Loss: tensor(0.3256)\n",
      "40075 Training Loss: tensor(0.3240)\n",
      "40076 Training Loss: tensor(0.3248)\n",
      "40077 Training Loss: tensor(0.3246)\n",
      "40078 Training Loss: tensor(0.3246)\n",
      "40079 Training Loss: tensor(0.3250)\n",
      "40080 Training Loss: tensor(0.3240)\n",
      "40081 Training Loss: tensor(0.3258)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40082 Training Loss: tensor(0.3252)\n",
      "40083 Training Loss: tensor(0.3246)\n",
      "40084 Training Loss: tensor(0.3255)\n",
      "40085 Training Loss: tensor(0.3237)\n",
      "40086 Training Loss: tensor(0.3244)\n",
      "40087 Training Loss: tensor(0.3252)\n",
      "40088 Training Loss: tensor(0.3251)\n",
      "40089 Training Loss: tensor(0.3240)\n",
      "40090 Training Loss: tensor(0.3239)\n",
      "40091 Training Loss: tensor(0.3241)\n",
      "40092 Training Loss: tensor(0.3245)\n",
      "40093 Training Loss: tensor(0.3242)\n",
      "40094 Training Loss: tensor(0.3244)\n",
      "40095 Training Loss: tensor(0.3244)\n",
      "40096 Training Loss: tensor(0.3243)\n",
      "40097 Training Loss: tensor(0.3240)\n",
      "40098 Training Loss: tensor(0.3243)\n",
      "40099 Training Loss: tensor(0.3250)\n",
      "40100 Training Loss: tensor(0.3244)\n",
      "40101 Training Loss: tensor(0.3255)\n",
      "40102 Training Loss: tensor(0.3242)\n",
      "40103 Training Loss: tensor(0.3244)\n",
      "40104 Training Loss: tensor(0.3242)\n",
      "40105 Training Loss: tensor(0.3243)\n",
      "40106 Training Loss: tensor(0.3259)\n",
      "40107 Training Loss: tensor(0.3249)\n",
      "40108 Training Loss: tensor(0.3245)\n",
      "40109 Training Loss: tensor(0.3251)\n",
      "40110 Training Loss: tensor(0.3247)\n",
      "40111 Training Loss: tensor(0.3247)\n",
      "40112 Training Loss: tensor(0.3244)\n",
      "40113 Training Loss: tensor(0.3248)\n",
      "40114 Training Loss: tensor(0.3247)\n",
      "40115 Training Loss: tensor(0.3248)\n",
      "40116 Training Loss: tensor(0.3247)\n",
      "40117 Training Loss: tensor(0.3245)\n",
      "40118 Training Loss: tensor(0.3241)\n",
      "40119 Training Loss: tensor(0.3259)\n",
      "40120 Training Loss: tensor(0.3254)\n",
      "40121 Training Loss: tensor(0.3246)\n",
      "40122 Training Loss: tensor(0.3243)\n",
      "40123 Training Loss: tensor(0.3238)\n",
      "40124 Training Loss: tensor(0.3245)\n",
      "40125 Training Loss: tensor(0.3254)\n",
      "40126 Training Loss: tensor(0.3244)\n",
      "40127 Training Loss: tensor(0.3244)\n",
      "40128 Training Loss: tensor(0.3250)\n",
      "40129 Training Loss: tensor(0.3246)\n",
      "40130 Training Loss: tensor(0.3244)\n",
      "40131 Training Loss: tensor(0.3255)\n",
      "40132 Training Loss: tensor(0.3252)\n",
      "40133 Training Loss: tensor(0.3248)\n",
      "40134 Training Loss: tensor(0.3246)\n",
      "40135 Training Loss: tensor(0.3245)\n",
      "40136 Training Loss: tensor(0.3239)\n",
      "40137 Training Loss: tensor(0.3253)\n",
      "40138 Training Loss: tensor(0.3241)\n",
      "40139 Training Loss: tensor(0.3245)\n",
      "40140 Training Loss: tensor(0.3245)\n",
      "40141 Training Loss: tensor(0.3252)\n",
      "40142 Training Loss: tensor(0.3244)\n",
      "40143 Training Loss: tensor(0.3241)\n",
      "40144 Training Loss: tensor(0.3247)\n",
      "40145 Training Loss: tensor(0.3244)\n",
      "40146 Training Loss: tensor(0.3257)\n",
      "40147 Training Loss: tensor(0.3243)\n",
      "40148 Training Loss: tensor(0.3249)\n",
      "40149 Training Loss: tensor(0.3239)\n",
      "40150 Training Loss: tensor(0.3239)\n",
      "40151 Training Loss: tensor(0.3252)\n",
      "40152 Training Loss: tensor(0.3253)\n",
      "40153 Training Loss: tensor(0.3248)\n",
      "40154 Training Loss: tensor(0.3244)\n",
      "40155 Training Loss: tensor(0.3242)\n",
      "40156 Training Loss: tensor(0.3238)\n",
      "40157 Training Loss: tensor(0.3257)\n",
      "40158 Training Loss: tensor(0.3260)\n",
      "40159 Training Loss: tensor(0.3249)\n",
      "40160 Training Loss: tensor(0.3246)\n",
      "40161 Training Loss: tensor(0.3248)\n",
      "40162 Training Loss: tensor(0.3244)\n",
      "40163 Training Loss: tensor(0.3250)\n",
      "40164 Training Loss: tensor(0.3240)\n",
      "40165 Training Loss: tensor(0.3250)\n",
      "40166 Training Loss: tensor(0.3245)\n",
      "40167 Training Loss: tensor(0.3244)\n",
      "40168 Training Loss: tensor(0.3255)\n",
      "40169 Training Loss: tensor(0.3241)\n",
      "40170 Training Loss: tensor(0.3242)\n",
      "40171 Training Loss: tensor(0.3245)\n",
      "40172 Training Loss: tensor(0.3239)\n",
      "40173 Training Loss: tensor(0.3245)\n",
      "40174 Training Loss: tensor(0.3243)\n",
      "40175 Training Loss: tensor(0.3242)\n",
      "40176 Training Loss: tensor(0.3248)\n",
      "40177 Training Loss: tensor(0.3241)\n",
      "40178 Training Loss: tensor(0.3238)\n",
      "40179 Training Loss: tensor(0.3246)\n",
      "40180 Training Loss: tensor(0.3257)\n",
      "40181 Training Loss: tensor(0.3257)\n",
      "40182 Training Loss: tensor(0.3240)\n",
      "40183 Training Loss: tensor(0.3253)\n",
      "40184 Training Loss: tensor(0.3243)\n",
      "40185 Training Loss: tensor(0.3242)\n",
      "40186 Training Loss: tensor(0.3245)\n",
      "40187 Training Loss: tensor(0.3251)\n",
      "40188 Training Loss: tensor(0.3249)\n",
      "40189 Training Loss: tensor(0.3247)\n",
      "40190 Training Loss: tensor(0.3243)\n",
      "40191 Training Loss: tensor(0.3257)\n",
      "40192 Training Loss: tensor(0.3241)\n",
      "40193 Training Loss: tensor(0.3245)\n",
      "40194 Training Loss: tensor(0.3248)\n",
      "40195 Training Loss: tensor(0.3244)\n",
      "40196 Training Loss: tensor(0.3258)\n",
      "40197 Training Loss: tensor(0.3245)\n",
      "40198 Training Loss: tensor(0.3254)\n",
      "40199 Training Loss: tensor(0.3250)\n",
      "40200 Training Loss: tensor(0.3244)\n",
      "40201 Training Loss: tensor(0.3244)\n",
      "40202 Training Loss: tensor(0.3246)\n",
      "40203 Training Loss: tensor(0.3245)\n",
      "40204 Training Loss: tensor(0.3259)\n",
      "40205 Training Loss: tensor(0.3247)\n",
      "40206 Training Loss: tensor(0.3243)\n",
      "40207 Training Loss: tensor(0.3255)\n",
      "40208 Training Loss: tensor(0.3245)\n",
      "40209 Training Loss: tensor(0.3252)\n",
      "40210 Training Loss: tensor(0.3246)\n",
      "40211 Training Loss: tensor(0.3242)\n",
      "40212 Training Loss: tensor(0.3238)\n",
      "40213 Training Loss: tensor(0.3247)\n",
      "40214 Training Loss: tensor(0.3243)\n",
      "40215 Training Loss: tensor(0.3242)\n",
      "40216 Training Loss: tensor(0.3254)\n",
      "40217 Training Loss: tensor(0.3244)\n",
      "40218 Training Loss: tensor(0.3246)\n",
      "40219 Training Loss: tensor(0.3246)\n",
      "40220 Training Loss: tensor(0.3249)\n",
      "40221 Training Loss: tensor(0.3254)\n",
      "40222 Training Loss: tensor(0.3244)\n",
      "40223 Training Loss: tensor(0.3258)\n",
      "40224 Training Loss: tensor(0.3251)\n",
      "40225 Training Loss: tensor(0.3264)\n",
      "40226 Training Loss: tensor(0.3243)\n",
      "40227 Training Loss: tensor(0.3253)\n",
      "40228 Training Loss: tensor(0.3242)\n",
      "40229 Training Loss: tensor(0.3250)\n",
      "40230 Training Loss: tensor(0.3248)\n",
      "40231 Training Loss: tensor(0.3256)\n",
      "40232 Training Loss: tensor(0.3242)\n",
      "40233 Training Loss: tensor(0.3251)\n",
      "40234 Training Loss: tensor(0.3249)\n",
      "40235 Training Loss: tensor(0.3253)\n",
      "40236 Training Loss: tensor(0.3250)\n",
      "40237 Training Loss: tensor(0.3250)\n",
      "40238 Training Loss: tensor(0.3248)\n",
      "40239 Training Loss: tensor(0.3249)\n",
      "40240 Training Loss: tensor(0.3247)\n",
      "40241 Training Loss: tensor(0.3244)\n",
      "40242 Training Loss: tensor(0.3250)\n",
      "40243 Training Loss: tensor(0.3246)\n",
      "40244 Training Loss: tensor(0.3257)\n",
      "40245 Training Loss: tensor(0.3246)\n",
      "40246 Training Loss: tensor(0.3250)\n",
      "40247 Training Loss: tensor(0.3251)\n",
      "40248 Training Loss: tensor(0.3248)\n",
      "40249 Training Loss: tensor(0.3259)\n",
      "40250 Training Loss: tensor(0.3250)\n",
      "40251 Training Loss: tensor(0.3244)\n",
      "40252 Training Loss: tensor(0.3244)\n",
      "40253 Training Loss: tensor(0.3251)\n",
      "40254 Training Loss: tensor(0.3242)\n",
      "40255 Training Loss: tensor(0.3254)\n",
      "40256 Training Loss: tensor(0.3258)\n",
      "40257 Training Loss: tensor(0.3246)\n",
      "40258 Training Loss: tensor(0.3244)\n",
      "40259 Training Loss: tensor(0.3245)\n",
      "40260 Training Loss: tensor(0.3243)\n",
      "40261 Training Loss: tensor(0.3253)\n",
      "40262 Training Loss: tensor(0.3246)\n",
      "40263 Training Loss: tensor(0.3240)\n",
      "40264 Training Loss: tensor(0.3247)\n",
      "40265 Training Loss: tensor(0.3249)\n",
      "40266 Training Loss: tensor(0.3249)\n",
      "40267 Training Loss: tensor(0.3248)\n",
      "40268 Training Loss: tensor(0.3241)\n",
      "40269 Training Loss: tensor(0.3264)\n",
      "40270 Training Loss: tensor(0.3246)\n",
      "40271 Training Loss: tensor(0.3252)\n",
      "40272 Training Loss: tensor(0.3241)\n",
      "40273 Training Loss: tensor(0.3242)\n",
      "40274 Training Loss: tensor(0.3246)\n",
      "40275 Training Loss: tensor(0.3252)\n",
      "40276 Training Loss: tensor(0.3248)\n",
      "40277 Training Loss: tensor(0.3242)\n",
      "40278 Training Loss: tensor(0.3250)\n",
      "40279 Training Loss: tensor(0.3243)\n",
      "40280 Training Loss: tensor(0.3241)\n",
      "40281 Training Loss: tensor(0.3243)\n",
      "40282 Training Loss: tensor(0.3247)\n",
      "40283 Training Loss: tensor(0.3258)\n",
      "40284 Training Loss: tensor(0.3250)\n",
      "40285 Training Loss: tensor(0.3246)\n",
      "40286 Training Loss: tensor(0.3236)\n",
      "40287 Training Loss: tensor(0.3248)\n",
      "40288 Training Loss: tensor(0.3248)\n",
      "40289 Training Loss: tensor(0.3237)\n",
      "40290 Training Loss: tensor(0.3248)\n",
      "40291 Training Loss: tensor(0.3244)\n",
      "40292 Training Loss: tensor(0.3241)\n",
      "40293 Training Loss: tensor(0.3245)\n",
      "40294 Training Loss: tensor(0.3244)\n",
      "40295 Training Loss: tensor(0.3249)\n",
      "40296 Training Loss: tensor(0.3247)\n",
      "40297 Training Loss: tensor(0.3259)\n",
      "40298 Training Loss: tensor(0.3238)\n",
      "40299 Training Loss: tensor(0.3251)\n",
      "40300 Training Loss: tensor(0.3249)\n",
      "40301 Training Loss: tensor(0.3248)\n",
      "40302 Training Loss: tensor(0.3244)\n",
      "40303 Training Loss: tensor(0.3256)\n",
      "40304 Training Loss: tensor(0.3242)\n",
      "40305 Training Loss: tensor(0.3242)\n",
      "40306 Training Loss: tensor(0.3250)\n",
      "40307 Training Loss: tensor(0.3241)\n",
      "40308 Training Loss: tensor(0.3249)\n",
      "40309 Training Loss: tensor(0.3250)\n",
      "40310 Training Loss: tensor(0.3255)\n",
      "40311 Training Loss: tensor(0.3250)\n",
      "40312 Training Loss: tensor(0.3248)\n",
      "40313 Training Loss: tensor(0.3247)\n",
      "40314 Training Loss: tensor(0.3243)\n",
      "40315 Training Loss: tensor(0.3245)\n",
      "40316 Training Loss: tensor(0.3258)\n",
      "40317 Training Loss: tensor(0.3252)\n",
      "40318 Training Loss: tensor(0.3246)\n",
      "40319 Training Loss: tensor(0.3254)\n",
      "40320 Training Loss: tensor(0.3245)\n",
      "40321 Training Loss: tensor(0.3254)\n",
      "40322 Training Loss: tensor(0.3249)\n",
      "40323 Training Loss: tensor(0.3254)\n",
      "40324 Training Loss: tensor(0.3241)\n",
      "40325 Training Loss: tensor(0.3242)\n",
      "40326 Training Loss: tensor(0.3248)\n",
      "40327 Training Loss: tensor(0.3244)\n",
      "40328 Training Loss: tensor(0.3242)\n",
      "40329 Training Loss: tensor(0.3243)\n",
      "40330 Training Loss: tensor(0.3244)\n",
      "40331 Training Loss: tensor(0.3245)\n",
      "40332 Training Loss: tensor(0.3246)\n",
      "40333 Training Loss: tensor(0.3238)\n",
      "40334 Training Loss: tensor(0.3242)\n",
      "40335 Training Loss: tensor(0.3248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40336 Training Loss: tensor(0.3242)\n",
      "40337 Training Loss: tensor(0.3244)\n",
      "40338 Training Loss: tensor(0.3253)\n",
      "40339 Training Loss: tensor(0.3255)\n",
      "40340 Training Loss: tensor(0.3245)\n",
      "40341 Training Loss: tensor(0.3247)\n",
      "40342 Training Loss: tensor(0.3239)\n",
      "40343 Training Loss: tensor(0.3247)\n",
      "40344 Training Loss: tensor(0.3255)\n",
      "40345 Training Loss: tensor(0.3242)\n",
      "40346 Training Loss: tensor(0.3244)\n",
      "40347 Training Loss: tensor(0.3250)\n",
      "40348 Training Loss: tensor(0.3241)\n",
      "40349 Training Loss: tensor(0.3251)\n",
      "40350 Training Loss: tensor(0.3245)\n",
      "40351 Training Loss: tensor(0.3244)\n",
      "40352 Training Loss: tensor(0.3254)\n",
      "40353 Training Loss: tensor(0.3246)\n",
      "40354 Training Loss: tensor(0.3246)\n",
      "40355 Training Loss: tensor(0.3246)\n",
      "40356 Training Loss: tensor(0.3251)\n",
      "40357 Training Loss: tensor(0.3237)\n",
      "40358 Training Loss: tensor(0.3248)\n",
      "40359 Training Loss: tensor(0.3250)\n",
      "40360 Training Loss: tensor(0.3249)\n",
      "40361 Training Loss: tensor(0.3241)\n",
      "40362 Training Loss: tensor(0.3242)\n",
      "40363 Training Loss: tensor(0.3248)\n",
      "40364 Training Loss: tensor(0.3256)\n",
      "40365 Training Loss: tensor(0.3252)\n",
      "40366 Training Loss: tensor(0.3248)\n",
      "40367 Training Loss: tensor(0.3251)\n",
      "40368 Training Loss: tensor(0.3246)\n",
      "40369 Training Loss: tensor(0.3246)\n",
      "40370 Training Loss: tensor(0.3246)\n",
      "40371 Training Loss: tensor(0.3254)\n",
      "40372 Training Loss: tensor(0.3252)\n",
      "40373 Training Loss: tensor(0.3246)\n",
      "40374 Training Loss: tensor(0.3255)\n",
      "40375 Training Loss: tensor(0.3244)\n",
      "40376 Training Loss: tensor(0.3259)\n",
      "40377 Training Loss: tensor(0.3241)\n",
      "40378 Training Loss: tensor(0.3246)\n",
      "40379 Training Loss: tensor(0.3240)\n",
      "40380 Training Loss: tensor(0.3243)\n",
      "40381 Training Loss: tensor(0.3239)\n",
      "40382 Training Loss: tensor(0.3247)\n",
      "40383 Training Loss: tensor(0.3253)\n",
      "40384 Training Loss: tensor(0.3238)\n",
      "40385 Training Loss: tensor(0.3247)\n",
      "40386 Training Loss: tensor(0.3249)\n",
      "40387 Training Loss: tensor(0.3250)\n",
      "40388 Training Loss: tensor(0.3249)\n",
      "40389 Training Loss: tensor(0.3256)\n",
      "40390 Training Loss: tensor(0.3247)\n",
      "40391 Training Loss: tensor(0.3255)\n",
      "40392 Training Loss: tensor(0.3248)\n",
      "40393 Training Loss: tensor(0.3252)\n",
      "40394 Training Loss: tensor(0.3247)\n",
      "40395 Training Loss: tensor(0.3245)\n",
      "40396 Training Loss: tensor(0.3238)\n",
      "40397 Training Loss: tensor(0.3255)\n",
      "40398 Training Loss: tensor(0.3253)\n",
      "40399 Training Loss: tensor(0.3247)\n",
      "40400 Training Loss: tensor(0.3246)\n",
      "40401 Training Loss: tensor(0.3238)\n",
      "40402 Training Loss: tensor(0.3251)\n",
      "40403 Training Loss: tensor(0.3245)\n",
      "40404 Training Loss: tensor(0.3242)\n",
      "40405 Training Loss: tensor(0.3242)\n",
      "40406 Training Loss: tensor(0.3253)\n",
      "40407 Training Loss: tensor(0.3246)\n",
      "40408 Training Loss: tensor(0.3242)\n",
      "40409 Training Loss: tensor(0.3246)\n",
      "40410 Training Loss: tensor(0.3238)\n",
      "40411 Training Loss: tensor(0.3245)\n",
      "40412 Training Loss: tensor(0.3252)\n",
      "40413 Training Loss: tensor(0.3253)\n",
      "40414 Training Loss: tensor(0.3246)\n",
      "40415 Training Loss: tensor(0.3244)\n",
      "40416 Training Loss: tensor(0.3250)\n",
      "40417 Training Loss: tensor(0.3245)\n",
      "40418 Training Loss: tensor(0.3260)\n",
      "40419 Training Loss: tensor(0.3249)\n",
      "40420 Training Loss: tensor(0.3259)\n",
      "40421 Training Loss: tensor(0.3242)\n",
      "40422 Training Loss: tensor(0.3241)\n",
      "40423 Training Loss: tensor(0.3250)\n",
      "40424 Training Loss: tensor(0.3243)\n",
      "40425 Training Loss: tensor(0.3243)\n",
      "40426 Training Loss: tensor(0.3243)\n",
      "40427 Training Loss: tensor(0.3243)\n",
      "40428 Training Loss: tensor(0.3244)\n",
      "40429 Training Loss: tensor(0.3242)\n",
      "40430 Training Loss: tensor(0.3246)\n",
      "40431 Training Loss: tensor(0.3244)\n",
      "40432 Training Loss: tensor(0.3252)\n",
      "40433 Training Loss: tensor(0.3250)\n",
      "40434 Training Loss: tensor(0.3238)\n",
      "40435 Training Loss: tensor(0.3258)\n",
      "40436 Training Loss: tensor(0.3254)\n",
      "40437 Training Loss: tensor(0.3238)\n",
      "40438 Training Loss: tensor(0.3244)\n",
      "40439 Training Loss: tensor(0.3242)\n",
      "40440 Training Loss: tensor(0.3242)\n",
      "40441 Training Loss: tensor(0.3243)\n",
      "40442 Training Loss: tensor(0.3244)\n",
      "40443 Training Loss: tensor(0.3242)\n",
      "40444 Training Loss: tensor(0.3244)\n",
      "40445 Training Loss: tensor(0.3245)\n",
      "40446 Training Loss: tensor(0.3249)\n",
      "40447 Training Loss: tensor(0.3241)\n",
      "40448 Training Loss: tensor(0.3244)\n",
      "40449 Training Loss: tensor(0.3246)\n",
      "40450 Training Loss: tensor(0.3241)\n",
      "40451 Training Loss: tensor(0.3251)\n",
      "40452 Training Loss: tensor(0.3243)\n",
      "40453 Training Loss: tensor(0.3249)\n",
      "40454 Training Loss: tensor(0.3251)\n",
      "40455 Training Loss: tensor(0.3242)\n",
      "40456 Training Loss: tensor(0.3245)\n",
      "40457 Training Loss: tensor(0.3256)\n",
      "40458 Training Loss: tensor(0.3249)\n",
      "40459 Training Loss: tensor(0.3246)\n",
      "40460 Training Loss: tensor(0.3244)\n",
      "40461 Training Loss: tensor(0.3247)\n",
      "40462 Training Loss: tensor(0.3249)\n",
      "40463 Training Loss: tensor(0.3244)\n",
      "40464 Training Loss: tensor(0.3241)\n",
      "40465 Training Loss: tensor(0.3249)\n",
      "40466 Training Loss: tensor(0.3248)\n",
      "40467 Training Loss: tensor(0.3246)\n",
      "40468 Training Loss: tensor(0.3238)\n",
      "40469 Training Loss: tensor(0.3245)\n",
      "40470 Training Loss: tensor(0.3245)\n",
      "40471 Training Loss: tensor(0.3246)\n",
      "40472 Training Loss: tensor(0.3239)\n",
      "40473 Training Loss: tensor(0.3242)\n",
      "40474 Training Loss: tensor(0.3239)\n",
      "40475 Training Loss: tensor(0.3249)\n",
      "40476 Training Loss: tensor(0.3241)\n",
      "40477 Training Loss: tensor(0.3238)\n",
      "40478 Training Loss: tensor(0.3258)\n",
      "40479 Training Loss: tensor(0.3241)\n",
      "40480 Training Loss: tensor(0.3241)\n",
      "40481 Training Loss: tensor(0.3246)\n",
      "40482 Training Loss: tensor(0.3246)\n",
      "40483 Training Loss: tensor(0.3241)\n",
      "40484 Training Loss: tensor(0.3247)\n",
      "40485 Training Loss: tensor(0.3243)\n",
      "40486 Training Loss: tensor(0.3242)\n",
      "40487 Training Loss: tensor(0.3239)\n",
      "40488 Training Loss: tensor(0.3239)\n",
      "40489 Training Loss: tensor(0.3248)\n",
      "40490 Training Loss: tensor(0.3241)\n",
      "40491 Training Loss: tensor(0.3247)\n",
      "40492 Training Loss: tensor(0.3249)\n",
      "40493 Training Loss: tensor(0.3254)\n",
      "40494 Training Loss: tensor(0.3249)\n",
      "40495 Training Loss: tensor(0.3249)\n",
      "40496 Training Loss: tensor(0.3244)\n",
      "40497 Training Loss: tensor(0.3254)\n",
      "40498 Training Loss: tensor(0.3252)\n",
      "40499 Training Loss: tensor(0.3249)\n",
      "40500 Training Loss: tensor(0.3256)\n",
      "40501 Training Loss: tensor(0.3249)\n",
      "40502 Training Loss: tensor(0.3239)\n",
      "40503 Training Loss: tensor(0.3249)\n",
      "40504 Training Loss: tensor(0.3243)\n",
      "40505 Training Loss: tensor(0.3258)\n",
      "40506 Training Loss: tensor(0.3246)\n",
      "40507 Training Loss: tensor(0.3240)\n",
      "40508 Training Loss: tensor(0.3242)\n",
      "40509 Training Loss: tensor(0.3245)\n",
      "40510 Training Loss: tensor(0.3251)\n",
      "40511 Training Loss: tensor(0.3248)\n",
      "40512 Training Loss: tensor(0.3239)\n",
      "40513 Training Loss: tensor(0.3241)\n",
      "40514 Training Loss: tensor(0.3245)\n",
      "40515 Training Loss: tensor(0.3237)\n",
      "40516 Training Loss: tensor(0.3252)\n",
      "40517 Training Loss: tensor(0.3241)\n",
      "40518 Training Loss: tensor(0.3246)\n",
      "40519 Training Loss: tensor(0.3246)\n",
      "40520 Training Loss: tensor(0.3246)\n",
      "40521 Training Loss: tensor(0.3244)\n",
      "40522 Training Loss: tensor(0.3248)\n",
      "40523 Training Loss: tensor(0.3250)\n",
      "40524 Training Loss: tensor(0.3266)\n",
      "40525 Training Loss: tensor(0.3242)\n",
      "40526 Training Loss: tensor(0.3249)\n",
      "40527 Training Loss: tensor(0.3243)\n",
      "40528 Training Loss: tensor(0.3245)\n",
      "40529 Training Loss: tensor(0.3247)\n",
      "40530 Training Loss: tensor(0.3245)\n",
      "40531 Training Loss: tensor(0.3242)\n",
      "40532 Training Loss: tensor(0.3249)\n",
      "40533 Training Loss: tensor(0.3247)\n",
      "40534 Training Loss: tensor(0.3253)\n",
      "40535 Training Loss: tensor(0.3253)\n",
      "40536 Training Loss: tensor(0.3241)\n",
      "40537 Training Loss: tensor(0.3247)\n",
      "40538 Training Loss: tensor(0.3254)\n",
      "40539 Training Loss: tensor(0.3254)\n",
      "40540 Training Loss: tensor(0.3239)\n",
      "40541 Training Loss: tensor(0.3243)\n",
      "40542 Training Loss: tensor(0.3249)\n",
      "40543 Training Loss: tensor(0.3263)\n",
      "40544 Training Loss: tensor(0.3257)\n",
      "40545 Training Loss: tensor(0.3250)\n",
      "40546 Training Loss: tensor(0.3257)\n",
      "40547 Training Loss: tensor(0.3247)\n",
      "40548 Training Loss: tensor(0.3249)\n",
      "40549 Training Loss: tensor(0.3250)\n",
      "40550 Training Loss: tensor(0.3255)\n",
      "40551 Training Loss: tensor(0.3239)\n",
      "40552 Training Loss: tensor(0.3250)\n",
      "40553 Training Loss: tensor(0.3255)\n",
      "40554 Training Loss: tensor(0.3252)\n",
      "40555 Training Loss: tensor(0.3246)\n",
      "40556 Training Loss: tensor(0.3255)\n",
      "40557 Training Loss: tensor(0.3243)\n",
      "40558 Training Loss: tensor(0.3255)\n",
      "40559 Training Loss: tensor(0.3239)\n",
      "40560 Training Loss: tensor(0.3248)\n",
      "40561 Training Loss: tensor(0.3248)\n",
      "40562 Training Loss: tensor(0.3252)\n",
      "40563 Training Loss: tensor(0.3247)\n",
      "40564 Training Loss: tensor(0.3256)\n",
      "40565 Training Loss: tensor(0.3243)\n",
      "40566 Training Loss: tensor(0.3248)\n",
      "40567 Training Loss: tensor(0.3247)\n",
      "40568 Training Loss: tensor(0.3253)\n",
      "40569 Training Loss: tensor(0.3248)\n",
      "40570 Training Loss: tensor(0.3244)\n",
      "40571 Training Loss: tensor(0.3251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40572 Training Loss: tensor(0.3244)\n",
      "40573 Training Loss: tensor(0.3246)\n",
      "40574 Training Loss: tensor(0.3255)\n",
      "40575 Training Loss: tensor(0.3242)\n",
      "40576 Training Loss: tensor(0.3239)\n",
      "40577 Training Loss: tensor(0.3247)\n",
      "40578 Training Loss: tensor(0.3249)\n",
      "40579 Training Loss: tensor(0.3248)\n",
      "40580 Training Loss: tensor(0.3254)\n",
      "40581 Training Loss: tensor(0.3247)\n",
      "40582 Training Loss: tensor(0.3249)\n",
      "40583 Training Loss: tensor(0.3242)\n",
      "40584 Training Loss: tensor(0.3241)\n",
      "40585 Training Loss: tensor(0.3244)\n",
      "40586 Training Loss: tensor(0.3240)\n",
      "40587 Training Loss: tensor(0.3243)\n",
      "40588 Training Loss: tensor(0.3244)\n",
      "40589 Training Loss: tensor(0.3249)\n",
      "40590 Training Loss: tensor(0.3241)\n",
      "40591 Training Loss: tensor(0.3246)\n",
      "40592 Training Loss: tensor(0.3244)\n",
      "40593 Training Loss: tensor(0.3244)\n",
      "40594 Training Loss: tensor(0.3248)\n",
      "40595 Training Loss: tensor(0.3256)\n",
      "40596 Training Loss: tensor(0.3249)\n",
      "40597 Training Loss: tensor(0.3249)\n",
      "40598 Training Loss: tensor(0.3242)\n",
      "40599 Training Loss: tensor(0.3242)\n",
      "40600 Training Loss: tensor(0.3255)\n",
      "40601 Training Loss: tensor(0.3252)\n",
      "40602 Training Loss: tensor(0.3244)\n",
      "40603 Training Loss: tensor(0.3240)\n",
      "40604 Training Loss: tensor(0.3255)\n",
      "40605 Training Loss: tensor(0.3239)\n",
      "40606 Training Loss: tensor(0.3247)\n",
      "40607 Training Loss: tensor(0.3241)\n",
      "40608 Training Loss: tensor(0.3242)\n",
      "40609 Training Loss: tensor(0.3254)\n",
      "40610 Training Loss: tensor(0.3248)\n",
      "40611 Training Loss: tensor(0.3248)\n",
      "40612 Training Loss: tensor(0.3250)\n",
      "40613 Training Loss: tensor(0.3245)\n",
      "40614 Training Loss: tensor(0.3248)\n",
      "40615 Training Loss: tensor(0.3245)\n",
      "40616 Training Loss: tensor(0.3237)\n",
      "40617 Training Loss: tensor(0.3245)\n",
      "40618 Training Loss: tensor(0.3246)\n",
      "40619 Training Loss: tensor(0.3254)\n",
      "40620 Training Loss: tensor(0.3245)\n",
      "40621 Training Loss: tensor(0.3257)\n",
      "40622 Training Loss: tensor(0.3239)\n",
      "40623 Training Loss: tensor(0.3242)\n",
      "40624 Training Loss: tensor(0.3244)\n",
      "40625 Training Loss: tensor(0.3247)\n",
      "40626 Training Loss: tensor(0.3241)\n",
      "40627 Training Loss: tensor(0.3252)\n",
      "40628 Training Loss: tensor(0.3249)\n",
      "40629 Training Loss: tensor(0.3246)\n",
      "40630 Training Loss: tensor(0.3239)\n",
      "40631 Training Loss: tensor(0.3247)\n",
      "40632 Training Loss: tensor(0.3245)\n",
      "40633 Training Loss: tensor(0.3251)\n",
      "40634 Training Loss: tensor(0.3239)\n",
      "40635 Training Loss: tensor(0.3247)\n",
      "40636 Training Loss: tensor(0.3244)\n",
      "40637 Training Loss: tensor(0.3239)\n",
      "40638 Training Loss: tensor(0.3246)\n",
      "40639 Training Loss: tensor(0.3251)\n",
      "40640 Training Loss: tensor(0.3246)\n",
      "40641 Training Loss: tensor(0.3240)\n",
      "40642 Training Loss: tensor(0.3249)\n",
      "40643 Training Loss: tensor(0.3251)\n",
      "40644 Training Loss: tensor(0.3264)\n",
      "40645 Training Loss: tensor(0.3245)\n",
      "40646 Training Loss: tensor(0.3245)\n",
      "40647 Training Loss: tensor(0.3246)\n",
      "40648 Training Loss: tensor(0.3242)\n",
      "40649 Training Loss: tensor(0.3244)\n",
      "40650 Training Loss: tensor(0.3238)\n",
      "40651 Training Loss: tensor(0.3247)\n",
      "40652 Training Loss: tensor(0.3241)\n",
      "40653 Training Loss: tensor(0.3248)\n",
      "40654 Training Loss: tensor(0.3245)\n",
      "40655 Training Loss: tensor(0.3262)\n",
      "40656 Training Loss: tensor(0.3250)\n",
      "40657 Training Loss: tensor(0.3242)\n",
      "40658 Training Loss: tensor(0.3251)\n",
      "40659 Training Loss: tensor(0.3256)\n",
      "40660 Training Loss: tensor(0.3240)\n",
      "40661 Training Loss: tensor(0.3252)\n",
      "40662 Training Loss: tensor(0.3244)\n",
      "40663 Training Loss: tensor(0.3246)\n",
      "40664 Training Loss: tensor(0.3250)\n",
      "40665 Training Loss: tensor(0.3256)\n",
      "40666 Training Loss: tensor(0.3242)\n",
      "40667 Training Loss: tensor(0.3242)\n",
      "40668 Training Loss: tensor(0.3247)\n",
      "40669 Training Loss: tensor(0.3248)\n",
      "40670 Training Loss: tensor(0.3248)\n",
      "40671 Training Loss: tensor(0.3242)\n",
      "40672 Training Loss: tensor(0.3241)\n",
      "40673 Training Loss: tensor(0.3239)\n",
      "40674 Training Loss: tensor(0.3241)\n",
      "40675 Training Loss: tensor(0.3242)\n",
      "40676 Training Loss: tensor(0.3245)\n",
      "40677 Training Loss: tensor(0.3244)\n",
      "40678 Training Loss: tensor(0.3247)\n",
      "40679 Training Loss: tensor(0.3240)\n",
      "40680 Training Loss: tensor(0.3240)\n",
      "40681 Training Loss: tensor(0.3238)\n",
      "40682 Training Loss: tensor(0.3238)\n",
      "40683 Training Loss: tensor(0.3243)\n",
      "40684 Training Loss: tensor(0.3240)\n",
      "40685 Training Loss: tensor(0.3252)\n",
      "40686 Training Loss: tensor(0.3244)\n",
      "40687 Training Loss: tensor(0.3244)\n",
      "40688 Training Loss: tensor(0.3245)\n",
      "40689 Training Loss: tensor(0.3262)\n",
      "40690 Training Loss: tensor(0.3237)\n",
      "40691 Training Loss: tensor(0.3241)\n",
      "40692 Training Loss: tensor(0.3252)\n",
      "40693 Training Loss: tensor(0.3257)\n",
      "40694 Training Loss: tensor(0.3259)\n",
      "40695 Training Loss: tensor(0.3249)\n",
      "40696 Training Loss: tensor(0.3240)\n",
      "40697 Training Loss: tensor(0.3241)\n",
      "40698 Training Loss: tensor(0.3258)\n",
      "40699 Training Loss: tensor(0.3248)\n",
      "40700 Training Loss: tensor(0.3241)\n",
      "40701 Training Loss: tensor(0.3243)\n",
      "40702 Training Loss: tensor(0.3247)\n",
      "40703 Training Loss: tensor(0.3244)\n",
      "40704 Training Loss: tensor(0.3243)\n",
      "40705 Training Loss: tensor(0.3243)\n",
      "40706 Training Loss: tensor(0.3249)\n",
      "40707 Training Loss: tensor(0.3238)\n",
      "40708 Training Loss: tensor(0.3245)\n",
      "40709 Training Loss: tensor(0.3240)\n",
      "40710 Training Loss: tensor(0.3236)\n",
      "40711 Training Loss: tensor(0.3237)\n",
      "40712 Training Loss: tensor(0.3241)\n",
      "40713 Training Loss: tensor(0.3248)\n",
      "40714 Training Loss: tensor(0.3253)\n",
      "40715 Training Loss: tensor(0.3251)\n",
      "40716 Training Loss: tensor(0.3238)\n",
      "40717 Training Loss: tensor(0.3243)\n",
      "40718 Training Loss: tensor(0.3239)\n",
      "40719 Training Loss: tensor(0.3247)\n",
      "40720 Training Loss: tensor(0.3246)\n",
      "40721 Training Loss: tensor(0.3242)\n",
      "40722 Training Loss: tensor(0.3250)\n",
      "40723 Training Loss: tensor(0.3255)\n",
      "40724 Training Loss: tensor(0.3246)\n",
      "40725 Training Loss: tensor(0.3246)\n",
      "40726 Training Loss: tensor(0.3244)\n",
      "40727 Training Loss: tensor(0.3239)\n",
      "40728 Training Loss: tensor(0.3244)\n",
      "40729 Training Loss: tensor(0.3243)\n",
      "40730 Training Loss: tensor(0.3243)\n",
      "40731 Training Loss: tensor(0.3248)\n",
      "40732 Training Loss: tensor(0.3243)\n",
      "40733 Training Loss: tensor(0.3245)\n",
      "40734 Training Loss: tensor(0.3252)\n",
      "40735 Training Loss: tensor(0.3246)\n",
      "40736 Training Loss: tensor(0.3259)\n",
      "40737 Training Loss: tensor(0.3240)\n",
      "40738 Training Loss: tensor(0.3242)\n",
      "40739 Training Loss: tensor(0.3243)\n",
      "40740 Training Loss: tensor(0.3248)\n",
      "40741 Training Loss: tensor(0.3237)\n",
      "40742 Training Loss: tensor(0.3238)\n",
      "40743 Training Loss: tensor(0.3255)\n",
      "40744 Training Loss: tensor(0.3247)\n",
      "40745 Training Loss: tensor(0.3249)\n",
      "40746 Training Loss: tensor(0.3241)\n",
      "40747 Training Loss: tensor(0.3251)\n",
      "40748 Training Loss: tensor(0.3241)\n",
      "40749 Training Loss: tensor(0.3248)\n",
      "40750 Training Loss: tensor(0.3254)\n",
      "40751 Training Loss: tensor(0.3243)\n",
      "40752 Training Loss: tensor(0.3245)\n",
      "40753 Training Loss: tensor(0.3245)\n",
      "40754 Training Loss: tensor(0.3245)\n",
      "40755 Training Loss: tensor(0.3247)\n",
      "40756 Training Loss: tensor(0.3247)\n",
      "40757 Training Loss: tensor(0.3251)\n",
      "40758 Training Loss: tensor(0.3248)\n",
      "40759 Training Loss: tensor(0.3244)\n",
      "40760 Training Loss: tensor(0.3245)\n",
      "40761 Training Loss: tensor(0.3243)\n",
      "40762 Training Loss: tensor(0.3249)\n",
      "40763 Training Loss: tensor(0.3250)\n",
      "40764 Training Loss: tensor(0.3241)\n",
      "40765 Training Loss: tensor(0.3246)\n",
      "40766 Training Loss: tensor(0.3241)\n",
      "40767 Training Loss: tensor(0.3241)\n",
      "40768 Training Loss: tensor(0.3237)\n",
      "40769 Training Loss: tensor(0.3243)\n",
      "40770 Training Loss: tensor(0.3249)\n",
      "40771 Training Loss: tensor(0.3245)\n",
      "40772 Training Loss: tensor(0.3247)\n",
      "40773 Training Loss: tensor(0.3242)\n",
      "40774 Training Loss: tensor(0.3249)\n",
      "40775 Training Loss: tensor(0.3250)\n",
      "40776 Training Loss: tensor(0.3267)\n",
      "40777 Training Loss: tensor(0.3239)\n",
      "40778 Training Loss: tensor(0.3252)\n",
      "40779 Training Loss: tensor(0.3242)\n",
      "40780 Training Loss: tensor(0.3237)\n",
      "40781 Training Loss: tensor(0.3243)\n",
      "40782 Training Loss: tensor(0.3242)\n",
      "40783 Training Loss: tensor(0.3266)\n",
      "40784 Training Loss: tensor(0.3245)\n",
      "40785 Training Loss: tensor(0.3243)\n",
      "40786 Training Loss: tensor(0.3241)\n",
      "40787 Training Loss: tensor(0.3239)\n",
      "40788 Training Loss: tensor(0.3240)\n",
      "40789 Training Loss: tensor(0.3241)\n",
      "40790 Training Loss: tensor(0.3241)\n",
      "40791 Training Loss: tensor(0.3239)\n",
      "40792 Training Loss: tensor(0.3258)\n",
      "40793 Training Loss: tensor(0.3242)\n",
      "40794 Training Loss: tensor(0.3253)\n",
      "40795 Training Loss: tensor(0.3248)\n",
      "40796 Training Loss: tensor(0.3255)\n",
      "40797 Training Loss: tensor(0.3246)\n",
      "40798 Training Loss: tensor(0.3250)\n",
      "40799 Training Loss: tensor(0.3242)\n",
      "40800 Training Loss: tensor(0.3251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40801 Training Loss: tensor(0.3254)\n",
      "40802 Training Loss: tensor(0.3244)\n",
      "40803 Training Loss: tensor(0.3253)\n",
      "40804 Training Loss: tensor(0.3255)\n",
      "40805 Training Loss: tensor(0.3242)\n",
      "40806 Training Loss: tensor(0.3250)\n",
      "40807 Training Loss: tensor(0.3253)\n",
      "40808 Training Loss: tensor(0.3242)\n",
      "40809 Training Loss: tensor(0.3243)\n",
      "40810 Training Loss: tensor(0.3245)\n",
      "40811 Training Loss: tensor(0.3247)\n",
      "40812 Training Loss: tensor(0.3244)\n",
      "40813 Training Loss: tensor(0.3253)\n",
      "40814 Training Loss: tensor(0.3246)\n",
      "40815 Training Loss: tensor(0.3252)\n",
      "40816 Training Loss: tensor(0.3247)\n",
      "40817 Training Loss: tensor(0.3245)\n",
      "40818 Training Loss: tensor(0.3249)\n",
      "40819 Training Loss: tensor(0.3248)\n",
      "40820 Training Loss: tensor(0.3245)\n",
      "40821 Training Loss: tensor(0.3249)\n",
      "40822 Training Loss: tensor(0.3243)\n",
      "40823 Training Loss: tensor(0.3258)\n",
      "40824 Training Loss: tensor(0.3243)\n",
      "40825 Training Loss: tensor(0.3250)\n",
      "40826 Training Loss: tensor(0.3244)\n",
      "40827 Training Loss: tensor(0.3253)\n",
      "40828 Training Loss: tensor(0.3243)\n",
      "40829 Training Loss: tensor(0.3242)\n",
      "40830 Training Loss: tensor(0.3255)\n",
      "40831 Training Loss: tensor(0.3245)\n",
      "40832 Training Loss: tensor(0.3240)\n",
      "40833 Training Loss: tensor(0.3247)\n",
      "40834 Training Loss: tensor(0.3253)\n",
      "40835 Training Loss: tensor(0.3249)\n",
      "40836 Training Loss: tensor(0.3254)\n",
      "40837 Training Loss: tensor(0.3245)\n",
      "40838 Training Loss: tensor(0.3244)\n",
      "40839 Training Loss: tensor(0.3243)\n",
      "40840 Training Loss: tensor(0.3238)\n",
      "40841 Training Loss: tensor(0.3241)\n",
      "40842 Training Loss: tensor(0.3242)\n",
      "40843 Training Loss: tensor(0.3241)\n",
      "40844 Training Loss: tensor(0.3245)\n",
      "40845 Training Loss: tensor(0.3250)\n",
      "40846 Training Loss: tensor(0.3255)\n",
      "40847 Training Loss: tensor(0.3243)\n",
      "40848 Training Loss: tensor(0.3255)\n",
      "40849 Training Loss: tensor(0.3243)\n",
      "40850 Training Loss: tensor(0.3246)\n",
      "40851 Training Loss: tensor(0.3246)\n",
      "40852 Training Loss: tensor(0.3242)\n",
      "40853 Training Loss: tensor(0.3241)\n",
      "40854 Training Loss: tensor(0.3242)\n",
      "40855 Training Loss: tensor(0.3237)\n",
      "40856 Training Loss: tensor(0.3252)\n",
      "40857 Training Loss: tensor(0.3236)\n",
      "40858 Training Loss: tensor(0.3243)\n",
      "40859 Training Loss: tensor(0.3247)\n",
      "40860 Training Loss: tensor(0.3244)\n",
      "40861 Training Loss: tensor(0.3253)\n",
      "40862 Training Loss: tensor(0.3239)\n",
      "40863 Training Loss: tensor(0.3240)\n",
      "40864 Training Loss: tensor(0.3250)\n",
      "40865 Training Loss: tensor(0.3235)\n",
      "40866 Training Loss: tensor(0.3248)\n",
      "40867 Training Loss: tensor(0.3244)\n",
      "40868 Training Loss: tensor(0.3244)\n",
      "40869 Training Loss: tensor(0.3243)\n",
      "40870 Training Loss: tensor(0.3239)\n",
      "40871 Training Loss: tensor(0.3240)\n",
      "40872 Training Loss: tensor(0.3237)\n",
      "40873 Training Loss: tensor(0.3243)\n",
      "40874 Training Loss: tensor(0.3246)\n",
      "40875 Training Loss: tensor(0.3252)\n",
      "40876 Training Loss: tensor(0.3248)\n",
      "40877 Training Loss: tensor(0.3248)\n",
      "40878 Training Loss: tensor(0.3249)\n",
      "40879 Training Loss: tensor(0.3250)\n",
      "40880 Training Loss: tensor(0.3253)\n",
      "40881 Training Loss: tensor(0.3245)\n",
      "40882 Training Loss: tensor(0.3243)\n",
      "40883 Training Loss: tensor(0.3247)\n",
      "40884 Training Loss: tensor(0.3260)\n",
      "40885 Training Loss: tensor(0.3236)\n",
      "40886 Training Loss: tensor(0.3258)\n",
      "40887 Training Loss: tensor(0.3249)\n",
      "40888 Training Loss: tensor(0.3240)\n",
      "40889 Training Loss: tensor(0.3242)\n",
      "40890 Training Loss: tensor(0.3241)\n",
      "40891 Training Loss: tensor(0.3242)\n",
      "40892 Training Loss: tensor(0.3249)\n",
      "40893 Training Loss: tensor(0.3246)\n",
      "40894 Training Loss: tensor(0.3248)\n",
      "40895 Training Loss: tensor(0.3248)\n",
      "40896 Training Loss: tensor(0.3248)\n",
      "40897 Training Loss: tensor(0.3248)\n",
      "40898 Training Loss: tensor(0.3249)\n",
      "40899 Training Loss: tensor(0.3246)\n",
      "40900 Training Loss: tensor(0.3243)\n",
      "40901 Training Loss: tensor(0.3249)\n",
      "40902 Training Loss: tensor(0.3244)\n",
      "40903 Training Loss: tensor(0.3248)\n",
      "40904 Training Loss: tensor(0.3237)\n",
      "40905 Training Loss: tensor(0.3254)\n",
      "40906 Training Loss: tensor(0.3240)\n",
      "40907 Training Loss: tensor(0.3246)\n",
      "40908 Training Loss: tensor(0.3237)\n",
      "40909 Training Loss: tensor(0.3241)\n",
      "40910 Training Loss: tensor(0.3247)\n",
      "40911 Training Loss: tensor(0.3243)\n",
      "40912 Training Loss: tensor(0.3247)\n",
      "40913 Training Loss: tensor(0.3251)\n",
      "40914 Training Loss: tensor(0.3243)\n",
      "40915 Training Loss: tensor(0.3255)\n",
      "40916 Training Loss: tensor(0.3239)\n",
      "40917 Training Loss: tensor(0.3257)\n",
      "40918 Training Loss: tensor(0.3251)\n",
      "40919 Training Loss: tensor(0.3262)\n",
      "40920 Training Loss: tensor(0.3247)\n",
      "40921 Training Loss: tensor(0.3252)\n",
      "40922 Training Loss: tensor(0.3249)\n",
      "40923 Training Loss: tensor(0.3237)\n",
      "40924 Training Loss: tensor(0.3243)\n",
      "40925 Training Loss: tensor(0.3241)\n",
      "40926 Training Loss: tensor(0.3246)\n",
      "40927 Training Loss: tensor(0.3240)\n",
      "40928 Training Loss: tensor(0.3245)\n",
      "40929 Training Loss: tensor(0.3257)\n",
      "40930 Training Loss: tensor(0.3246)\n",
      "40931 Training Loss: tensor(0.3241)\n",
      "40932 Training Loss: tensor(0.3245)\n",
      "40933 Training Loss: tensor(0.3243)\n",
      "40934 Training Loss: tensor(0.3259)\n",
      "40935 Training Loss: tensor(0.3243)\n",
      "40936 Training Loss: tensor(0.3252)\n",
      "40937 Training Loss: tensor(0.3246)\n",
      "40938 Training Loss: tensor(0.3238)\n",
      "40939 Training Loss: tensor(0.3244)\n",
      "40940 Training Loss: tensor(0.3246)\n",
      "40941 Training Loss: tensor(0.3242)\n",
      "40942 Training Loss: tensor(0.3239)\n",
      "40943 Training Loss: tensor(0.3252)\n",
      "40944 Training Loss: tensor(0.3248)\n",
      "40945 Training Loss: tensor(0.3245)\n",
      "40946 Training Loss: tensor(0.3247)\n",
      "40947 Training Loss: tensor(0.3239)\n",
      "40948 Training Loss: tensor(0.3249)\n",
      "40949 Training Loss: tensor(0.3246)\n",
      "40950 Training Loss: tensor(0.3249)\n",
      "40951 Training Loss: tensor(0.3244)\n",
      "40952 Training Loss: tensor(0.3258)\n",
      "40953 Training Loss: tensor(0.3243)\n",
      "40954 Training Loss: tensor(0.3242)\n",
      "40955 Training Loss: tensor(0.3249)\n",
      "40956 Training Loss: tensor(0.3245)\n",
      "40957 Training Loss: tensor(0.3239)\n",
      "40958 Training Loss: tensor(0.3243)\n",
      "40959 Training Loss: tensor(0.3248)\n",
      "40960 Training Loss: tensor(0.3251)\n",
      "40961 Training Loss: tensor(0.3250)\n",
      "40962 Training Loss: tensor(0.3255)\n",
      "40963 Training Loss: tensor(0.3251)\n",
      "40964 Training Loss: tensor(0.3246)\n",
      "40965 Training Loss: tensor(0.3255)\n",
      "40966 Training Loss: tensor(0.3249)\n",
      "40967 Training Loss: tensor(0.3258)\n",
      "40968 Training Loss: tensor(0.3244)\n",
      "40969 Training Loss: tensor(0.3267)\n",
      "40970 Training Loss: tensor(0.3251)\n",
      "40971 Training Loss: tensor(0.3249)\n",
      "40972 Training Loss: tensor(0.3257)\n",
      "40973 Training Loss: tensor(0.3239)\n",
      "40974 Training Loss: tensor(0.3244)\n",
      "40975 Training Loss: tensor(0.3258)\n",
      "40976 Training Loss: tensor(0.3242)\n",
      "40977 Training Loss: tensor(0.3245)\n",
      "40978 Training Loss: tensor(0.3243)\n",
      "40979 Training Loss: tensor(0.3244)\n",
      "40980 Training Loss: tensor(0.3242)\n",
      "40981 Training Loss: tensor(0.3249)\n",
      "40982 Training Loss: tensor(0.3262)\n",
      "40983 Training Loss: tensor(0.3241)\n",
      "40984 Training Loss: tensor(0.3259)\n",
      "40985 Training Loss: tensor(0.3247)\n",
      "40986 Training Loss: tensor(0.3247)\n",
      "40987 Training Loss: tensor(0.3246)\n",
      "40988 Training Loss: tensor(0.3248)\n",
      "40989 Training Loss: tensor(0.3250)\n",
      "40990 Training Loss: tensor(0.3246)\n",
      "40991 Training Loss: tensor(0.3253)\n",
      "40992 Training Loss: tensor(0.3246)\n",
      "40993 Training Loss: tensor(0.3252)\n",
      "40994 Training Loss: tensor(0.3246)\n",
      "40995 Training Loss: tensor(0.3245)\n",
      "40996 Training Loss: tensor(0.3248)\n",
      "40997 Training Loss: tensor(0.3249)\n",
      "40998 Training Loss: tensor(0.3244)\n",
      "40999 Training Loss: tensor(0.3243)\n",
      "41000 Training Loss: tensor(0.3243)\n",
      "41001 Training Loss: tensor(0.3247)\n",
      "41002 Training Loss: tensor(0.3252)\n",
      "41003 Training Loss: tensor(0.3249)\n",
      "41004 Training Loss: tensor(0.3241)\n",
      "41005 Training Loss: tensor(0.3242)\n",
      "41006 Training Loss: tensor(0.3243)\n",
      "41007 Training Loss: tensor(0.3243)\n",
      "41008 Training Loss: tensor(0.3242)\n",
      "41009 Training Loss: tensor(0.3254)\n",
      "41010 Training Loss: tensor(0.3260)\n",
      "41011 Training Loss: tensor(0.3246)\n",
      "41012 Training Loss: tensor(0.3243)\n",
      "41013 Training Loss: tensor(0.3245)\n",
      "41014 Training Loss: tensor(0.3244)\n",
      "41015 Training Loss: tensor(0.3248)\n",
      "41016 Training Loss: tensor(0.3246)\n",
      "41017 Training Loss: tensor(0.3248)\n",
      "41018 Training Loss: tensor(0.3249)\n",
      "41019 Training Loss: tensor(0.3247)\n",
      "41020 Training Loss: tensor(0.3244)\n",
      "41021 Training Loss: tensor(0.3247)\n",
      "41022 Training Loss: tensor(0.3240)\n",
      "41023 Training Loss: tensor(0.3245)\n",
      "41024 Training Loss: tensor(0.3240)\n",
      "41025 Training Loss: tensor(0.3236)\n",
      "41026 Training Loss: tensor(0.3245)\n",
      "41027 Training Loss: tensor(0.3244)\n",
      "41028 Training Loss: tensor(0.3251)\n",
      "41029 Training Loss: tensor(0.3237)\n",
      "41030 Training Loss: tensor(0.3242)\n",
      "41031 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41032 Training Loss: tensor(0.3242)\n",
      "41033 Training Loss: tensor(0.3263)\n",
      "41034 Training Loss: tensor(0.3247)\n",
      "41035 Training Loss: tensor(0.3250)\n",
      "41036 Training Loss: tensor(0.3240)\n",
      "41037 Training Loss: tensor(0.3249)\n",
      "41038 Training Loss: tensor(0.3241)\n",
      "41039 Training Loss: tensor(0.3244)\n",
      "41040 Training Loss: tensor(0.3247)\n",
      "41041 Training Loss: tensor(0.3248)\n",
      "41042 Training Loss: tensor(0.3242)\n",
      "41043 Training Loss: tensor(0.3242)\n",
      "41044 Training Loss: tensor(0.3259)\n",
      "41045 Training Loss: tensor(0.3239)\n",
      "41046 Training Loss: tensor(0.3256)\n",
      "41047 Training Loss: tensor(0.3244)\n",
      "41048 Training Loss: tensor(0.3247)\n",
      "41049 Training Loss: tensor(0.3249)\n",
      "41050 Training Loss: tensor(0.3245)\n",
      "41051 Training Loss: tensor(0.3250)\n",
      "41052 Training Loss: tensor(0.3241)\n",
      "41053 Training Loss: tensor(0.3247)\n",
      "41054 Training Loss: tensor(0.3247)\n",
      "41055 Training Loss: tensor(0.3242)\n",
      "41056 Training Loss: tensor(0.3254)\n",
      "41057 Training Loss: tensor(0.3240)\n",
      "41058 Training Loss: tensor(0.3241)\n",
      "41059 Training Loss: tensor(0.3244)\n",
      "41060 Training Loss: tensor(0.3238)\n",
      "41061 Training Loss: tensor(0.3254)\n",
      "41062 Training Loss: tensor(0.3242)\n",
      "41063 Training Loss: tensor(0.3242)\n",
      "41064 Training Loss: tensor(0.3239)\n",
      "41065 Training Loss: tensor(0.3253)\n",
      "41066 Training Loss: tensor(0.3241)\n",
      "41067 Training Loss: tensor(0.3238)\n",
      "41068 Training Loss: tensor(0.3245)\n",
      "41069 Training Loss: tensor(0.3249)\n",
      "41070 Training Loss: tensor(0.3252)\n",
      "41071 Training Loss: tensor(0.3254)\n",
      "41072 Training Loss: tensor(0.3248)\n",
      "41073 Training Loss: tensor(0.3252)\n",
      "41074 Training Loss: tensor(0.3246)\n",
      "41075 Training Loss: tensor(0.3241)\n",
      "41076 Training Loss: tensor(0.3242)\n",
      "41077 Training Loss: tensor(0.3238)\n",
      "41078 Training Loss: tensor(0.3244)\n",
      "41079 Training Loss: tensor(0.3251)\n",
      "41080 Training Loss: tensor(0.3245)\n",
      "41081 Training Loss: tensor(0.3248)\n",
      "41082 Training Loss: tensor(0.3256)\n",
      "41083 Training Loss: tensor(0.3244)\n",
      "41084 Training Loss: tensor(0.3258)\n",
      "41085 Training Loss: tensor(0.3240)\n",
      "41086 Training Loss: tensor(0.3247)\n",
      "41087 Training Loss: tensor(0.3247)\n",
      "41088 Training Loss: tensor(0.3252)\n",
      "41089 Training Loss: tensor(0.3253)\n",
      "41090 Training Loss: tensor(0.3248)\n",
      "41091 Training Loss: tensor(0.3253)\n",
      "41092 Training Loss: tensor(0.3254)\n",
      "41093 Training Loss: tensor(0.3247)\n",
      "41094 Training Loss: tensor(0.3247)\n",
      "41095 Training Loss: tensor(0.3246)\n",
      "41096 Training Loss: tensor(0.3240)\n",
      "41097 Training Loss: tensor(0.3240)\n",
      "41098 Training Loss: tensor(0.3252)\n",
      "41099 Training Loss: tensor(0.3245)\n",
      "41100 Training Loss: tensor(0.3246)\n",
      "41101 Training Loss: tensor(0.3246)\n",
      "41102 Training Loss: tensor(0.3246)\n",
      "41103 Training Loss: tensor(0.3239)\n",
      "41104 Training Loss: tensor(0.3252)\n",
      "41105 Training Loss: tensor(0.3244)\n",
      "41106 Training Loss: tensor(0.3248)\n",
      "41107 Training Loss: tensor(0.3244)\n",
      "41108 Training Loss: tensor(0.3244)\n",
      "41109 Training Loss: tensor(0.3247)\n",
      "41110 Training Loss: tensor(0.3246)\n",
      "41111 Training Loss: tensor(0.3238)\n",
      "41112 Training Loss: tensor(0.3248)\n",
      "41113 Training Loss: tensor(0.3263)\n",
      "41114 Training Loss: tensor(0.3252)\n",
      "41115 Training Loss: tensor(0.3255)\n",
      "41116 Training Loss: tensor(0.3248)\n",
      "41117 Training Loss: tensor(0.3249)\n",
      "41118 Training Loss: tensor(0.3245)\n",
      "41119 Training Loss: tensor(0.3248)\n",
      "41120 Training Loss: tensor(0.3247)\n",
      "41121 Training Loss: tensor(0.3249)\n",
      "41122 Training Loss: tensor(0.3241)\n",
      "41123 Training Loss: tensor(0.3258)\n",
      "41124 Training Loss: tensor(0.3244)\n",
      "41125 Training Loss: tensor(0.3243)\n",
      "41126 Training Loss: tensor(0.3243)\n",
      "41127 Training Loss: tensor(0.3252)\n",
      "41128 Training Loss: tensor(0.3251)\n",
      "41129 Training Loss: tensor(0.3246)\n",
      "41130 Training Loss: tensor(0.3248)\n",
      "41131 Training Loss: tensor(0.3253)\n",
      "41132 Training Loss: tensor(0.3243)\n",
      "41133 Training Loss: tensor(0.3248)\n",
      "41134 Training Loss: tensor(0.3242)\n",
      "41135 Training Loss: tensor(0.3242)\n",
      "41136 Training Loss: tensor(0.3253)\n",
      "41137 Training Loss: tensor(0.3250)\n",
      "41138 Training Loss: tensor(0.3240)\n",
      "41139 Training Loss: tensor(0.3245)\n",
      "41140 Training Loss: tensor(0.3251)\n",
      "41141 Training Loss: tensor(0.3245)\n",
      "41142 Training Loss: tensor(0.3239)\n",
      "41143 Training Loss: tensor(0.3240)\n",
      "41144 Training Loss: tensor(0.3241)\n",
      "41145 Training Loss: tensor(0.3246)\n",
      "41146 Training Loss: tensor(0.3246)\n",
      "41147 Training Loss: tensor(0.3244)\n",
      "41148 Training Loss: tensor(0.3261)\n",
      "41149 Training Loss: tensor(0.3257)\n",
      "41150 Training Loss: tensor(0.3240)\n",
      "41151 Training Loss: tensor(0.3253)\n",
      "41152 Training Loss: tensor(0.3241)\n",
      "41153 Training Loss: tensor(0.3246)\n",
      "41154 Training Loss: tensor(0.3242)\n",
      "41155 Training Loss: tensor(0.3253)\n",
      "41156 Training Loss: tensor(0.3253)\n",
      "41157 Training Loss: tensor(0.3246)\n",
      "41158 Training Loss: tensor(0.3247)\n",
      "41159 Training Loss: tensor(0.3248)\n",
      "41160 Training Loss: tensor(0.3251)\n",
      "41161 Training Loss: tensor(0.3251)\n",
      "41162 Training Loss: tensor(0.3242)\n",
      "41163 Training Loss: tensor(0.3250)\n",
      "41164 Training Loss: tensor(0.3249)\n",
      "41165 Training Loss: tensor(0.3249)\n",
      "41166 Training Loss: tensor(0.3247)\n",
      "41167 Training Loss: tensor(0.3245)\n",
      "41168 Training Loss: tensor(0.3252)\n",
      "41169 Training Loss: tensor(0.3250)\n",
      "41170 Training Loss: tensor(0.3247)\n",
      "41171 Training Loss: tensor(0.3242)\n",
      "41172 Training Loss: tensor(0.3247)\n",
      "41173 Training Loss: tensor(0.3245)\n",
      "41174 Training Loss: tensor(0.3243)\n",
      "41175 Training Loss: tensor(0.3247)\n",
      "41176 Training Loss: tensor(0.3255)\n",
      "41177 Training Loss: tensor(0.3248)\n",
      "41178 Training Loss: tensor(0.3242)\n",
      "41179 Training Loss: tensor(0.3245)\n",
      "41180 Training Loss: tensor(0.3247)\n",
      "41181 Training Loss: tensor(0.3242)\n",
      "41182 Training Loss: tensor(0.3241)\n",
      "41183 Training Loss: tensor(0.3255)\n",
      "41184 Training Loss: tensor(0.3249)\n",
      "41185 Training Loss: tensor(0.3268)\n",
      "41186 Training Loss: tensor(0.3242)\n",
      "41187 Training Loss: tensor(0.3248)\n",
      "41188 Training Loss: tensor(0.3254)\n",
      "41189 Training Loss: tensor(0.3244)\n",
      "41190 Training Loss: tensor(0.3252)\n",
      "41191 Training Loss: tensor(0.3246)\n",
      "41192 Training Loss: tensor(0.3242)\n",
      "41193 Training Loss: tensor(0.3251)\n",
      "41194 Training Loss: tensor(0.3242)\n",
      "41195 Training Loss: tensor(0.3247)\n",
      "41196 Training Loss: tensor(0.3246)\n",
      "41197 Training Loss: tensor(0.3241)\n",
      "41198 Training Loss: tensor(0.3242)\n",
      "41199 Training Loss: tensor(0.3241)\n",
      "41200 Training Loss: tensor(0.3245)\n",
      "41201 Training Loss: tensor(0.3242)\n",
      "41202 Training Loss: tensor(0.3249)\n",
      "41203 Training Loss: tensor(0.3247)\n",
      "41204 Training Loss: tensor(0.3241)\n",
      "41205 Training Loss: tensor(0.3237)\n",
      "41206 Training Loss: tensor(0.3247)\n",
      "41207 Training Loss: tensor(0.3248)\n",
      "41208 Training Loss: tensor(0.3257)\n",
      "41209 Training Loss: tensor(0.3242)\n",
      "41210 Training Loss: tensor(0.3243)\n",
      "41211 Training Loss: tensor(0.3241)\n",
      "41212 Training Loss: tensor(0.3249)\n",
      "41213 Training Loss: tensor(0.3239)\n",
      "41214 Training Loss: tensor(0.3252)\n",
      "41215 Training Loss: tensor(0.3251)\n",
      "41216 Training Loss: tensor(0.3245)\n",
      "41217 Training Loss: tensor(0.3260)\n",
      "41218 Training Loss: tensor(0.3252)\n",
      "41219 Training Loss: tensor(0.3247)\n",
      "41220 Training Loss: tensor(0.3249)\n",
      "41221 Training Loss: tensor(0.3242)\n",
      "41222 Training Loss: tensor(0.3242)\n",
      "41223 Training Loss: tensor(0.3239)\n",
      "41224 Training Loss: tensor(0.3244)\n",
      "41225 Training Loss: tensor(0.3250)\n",
      "41226 Training Loss: tensor(0.3246)\n",
      "41227 Training Loss: tensor(0.3249)\n",
      "41228 Training Loss: tensor(0.3252)\n",
      "41229 Training Loss: tensor(0.3246)\n",
      "41230 Training Loss: tensor(0.3236)\n",
      "41231 Training Loss: tensor(0.3253)\n",
      "41232 Training Loss: tensor(0.3248)\n",
      "41233 Training Loss: tensor(0.3251)\n",
      "41234 Training Loss: tensor(0.3247)\n",
      "41235 Training Loss: tensor(0.3253)\n",
      "41236 Training Loss: tensor(0.3249)\n",
      "41237 Training Loss: tensor(0.3243)\n",
      "41238 Training Loss: tensor(0.3257)\n",
      "41239 Training Loss: tensor(0.3244)\n",
      "41240 Training Loss: tensor(0.3247)\n",
      "41241 Training Loss: tensor(0.3243)\n",
      "41242 Training Loss: tensor(0.3257)\n",
      "41243 Training Loss: tensor(0.3253)\n",
      "41244 Training Loss: tensor(0.3244)\n",
      "41245 Training Loss: tensor(0.3242)\n",
      "41246 Training Loss: tensor(0.3247)\n",
      "41247 Training Loss: tensor(0.3253)\n",
      "41248 Training Loss: tensor(0.3242)\n",
      "41249 Training Loss: tensor(0.3246)\n",
      "41250 Training Loss: tensor(0.3246)\n",
      "41251 Training Loss: tensor(0.3249)\n",
      "41252 Training Loss: tensor(0.3257)\n",
      "41253 Training Loss: tensor(0.3253)\n",
      "41254 Training Loss: tensor(0.3247)\n",
      "41255 Training Loss: tensor(0.3245)\n",
      "41256 Training Loss: tensor(0.3240)\n",
      "41257 Training Loss: tensor(0.3247)\n",
      "41258 Training Loss: tensor(0.3240)\n",
      "41259 Training Loss: tensor(0.3250)\n",
      "41260 Training Loss: tensor(0.3242)\n",
      "41261 Training Loss: tensor(0.3250)\n",
      "41262 Training Loss: tensor(0.3245)\n",
      "41263 Training Loss: tensor(0.3250)\n",
      "41264 Training Loss: tensor(0.3251)\n",
      "41265 Training Loss: tensor(0.3243)\n",
      "41266 Training Loss: tensor(0.3247)\n",
      "41267 Training Loss: tensor(0.3243)\n",
      "41268 Training Loss: tensor(0.3245)\n",
      "41269 Training Loss: tensor(0.3244)\n",
      "41270 Training Loss: tensor(0.3243)\n",
      "41271 Training Loss: tensor(0.3243)\n",
      "41272 Training Loss: tensor(0.3246)\n",
      "41273 Training Loss: tensor(0.3237)\n",
      "41274 Training Loss: tensor(0.3243)\n",
      "41275 Training Loss: tensor(0.3242)\n",
      "41276 Training Loss: tensor(0.3240)\n",
      "41277 Training Loss: tensor(0.3239)\n",
      "41278 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41279 Training Loss: tensor(0.3249)\n",
      "41280 Training Loss: tensor(0.3247)\n",
      "41281 Training Loss: tensor(0.3247)\n",
      "41282 Training Loss: tensor(0.3242)\n",
      "41283 Training Loss: tensor(0.3248)\n",
      "41284 Training Loss: tensor(0.3243)\n",
      "41285 Training Loss: tensor(0.3257)\n",
      "41286 Training Loss: tensor(0.3237)\n",
      "41287 Training Loss: tensor(0.3246)\n",
      "41288 Training Loss: tensor(0.3243)\n",
      "41289 Training Loss: tensor(0.3237)\n",
      "41290 Training Loss: tensor(0.3242)\n",
      "41291 Training Loss: tensor(0.3248)\n",
      "41292 Training Loss: tensor(0.3241)\n",
      "41293 Training Loss: tensor(0.3243)\n",
      "41294 Training Loss: tensor(0.3242)\n",
      "41295 Training Loss: tensor(0.3236)\n",
      "41296 Training Loss: tensor(0.3249)\n",
      "41297 Training Loss: tensor(0.3236)\n",
      "41298 Training Loss: tensor(0.3251)\n",
      "41299 Training Loss: tensor(0.3244)\n",
      "41300 Training Loss: tensor(0.3239)\n",
      "41301 Training Loss: tensor(0.3249)\n",
      "41302 Training Loss: tensor(0.3242)\n",
      "41303 Training Loss: tensor(0.3236)\n",
      "41304 Training Loss: tensor(0.3246)\n",
      "41305 Training Loss: tensor(0.3255)\n",
      "41306 Training Loss: tensor(0.3240)\n",
      "41307 Training Loss: tensor(0.3253)\n",
      "41308 Training Loss: tensor(0.3259)\n",
      "41309 Training Loss: tensor(0.3238)\n",
      "41310 Training Loss: tensor(0.3239)\n",
      "41311 Training Loss: tensor(0.3255)\n",
      "41312 Training Loss: tensor(0.3240)\n",
      "41313 Training Loss: tensor(0.3247)\n",
      "41314 Training Loss: tensor(0.3250)\n",
      "41315 Training Loss: tensor(0.3242)\n",
      "41316 Training Loss: tensor(0.3241)\n",
      "41317 Training Loss: tensor(0.3243)\n",
      "41318 Training Loss: tensor(0.3240)\n",
      "41319 Training Loss: tensor(0.3249)\n",
      "41320 Training Loss: tensor(0.3255)\n",
      "41321 Training Loss: tensor(0.3248)\n",
      "41322 Training Loss: tensor(0.3240)\n",
      "41323 Training Loss: tensor(0.3247)\n",
      "41324 Training Loss: tensor(0.3248)\n",
      "41325 Training Loss: tensor(0.3245)\n",
      "41326 Training Loss: tensor(0.3252)\n",
      "41327 Training Loss: tensor(0.3264)\n",
      "41328 Training Loss: tensor(0.3256)\n",
      "41329 Training Loss: tensor(0.3250)\n",
      "41330 Training Loss: tensor(0.3247)\n",
      "41331 Training Loss: tensor(0.3245)\n",
      "41332 Training Loss: tensor(0.3240)\n",
      "41333 Training Loss: tensor(0.3241)\n",
      "41334 Training Loss: tensor(0.3248)\n",
      "41335 Training Loss: tensor(0.3240)\n",
      "41336 Training Loss: tensor(0.3241)\n",
      "41337 Training Loss: tensor(0.3240)\n",
      "41338 Training Loss: tensor(0.3251)\n",
      "41339 Training Loss: tensor(0.3244)\n",
      "41340 Training Loss: tensor(0.3246)\n",
      "41341 Training Loss: tensor(0.3251)\n",
      "41342 Training Loss: tensor(0.3240)\n",
      "41343 Training Loss: tensor(0.3241)\n",
      "41344 Training Loss: tensor(0.3245)\n",
      "41345 Training Loss: tensor(0.3238)\n",
      "41346 Training Loss: tensor(0.3251)\n",
      "41347 Training Loss: tensor(0.3244)\n",
      "41348 Training Loss: tensor(0.3248)\n",
      "41349 Training Loss: tensor(0.3248)\n",
      "41350 Training Loss: tensor(0.3242)\n",
      "41351 Training Loss: tensor(0.3239)\n",
      "41352 Training Loss: tensor(0.3252)\n",
      "41353 Training Loss: tensor(0.3245)\n",
      "41354 Training Loss: tensor(0.3241)\n",
      "41355 Training Loss: tensor(0.3245)\n",
      "41356 Training Loss: tensor(0.3234)\n",
      "41357 Training Loss: tensor(0.3248)\n",
      "41358 Training Loss: tensor(0.3247)\n",
      "41359 Training Loss: tensor(0.3245)\n",
      "41360 Training Loss: tensor(0.3243)\n",
      "41361 Training Loss: tensor(0.3243)\n",
      "41362 Training Loss: tensor(0.3242)\n",
      "41363 Training Loss: tensor(0.3244)\n",
      "41364 Training Loss: tensor(0.3244)\n",
      "41365 Training Loss: tensor(0.3240)\n",
      "41366 Training Loss: tensor(0.3240)\n",
      "41367 Training Loss: tensor(0.3245)\n",
      "41368 Training Loss: tensor(0.3240)\n",
      "41369 Training Loss: tensor(0.3245)\n",
      "41370 Training Loss: tensor(0.3260)\n",
      "41371 Training Loss: tensor(0.3249)\n",
      "41372 Training Loss: tensor(0.3241)\n",
      "41373 Training Loss: tensor(0.3246)\n",
      "41374 Training Loss: tensor(0.3265)\n",
      "41375 Training Loss: tensor(0.3244)\n",
      "41376 Training Loss: tensor(0.3242)\n",
      "41377 Training Loss: tensor(0.3244)\n",
      "41378 Training Loss: tensor(0.3236)\n",
      "41379 Training Loss: tensor(0.3252)\n",
      "41380 Training Loss: tensor(0.3250)\n",
      "41381 Training Loss: tensor(0.3244)\n",
      "41382 Training Loss: tensor(0.3245)\n",
      "41383 Training Loss: tensor(0.3245)\n",
      "41384 Training Loss: tensor(0.3245)\n",
      "41385 Training Loss: tensor(0.3236)\n",
      "41386 Training Loss: tensor(0.3246)\n",
      "41387 Training Loss: tensor(0.3242)\n",
      "41388 Training Loss: tensor(0.3249)\n",
      "41389 Training Loss: tensor(0.3244)\n",
      "41390 Training Loss: tensor(0.3248)\n",
      "41391 Training Loss: tensor(0.3246)\n",
      "41392 Training Loss: tensor(0.3234)\n",
      "41393 Training Loss: tensor(0.3241)\n",
      "41394 Training Loss: tensor(0.3241)\n",
      "41395 Training Loss: tensor(0.3247)\n",
      "41396 Training Loss: tensor(0.3243)\n",
      "41397 Training Loss: tensor(0.3238)\n",
      "41398 Training Loss: tensor(0.3242)\n",
      "41399 Training Loss: tensor(0.3249)\n",
      "41400 Training Loss: tensor(0.3249)\n",
      "41401 Training Loss: tensor(0.3249)\n",
      "41402 Training Loss: tensor(0.3247)\n",
      "41403 Training Loss: tensor(0.3249)\n",
      "41404 Training Loss: tensor(0.3240)\n",
      "41405 Training Loss: tensor(0.3245)\n",
      "41406 Training Loss: tensor(0.3244)\n",
      "41407 Training Loss: tensor(0.3247)\n",
      "41408 Training Loss: tensor(0.3242)\n",
      "41409 Training Loss: tensor(0.3249)\n",
      "41410 Training Loss: tensor(0.3259)\n",
      "41411 Training Loss: tensor(0.3261)\n",
      "41412 Training Loss: tensor(0.3245)\n",
      "41413 Training Loss: tensor(0.3241)\n",
      "41414 Training Loss: tensor(0.3243)\n",
      "41415 Training Loss: tensor(0.3249)\n",
      "41416 Training Loss: tensor(0.3248)\n",
      "41417 Training Loss: tensor(0.3239)\n",
      "41418 Training Loss: tensor(0.3243)\n",
      "41419 Training Loss: tensor(0.3243)\n",
      "41420 Training Loss: tensor(0.3255)\n",
      "41421 Training Loss: tensor(0.3248)\n",
      "41422 Training Loss: tensor(0.3256)\n",
      "41423 Training Loss: tensor(0.3239)\n",
      "41424 Training Loss: tensor(0.3246)\n",
      "41425 Training Loss: tensor(0.3246)\n",
      "41426 Training Loss: tensor(0.3245)\n",
      "41427 Training Loss: tensor(0.3248)\n",
      "41428 Training Loss: tensor(0.3248)\n",
      "41429 Training Loss: tensor(0.3247)\n",
      "41430 Training Loss: tensor(0.3247)\n",
      "41431 Training Loss: tensor(0.3246)\n",
      "41432 Training Loss: tensor(0.3248)\n",
      "41433 Training Loss: tensor(0.3244)\n",
      "41434 Training Loss: tensor(0.3240)\n",
      "41435 Training Loss: tensor(0.3248)\n",
      "41436 Training Loss: tensor(0.3242)\n",
      "41437 Training Loss: tensor(0.3240)\n",
      "41438 Training Loss: tensor(0.3249)\n",
      "41439 Training Loss: tensor(0.3247)\n",
      "41440 Training Loss: tensor(0.3247)\n",
      "41441 Training Loss: tensor(0.3246)\n",
      "41442 Training Loss: tensor(0.3237)\n",
      "41443 Training Loss: tensor(0.3252)\n",
      "41444 Training Loss: tensor(0.3248)\n",
      "41445 Training Loss: tensor(0.3252)\n",
      "41446 Training Loss: tensor(0.3244)\n",
      "41447 Training Loss: tensor(0.3241)\n",
      "41448 Training Loss: tensor(0.3250)\n",
      "41449 Training Loss: tensor(0.3251)\n",
      "41450 Training Loss: tensor(0.3251)\n",
      "41451 Training Loss: tensor(0.3246)\n",
      "41452 Training Loss: tensor(0.3241)\n",
      "41453 Training Loss: tensor(0.3244)\n",
      "41454 Training Loss: tensor(0.3241)\n",
      "41455 Training Loss: tensor(0.3241)\n",
      "41456 Training Loss: tensor(0.3242)\n",
      "41457 Training Loss: tensor(0.3246)\n",
      "41458 Training Loss: tensor(0.3238)\n",
      "41459 Training Loss: tensor(0.3242)\n",
      "41460 Training Loss: tensor(0.3244)\n",
      "41461 Training Loss: tensor(0.3241)\n",
      "41462 Training Loss: tensor(0.3246)\n",
      "41463 Training Loss: tensor(0.3242)\n",
      "41464 Training Loss: tensor(0.3247)\n",
      "41465 Training Loss: tensor(0.3260)\n",
      "41466 Training Loss: tensor(0.3238)\n",
      "41467 Training Loss: tensor(0.3249)\n",
      "41468 Training Loss: tensor(0.3251)\n",
      "41469 Training Loss: tensor(0.3241)\n",
      "41470 Training Loss: tensor(0.3239)\n",
      "41471 Training Loss: tensor(0.3241)\n",
      "41472 Training Loss: tensor(0.3236)\n",
      "41473 Training Loss: tensor(0.3244)\n",
      "41474 Training Loss: tensor(0.3254)\n",
      "41475 Training Loss: tensor(0.3249)\n",
      "41476 Training Loss: tensor(0.3239)\n",
      "41477 Training Loss: tensor(0.3239)\n",
      "41478 Training Loss: tensor(0.3247)\n",
      "41479 Training Loss: tensor(0.3247)\n",
      "41480 Training Loss: tensor(0.3238)\n",
      "41481 Training Loss: tensor(0.3242)\n",
      "41482 Training Loss: tensor(0.3239)\n",
      "41483 Training Loss: tensor(0.3239)\n",
      "41484 Training Loss: tensor(0.3245)\n",
      "41485 Training Loss: tensor(0.3246)\n",
      "41486 Training Loss: tensor(0.3244)\n",
      "41487 Training Loss: tensor(0.3244)\n",
      "41488 Training Loss: tensor(0.3246)\n",
      "41489 Training Loss: tensor(0.3246)\n",
      "41490 Training Loss: tensor(0.3239)\n",
      "41491 Training Loss: tensor(0.3244)\n",
      "41492 Training Loss: tensor(0.3241)\n",
      "41493 Training Loss: tensor(0.3246)\n",
      "41494 Training Loss: tensor(0.3238)\n",
      "41495 Training Loss: tensor(0.3242)\n",
      "41496 Training Loss: tensor(0.3252)\n",
      "41497 Training Loss: tensor(0.3248)\n",
      "41498 Training Loss: tensor(0.3242)\n",
      "41499 Training Loss: tensor(0.3253)\n",
      "41500 Training Loss: tensor(0.3250)\n",
      "41501 Training Loss: tensor(0.3235)\n",
      "41502 Training Loss: tensor(0.3238)\n",
      "41503 Training Loss: tensor(0.3244)\n",
      "41504 Training Loss: tensor(0.3249)\n",
      "41505 Training Loss: tensor(0.3253)\n",
      "41506 Training Loss: tensor(0.3249)\n",
      "41507 Training Loss: tensor(0.3245)\n",
      "41508 Training Loss: tensor(0.3246)\n",
      "41509 Training Loss: tensor(0.3250)\n",
      "41510 Training Loss: tensor(0.3253)\n",
      "41511 Training Loss: tensor(0.3266)\n",
      "41512 Training Loss: tensor(0.3251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41513 Training Loss: tensor(0.3245)\n",
      "41514 Training Loss: tensor(0.3253)\n",
      "41515 Training Loss: tensor(0.3240)\n",
      "41516 Training Loss: tensor(0.3243)\n",
      "41517 Training Loss: tensor(0.3244)\n",
      "41518 Training Loss: tensor(0.3249)\n",
      "41519 Training Loss: tensor(0.3237)\n",
      "41520 Training Loss: tensor(0.3243)\n",
      "41521 Training Loss: tensor(0.3245)\n",
      "41522 Training Loss: tensor(0.3241)\n",
      "41523 Training Loss: tensor(0.3250)\n",
      "41524 Training Loss: tensor(0.3238)\n",
      "41525 Training Loss: tensor(0.3245)\n",
      "41526 Training Loss: tensor(0.3252)\n",
      "41527 Training Loss: tensor(0.3255)\n",
      "41528 Training Loss: tensor(0.3249)\n",
      "41529 Training Loss: tensor(0.3241)\n",
      "41530 Training Loss: tensor(0.3249)\n",
      "41531 Training Loss: tensor(0.3249)\n",
      "41532 Training Loss: tensor(0.3238)\n",
      "41533 Training Loss: tensor(0.3252)\n",
      "41534 Training Loss: tensor(0.3247)\n",
      "41535 Training Loss: tensor(0.3247)\n",
      "41536 Training Loss: tensor(0.3247)\n",
      "41537 Training Loss: tensor(0.3250)\n",
      "41538 Training Loss: tensor(0.3240)\n",
      "41539 Training Loss: tensor(0.3236)\n",
      "41540 Training Loss: tensor(0.3241)\n",
      "41541 Training Loss: tensor(0.3254)\n",
      "41542 Training Loss: tensor(0.3244)\n",
      "41543 Training Loss: tensor(0.3247)\n",
      "41544 Training Loss: tensor(0.3238)\n",
      "41545 Training Loss: tensor(0.3246)\n",
      "41546 Training Loss: tensor(0.3239)\n",
      "41547 Training Loss: tensor(0.3242)\n",
      "41548 Training Loss: tensor(0.3252)\n",
      "41549 Training Loss: tensor(0.3235)\n",
      "41550 Training Loss: tensor(0.3254)\n",
      "41551 Training Loss: tensor(0.3243)\n",
      "41552 Training Loss: tensor(0.3241)\n",
      "41553 Training Loss: tensor(0.3247)\n",
      "41554 Training Loss: tensor(0.3240)\n",
      "41555 Training Loss: tensor(0.3248)\n",
      "41556 Training Loss: tensor(0.3237)\n",
      "41557 Training Loss: tensor(0.3249)\n",
      "41558 Training Loss: tensor(0.3252)\n",
      "41559 Training Loss: tensor(0.3249)\n",
      "41560 Training Loss: tensor(0.3244)\n",
      "41561 Training Loss: tensor(0.3241)\n",
      "41562 Training Loss: tensor(0.3247)\n",
      "41563 Training Loss: tensor(0.3244)\n",
      "41564 Training Loss: tensor(0.3244)\n",
      "41565 Training Loss: tensor(0.3257)\n",
      "41566 Training Loss: tensor(0.3249)\n",
      "41567 Training Loss: tensor(0.3240)\n",
      "41568 Training Loss: tensor(0.3243)\n",
      "41569 Training Loss: tensor(0.3243)\n",
      "41570 Training Loss: tensor(0.3242)\n",
      "41571 Training Loss: tensor(0.3246)\n",
      "41572 Training Loss: tensor(0.3240)\n",
      "41573 Training Loss: tensor(0.3252)\n",
      "41574 Training Loss: tensor(0.3245)\n",
      "41575 Training Loss: tensor(0.3246)\n",
      "41576 Training Loss: tensor(0.3247)\n",
      "41577 Training Loss: tensor(0.3247)\n",
      "41578 Training Loss: tensor(0.3249)\n",
      "41579 Training Loss: tensor(0.3242)\n",
      "41580 Training Loss: tensor(0.3247)\n",
      "41581 Training Loss: tensor(0.3250)\n",
      "41582 Training Loss: tensor(0.3243)\n",
      "41583 Training Loss: tensor(0.3240)\n",
      "41584 Training Loss: tensor(0.3249)\n",
      "41585 Training Loss: tensor(0.3246)\n",
      "41586 Training Loss: tensor(0.3247)\n",
      "41587 Training Loss: tensor(0.3240)\n",
      "41588 Training Loss: tensor(0.3240)\n",
      "41589 Training Loss: tensor(0.3240)\n",
      "41590 Training Loss: tensor(0.3245)\n",
      "41591 Training Loss: tensor(0.3241)\n",
      "41592 Training Loss: tensor(0.3241)\n",
      "41593 Training Loss: tensor(0.3243)\n",
      "41594 Training Loss: tensor(0.3243)\n",
      "41595 Training Loss: tensor(0.3250)\n",
      "41596 Training Loss: tensor(0.3236)\n",
      "41597 Training Loss: tensor(0.3244)\n",
      "41598 Training Loss: tensor(0.3242)\n",
      "41599 Training Loss: tensor(0.3242)\n",
      "41600 Training Loss: tensor(0.3242)\n",
      "41601 Training Loss: tensor(0.3248)\n",
      "41602 Training Loss: tensor(0.3251)\n",
      "41603 Training Loss: tensor(0.3246)\n",
      "41604 Training Loss: tensor(0.3245)\n",
      "41605 Training Loss: tensor(0.3253)\n",
      "41606 Training Loss: tensor(0.3239)\n",
      "41607 Training Loss: tensor(0.3245)\n",
      "41608 Training Loss: tensor(0.3242)\n",
      "41609 Training Loss: tensor(0.3240)\n",
      "41610 Training Loss: tensor(0.3247)\n",
      "41611 Training Loss: tensor(0.3243)\n",
      "41612 Training Loss: tensor(0.3245)\n",
      "41613 Training Loss: tensor(0.3257)\n",
      "41614 Training Loss: tensor(0.3250)\n",
      "41615 Training Loss: tensor(0.3244)\n",
      "41616 Training Loss: tensor(0.3242)\n",
      "41617 Training Loss: tensor(0.3243)\n",
      "41618 Training Loss: tensor(0.3254)\n",
      "41619 Training Loss: tensor(0.3243)\n",
      "41620 Training Loss: tensor(0.3247)\n",
      "41621 Training Loss: tensor(0.3243)\n",
      "41622 Training Loss: tensor(0.3246)\n",
      "41623 Training Loss: tensor(0.3251)\n",
      "41624 Training Loss: tensor(0.3245)\n",
      "41625 Training Loss: tensor(0.3240)\n",
      "41626 Training Loss: tensor(0.3247)\n",
      "41627 Training Loss: tensor(0.3243)\n",
      "41628 Training Loss: tensor(0.3252)\n",
      "41629 Training Loss: tensor(0.3240)\n",
      "41630 Training Loss: tensor(0.3239)\n",
      "41631 Training Loss: tensor(0.3253)\n",
      "41632 Training Loss: tensor(0.3239)\n",
      "41633 Training Loss: tensor(0.3245)\n",
      "41634 Training Loss: tensor(0.3237)\n",
      "41635 Training Loss: tensor(0.3244)\n",
      "41636 Training Loss: tensor(0.3247)\n",
      "41637 Training Loss: tensor(0.3247)\n",
      "41638 Training Loss: tensor(0.3242)\n",
      "41639 Training Loss: tensor(0.3252)\n",
      "41640 Training Loss: tensor(0.3240)\n",
      "41641 Training Loss: tensor(0.3243)\n",
      "41642 Training Loss: tensor(0.3248)\n",
      "41643 Training Loss: tensor(0.3257)\n",
      "41644 Training Loss: tensor(0.3259)\n",
      "41645 Training Loss: tensor(0.3255)\n",
      "41646 Training Loss: tensor(0.3250)\n",
      "41647 Training Loss: tensor(0.3243)\n",
      "41648 Training Loss: tensor(0.3247)\n",
      "41649 Training Loss: tensor(0.3240)\n",
      "41650 Training Loss: tensor(0.3241)\n",
      "41651 Training Loss: tensor(0.3242)\n",
      "41652 Training Loss: tensor(0.3243)\n",
      "41653 Training Loss: tensor(0.3248)\n",
      "41654 Training Loss: tensor(0.3250)\n",
      "41655 Training Loss: tensor(0.3242)\n",
      "41656 Training Loss: tensor(0.3243)\n",
      "41657 Training Loss: tensor(0.3241)\n",
      "41658 Training Loss: tensor(0.3259)\n",
      "41659 Training Loss: tensor(0.3241)\n",
      "41660 Training Loss: tensor(0.3246)\n",
      "41661 Training Loss: tensor(0.3246)\n",
      "41662 Training Loss: tensor(0.3245)\n",
      "41663 Training Loss: tensor(0.3241)\n",
      "41664 Training Loss: tensor(0.3245)\n",
      "41665 Training Loss: tensor(0.3242)\n",
      "41666 Training Loss: tensor(0.3256)\n",
      "41667 Training Loss: tensor(0.3245)\n",
      "41668 Training Loss: tensor(0.3250)\n",
      "41669 Training Loss: tensor(0.3236)\n",
      "41670 Training Loss: tensor(0.3249)\n",
      "41671 Training Loss: tensor(0.3249)\n",
      "41672 Training Loss: tensor(0.3242)\n",
      "41673 Training Loss: tensor(0.3242)\n",
      "41674 Training Loss: tensor(0.3240)\n",
      "41675 Training Loss: tensor(0.3246)\n",
      "41676 Training Loss: tensor(0.3251)\n",
      "41677 Training Loss: tensor(0.3238)\n",
      "41678 Training Loss: tensor(0.3251)\n",
      "41679 Training Loss: tensor(0.3238)\n",
      "41680 Training Loss: tensor(0.3244)\n",
      "41681 Training Loss: tensor(0.3243)\n",
      "41682 Training Loss: tensor(0.3243)\n",
      "41683 Training Loss: tensor(0.3255)\n",
      "41684 Training Loss: tensor(0.3249)\n",
      "41685 Training Loss: tensor(0.3255)\n",
      "41686 Training Loss: tensor(0.3240)\n",
      "41687 Training Loss: tensor(0.3248)\n",
      "41688 Training Loss: tensor(0.3254)\n",
      "41689 Training Loss: tensor(0.3252)\n",
      "41690 Training Loss: tensor(0.3241)\n",
      "41691 Training Loss: tensor(0.3242)\n",
      "41692 Training Loss: tensor(0.3242)\n",
      "41693 Training Loss: tensor(0.3252)\n",
      "41694 Training Loss: tensor(0.3248)\n",
      "41695 Training Loss: tensor(0.3243)\n",
      "41696 Training Loss: tensor(0.3246)\n",
      "41697 Training Loss: tensor(0.3243)\n",
      "41698 Training Loss: tensor(0.3241)\n",
      "41699 Training Loss: tensor(0.3240)\n",
      "41700 Training Loss: tensor(0.3244)\n",
      "41701 Training Loss: tensor(0.3241)\n",
      "41702 Training Loss: tensor(0.3252)\n",
      "41703 Training Loss: tensor(0.3251)\n",
      "41704 Training Loss: tensor(0.3243)\n",
      "41705 Training Loss: tensor(0.3256)\n",
      "41706 Training Loss: tensor(0.3248)\n",
      "41707 Training Loss: tensor(0.3245)\n",
      "41708 Training Loss: tensor(0.3249)\n",
      "41709 Training Loss: tensor(0.3240)\n",
      "41710 Training Loss: tensor(0.3254)\n",
      "41711 Training Loss: tensor(0.3246)\n",
      "41712 Training Loss: tensor(0.3242)\n",
      "41713 Training Loss: tensor(0.3245)\n",
      "41714 Training Loss: tensor(0.3246)\n",
      "41715 Training Loss: tensor(0.3252)\n",
      "41716 Training Loss: tensor(0.3248)\n",
      "41717 Training Loss: tensor(0.3248)\n",
      "41718 Training Loss: tensor(0.3245)\n",
      "41719 Training Loss: tensor(0.3240)\n",
      "41720 Training Loss: tensor(0.3248)\n",
      "41721 Training Loss: tensor(0.3247)\n",
      "41722 Training Loss: tensor(0.3246)\n",
      "41723 Training Loss: tensor(0.3246)\n",
      "41724 Training Loss: tensor(0.3248)\n",
      "41725 Training Loss: tensor(0.3241)\n",
      "41726 Training Loss: tensor(0.3243)\n",
      "41727 Training Loss: tensor(0.3242)\n",
      "41728 Training Loss: tensor(0.3242)\n",
      "41729 Training Loss: tensor(0.3244)\n",
      "41730 Training Loss: tensor(0.3243)\n",
      "41731 Training Loss: tensor(0.3242)\n",
      "41732 Training Loss: tensor(0.3243)\n",
      "41733 Training Loss: tensor(0.3243)\n",
      "41734 Training Loss: tensor(0.3238)\n",
      "41735 Training Loss: tensor(0.3242)\n",
      "41736 Training Loss: tensor(0.3241)\n",
      "41737 Training Loss: tensor(0.3244)\n",
      "41738 Training Loss: tensor(0.3243)\n",
      "41739 Training Loss: tensor(0.3254)\n",
      "41740 Training Loss: tensor(0.3246)\n",
      "41741 Training Loss: tensor(0.3238)\n",
      "41742 Training Loss: tensor(0.3240)\n",
      "41743 Training Loss: tensor(0.3249)\n",
      "41744 Training Loss: tensor(0.3244)\n",
      "41745 Training Loss: tensor(0.3248)\n",
      "41746 Training Loss: tensor(0.3237)\n",
      "41747 Training Loss: tensor(0.3251)\n",
      "41748 Training Loss: tensor(0.3255)\n",
      "41749 Training Loss: tensor(0.3240)\n",
      "41750 Training Loss: tensor(0.3244)\n",
      "41751 Training Loss: tensor(0.3246)\n",
      "41752 Training Loss: tensor(0.3252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41753 Training Loss: tensor(0.3247)\n",
      "41754 Training Loss: tensor(0.3246)\n",
      "41755 Training Loss: tensor(0.3253)\n",
      "41756 Training Loss: tensor(0.3250)\n",
      "41757 Training Loss: tensor(0.3256)\n",
      "41758 Training Loss: tensor(0.3251)\n",
      "41759 Training Loss: tensor(0.3236)\n",
      "41760 Training Loss: tensor(0.3244)\n",
      "41761 Training Loss: tensor(0.3256)\n",
      "41762 Training Loss: tensor(0.3256)\n",
      "41763 Training Loss: tensor(0.3251)\n",
      "41764 Training Loss: tensor(0.3260)\n",
      "41765 Training Loss: tensor(0.3258)\n",
      "41766 Training Loss: tensor(0.3267)\n",
      "41767 Training Loss: tensor(0.3245)\n",
      "41768 Training Loss: tensor(0.3247)\n",
      "41769 Training Loss: tensor(0.3254)\n",
      "41770 Training Loss: tensor(0.3257)\n",
      "41771 Training Loss: tensor(0.3247)\n",
      "41772 Training Loss: tensor(0.3250)\n",
      "41773 Training Loss: tensor(0.3241)\n",
      "41774 Training Loss: tensor(0.3246)\n",
      "41775 Training Loss: tensor(0.3252)\n",
      "41776 Training Loss: tensor(0.3246)\n",
      "41777 Training Loss: tensor(0.3241)\n",
      "41778 Training Loss: tensor(0.3244)\n",
      "41779 Training Loss: tensor(0.3240)\n",
      "41780 Training Loss: tensor(0.3248)\n",
      "41781 Training Loss: tensor(0.3260)\n",
      "41782 Training Loss: tensor(0.3246)\n",
      "41783 Training Loss: tensor(0.3249)\n",
      "41784 Training Loss: tensor(0.3242)\n",
      "41785 Training Loss: tensor(0.3244)\n",
      "41786 Training Loss: tensor(0.3250)\n",
      "41787 Training Loss: tensor(0.3246)\n",
      "41788 Training Loss: tensor(0.3241)\n",
      "41789 Training Loss: tensor(0.3237)\n",
      "41790 Training Loss: tensor(0.3241)\n",
      "41791 Training Loss: tensor(0.3243)\n",
      "41792 Training Loss: tensor(0.3250)\n",
      "41793 Training Loss: tensor(0.3266)\n",
      "41794 Training Loss: tensor(0.3252)\n",
      "41795 Training Loss: tensor(0.3237)\n",
      "41796 Training Loss: tensor(0.3242)\n",
      "41797 Training Loss: tensor(0.3245)\n",
      "41798 Training Loss: tensor(0.3252)\n",
      "41799 Training Loss: tensor(0.3236)\n",
      "41800 Training Loss: tensor(0.3245)\n",
      "41801 Training Loss: tensor(0.3247)\n",
      "41802 Training Loss: tensor(0.3257)\n",
      "41803 Training Loss: tensor(0.3254)\n",
      "41804 Training Loss: tensor(0.3247)\n",
      "41805 Training Loss: tensor(0.3252)\n",
      "41806 Training Loss: tensor(0.3242)\n",
      "41807 Training Loss: tensor(0.3249)\n",
      "41808 Training Loss: tensor(0.3252)\n",
      "41809 Training Loss: tensor(0.3245)\n",
      "41810 Training Loss: tensor(0.3256)\n",
      "41811 Training Loss: tensor(0.3243)\n",
      "41812 Training Loss: tensor(0.3246)\n",
      "41813 Training Loss: tensor(0.3248)\n",
      "41814 Training Loss: tensor(0.3248)\n",
      "41815 Training Loss: tensor(0.3248)\n",
      "41816 Training Loss: tensor(0.3242)\n",
      "41817 Training Loss: tensor(0.3253)\n",
      "41818 Training Loss: tensor(0.3245)\n",
      "41819 Training Loss: tensor(0.3237)\n",
      "41820 Training Loss: tensor(0.3251)\n",
      "41821 Training Loss: tensor(0.3251)\n",
      "41822 Training Loss: tensor(0.3243)\n",
      "41823 Training Loss: tensor(0.3246)\n",
      "41824 Training Loss: tensor(0.3240)\n",
      "41825 Training Loss: tensor(0.3243)\n",
      "41826 Training Loss: tensor(0.3246)\n",
      "41827 Training Loss: tensor(0.3244)\n",
      "41828 Training Loss: tensor(0.3245)\n",
      "41829 Training Loss: tensor(0.3242)\n",
      "41830 Training Loss: tensor(0.3240)\n",
      "41831 Training Loss: tensor(0.3240)\n",
      "41832 Training Loss: tensor(0.3242)\n",
      "41833 Training Loss: tensor(0.3238)\n",
      "41834 Training Loss: tensor(0.3238)\n",
      "41835 Training Loss: tensor(0.3251)\n",
      "41836 Training Loss: tensor(0.3245)\n",
      "41837 Training Loss: tensor(0.3247)\n",
      "41838 Training Loss: tensor(0.3246)\n",
      "41839 Training Loss: tensor(0.3240)\n",
      "41840 Training Loss: tensor(0.3246)\n",
      "41841 Training Loss: tensor(0.3247)\n",
      "41842 Training Loss: tensor(0.3251)\n",
      "41843 Training Loss: tensor(0.3241)\n",
      "41844 Training Loss: tensor(0.3242)\n",
      "41845 Training Loss: tensor(0.3246)\n",
      "41846 Training Loss: tensor(0.3238)\n",
      "41847 Training Loss: tensor(0.3252)\n",
      "41848 Training Loss: tensor(0.3242)\n",
      "41849 Training Loss: tensor(0.3255)\n",
      "41850 Training Loss: tensor(0.3247)\n",
      "41851 Training Loss: tensor(0.3250)\n",
      "41852 Training Loss: tensor(0.3249)\n",
      "41853 Training Loss: tensor(0.3245)\n",
      "41854 Training Loss: tensor(0.3241)\n",
      "41855 Training Loss: tensor(0.3244)\n",
      "41856 Training Loss: tensor(0.3248)\n",
      "41857 Training Loss: tensor(0.3238)\n",
      "41858 Training Loss: tensor(0.3240)\n",
      "41859 Training Loss: tensor(0.3247)\n",
      "41860 Training Loss: tensor(0.3257)\n",
      "41861 Training Loss: tensor(0.3246)\n",
      "41862 Training Loss: tensor(0.3240)\n",
      "41863 Training Loss: tensor(0.3242)\n",
      "41864 Training Loss: tensor(0.3250)\n",
      "41865 Training Loss: tensor(0.3247)\n",
      "41866 Training Loss: tensor(0.3240)\n",
      "41867 Training Loss: tensor(0.3254)\n",
      "41868 Training Loss: tensor(0.3246)\n",
      "41869 Training Loss: tensor(0.3241)\n",
      "41870 Training Loss: tensor(0.3247)\n",
      "41871 Training Loss: tensor(0.3245)\n",
      "41872 Training Loss: tensor(0.3242)\n",
      "41873 Training Loss: tensor(0.3244)\n",
      "41874 Training Loss: tensor(0.3261)\n",
      "41875 Training Loss: tensor(0.3245)\n",
      "41876 Training Loss: tensor(0.3255)\n",
      "41877 Training Loss: tensor(0.3250)\n",
      "41878 Training Loss: tensor(0.3246)\n",
      "41879 Training Loss: tensor(0.3245)\n",
      "41880 Training Loss: tensor(0.3237)\n",
      "41881 Training Loss: tensor(0.3240)\n",
      "41882 Training Loss: tensor(0.3254)\n",
      "41883 Training Loss: tensor(0.3247)\n",
      "41884 Training Loss: tensor(0.3246)\n",
      "41885 Training Loss: tensor(0.3247)\n",
      "41886 Training Loss: tensor(0.3248)\n",
      "41887 Training Loss: tensor(0.3260)\n",
      "41888 Training Loss: tensor(0.3257)\n",
      "41889 Training Loss: tensor(0.3253)\n",
      "41890 Training Loss: tensor(0.3251)\n",
      "41891 Training Loss: tensor(0.3248)\n",
      "41892 Training Loss: tensor(0.3246)\n",
      "41893 Training Loss: tensor(0.3242)\n",
      "41894 Training Loss: tensor(0.3256)\n",
      "41895 Training Loss: tensor(0.3255)\n",
      "41896 Training Loss: tensor(0.3254)\n",
      "41897 Training Loss: tensor(0.3259)\n",
      "41898 Training Loss: tensor(0.3246)\n",
      "41899 Training Loss: tensor(0.3257)\n",
      "41900 Training Loss: tensor(0.3250)\n",
      "41901 Training Loss: tensor(0.3242)\n",
      "41902 Training Loss: tensor(0.3241)\n",
      "41903 Training Loss: tensor(0.3250)\n",
      "41904 Training Loss: tensor(0.3251)\n",
      "41905 Training Loss: tensor(0.3240)\n",
      "41906 Training Loss: tensor(0.3247)\n",
      "41907 Training Loss: tensor(0.3247)\n",
      "41908 Training Loss: tensor(0.3253)\n",
      "41909 Training Loss: tensor(0.3245)\n",
      "41910 Training Loss: tensor(0.3247)\n",
      "41911 Training Loss: tensor(0.3241)\n",
      "41912 Training Loss: tensor(0.3243)\n",
      "41913 Training Loss: tensor(0.3245)\n",
      "41914 Training Loss: tensor(0.3241)\n",
      "41915 Training Loss: tensor(0.3250)\n",
      "41916 Training Loss: tensor(0.3243)\n",
      "41917 Training Loss: tensor(0.3241)\n",
      "41918 Training Loss: tensor(0.3248)\n",
      "41919 Training Loss: tensor(0.3247)\n",
      "41920 Training Loss: tensor(0.3255)\n",
      "41921 Training Loss: tensor(0.3251)\n",
      "41922 Training Loss: tensor(0.3243)\n",
      "41923 Training Loss: tensor(0.3241)\n",
      "41924 Training Loss: tensor(0.3246)\n",
      "41925 Training Loss: tensor(0.3248)\n",
      "41926 Training Loss: tensor(0.3252)\n",
      "41927 Training Loss: tensor(0.3249)\n",
      "41928 Training Loss: tensor(0.3246)\n",
      "41929 Training Loss: tensor(0.3247)\n",
      "41930 Training Loss: tensor(0.3246)\n",
      "41931 Training Loss: tensor(0.3250)\n",
      "41932 Training Loss: tensor(0.3243)\n",
      "41933 Training Loss: tensor(0.3244)\n",
      "41934 Training Loss: tensor(0.3242)\n",
      "41935 Training Loss: tensor(0.3245)\n",
      "41936 Training Loss: tensor(0.3257)\n",
      "41937 Training Loss: tensor(0.3238)\n",
      "41938 Training Loss: tensor(0.3244)\n",
      "41939 Training Loss: tensor(0.3249)\n",
      "41940 Training Loss: tensor(0.3253)\n",
      "41941 Training Loss: tensor(0.3242)\n",
      "41942 Training Loss: tensor(0.3251)\n",
      "41943 Training Loss: tensor(0.3242)\n",
      "41944 Training Loss: tensor(0.3247)\n",
      "41945 Training Loss: tensor(0.3237)\n",
      "41946 Training Loss: tensor(0.3251)\n",
      "41947 Training Loss: tensor(0.3240)\n",
      "41948 Training Loss: tensor(0.3245)\n",
      "41949 Training Loss: tensor(0.3236)\n",
      "41950 Training Loss: tensor(0.3248)\n",
      "41951 Training Loss: tensor(0.3246)\n",
      "41952 Training Loss: tensor(0.3248)\n",
      "41953 Training Loss: tensor(0.3252)\n",
      "41954 Training Loss: tensor(0.3237)\n",
      "41955 Training Loss: tensor(0.3241)\n",
      "41956 Training Loss: tensor(0.3238)\n",
      "41957 Training Loss: tensor(0.3241)\n",
      "41958 Training Loss: tensor(0.3245)\n",
      "41959 Training Loss: tensor(0.3247)\n",
      "41960 Training Loss: tensor(0.3247)\n",
      "41961 Training Loss: tensor(0.3249)\n",
      "41962 Training Loss: tensor(0.3239)\n",
      "41963 Training Loss: tensor(0.3239)\n",
      "41964 Training Loss: tensor(0.3247)\n",
      "41965 Training Loss: tensor(0.3261)\n",
      "41966 Training Loss: tensor(0.3241)\n",
      "41967 Training Loss: tensor(0.3239)\n",
      "41968 Training Loss: tensor(0.3243)\n",
      "41969 Training Loss: tensor(0.3254)\n",
      "41970 Training Loss: tensor(0.3247)\n",
      "41971 Training Loss: tensor(0.3245)\n",
      "41972 Training Loss: tensor(0.3250)\n",
      "41973 Training Loss: tensor(0.3251)\n",
      "41974 Training Loss: tensor(0.3256)\n",
      "41975 Training Loss: tensor(0.3244)\n",
      "41976 Training Loss: tensor(0.3254)\n",
      "41977 Training Loss: tensor(0.3244)\n",
      "41978 Training Loss: tensor(0.3247)\n",
      "41979 Training Loss: tensor(0.3246)\n",
      "41980 Training Loss: tensor(0.3247)\n",
      "41981 Training Loss: tensor(0.3240)\n",
      "41982 Training Loss: tensor(0.3244)\n",
      "41983 Training Loss: tensor(0.3249)\n",
      "41984 Training Loss: tensor(0.3243)\n",
      "41985 Training Loss: tensor(0.3251)\n",
      "41986 Training Loss: tensor(0.3252)\n",
      "41987 Training Loss: tensor(0.3248)\n",
      "41988 Training Loss: tensor(0.3239)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41989 Training Loss: tensor(0.3246)\n",
      "41990 Training Loss: tensor(0.3253)\n",
      "41991 Training Loss: tensor(0.3248)\n",
      "41992 Training Loss: tensor(0.3252)\n",
      "41993 Training Loss: tensor(0.3246)\n",
      "41994 Training Loss: tensor(0.3248)\n",
      "41995 Training Loss: tensor(0.3247)\n",
      "41996 Training Loss: tensor(0.3255)\n",
      "41997 Training Loss: tensor(0.3240)\n",
      "41998 Training Loss: tensor(0.3240)\n",
      "41999 Training Loss: tensor(0.3249)\n",
      "42000 Training Loss: tensor(0.3249)\n",
      "42001 Training Loss: tensor(0.3248)\n",
      "42002 Training Loss: tensor(0.3240)\n",
      "42003 Training Loss: tensor(0.3245)\n",
      "42004 Training Loss: tensor(0.3242)\n",
      "42005 Training Loss: tensor(0.3239)\n",
      "42006 Training Loss: tensor(0.3250)\n",
      "42007 Training Loss: tensor(0.3244)\n",
      "42008 Training Loss: tensor(0.3247)\n",
      "42009 Training Loss: tensor(0.3251)\n",
      "42010 Training Loss: tensor(0.3246)\n",
      "42011 Training Loss: tensor(0.3237)\n",
      "42012 Training Loss: tensor(0.3241)\n",
      "42013 Training Loss: tensor(0.3238)\n",
      "42014 Training Loss: tensor(0.3245)\n",
      "42015 Training Loss: tensor(0.3239)\n",
      "42016 Training Loss: tensor(0.3240)\n",
      "42017 Training Loss: tensor(0.3261)\n",
      "42018 Training Loss: tensor(0.3249)\n",
      "42019 Training Loss: tensor(0.3243)\n",
      "42020 Training Loss: tensor(0.3242)\n",
      "42021 Training Loss: tensor(0.3251)\n",
      "42022 Training Loss: tensor(0.3238)\n",
      "42023 Training Loss: tensor(0.3248)\n",
      "42024 Training Loss: tensor(0.3255)\n",
      "42025 Training Loss: tensor(0.3243)\n",
      "42026 Training Loss: tensor(0.3236)\n",
      "42027 Training Loss: tensor(0.3243)\n",
      "42028 Training Loss: tensor(0.3252)\n",
      "42029 Training Loss: tensor(0.3246)\n",
      "42030 Training Loss: tensor(0.3245)\n",
      "42031 Training Loss: tensor(0.3241)\n",
      "42032 Training Loss: tensor(0.3241)\n",
      "42033 Training Loss: tensor(0.3246)\n",
      "42034 Training Loss: tensor(0.3241)\n",
      "42035 Training Loss: tensor(0.3268)\n",
      "42036 Training Loss: tensor(0.3239)\n",
      "42037 Training Loss: tensor(0.3244)\n",
      "42038 Training Loss: tensor(0.3239)\n",
      "42039 Training Loss: tensor(0.3242)\n",
      "42040 Training Loss: tensor(0.3249)\n",
      "42041 Training Loss: tensor(0.3241)\n",
      "42042 Training Loss: tensor(0.3248)\n",
      "42043 Training Loss: tensor(0.3244)\n",
      "42044 Training Loss: tensor(0.3242)\n",
      "42045 Training Loss: tensor(0.3238)\n",
      "42046 Training Loss: tensor(0.3241)\n",
      "42047 Training Loss: tensor(0.3243)\n",
      "42048 Training Loss: tensor(0.3244)\n",
      "42049 Training Loss: tensor(0.3246)\n",
      "42050 Training Loss: tensor(0.3253)\n",
      "42051 Training Loss: tensor(0.3252)\n",
      "42052 Training Loss: tensor(0.3238)\n",
      "42053 Training Loss: tensor(0.3250)\n",
      "42054 Training Loss: tensor(0.3251)\n",
      "42055 Training Loss: tensor(0.3249)\n",
      "42056 Training Loss: tensor(0.3248)\n",
      "42057 Training Loss: tensor(0.3244)\n",
      "42058 Training Loss: tensor(0.3245)\n",
      "42059 Training Loss: tensor(0.3249)\n",
      "42060 Training Loss: tensor(0.3245)\n",
      "42061 Training Loss: tensor(0.3237)\n",
      "42062 Training Loss: tensor(0.3245)\n",
      "42063 Training Loss: tensor(0.3250)\n",
      "42064 Training Loss: tensor(0.3242)\n",
      "42065 Training Loss: tensor(0.3248)\n",
      "42066 Training Loss: tensor(0.3244)\n",
      "42067 Training Loss: tensor(0.3244)\n",
      "42068 Training Loss: tensor(0.3250)\n",
      "42069 Training Loss: tensor(0.3251)\n",
      "42070 Training Loss: tensor(0.3260)\n",
      "42071 Training Loss: tensor(0.3253)\n",
      "42072 Training Loss: tensor(0.3239)\n",
      "42073 Training Loss: tensor(0.3241)\n",
      "42074 Training Loss: tensor(0.3243)\n",
      "42075 Training Loss: tensor(0.3252)\n",
      "42076 Training Loss: tensor(0.3242)\n",
      "42077 Training Loss: tensor(0.3251)\n",
      "42078 Training Loss: tensor(0.3241)\n",
      "42079 Training Loss: tensor(0.3250)\n",
      "42080 Training Loss: tensor(0.3250)\n",
      "42081 Training Loss: tensor(0.3245)\n",
      "42082 Training Loss: tensor(0.3243)\n",
      "42083 Training Loss: tensor(0.3243)\n",
      "42084 Training Loss: tensor(0.3257)\n",
      "42085 Training Loss: tensor(0.3255)\n",
      "42086 Training Loss: tensor(0.3242)\n",
      "42087 Training Loss: tensor(0.3252)\n",
      "42088 Training Loss: tensor(0.3243)\n",
      "42089 Training Loss: tensor(0.3250)\n",
      "42090 Training Loss: tensor(0.3243)\n",
      "42091 Training Loss: tensor(0.3249)\n",
      "42092 Training Loss: tensor(0.3244)\n",
      "42093 Training Loss: tensor(0.3245)\n",
      "42094 Training Loss: tensor(0.3242)\n",
      "42095 Training Loss: tensor(0.3243)\n",
      "42096 Training Loss: tensor(0.3257)\n",
      "42097 Training Loss: tensor(0.3237)\n",
      "42098 Training Loss: tensor(0.3242)\n",
      "42099 Training Loss: tensor(0.3243)\n",
      "42100 Training Loss: tensor(0.3242)\n",
      "42101 Training Loss: tensor(0.3239)\n",
      "42102 Training Loss: tensor(0.3240)\n",
      "42103 Training Loss: tensor(0.3245)\n",
      "42104 Training Loss: tensor(0.3247)\n",
      "42105 Training Loss: tensor(0.3237)\n",
      "42106 Training Loss: tensor(0.3246)\n",
      "42107 Training Loss: tensor(0.3240)\n",
      "42108 Training Loss: tensor(0.3245)\n",
      "42109 Training Loss: tensor(0.3261)\n",
      "42110 Training Loss: tensor(0.3251)\n",
      "42111 Training Loss: tensor(0.3241)\n",
      "42112 Training Loss: tensor(0.3245)\n",
      "42113 Training Loss: tensor(0.3238)\n",
      "42114 Training Loss: tensor(0.3259)\n",
      "42115 Training Loss: tensor(0.3256)\n",
      "42116 Training Loss: tensor(0.3237)\n",
      "42117 Training Loss: tensor(0.3251)\n",
      "42118 Training Loss: tensor(0.3263)\n",
      "42119 Training Loss: tensor(0.3241)\n",
      "42120 Training Loss: tensor(0.3246)\n",
      "42121 Training Loss: tensor(0.3242)\n",
      "42122 Training Loss: tensor(0.3241)\n",
      "42123 Training Loss: tensor(0.3241)\n",
      "42124 Training Loss: tensor(0.3250)\n",
      "42125 Training Loss: tensor(0.3255)\n",
      "42126 Training Loss: tensor(0.3248)\n",
      "42127 Training Loss: tensor(0.3262)\n",
      "42128 Training Loss: tensor(0.3253)\n",
      "42129 Training Loss: tensor(0.3249)\n",
      "42130 Training Loss: tensor(0.3249)\n",
      "42131 Training Loss: tensor(0.3259)\n",
      "42132 Training Loss: tensor(0.3241)\n",
      "42133 Training Loss: tensor(0.3246)\n",
      "42134 Training Loss: tensor(0.3243)\n",
      "42135 Training Loss: tensor(0.3241)\n",
      "42136 Training Loss: tensor(0.3245)\n",
      "42137 Training Loss: tensor(0.3244)\n",
      "42138 Training Loss: tensor(0.3239)\n",
      "42139 Training Loss: tensor(0.3247)\n",
      "42140 Training Loss: tensor(0.3244)\n",
      "42141 Training Loss: tensor(0.3248)\n",
      "42142 Training Loss: tensor(0.3243)\n",
      "42143 Training Loss: tensor(0.3238)\n",
      "42144 Training Loss: tensor(0.3247)\n",
      "42145 Training Loss: tensor(0.3243)\n",
      "42146 Training Loss: tensor(0.3244)\n",
      "42147 Training Loss: tensor(0.3243)\n",
      "42148 Training Loss: tensor(0.3252)\n",
      "42149 Training Loss: tensor(0.3251)\n",
      "42150 Training Loss: tensor(0.3250)\n",
      "42151 Training Loss: tensor(0.3243)\n",
      "42152 Training Loss: tensor(0.3252)\n",
      "42153 Training Loss: tensor(0.3262)\n",
      "42154 Training Loss: tensor(0.3248)\n",
      "42155 Training Loss: tensor(0.3244)\n",
      "42156 Training Loss: tensor(0.3237)\n",
      "42157 Training Loss: tensor(0.3245)\n",
      "42158 Training Loss: tensor(0.3243)\n",
      "42159 Training Loss: tensor(0.3247)\n",
      "42160 Training Loss: tensor(0.3248)\n",
      "42161 Training Loss: tensor(0.3238)\n",
      "42162 Training Loss: tensor(0.3238)\n",
      "42163 Training Loss: tensor(0.3254)\n",
      "42164 Training Loss: tensor(0.3245)\n",
      "42165 Training Loss: tensor(0.3245)\n",
      "42166 Training Loss: tensor(0.3240)\n",
      "42167 Training Loss: tensor(0.3251)\n",
      "42168 Training Loss: tensor(0.3246)\n",
      "42169 Training Loss: tensor(0.3242)\n",
      "42170 Training Loss: tensor(0.3258)\n",
      "42171 Training Loss: tensor(0.3249)\n",
      "42172 Training Loss: tensor(0.3240)\n",
      "42173 Training Loss: tensor(0.3244)\n",
      "42174 Training Loss: tensor(0.3253)\n",
      "42175 Training Loss: tensor(0.3253)\n",
      "42176 Training Loss: tensor(0.3254)\n",
      "42177 Training Loss: tensor(0.3240)\n",
      "42178 Training Loss: tensor(0.3244)\n",
      "42179 Training Loss: tensor(0.3245)\n",
      "42180 Training Loss: tensor(0.3258)\n",
      "42181 Training Loss: tensor(0.3245)\n",
      "42182 Training Loss: tensor(0.3241)\n",
      "42183 Training Loss: tensor(0.3245)\n",
      "42184 Training Loss: tensor(0.3252)\n",
      "42185 Training Loss: tensor(0.3258)\n",
      "42186 Training Loss: tensor(0.3243)\n",
      "42187 Training Loss: tensor(0.3240)\n",
      "42188 Training Loss: tensor(0.3244)\n",
      "42189 Training Loss: tensor(0.3239)\n",
      "42190 Training Loss: tensor(0.3243)\n",
      "42191 Training Loss: tensor(0.3244)\n",
      "42192 Training Loss: tensor(0.3257)\n",
      "42193 Training Loss: tensor(0.3247)\n",
      "42194 Training Loss: tensor(0.3249)\n",
      "42195 Training Loss: tensor(0.3245)\n",
      "42196 Training Loss: tensor(0.3250)\n",
      "42197 Training Loss: tensor(0.3244)\n",
      "42198 Training Loss: tensor(0.3238)\n",
      "42199 Training Loss: tensor(0.3246)\n",
      "42200 Training Loss: tensor(0.3242)\n",
      "42201 Training Loss: tensor(0.3240)\n",
      "42202 Training Loss: tensor(0.3246)\n",
      "42203 Training Loss: tensor(0.3239)\n",
      "42204 Training Loss: tensor(0.3252)\n",
      "42205 Training Loss: tensor(0.3251)\n",
      "42206 Training Loss: tensor(0.3255)\n",
      "42207 Training Loss: tensor(0.3255)\n",
      "42208 Training Loss: tensor(0.3241)\n",
      "42209 Training Loss: tensor(0.3246)\n",
      "42210 Training Loss: tensor(0.3249)\n",
      "42211 Training Loss: tensor(0.3256)\n",
      "42212 Training Loss: tensor(0.3242)\n",
      "42213 Training Loss: tensor(0.3244)\n",
      "42214 Training Loss: tensor(0.3245)\n",
      "42215 Training Loss: tensor(0.3246)\n",
      "42216 Training Loss: tensor(0.3254)\n",
      "42217 Training Loss: tensor(0.3246)\n",
      "42218 Training Loss: tensor(0.3247)\n",
      "42219 Training Loss: tensor(0.3253)\n",
      "42220 Training Loss: tensor(0.3240)\n",
      "42221 Training Loss: tensor(0.3242)\n",
      "42222 Training Loss: tensor(0.3248)\n",
      "42223 Training Loss: tensor(0.3244)\n",
      "42224 Training Loss: tensor(0.3247)\n",
      "42225 Training Loss: tensor(0.3242)\n",
      "42226 Training Loss: tensor(0.3247)\n",
      "42227 Training Loss: tensor(0.3252)\n",
      "42228 Training Loss: tensor(0.3256)\n",
      "42229 Training Loss: tensor(0.3248)\n",
      "42230 Training Loss: tensor(0.3242)\n",
      "42231 Training Loss: tensor(0.3245)\n",
      "42232 Training Loss: tensor(0.3243)\n",
      "42233 Training Loss: tensor(0.3251)\n",
      "42234 Training Loss: tensor(0.3248)\n",
      "42235 Training Loss: tensor(0.3252)\n",
      "42236 Training Loss: tensor(0.3243)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42237 Training Loss: tensor(0.3251)\n",
      "42238 Training Loss: tensor(0.3247)\n",
      "42239 Training Loss: tensor(0.3244)\n",
      "42240 Training Loss: tensor(0.3252)\n",
      "42241 Training Loss: tensor(0.3250)\n",
      "42242 Training Loss: tensor(0.3252)\n",
      "42243 Training Loss: tensor(0.3241)\n",
      "42244 Training Loss: tensor(0.3243)\n",
      "42245 Training Loss: tensor(0.3252)\n",
      "42246 Training Loss: tensor(0.3248)\n",
      "42247 Training Loss: tensor(0.3240)\n",
      "42248 Training Loss: tensor(0.3244)\n",
      "42249 Training Loss: tensor(0.3244)\n",
      "42250 Training Loss: tensor(0.3246)\n",
      "42251 Training Loss: tensor(0.3245)\n",
      "42252 Training Loss: tensor(0.3244)\n",
      "42253 Training Loss: tensor(0.3251)\n",
      "42254 Training Loss: tensor(0.3237)\n",
      "42255 Training Loss: tensor(0.3243)\n",
      "42256 Training Loss: tensor(0.3247)\n",
      "42257 Training Loss: tensor(0.3240)\n",
      "42258 Training Loss: tensor(0.3247)\n",
      "42259 Training Loss: tensor(0.3245)\n",
      "42260 Training Loss: tensor(0.3243)\n",
      "42261 Training Loss: tensor(0.3242)\n",
      "42262 Training Loss: tensor(0.3242)\n",
      "42263 Training Loss: tensor(0.3245)\n",
      "42264 Training Loss: tensor(0.3247)\n",
      "42265 Training Loss: tensor(0.3241)\n",
      "42266 Training Loss: tensor(0.3248)\n",
      "42267 Training Loss: tensor(0.3253)\n",
      "42268 Training Loss: tensor(0.3254)\n",
      "42269 Training Loss: tensor(0.3254)\n",
      "42270 Training Loss: tensor(0.3245)\n",
      "42271 Training Loss: tensor(0.3241)\n",
      "42272 Training Loss: tensor(0.3239)\n",
      "42273 Training Loss: tensor(0.3253)\n",
      "42274 Training Loss: tensor(0.3249)\n",
      "42275 Training Loss: tensor(0.3242)\n",
      "42276 Training Loss: tensor(0.3244)\n",
      "42277 Training Loss: tensor(0.3243)\n",
      "42278 Training Loss: tensor(0.3251)\n",
      "42279 Training Loss: tensor(0.3247)\n",
      "42280 Training Loss: tensor(0.3246)\n",
      "42281 Training Loss: tensor(0.3244)\n",
      "42282 Training Loss: tensor(0.3251)\n",
      "42283 Training Loss: tensor(0.3243)\n",
      "42284 Training Loss: tensor(0.3254)\n",
      "42285 Training Loss: tensor(0.3251)\n",
      "42286 Training Loss: tensor(0.3246)\n",
      "42287 Training Loss: tensor(0.3244)\n",
      "42288 Training Loss: tensor(0.3243)\n",
      "42289 Training Loss: tensor(0.3238)\n",
      "42290 Training Loss: tensor(0.3239)\n",
      "42291 Training Loss: tensor(0.3242)\n",
      "42292 Training Loss: tensor(0.3243)\n",
      "42293 Training Loss: tensor(0.3245)\n",
      "42294 Training Loss: tensor(0.3253)\n",
      "42295 Training Loss: tensor(0.3241)\n",
      "42296 Training Loss: tensor(0.3244)\n",
      "42297 Training Loss: tensor(0.3253)\n",
      "42298 Training Loss: tensor(0.3242)\n",
      "42299 Training Loss: tensor(0.3249)\n",
      "42300 Training Loss: tensor(0.3244)\n",
      "42301 Training Loss: tensor(0.3241)\n",
      "42302 Training Loss: tensor(0.3242)\n",
      "42303 Training Loss: tensor(0.3246)\n",
      "42304 Training Loss: tensor(0.3249)\n",
      "42305 Training Loss: tensor(0.3253)\n",
      "42306 Training Loss: tensor(0.3247)\n",
      "42307 Training Loss: tensor(0.3249)\n",
      "42308 Training Loss: tensor(0.3252)\n",
      "42309 Training Loss: tensor(0.3245)\n",
      "42310 Training Loss: tensor(0.3242)\n",
      "42311 Training Loss: tensor(0.3238)\n",
      "42312 Training Loss: tensor(0.3239)\n",
      "42313 Training Loss: tensor(0.3246)\n",
      "42314 Training Loss: tensor(0.3247)\n",
      "42315 Training Loss: tensor(0.3241)\n",
      "42316 Training Loss: tensor(0.3243)\n",
      "42317 Training Loss: tensor(0.3246)\n",
      "42318 Training Loss: tensor(0.3247)\n",
      "42319 Training Loss: tensor(0.3258)\n",
      "42320 Training Loss: tensor(0.3261)\n",
      "42321 Training Loss: tensor(0.3241)\n",
      "42322 Training Loss: tensor(0.3249)\n",
      "42323 Training Loss: tensor(0.3246)\n",
      "42324 Training Loss: tensor(0.3242)\n",
      "42325 Training Loss: tensor(0.3241)\n",
      "42326 Training Loss: tensor(0.3245)\n",
      "42327 Training Loss: tensor(0.3235)\n",
      "42328 Training Loss: tensor(0.3241)\n",
      "42329 Training Loss: tensor(0.3246)\n",
      "42330 Training Loss: tensor(0.3238)\n",
      "42331 Training Loss: tensor(0.3261)\n",
      "42332 Training Loss: tensor(0.3240)\n",
      "42333 Training Loss: tensor(0.3242)\n",
      "42334 Training Loss: tensor(0.3244)\n",
      "42335 Training Loss: tensor(0.3240)\n",
      "42336 Training Loss: tensor(0.3242)\n",
      "42337 Training Loss: tensor(0.3247)\n",
      "42338 Training Loss: tensor(0.3238)\n",
      "42339 Training Loss: tensor(0.3249)\n",
      "42340 Training Loss: tensor(0.3244)\n",
      "42341 Training Loss: tensor(0.3241)\n",
      "42342 Training Loss: tensor(0.3240)\n",
      "42343 Training Loss: tensor(0.3236)\n",
      "42344 Training Loss: tensor(0.3247)\n",
      "42345 Training Loss: tensor(0.3240)\n",
      "42346 Training Loss: tensor(0.3245)\n",
      "42347 Training Loss: tensor(0.3250)\n",
      "42348 Training Loss: tensor(0.3255)\n",
      "42349 Training Loss: tensor(0.3249)\n",
      "42350 Training Loss: tensor(0.3247)\n",
      "42351 Training Loss: tensor(0.3251)\n",
      "42352 Training Loss: tensor(0.3237)\n",
      "42353 Training Loss: tensor(0.3243)\n",
      "42354 Training Loss: tensor(0.3242)\n",
      "42355 Training Loss: tensor(0.3256)\n",
      "42356 Training Loss: tensor(0.3244)\n",
      "42357 Training Loss: tensor(0.3244)\n",
      "42358 Training Loss: tensor(0.3242)\n",
      "42359 Training Loss: tensor(0.3255)\n",
      "42360 Training Loss: tensor(0.3251)\n",
      "42361 Training Loss: tensor(0.3251)\n",
      "42362 Training Loss: tensor(0.3241)\n",
      "42363 Training Loss: tensor(0.3247)\n",
      "42364 Training Loss: tensor(0.3252)\n",
      "42365 Training Loss: tensor(0.3245)\n",
      "42366 Training Loss: tensor(0.3240)\n",
      "42367 Training Loss: tensor(0.3247)\n",
      "42368 Training Loss: tensor(0.3246)\n",
      "42369 Training Loss: tensor(0.3240)\n",
      "42370 Training Loss: tensor(0.3241)\n",
      "42371 Training Loss: tensor(0.3243)\n",
      "42372 Training Loss: tensor(0.3241)\n",
      "42373 Training Loss: tensor(0.3244)\n",
      "42374 Training Loss: tensor(0.3248)\n",
      "42375 Training Loss: tensor(0.3248)\n",
      "42376 Training Loss: tensor(0.3241)\n",
      "42377 Training Loss: tensor(0.3253)\n",
      "42378 Training Loss: tensor(0.3248)\n",
      "42379 Training Loss: tensor(0.3245)\n",
      "42380 Training Loss: tensor(0.3244)\n",
      "42381 Training Loss: tensor(0.3242)\n",
      "42382 Training Loss: tensor(0.3251)\n",
      "42383 Training Loss: tensor(0.3246)\n",
      "42384 Training Loss: tensor(0.3244)\n",
      "42385 Training Loss: tensor(0.3244)\n",
      "42386 Training Loss: tensor(0.3244)\n",
      "42387 Training Loss: tensor(0.3244)\n",
      "42388 Training Loss: tensor(0.3242)\n",
      "42389 Training Loss: tensor(0.3244)\n",
      "42390 Training Loss: tensor(0.3240)\n",
      "42391 Training Loss: tensor(0.3241)\n",
      "42392 Training Loss: tensor(0.3243)\n",
      "42393 Training Loss: tensor(0.3246)\n",
      "42394 Training Loss: tensor(0.3243)\n",
      "42395 Training Loss: tensor(0.3247)\n",
      "42396 Training Loss: tensor(0.3241)\n",
      "42397 Training Loss: tensor(0.3255)\n",
      "42398 Training Loss: tensor(0.3246)\n",
      "42399 Training Loss: tensor(0.3241)\n",
      "42400 Training Loss: tensor(0.3245)\n",
      "42401 Training Loss: tensor(0.3239)\n",
      "42402 Training Loss: tensor(0.3243)\n",
      "42403 Training Loss: tensor(0.3258)\n",
      "42404 Training Loss: tensor(0.3249)\n",
      "42405 Training Loss: tensor(0.3255)\n",
      "42406 Training Loss: tensor(0.3239)\n",
      "42407 Training Loss: tensor(0.3239)\n",
      "42408 Training Loss: tensor(0.3243)\n",
      "42409 Training Loss: tensor(0.3247)\n",
      "42410 Training Loss: tensor(0.3240)\n",
      "42411 Training Loss: tensor(0.3238)\n",
      "42412 Training Loss: tensor(0.3249)\n",
      "42413 Training Loss: tensor(0.3255)\n",
      "42414 Training Loss: tensor(0.3249)\n",
      "42415 Training Loss: tensor(0.3243)\n",
      "42416 Training Loss: tensor(0.3253)\n",
      "42417 Training Loss: tensor(0.3242)\n",
      "42418 Training Loss: tensor(0.3245)\n",
      "42419 Training Loss: tensor(0.3241)\n",
      "42420 Training Loss: tensor(0.3239)\n",
      "42421 Training Loss: tensor(0.3252)\n",
      "42422 Training Loss: tensor(0.3240)\n",
      "42423 Training Loss: tensor(0.3253)\n",
      "42424 Training Loss: tensor(0.3243)\n",
      "42425 Training Loss: tensor(0.3246)\n",
      "42426 Training Loss: tensor(0.3246)\n",
      "42427 Training Loss: tensor(0.3240)\n",
      "42428 Training Loss: tensor(0.3240)\n",
      "42429 Training Loss: tensor(0.3241)\n",
      "42430 Training Loss: tensor(0.3244)\n",
      "42431 Training Loss: tensor(0.3243)\n",
      "42432 Training Loss: tensor(0.3248)\n",
      "42433 Training Loss: tensor(0.3251)\n",
      "42434 Training Loss: tensor(0.3247)\n",
      "42435 Training Loss: tensor(0.3245)\n",
      "42436 Training Loss: tensor(0.3246)\n",
      "42437 Training Loss: tensor(0.3249)\n",
      "42438 Training Loss: tensor(0.3242)\n",
      "42439 Training Loss: tensor(0.3243)\n",
      "42440 Training Loss: tensor(0.3242)\n",
      "42441 Training Loss: tensor(0.3239)\n",
      "42442 Training Loss: tensor(0.3247)\n",
      "42443 Training Loss: tensor(0.3242)\n",
      "42444 Training Loss: tensor(0.3246)\n",
      "42445 Training Loss: tensor(0.3243)\n",
      "42446 Training Loss: tensor(0.3245)\n",
      "42447 Training Loss: tensor(0.3246)\n",
      "42448 Training Loss: tensor(0.3243)\n",
      "42449 Training Loss: tensor(0.3252)\n",
      "42450 Training Loss: tensor(0.3245)\n",
      "42451 Training Loss: tensor(0.3241)\n",
      "42452 Training Loss: tensor(0.3238)\n",
      "42453 Training Loss: tensor(0.3243)\n",
      "42454 Training Loss: tensor(0.3247)\n",
      "42455 Training Loss: tensor(0.3243)\n",
      "42456 Training Loss: tensor(0.3240)\n",
      "42457 Training Loss: tensor(0.3237)\n",
      "42458 Training Loss: tensor(0.3260)\n",
      "42459 Training Loss: tensor(0.3253)\n",
      "42460 Training Loss: tensor(0.3246)\n",
      "42461 Training Loss: tensor(0.3240)\n",
      "42462 Training Loss: tensor(0.3242)\n",
      "42463 Training Loss: tensor(0.3238)\n",
      "42464 Training Loss: tensor(0.3255)\n",
      "42465 Training Loss: tensor(0.3242)\n",
      "42466 Training Loss: tensor(0.3246)\n",
      "42467 Training Loss: tensor(0.3253)\n",
      "42468 Training Loss: tensor(0.3242)\n",
      "42469 Training Loss: tensor(0.3249)\n",
      "42470 Training Loss: tensor(0.3243)\n",
      "42471 Training Loss: tensor(0.3238)\n",
      "42472 Training Loss: tensor(0.3240)\n",
      "42473 Training Loss: tensor(0.3248)\n",
      "42474 Training Loss: tensor(0.3242)\n",
      "42475 Training Loss: tensor(0.3238)\n",
      "42476 Training Loss: tensor(0.3246)\n",
      "42477 Training Loss: tensor(0.3244)\n",
      "42478 Training Loss: tensor(0.3247)\n",
      "42479 Training Loss: tensor(0.3236)\n",
      "42480 Training Loss: tensor(0.3238)\n",
      "42481 Training Loss: tensor(0.3243)\n",
      "42482 Training Loss: tensor(0.3255)\n",
      "42483 Training Loss: tensor(0.3247)\n",
      "42484 Training Loss: tensor(0.3245)\n",
      "42485 Training Loss: tensor(0.3242)\n",
      "42486 Training Loss: tensor(0.3241)\n",
      "42487 Training Loss: tensor(0.3256)\n",
      "42488 Training Loss: tensor(0.3248)\n",
      "42489 Training Loss: tensor(0.3251)\n",
      "42490 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42491 Training Loss: tensor(0.3250)\n",
      "42492 Training Loss: tensor(0.3241)\n",
      "42493 Training Loss: tensor(0.3252)\n",
      "42494 Training Loss: tensor(0.3248)\n",
      "42495 Training Loss: tensor(0.3249)\n",
      "42496 Training Loss: tensor(0.3252)\n",
      "42497 Training Loss: tensor(0.3244)\n",
      "42498 Training Loss: tensor(0.3251)\n",
      "42499 Training Loss: tensor(0.3248)\n",
      "42500 Training Loss: tensor(0.3250)\n",
      "42501 Training Loss: tensor(0.3240)\n",
      "42502 Training Loss: tensor(0.3247)\n",
      "42503 Training Loss: tensor(0.3249)\n",
      "42504 Training Loss: tensor(0.3242)\n",
      "42505 Training Loss: tensor(0.3247)\n",
      "42506 Training Loss: tensor(0.3243)\n",
      "42507 Training Loss: tensor(0.3239)\n",
      "42508 Training Loss: tensor(0.3242)\n",
      "42509 Training Loss: tensor(0.3245)\n",
      "42510 Training Loss: tensor(0.3246)\n",
      "42511 Training Loss: tensor(0.3253)\n",
      "42512 Training Loss: tensor(0.3237)\n",
      "42513 Training Loss: tensor(0.3244)\n",
      "42514 Training Loss: tensor(0.3241)\n",
      "42515 Training Loss: tensor(0.3244)\n",
      "42516 Training Loss: tensor(0.3242)\n",
      "42517 Training Loss: tensor(0.3269)\n",
      "42518 Training Loss: tensor(0.3251)\n",
      "42519 Training Loss: tensor(0.3256)\n",
      "42520 Training Loss: tensor(0.3242)\n",
      "42521 Training Loss: tensor(0.3243)\n",
      "42522 Training Loss: tensor(0.3245)\n",
      "42523 Training Loss: tensor(0.3247)\n",
      "42524 Training Loss: tensor(0.3243)\n",
      "42525 Training Loss: tensor(0.3243)\n",
      "42526 Training Loss: tensor(0.3248)\n",
      "42527 Training Loss: tensor(0.3247)\n",
      "42528 Training Loss: tensor(0.3252)\n",
      "42529 Training Loss: tensor(0.3254)\n",
      "42530 Training Loss: tensor(0.3246)\n",
      "42531 Training Loss: tensor(0.3257)\n",
      "42532 Training Loss: tensor(0.3262)\n",
      "42533 Training Loss: tensor(0.3242)\n",
      "42534 Training Loss: tensor(0.3246)\n",
      "42535 Training Loss: tensor(0.3241)\n",
      "42536 Training Loss: tensor(0.3244)\n",
      "42537 Training Loss: tensor(0.3246)\n",
      "42538 Training Loss: tensor(0.3246)\n",
      "42539 Training Loss: tensor(0.3246)\n",
      "42540 Training Loss: tensor(0.3249)\n",
      "42541 Training Loss: tensor(0.3243)\n",
      "42542 Training Loss: tensor(0.3257)\n",
      "42543 Training Loss: tensor(0.3242)\n",
      "42544 Training Loss: tensor(0.3251)\n",
      "42545 Training Loss: tensor(0.3241)\n",
      "42546 Training Loss: tensor(0.3256)\n",
      "42547 Training Loss: tensor(0.3246)\n",
      "42548 Training Loss: tensor(0.3245)\n",
      "42549 Training Loss: tensor(0.3247)\n",
      "42550 Training Loss: tensor(0.3253)\n",
      "42551 Training Loss: tensor(0.3247)\n",
      "42552 Training Loss: tensor(0.3240)\n",
      "42553 Training Loss: tensor(0.3237)\n",
      "42554 Training Loss: tensor(0.3261)\n",
      "42555 Training Loss: tensor(0.3246)\n",
      "42556 Training Loss: tensor(0.3239)\n",
      "42557 Training Loss: tensor(0.3249)\n",
      "42558 Training Loss: tensor(0.3243)\n",
      "42559 Training Loss: tensor(0.3237)\n",
      "42560 Training Loss: tensor(0.3254)\n",
      "42561 Training Loss: tensor(0.3242)\n",
      "42562 Training Loss: tensor(0.3240)\n",
      "42563 Training Loss: tensor(0.3241)\n",
      "42564 Training Loss: tensor(0.3246)\n",
      "42565 Training Loss: tensor(0.3245)\n",
      "42566 Training Loss: tensor(0.3244)\n",
      "42567 Training Loss: tensor(0.3237)\n",
      "42568 Training Loss: tensor(0.3238)\n",
      "42569 Training Loss: tensor(0.3252)\n",
      "42570 Training Loss: tensor(0.3242)\n",
      "42571 Training Loss: tensor(0.3251)\n",
      "42572 Training Loss: tensor(0.3244)\n",
      "42573 Training Loss: tensor(0.3246)\n",
      "42574 Training Loss: tensor(0.3239)\n",
      "42575 Training Loss: tensor(0.3236)\n",
      "42576 Training Loss: tensor(0.3245)\n",
      "42577 Training Loss: tensor(0.3243)\n",
      "42578 Training Loss: tensor(0.3252)\n",
      "42579 Training Loss: tensor(0.3237)\n",
      "42580 Training Loss: tensor(0.3243)\n",
      "42581 Training Loss: tensor(0.3244)\n",
      "42582 Training Loss: tensor(0.3245)\n",
      "42583 Training Loss: tensor(0.3249)\n",
      "42584 Training Loss: tensor(0.3245)\n",
      "42585 Training Loss: tensor(0.3238)\n",
      "42586 Training Loss: tensor(0.3249)\n",
      "42587 Training Loss: tensor(0.3250)\n",
      "42588 Training Loss: tensor(0.3240)\n",
      "42589 Training Loss: tensor(0.3250)\n",
      "42590 Training Loss: tensor(0.3239)\n",
      "42591 Training Loss: tensor(0.3246)\n",
      "42592 Training Loss: tensor(0.3247)\n",
      "42593 Training Loss: tensor(0.3238)\n",
      "42594 Training Loss: tensor(0.3246)\n",
      "42595 Training Loss: tensor(0.3237)\n",
      "42596 Training Loss: tensor(0.3249)\n",
      "42597 Training Loss: tensor(0.3241)\n",
      "42598 Training Loss: tensor(0.3244)\n",
      "42599 Training Loss: tensor(0.3239)\n",
      "42600 Training Loss: tensor(0.3237)\n",
      "42601 Training Loss: tensor(0.3246)\n",
      "42602 Training Loss: tensor(0.3236)\n",
      "42603 Training Loss: tensor(0.3243)\n",
      "42604 Training Loss: tensor(0.3240)\n",
      "42605 Training Loss: tensor(0.3244)\n",
      "42606 Training Loss: tensor(0.3247)\n",
      "42607 Training Loss: tensor(0.3242)\n",
      "42608 Training Loss: tensor(0.3242)\n",
      "42609 Training Loss: tensor(0.3246)\n",
      "42610 Training Loss: tensor(0.3256)\n",
      "42611 Training Loss: tensor(0.3249)\n",
      "42612 Training Loss: tensor(0.3248)\n",
      "42613 Training Loss: tensor(0.3240)\n",
      "42614 Training Loss: tensor(0.3241)\n",
      "42615 Training Loss: tensor(0.3237)\n",
      "42616 Training Loss: tensor(0.3249)\n",
      "42617 Training Loss: tensor(0.3246)\n",
      "42618 Training Loss: tensor(0.3260)\n",
      "42619 Training Loss: tensor(0.3245)\n",
      "42620 Training Loss: tensor(0.3256)\n",
      "42621 Training Loss: tensor(0.3245)\n",
      "42622 Training Loss: tensor(0.3247)\n",
      "42623 Training Loss: tensor(0.3240)\n",
      "42624 Training Loss: tensor(0.3240)\n",
      "42625 Training Loss: tensor(0.3256)\n",
      "42626 Training Loss: tensor(0.3239)\n",
      "42627 Training Loss: tensor(0.3254)\n",
      "42628 Training Loss: tensor(0.3246)\n",
      "42629 Training Loss: tensor(0.3245)\n",
      "42630 Training Loss: tensor(0.3245)\n",
      "42631 Training Loss: tensor(0.3245)\n",
      "42632 Training Loss: tensor(0.3252)\n",
      "42633 Training Loss: tensor(0.3242)\n",
      "42634 Training Loss: tensor(0.3250)\n",
      "42635 Training Loss: tensor(0.3241)\n",
      "42636 Training Loss: tensor(0.3246)\n",
      "42637 Training Loss: tensor(0.3247)\n",
      "42638 Training Loss: tensor(0.3236)\n",
      "42639 Training Loss: tensor(0.3242)\n",
      "42640 Training Loss: tensor(0.3248)\n",
      "42641 Training Loss: tensor(0.3250)\n",
      "42642 Training Loss: tensor(0.3239)\n",
      "42643 Training Loss: tensor(0.3243)\n",
      "42644 Training Loss: tensor(0.3246)\n",
      "42645 Training Loss: tensor(0.3240)\n",
      "42646 Training Loss: tensor(0.3245)\n",
      "42647 Training Loss: tensor(0.3245)\n",
      "42648 Training Loss: tensor(0.3250)\n",
      "42649 Training Loss: tensor(0.3252)\n",
      "42650 Training Loss: tensor(0.3254)\n",
      "42651 Training Loss: tensor(0.3248)\n",
      "42652 Training Loss: tensor(0.3244)\n",
      "42653 Training Loss: tensor(0.3256)\n",
      "42654 Training Loss: tensor(0.3243)\n",
      "42655 Training Loss: tensor(0.3245)\n",
      "42656 Training Loss: tensor(0.3252)\n",
      "42657 Training Loss: tensor(0.3246)\n",
      "42658 Training Loss: tensor(0.3257)\n",
      "42659 Training Loss: tensor(0.3241)\n",
      "42660 Training Loss: tensor(0.3246)\n",
      "42661 Training Loss: tensor(0.3238)\n",
      "42662 Training Loss: tensor(0.3246)\n",
      "42663 Training Loss: tensor(0.3239)\n",
      "42664 Training Loss: tensor(0.3250)\n",
      "42665 Training Loss: tensor(0.3245)\n",
      "42666 Training Loss: tensor(0.3244)\n",
      "42667 Training Loss: tensor(0.3241)\n",
      "42668 Training Loss: tensor(0.3244)\n",
      "42669 Training Loss: tensor(0.3244)\n",
      "42670 Training Loss: tensor(0.3247)\n",
      "42671 Training Loss: tensor(0.3261)\n",
      "42672 Training Loss: tensor(0.3245)\n",
      "42673 Training Loss: tensor(0.3246)\n",
      "42674 Training Loss: tensor(0.3239)\n",
      "42675 Training Loss: tensor(0.3243)\n",
      "42676 Training Loss: tensor(0.3239)\n",
      "42677 Training Loss: tensor(0.3244)\n",
      "42678 Training Loss: tensor(0.3244)\n",
      "42679 Training Loss: tensor(0.3255)\n",
      "42680 Training Loss: tensor(0.3253)\n",
      "42681 Training Loss: tensor(0.3250)\n",
      "42682 Training Loss: tensor(0.3241)\n",
      "42683 Training Loss: tensor(0.3237)\n",
      "42684 Training Loss: tensor(0.3238)\n",
      "42685 Training Loss: tensor(0.3250)\n",
      "42686 Training Loss: tensor(0.3243)\n",
      "42687 Training Loss: tensor(0.3242)\n",
      "42688 Training Loss: tensor(0.3245)\n",
      "42689 Training Loss: tensor(0.3251)\n",
      "42690 Training Loss: tensor(0.3243)\n",
      "42691 Training Loss: tensor(0.3244)\n",
      "42692 Training Loss: tensor(0.3251)\n",
      "42693 Training Loss: tensor(0.3241)\n",
      "42694 Training Loss: tensor(0.3248)\n",
      "42695 Training Loss: tensor(0.3240)\n",
      "42696 Training Loss: tensor(0.3239)\n",
      "42697 Training Loss: tensor(0.3249)\n",
      "42698 Training Loss: tensor(0.3243)\n",
      "42699 Training Loss: tensor(0.3243)\n",
      "42700 Training Loss: tensor(0.3246)\n",
      "42701 Training Loss: tensor(0.3239)\n",
      "42702 Training Loss: tensor(0.3256)\n",
      "42703 Training Loss: tensor(0.3249)\n",
      "42704 Training Loss: tensor(0.3244)\n",
      "42705 Training Loss: tensor(0.3249)\n",
      "42706 Training Loss: tensor(0.3246)\n",
      "42707 Training Loss: tensor(0.3244)\n",
      "42708 Training Loss: tensor(0.3245)\n",
      "42709 Training Loss: tensor(0.3238)\n",
      "42710 Training Loss: tensor(0.3244)\n",
      "42711 Training Loss: tensor(0.3248)\n",
      "42712 Training Loss: tensor(0.3244)\n",
      "42713 Training Loss: tensor(0.3247)\n",
      "42714 Training Loss: tensor(0.3247)\n",
      "42715 Training Loss: tensor(0.3241)\n",
      "42716 Training Loss: tensor(0.3236)\n",
      "42717 Training Loss: tensor(0.3246)\n",
      "42718 Training Loss: tensor(0.3243)\n",
      "42719 Training Loss: tensor(0.3251)\n",
      "42720 Training Loss: tensor(0.3245)\n",
      "42721 Training Loss: tensor(0.3240)\n",
      "42722 Training Loss: tensor(0.3242)\n",
      "42723 Training Loss: tensor(0.3243)\n",
      "42724 Training Loss: tensor(0.3245)\n",
      "42725 Training Loss: tensor(0.3246)\n",
      "42726 Training Loss: tensor(0.3243)\n",
      "42727 Training Loss: tensor(0.3249)\n",
      "42728 Training Loss: tensor(0.3241)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42729 Training Loss: tensor(0.3245)\n",
      "42730 Training Loss: tensor(0.3244)\n",
      "42731 Training Loss: tensor(0.3243)\n",
      "42732 Training Loss: tensor(0.3244)\n",
      "42733 Training Loss: tensor(0.3243)\n",
      "42734 Training Loss: tensor(0.3243)\n",
      "42735 Training Loss: tensor(0.3243)\n",
      "42736 Training Loss: tensor(0.3237)\n",
      "42737 Training Loss: tensor(0.3238)\n",
      "42738 Training Loss: tensor(0.3248)\n",
      "42739 Training Loss: tensor(0.3235)\n",
      "42740 Training Loss: tensor(0.3242)\n",
      "42741 Training Loss: tensor(0.3246)\n",
      "42742 Training Loss: tensor(0.3250)\n",
      "42743 Training Loss: tensor(0.3242)\n",
      "42744 Training Loss: tensor(0.3248)\n",
      "42745 Training Loss: tensor(0.3246)\n",
      "42746 Training Loss: tensor(0.3244)\n",
      "42747 Training Loss: tensor(0.3242)\n",
      "42748 Training Loss: tensor(0.3249)\n",
      "42749 Training Loss: tensor(0.3260)\n",
      "42750 Training Loss: tensor(0.3250)\n",
      "42751 Training Loss: tensor(0.3244)\n",
      "42752 Training Loss: tensor(0.3240)\n",
      "42753 Training Loss: tensor(0.3245)\n",
      "42754 Training Loss: tensor(0.3240)\n",
      "42755 Training Loss: tensor(0.3244)\n",
      "42756 Training Loss: tensor(0.3249)\n",
      "42757 Training Loss: tensor(0.3238)\n",
      "42758 Training Loss: tensor(0.3243)\n",
      "42759 Training Loss: tensor(0.3244)\n",
      "42760 Training Loss: tensor(0.3245)\n",
      "42761 Training Loss: tensor(0.3242)\n",
      "42762 Training Loss: tensor(0.3240)\n",
      "42763 Training Loss: tensor(0.3236)\n",
      "42764 Training Loss: tensor(0.3243)\n",
      "42765 Training Loss: tensor(0.3249)\n",
      "42766 Training Loss: tensor(0.3240)\n",
      "42767 Training Loss: tensor(0.3253)\n",
      "42768 Training Loss: tensor(0.3240)\n",
      "42769 Training Loss: tensor(0.3243)\n",
      "42770 Training Loss: tensor(0.3251)\n",
      "42771 Training Loss: tensor(0.3246)\n",
      "42772 Training Loss: tensor(0.3251)\n",
      "42773 Training Loss: tensor(0.3247)\n",
      "42774 Training Loss: tensor(0.3241)\n",
      "42775 Training Loss: tensor(0.3237)\n",
      "42776 Training Loss: tensor(0.3246)\n",
      "42777 Training Loss: tensor(0.3253)\n",
      "42778 Training Loss: tensor(0.3236)\n",
      "42779 Training Loss: tensor(0.3247)\n",
      "42780 Training Loss: tensor(0.3247)\n",
      "42781 Training Loss: tensor(0.3246)\n",
      "42782 Training Loss: tensor(0.3244)\n",
      "42783 Training Loss: tensor(0.3237)\n",
      "42784 Training Loss: tensor(0.3273)\n",
      "42785 Training Loss: tensor(0.3247)\n",
      "42786 Training Loss: tensor(0.3255)\n",
      "42787 Training Loss: tensor(0.3245)\n",
      "42788 Training Loss: tensor(0.3252)\n",
      "42789 Training Loss: tensor(0.3242)\n",
      "42790 Training Loss: tensor(0.3243)\n",
      "42791 Training Loss: tensor(0.3244)\n",
      "42792 Training Loss: tensor(0.3242)\n",
      "42793 Training Loss: tensor(0.3248)\n",
      "42794 Training Loss: tensor(0.3247)\n",
      "42795 Training Loss: tensor(0.3235)\n",
      "42796 Training Loss: tensor(0.3241)\n",
      "42797 Training Loss: tensor(0.3243)\n",
      "42798 Training Loss: tensor(0.3247)\n",
      "42799 Training Loss: tensor(0.3241)\n",
      "42800 Training Loss: tensor(0.3242)\n",
      "42801 Training Loss: tensor(0.3241)\n",
      "42802 Training Loss: tensor(0.3240)\n",
      "42803 Training Loss: tensor(0.3246)\n",
      "42804 Training Loss: tensor(0.3249)\n",
      "42805 Training Loss: tensor(0.3246)\n",
      "42806 Training Loss: tensor(0.3247)\n",
      "42807 Training Loss: tensor(0.3243)\n",
      "42808 Training Loss: tensor(0.3244)\n",
      "42809 Training Loss: tensor(0.3240)\n",
      "42810 Training Loss: tensor(0.3251)\n",
      "42811 Training Loss: tensor(0.3250)\n",
      "42812 Training Loss: tensor(0.3235)\n",
      "42813 Training Loss: tensor(0.3254)\n",
      "42814 Training Loss: tensor(0.3250)\n",
      "42815 Training Loss: tensor(0.3238)\n",
      "42816 Training Loss: tensor(0.3253)\n",
      "42817 Training Loss: tensor(0.3250)\n",
      "42818 Training Loss: tensor(0.3258)\n",
      "42819 Training Loss: tensor(0.3257)\n",
      "42820 Training Loss: tensor(0.3241)\n",
      "42821 Training Loss: tensor(0.3240)\n",
      "42822 Training Loss: tensor(0.3246)\n",
      "42823 Training Loss: tensor(0.3240)\n",
      "42824 Training Loss: tensor(0.3245)\n",
      "42825 Training Loss: tensor(0.3240)\n",
      "42826 Training Loss: tensor(0.3253)\n",
      "42827 Training Loss: tensor(0.3251)\n",
      "42828 Training Loss: tensor(0.3241)\n",
      "42829 Training Loss: tensor(0.3245)\n",
      "42830 Training Loss: tensor(0.3257)\n",
      "42831 Training Loss: tensor(0.3242)\n",
      "42832 Training Loss: tensor(0.3241)\n",
      "42833 Training Loss: tensor(0.3247)\n",
      "42834 Training Loss: tensor(0.3249)\n",
      "42835 Training Loss: tensor(0.3240)\n",
      "42836 Training Loss: tensor(0.3246)\n",
      "42837 Training Loss: tensor(0.3244)\n",
      "42838 Training Loss: tensor(0.3242)\n",
      "42839 Training Loss: tensor(0.3246)\n",
      "42840 Training Loss: tensor(0.3247)\n",
      "42841 Training Loss: tensor(0.3249)\n",
      "42842 Training Loss: tensor(0.3245)\n",
      "42843 Training Loss: tensor(0.3241)\n",
      "42844 Training Loss: tensor(0.3247)\n",
      "42845 Training Loss: tensor(0.3251)\n",
      "42846 Training Loss: tensor(0.3251)\n",
      "42847 Training Loss: tensor(0.3239)\n",
      "42848 Training Loss: tensor(0.3254)\n",
      "42849 Training Loss: tensor(0.3249)\n",
      "42850 Training Loss: tensor(0.3238)\n",
      "42851 Training Loss: tensor(0.3242)\n",
      "42852 Training Loss: tensor(0.3240)\n",
      "42853 Training Loss: tensor(0.3252)\n",
      "42854 Training Loss: tensor(0.3250)\n",
      "42855 Training Loss: tensor(0.3237)\n",
      "42856 Training Loss: tensor(0.3236)\n",
      "42857 Training Loss: tensor(0.3251)\n",
      "42858 Training Loss: tensor(0.3237)\n",
      "42859 Training Loss: tensor(0.3246)\n",
      "42860 Training Loss: tensor(0.3244)\n",
      "42861 Training Loss: tensor(0.3248)\n",
      "42862 Training Loss: tensor(0.3243)\n",
      "42863 Training Loss: tensor(0.3242)\n",
      "42864 Training Loss: tensor(0.3249)\n",
      "42865 Training Loss: tensor(0.3240)\n",
      "42866 Training Loss: tensor(0.3250)\n",
      "42867 Training Loss: tensor(0.3243)\n",
      "42868 Training Loss: tensor(0.3241)\n",
      "42869 Training Loss: tensor(0.3240)\n",
      "42870 Training Loss: tensor(0.3249)\n",
      "42871 Training Loss: tensor(0.3245)\n",
      "42872 Training Loss: tensor(0.3246)\n",
      "42873 Training Loss: tensor(0.3257)\n",
      "42874 Training Loss: tensor(0.3245)\n",
      "42875 Training Loss: tensor(0.3247)\n",
      "42876 Training Loss: tensor(0.3252)\n",
      "42877 Training Loss: tensor(0.3244)\n",
      "42878 Training Loss: tensor(0.3243)\n",
      "42879 Training Loss: tensor(0.3257)\n",
      "42880 Training Loss: tensor(0.3239)\n",
      "42881 Training Loss: tensor(0.3249)\n",
      "42882 Training Loss: tensor(0.3244)\n",
      "42883 Training Loss: tensor(0.3247)\n",
      "42884 Training Loss: tensor(0.3242)\n",
      "42885 Training Loss: tensor(0.3241)\n",
      "42886 Training Loss: tensor(0.3247)\n",
      "42887 Training Loss: tensor(0.3241)\n",
      "42888 Training Loss: tensor(0.3244)\n",
      "42889 Training Loss: tensor(0.3241)\n",
      "42890 Training Loss: tensor(0.3245)\n",
      "42891 Training Loss: tensor(0.3250)\n",
      "42892 Training Loss: tensor(0.3258)\n",
      "42893 Training Loss: tensor(0.3241)\n",
      "42894 Training Loss: tensor(0.3239)\n",
      "42895 Training Loss: tensor(0.3244)\n",
      "42896 Training Loss: tensor(0.3234)\n",
      "42897 Training Loss: tensor(0.3245)\n",
      "42898 Training Loss: tensor(0.3244)\n",
      "42899 Training Loss: tensor(0.3243)\n",
      "42900 Training Loss: tensor(0.3245)\n",
      "42901 Training Loss: tensor(0.3250)\n",
      "42902 Training Loss: tensor(0.3242)\n",
      "42903 Training Loss: tensor(0.3238)\n",
      "42904 Training Loss: tensor(0.3248)\n",
      "42905 Training Loss: tensor(0.3248)\n",
      "42906 Training Loss: tensor(0.3242)\n",
      "42907 Training Loss: tensor(0.3249)\n",
      "42908 Training Loss: tensor(0.3240)\n",
      "42909 Training Loss: tensor(0.3241)\n",
      "42910 Training Loss: tensor(0.3250)\n",
      "42911 Training Loss: tensor(0.3243)\n",
      "42912 Training Loss: tensor(0.3247)\n",
      "42913 Training Loss: tensor(0.3246)\n",
      "42914 Training Loss: tensor(0.3242)\n",
      "42915 Training Loss: tensor(0.3251)\n",
      "42916 Training Loss: tensor(0.3240)\n",
      "42917 Training Loss: tensor(0.3248)\n",
      "42918 Training Loss: tensor(0.3251)\n",
      "42919 Training Loss: tensor(0.3239)\n",
      "42920 Training Loss: tensor(0.3248)\n",
      "42921 Training Loss: tensor(0.3245)\n",
      "42922 Training Loss: tensor(0.3253)\n",
      "42923 Training Loss: tensor(0.3246)\n",
      "42924 Training Loss: tensor(0.3243)\n",
      "42925 Training Loss: tensor(0.3248)\n",
      "42926 Training Loss: tensor(0.3249)\n",
      "42927 Training Loss: tensor(0.3246)\n",
      "42928 Training Loss: tensor(0.3251)\n",
      "42929 Training Loss: tensor(0.3242)\n",
      "42930 Training Loss: tensor(0.3253)\n",
      "42931 Training Loss: tensor(0.3250)\n",
      "42932 Training Loss: tensor(0.3245)\n",
      "42933 Training Loss: tensor(0.3250)\n",
      "42934 Training Loss: tensor(0.3249)\n",
      "42935 Training Loss: tensor(0.3252)\n",
      "42936 Training Loss: tensor(0.3255)\n",
      "42937 Training Loss: tensor(0.3259)\n",
      "42938 Training Loss: tensor(0.3246)\n",
      "42939 Training Loss: tensor(0.3245)\n",
      "42940 Training Loss: tensor(0.3259)\n",
      "42941 Training Loss: tensor(0.3245)\n",
      "42942 Training Loss: tensor(0.3267)\n",
      "42943 Training Loss: tensor(0.3244)\n",
      "42944 Training Loss: tensor(0.3251)\n",
      "42945 Training Loss: tensor(0.3250)\n",
      "42946 Training Loss: tensor(0.3246)\n",
      "42947 Training Loss: tensor(0.3253)\n",
      "42948 Training Loss: tensor(0.3250)\n",
      "42949 Training Loss: tensor(0.3247)\n",
      "42950 Training Loss: tensor(0.3255)\n",
      "42951 Training Loss: tensor(0.3245)\n",
      "42952 Training Loss: tensor(0.3251)\n",
      "42953 Training Loss: tensor(0.3244)\n",
      "42954 Training Loss: tensor(0.3240)\n",
      "42955 Training Loss: tensor(0.3251)\n",
      "42956 Training Loss: tensor(0.3247)\n",
      "42957 Training Loss: tensor(0.3260)\n",
      "42958 Training Loss: tensor(0.3246)\n",
      "42959 Training Loss: tensor(0.3246)\n",
      "42960 Training Loss: tensor(0.3257)\n",
      "42961 Training Loss: tensor(0.3254)\n",
      "42962 Training Loss: tensor(0.3243)\n",
      "42963 Training Loss: tensor(0.3240)\n",
      "42964 Training Loss: tensor(0.3242)\n",
      "42965 Training Loss: tensor(0.3241)\n",
      "42966 Training Loss: tensor(0.3251)\n",
      "42967 Training Loss: tensor(0.3244)\n",
      "42968 Training Loss: tensor(0.3249)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42969 Training Loss: tensor(0.3244)\n",
      "42970 Training Loss: tensor(0.3257)\n",
      "42971 Training Loss: tensor(0.3245)\n",
      "42972 Training Loss: tensor(0.3250)\n",
      "42973 Training Loss: tensor(0.3246)\n",
      "42974 Training Loss: tensor(0.3249)\n",
      "42975 Training Loss: tensor(0.3242)\n",
      "42976 Training Loss: tensor(0.3250)\n",
      "42977 Training Loss: tensor(0.3245)\n",
      "42978 Training Loss: tensor(0.3245)\n",
      "42979 Training Loss: tensor(0.3241)\n",
      "42980 Training Loss: tensor(0.3236)\n",
      "42981 Training Loss: tensor(0.3245)\n",
      "42982 Training Loss: tensor(0.3242)\n",
      "42983 Training Loss: tensor(0.3244)\n",
      "42984 Training Loss: tensor(0.3246)\n",
      "42985 Training Loss: tensor(0.3236)\n",
      "42986 Training Loss: tensor(0.3242)\n",
      "42987 Training Loss: tensor(0.3239)\n",
      "42988 Training Loss: tensor(0.3247)\n",
      "42989 Training Loss: tensor(0.3249)\n",
      "42990 Training Loss: tensor(0.3252)\n",
      "42991 Training Loss: tensor(0.3249)\n",
      "42992 Training Loss: tensor(0.3255)\n",
      "42993 Training Loss: tensor(0.3244)\n",
      "42994 Training Loss: tensor(0.3238)\n",
      "42995 Training Loss: tensor(0.3250)\n",
      "42996 Training Loss: tensor(0.3240)\n",
      "42997 Training Loss: tensor(0.3253)\n",
      "42998 Training Loss: tensor(0.3256)\n",
      "42999 Training Loss: tensor(0.3241)\n",
      "43000 Training Loss: tensor(0.3248)\n",
      "43001 Training Loss: tensor(0.3244)\n",
      "43002 Training Loss: tensor(0.3245)\n",
      "43003 Training Loss: tensor(0.3244)\n",
      "43004 Training Loss: tensor(0.3248)\n",
      "43005 Training Loss: tensor(0.3240)\n",
      "43006 Training Loss: tensor(0.3250)\n",
      "43007 Training Loss: tensor(0.3244)\n",
      "43008 Training Loss: tensor(0.3249)\n",
      "43009 Training Loss: tensor(0.3240)\n",
      "43010 Training Loss: tensor(0.3249)\n",
      "43011 Training Loss: tensor(0.3248)\n",
      "43012 Training Loss: tensor(0.3239)\n",
      "43013 Training Loss: tensor(0.3243)\n",
      "43014 Training Loss: tensor(0.3243)\n",
      "43015 Training Loss: tensor(0.3241)\n",
      "43016 Training Loss: tensor(0.3243)\n",
      "43017 Training Loss: tensor(0.3245)\n",
      "43018 Training Loss: tensor(0.3244)\n",
      "43019 Training Loss: tensor(0.3250)\n",
      "43020 Training Loss: tensor(0.3237)\n",
      "43021 Training Loss: tensor(0.3243)\n",
      "43022 Training Loss: tensor(0.3240)\n",
      "43023 Training Loss: tensor(0.3244)\n",
      "43024 Training Loss: tensor(0.3244)\n",
      "43025 Training Loss: tensor(0.3248)\n",
      "43026 Training Loss: tensor(0.3242)\n",
      "43027 Training Loss: tensor(0.3237)\n",
      "43028 Training Loss: tensor(0.3260)\n",
      "43029 Training Loss: tensor(0.3239)\n",
      "43030 Training Loss: tensor(0.3241)\n",
      "43031 Training Loss: tensor(0.3243)\n",
      "43032 Training Loss: tensor(0.3257)\n",
      "43033 Training Loss: tensor(0.3244)\n",
      "43034 Training Loss: tensor(0.3239)\n",
      "43035 Training Loss: tensor(0.3248)\n",
      "43036 Training Loss: tensor(0.3248)\n",
      "43037 Training Loss: tensor(0.3265)\n",
      "43038 Training Loss: tensor(0.3236)\n",
      "43039 Training Loss: tensor(0.3242)\n",
      "43040 Training Loss: tensor(0.3235)\n",
      "43041 Training Loss: tensor(0.3250)\n",
      "43042 Training Loss: tensor(0.3240)\n",
      "43043 Training Loss: tensor(0.3248)\n",
      "43044 Training Loss: tensor(0.3251)\n",
      "43045 Training Loss: tensor(0.3246)\n",
      "43046 Training Loss: tensor(0.3239)\n",
      "43047 Training Loss: tensor(0.3245)\n",
      "43048 Training Loss: tensor(0.3249)\n",
      "43049 Training Loss: tensor(0.3246)\n",
      "43050 Training Loss: tensor(0.3242)\n",
      "43051 Training Loss: tensor(0.3240)\n",
      "43052 Training Loss: tensor(0.3242)\n",
      "43053 Training Loss: tensor(0.3250)\n",
      "43054 Training Loss: tensor(0.3246)\n",
      "43055 Training Loss: tensor(0.3245)\n",
      "43056 Training Loss: tensor(0.3245)\n",
      "43057 Training Loss: tensor(0.3239)\n",
      "43058 Training Loss: tensor(0.3245)\n",
      "43059 Training Loss: tensor(0.3246)\n",
      "43060 Training Loss: tensor(0.3240)\n",
      "43061 Training Loss: tensor(0.3260)\n",
      "43062 Training Loss: tensor(0.3241)\n",
      "43063 Training Loss: tensor(0.3243)\n",
      "43064 Training Loss: tensor(0.3251)\n",
      "43065 Training Loss: tensor(0.3250)\n",
      "43066 Training Loss: tensor(0.3244)\n",
      "43067 Training Loss: tensor(0.3249)\n",
      "43068 Training Loss: tensor(0.3244)\n",
      "43069 Training Loss: tensor(0.3237)\n",
      "43070 Training Loss: tensor(0.3240)\n",
      "43071 Training Loss: tensor(0.3245)\n",
      "43072 Training Loss: tensor(0.3248)\n",
      "43073 Training Loss: tensor(0.3247)\n",
      "43074 Training Loss: tensor(0.3250)\n",
      "43075 Training Loss: tensor(0.3246)\n",
      "43076 Training Loss: tensor(0.3242)\n",
      "43077 Training Loss: tensor(0.3243)\n",
      "43078 Training Loss: tensor(0.3235)\n",
      "43079 Training Loss: tensor(0.3246)\n",
      "43080 Training Loss: tensor(0.3252)\n",
      "43081 Training Loss: tensor(0.3243)\n",
      "43082 Training Loss: tensor(0.3243)\n",
      "43083 Training Loss: tensor(0.3235)\n",
      "43084 Training Loss: tensor(0.3250)\n",
      "43085 Training Loss: tensor(0.3240)\n",
      "43086 Training Loss: tensor(0.3260)\n",
      "43087 Training Loss: tensor(0.3240)\n",
      "43088 Training Loss: tensor(0.3240)\n",
      "43089 Training Loss: tensor(0.3243)\n",
      "43090 Training Loss: tensor(0.3247)\n",
      "43091 Training Loss: tensor(0.3245)\n",
      "43092 Training Loss: tensor(0.3237)\n",
      "43093 Training Loss: tensor(0.3246)\n",
      "43094 Training Loss: tensor(0.3250)\n",
      "43095 Training Loss: tensor(0.3246)\n",
      "43096 Training Loss: tensor(0.3238)\n",
      "43097 Training Loss: tensor(0.3242)\n",
      "43098 Training Loss: tensor(0.3244)\n",
      "43099 Training Loss: tensor(0.3245)\n",
      "43100 Training Loss: tensor(0.3241)\n",
      "43101 Training Loss: tensor(0.3251)\n",
      "43102 Training Loss: tensor(0.3248)\n",
      "43103 Training Loss: tensor(0.3236)\n",
      "43104 Training Loss: tensor(0.3257)\n",
      "43105 Training Loss: tensor(0.3243)\n",
      "43106 Training Loss: tensor(0.3248)\n",
      "43107 Training Loss: tensor(0.3239)\n",
      "43108 Training Loss: tensor(0.3244)\n",
      "43109 Training Loss: tensor(0.3242)\n",
      "43110 Training Loss: tensor(0.3249)\n",
      "43111 Training Loss: tensor(0.3247)\n",
      "43112 Training Loss: tensor(0.3245)\n",
      "43113 Training Loss: tensor(0.3240)\n",
      "43114 Training Loss: tensor(0.3242)\n",
      "43115 Training Loss: tensor(0.3241)\n",
      "43116 Training Loss: tensor(0.3243)\n",
      "43117 Training Loss: tensor(0.3242)\n",
      "43118 Training Loss: tensor(0.3251)\n",
      "43119 Training Loss: tensor(0.3247)\n",
      "43120 Training Loss: tensor(0.3244)\n",
      "43121 Training Loss: tensor(0.3240)\n",
      "43122 Training Loss: tensor(0.3238)\n",
      "43123 Training Loss: tensor(0.3244)\n",
      "43124 Training Loss: tensor(0.3246)\n",
      "43125 Training Loss: tensor(0.3246)\n",
      "43126 Training Loss: tensor(0.3238)\n",
      "43127 Training Loss: tensor(0.3245)\n",
      "43128 Training Loss: tensor(0.3248)\n",
      "43129 Training Loss: tensor(0.3243)\n",
      "43130 Training Loss: tensor(0.3246)\n",
      "43131 Training Loss: tensor(0.3248)\n",
      "43132 Training Loss: tensor(0.3235)\n",
      "43133 Training Loss: tensor(0.3240)\n",
      "43134 Training Loss: tensor(0.3247)\n",
      "43135 Training Loss: tensor(0.3249)\n",
      "43136 Training Loss: tensor(0.3242)\n",
      "43137 Training Loss: tensor(0.3257)\n",
      "43138 Training Loss: tensor(0.3254)\n",
      "43139 Training Loss: tensor(0.3247)\n",
      "43140 Training Loss: tensor(0.3241)\n",
      "43141 Training Loss: tensor(0.3251)\n",
      "43142 Training Loss: tensor(0.3255)\n",
      "43143 Training Loss: tensor(0.3238)\n",
      "43144 Training Loss: tensor(0.3241)\n",
      "43145 Training Loss: tensor(0.3247)\n",
      "43146 Training Loss: tensor(0.3246)\n",
      "43147 Training Loss: tensor(0.3244)\n",
      "43148 Training Loss: tensor(0.3246)\n",
      "43149 Training Loss: tensor(0.3252)\n",
      "43150 Training Loss: tensor(0.3243)\n",
      "43151 Training Loss: tensor(0.3242)\n",
      "43152 Training Loss: tensor(0.3239)\n",
      "43153 Training Loss: tensor(0.3240)\n",
      "43154 Training Loss: tensor(0.3245)\n",
      "43155 Training Loss: tensor(0.3250)\n",
      "43156 Training Loss: tensor(0.3253)\n",
      "43157 Training Loss: tensor(0.3244)\n",
      "43158 Training Loss: tensor(0.3239)\n",
      "43159 Training Loss: tensor(0.3246)\n",
      "43160 Training Loss: tensor(0.3250)\n",
      "43161 Training Loss: tensor(0.3238)\n",
      "43162 Training Loss: tensor(0.3244)\n",
      "43163 Training Loss: tensor(0.3246)\n",
      "43164 Training Loss: tensor(0.3244)\n",
      "43165 Training Loss: tensor(0.3238)\n",
      "43166 Training Loss: tensor(0.3243)\n",
      "43167 Training Loss: tensor(0.3243)\n",
      "43168 Training Loss: tensor(0.3251)\n",
      "43169 Training Loss: tensor(0.3237)\n",
      "43170 Training Loss: tensor(0.3234)\n",
      "43171 Training Loss: tensor(0.3241)\n",
      "43172 Training Loss: tensor(0.3249)\n",
      "43173 Training Loss: tensor(0.3255)\n",
      "43174 Training Loss: tensor(0.3244)\n",
      "43175 Training Loss: tensor(0.3244)\n",
      "43176 Training Loss: tensor(0.3247)\n",
      "43177 Training Loss: tensor(0.3253)\n",
      "43178 Training Loss: tensor(0.3240)\n",
      "43179 Training Loss: tensor(0.3239)\n",
      "43180 Training Loss: tensor(0.3259)\n",
      "43181 Training Loss: tensor(0.3243)\n",
      "43182 Training Loss: tensor(0.3242)\n",
      "43183 Training Loss: tensor(0.3244)\n",
      "43184 Training Loss: tensor(0.3237)\n",
      "43185 Training Loss: tensor(0.3247)\n",
      "43186 Training Loss: tensor(0.3244)\n",
      "43187 Training Loss: tensor(0.3243)\n",
      "43188 Training Loss: tensor(0.3239)\n",
      "43189 Training Loss: tensor(0.3242)\n",
      "43190 Training Loss: tensor(0.3242)\n",
      "43191 Training Loss: tensor(0.3248)\n",
      "43192 Training Loss: tensor(0.3250)\n",
      "43193 Training Loss: tensor(0.3255)\n",
      "43194 Training Loss: tensor(0.3245)\n",
      "43195 Training Loss: tensor(0.3235)\n",
      "43196 Training Loss: tensor(0.3239)\n",
      "43197 Training Loss: tensor(0.3249)\n",
      "43198 Training Loss: tensor(0.3248)\n",
      "43199 Training Loss: tensor(0.3240)\n",
      "43200 Training Loss: tensor(0.3245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43201 Training Loss: tensor(0.3245)\n",
      "43202 Training Loss: tensor(0.3242)\n",
      "43203 Training Loss: tensor(0.3251)\n",
      "43204 Training Loss: tensor(0.3247)\n",
      "43205 Training Loss: tensor(0.3236)\n",
      "43206 Training Loss: tensor(0.3244)\n",
      "43207 Training Loss: tensor(0.3245)\n",
      "43208 Training Loss: tensor(0.3255)\n",
      "43209 Training Loss: tensor(0.3243)\n",
      "43210 Training Loss: tensor(0.3236)\n",
      "43211 Training Loss: tensor(0.3246)\n",
      "43212 Training Loss: tensor(0.3240)\n",
      "43213 Training Loss: tensor(0.3245)\n",
      "43214 Training Loss: tensor(0.3243)\n",
      "43215 Training Loss: tensor(0.3242)\n",
      "43216 Training Loss: tensor(0.3241)\n",
      "43217 Training Loss: tensor(0.3249)\n",
      "43218 Training Loss: tensor(0.3248)\n",
      "43219 Training Loss: tensor(0.3240)\n",
      "43220 Training Loss: tensor(0.3240)\n",
      "43221 Training Loss: tensor(0.3237)\n",
      "43222 Training Loss: tensor(0.3237)\n",
      "43223 Training Loss: tensor(0.3240)\n",
      "43224 Training Loss: tensor(0.3237)\n",
      "43225 Training Loss: tensor(0.3243)\n",
      "43226 Training Loss: tensor(0.3239)\n",
      "43227 Training Loss: tensor(0.3243)\n",
      "43228 Training Loss: tensor(0.3237)\n",
      "43229 Training Loss: tensor(0.3246)\n",
      "43230 Training Loss: tensor(0.3235)\n",
      "43231 Training Loss: tensor(0.3246)\n",
      "43232 Training Loss: tensor(0.3250)\n",
      "43233 Training Loss: tensor(0.3244)\n",
      "43234 Training Loss: tensor(0.3242)\n",
      "43235 Training Loss: tensor(0.3245)\n",
      "43236 Training Loss: tensor(0.3240)\n",
      "43237 Training Loss: tensor(0.3248)\n",
      "43238 Training Loss: tensor(0.3247)\n",
      "43239 Training Loss: tensor(0.3254)\n",
      "43240 Training Loss: tensor(0.3259)\n",
      "43241 Training Loss: tensor(0.3240)\n",
      "43242 Training Loss: tensor(0.3240)\n",
      "43243 Training Loss: tensor(0.3254)\n",
      "43244 Training Loss: tensor(0.3242)\n",
      "43245 Training Loss: tensor(0.3241)\n",
      "43246 Training Loss: tensor(0.3243)\n",
      "43247 Training Loss: tensor(0.3244)\n",
      "43248 Training Loss: tensor(0.3243)\n",
      "43249 Training Loss: tensor(0.3240)\n",
      "43250 Training Loss: tensor(0.3250)\n",
      "43251 Training Loss: tensor(0.3244)\n",
      "43252 Training Loss: tensor(0.3255)\n",
      "43253 Training Loss: tensor(0.3242)\n",
      "43254 Training Loss: tensor(0.3241)\n",
      "43255 Training Loss: tensor(0.3240)\n",
      "43256 Training Loss: tensor(0.3254)\n",
      "43257 Training Loss: tensor(0.3236)\n",
      "43258 Training Loss: tensor(0.3250)\n",
      "43259 Training Loss: tensor(0.3262)\n",
      "43260 Training Loss: tensor(0.3235)\n",
      "43261 Training Loss: tensor(0.3244)\n",
      "43262 Training Loss: tensor(0.3243)\n",
      "43263 Training Loss: tensor(0.3257)\n",
      "43264 Training Loss: tensor(0.3252)\n",
      "43265 Training Loss: tensor(0.3240)\n",
      "43266 Training Loss: tensor(0.3250)\n",
      "43267 Training Loss: tensor(0.3242)\n",
      "43268 Training Loss: tensor(0.3244)\n",
      "43269 Training Loss: tensor(0.3240)\n",
      "43270 Training Loss: tensor(0.3253)\n",
      "43271 Training Loss: tensor(0.3243)\n",
      "43272 Training Loss: tensor(0.3242)\n",
      "43273 Training Loss: tensor(0.3238)\n",
      "43274 Training Loss: tensor(0.3245)\n",
      "43275 Training Loss: tensor(0.3255)\n",
      "43276 Training Loss: tensor(0.3244)\n",
      "43277 Training Loss: tensor(0.3243)\n",
      "43278 Training Loss: tensor(0.3243)\n",
      "43279 Training Loss: tensor(0.3239)\n",
      "43280 Training Loss: tensor(0.3237)\n",
      "43281 Training Loss: tensor(0.3237)\n",
      "43282 Training Loss: tensor(0.3242)\n",
      "43283 Training Loss: tensor(0.3252)\n",
      "43284 Training Loss: tensor(0.3245)\n",
      "43285 Training Loss: tensor(0.3247)\n",
      "43286 Training Loss: tensor(0.3247)\n",
      "43287 Training Loss: tensor(0.3242)\n",
      "43288 Training Loss: tensor(0.3241)\n",
      "43289 Training Loss: tensor(0.3243)\n",
      "43290 Training Loss: tensor(0.3241)\n",
      "43291 Training Loss: tensor(0.3234)\n",
      "43292 Training Loss: tensor(0.3245)\n",
      "43293 Training Loss: tensor(0.3259)\n",
      "43294 Training Loss: tensor(0.3243)\n",
      "43295 Training Loss: tensor(0.3247)\n",
      "43296 Training Loss: tensor(0.3245)\n",
      "43297 Training Loss: tensor(0.3250)\n",
      "43298 Training Loss: tensor(0.3244)\n",
      "43299 Training Loss: tensor(0.3245)\n",
      "43300 Training Loss: tensor(0.3245)\n",
      "43301 Training Loss: tensor(0.3243)\n",
      "43302 Training Loss: tensor(0.3246)\n",
      "43303 Training Loss: tensor(0.3241)\n",
      "43304 Training Loss: tensor(0.3245)\n",
      "43305 Training Loss: tensor(0.3241)\n",
      "43306 Training Loss: tensor(0.3244)\n",
      "43307 Training Loss: tensor(0.3246)\n",
      "43308 Training Loss: tensor(0.3240)\n",
      "43309 Training Loss: tensor(0.3243)\n",
      "43310 Training Loss: tensor(0.3242)\n",
      "43311 Training Loss: tensor(0.3239)\n",
      "43312 Training Loss: tensor(0.3251)\n",
      "43313 Training Loss: tensor(0.3248)\n",
      "43314 Training Loss: tensor(0.3237)\n",
      "43315 Training Loss: tensor(0.3241)\n",
      "43316 Training Loss: tensor(0.3247)\n",
      "43317 Training Loss: tensor(0.3241)\n",
      "43318 Training Loss: tensor(0.3242)\n",
      "43319 Training Loss: tensor(0.3253)\n",
      "43320 Training Loss: tensor(0.3256)\n",
      "43321 Training Loss: tensor(0.3243)\n",
      "43322 Training Loss: tensor(0.3249)\n",
      "43323 Training Loss: tensor(0.3237)\n",
      "43324 Training Loss: tensor(0.3237)\n",
      "43325 Training Loss: tensor(0.3255)\n",
      "43326 Training Loss: tensor(0.3240)\n",
      "43327 Training Loss: tensor(0.3256)\n",
      "43328 Training Loss: tensor(0.3243)\n",
      "43329 Training Loss: tensor(0.3240)\n",
      "43330 Training Loss: tensor(0.3250)\n",
      "43331 Training Loss: tensor(0.3243)\n",
      "43332 Training Loss: tensor(0.3247)\n",
      "43333 Training Loss: tensor(0.3241)\n",
      "43334 Training Loss: tensor(0.3238)\n",
      "43335 Training Loss: tensor(0.3238)\n",
      "43336 Training Loss: tensor(0.3243)\n",
      "43337 Training Loss: tensor(0.3256)\n",
      "43338 Training Loss: tensor(0.3247)\n",
      "43339 Training Loss: tensor(0.3247)\n",
      "43340 Training Loss: tensor(0.3251)\n",
      "43341 Training Loss: tensor(0.3239)\n",
      "43342 Training Loss: tensor(0.3243)\n",
      "43343 Training Loss: tensor(0.3239)\n",
      "43344 Training Loss: tensor(0.3246)\n",
      "43345 Training Loss: tensor(0.3247)\n",
      "43346 Training Loss: tensor(0.3244)\n",
      "43347 Training Loss: tensor(0.3239)\n",
      "43348 Training Loss: tensor(0.3242)\n",
      "43349 Training Loss: tensor(0.3244)\n",
      "43350 Training Loss: tensor(0.3241)\n",
      "43351 Training Loss: tensor(0.3249)\n",
      "43352 Training Loss: tensor(0.3241)\n",
      "43353 Training Loss: tensor(0.3250)\n",
      "43354 Training Loss: tensor(0.3242)\n",
      "43355 Training Loss: tensor(0.3244)\n",
      "43356 Training Loss: tensor(0.3243)\n",
      "43357 Training Loss: tensor(0.3238)\n",
      "43358 Training Loss: tensor(0.3239)\n",
      "43359 Training Loss: tensor(0.3243)\n",
      "43360 Training Loss: tensor(0.3256)\n",
      "43361 Training Loss: tensor(0.3246)\n",
      "43362 Training Loss: tensor(0.3244)\n",
      "43363 Training Loss: tensor(0.3242)\n",
      "43364 Training Loss: tensor(0.3243)\n",
      "43365 Training Loss: tensor(0.3248)\n",
      "43366 Training Loss: tensor(0.3248)\n",
      "43367 Training Loss: tensor(0.3242)\n",
      "43368 Training Loss: tensor(0.3252)\n",
      "43369 Training Loss: tensor(0.3241)\n",
      "43370 Training Loss: tensor(0.3254)\n",
      "43371 Training Loss: tensor(0.3242)\n",
      "43372 Training Loss: tensor(0.3250)\n",
      "43373 Training Loss: tensor(0.3241)\n",
      "43374 Training Loss: tensor(0.3244)\n",
      "43375 Training Loss: tensor(0.3250)\n",
      "43376 Training Loss: tensor(0.3253)\n",
      "43377 Training Loss: tensor(0.3250)\n",
      "43378 Training Loss: tensor(0.3245)\n",
      "43379 Training Loss: tensor(0.3247)\n",
      "43380 Training Loss: tensor(0.3246)\n",
      "43381 Training Loss: tensor(0.3243)\n",
      "43382 Training Loss: tensor(0.3245)\n",
      "43383 Training Loss: tensor(0.3240)\n",
      "43384 Training Loss: tensor(0.3258)\n",
      "43385 Training Loss: tensor(0.3252)\n",
      "43386 Training Loss: tensor(0.3250)\n",
      "43387 Training Loss: tensor(0.3251)\n",
      "43388 Training Loss: tensor(0.3243)\n",
      "43389 Training Loss: tensor(0.3243)\n",
      "43390 Training Loss: tensor(0.3249)\n",
      "43391 Training Loss: tensor(0.3245)\n",
      "43392 Training Loss: tensor(0.3246)\n",
      "43393 Training Loss: tensor(0.3248)\n",
      "43394 Training Loss: tensor(0.3243)\n",
      "43395 Training Loss: tensor(0.3241)\n",
      "43396 Training Loss: tensor(0.3244)\n",
      "43397 Training Loss: tensor(0.3241)\n",
      "43398 Training Loss: tensor(0.3248)\n",
      "43399 Training Loss: tensor(0.3250)\n",
      "43400 Training Loss: tensor(0.3246)\n",
      "43401 Training Loss: tensor(0.3246)\n",
      "43402 Training Loss: tensor(0.3242)\n",
      "43403 Training Loss: tensor(0.3252)\n",
      "43404 Training Loss: tensor(0.3249)\n",
      "43405 Training Loss: tensor(0.3248)\n",
      "43406 Training Loss: tensor(0.3247)\n",
      "43407 Training Loss: tensor(0.3255)\n",
      "43408 Training Loss: tensor(0.3249)\n",
      "43409 Training Loss: tensor(0.3247)\n",
      "43410 Training Loss: tensor(0.3245)\n",
      "43411 Training Loss: tensor(0.3250)\n",
      "43412 Training Loss: tensor(0.3256)\n",
      "43413 Training Loss: tensor(0.3251)\n",
      "43414 Training Loss: tensor(0.3244)\n",
      "43415 Training Loss: tensor(0.3241)\n",
      "43416 Training Loss: tensor(0.3243)\n",
      "43417 Training Loss: tensor(0.3247)\n",
      "43418 Training Loss: tensor(0.3253)\n",
      "43419 Training Loss: tensor(0.3251)\n",
      "43420 Training Loss: tensor(0.3249)\n",
      "43421 Training Loss: tensor(0.3253)\n",
      "43422 Training Loss: tensor(0.3248)\n",
      "43423 Training Loss: tensor(0.3250)\n",
      "43424 Training Loss: tensor(0.3241)\n",
      "43425 Training Loss: tensor(0.3241)\n",
      "43426 Training Loss: tensor(0.3245)\n",
      "43427 Training Loss: tensor(0.3241)\n",
      "43428 Training Loss: tensor(0.3253)\n",
      "43429 Training Loss: tensor(0.3241)\n",
      "43430 Training Loss: tensor(0.3242)\n",
      "43431 Training Loss: tensor(0.3243)\n",
      "43432 Training Loss: tensor(0.3251)\n",
      "43433 Training Loss: tensor(0.3243)\n",
      "43434 Training Loss: tensor(0.3251)\n",
      "43435 Training Loss: tensor(0.3246)\n",
      "43436 Training Loss: tensor(0.3243)\n",
      "43437 Training Loss: tensor(0.3242)\n",
      "43438 Training Loss: tensor(0.3247)\n",
      "43439 Training Loss: tensor(0.3255)\n",
      "43440 Training Loss: tensor(0.3239)\n",
      "43441 Training Loss: tensor(0.3240)\n",
      "43442 Training Loss: tensor(0.3241)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43443 Training Loss: tensor(0.3251)\n",
      "43444 Training Loss: tensor(0.3250)\n",
      "43445 Training Loss: tensor(0.3250)\n",
      "43446 Training Loss: tensor(0.3239)\n",
      "43447 Training Loss: tensor(0.3247)\n",
      "43448 Training Loss: tensor(0.3251)\n",
      "43449 Training Loss: tensor(0.3247)\n",
      "43450 Training Loss: tensor(0.3240)\n",
      "43451 Training Loss: tensor(0.3242)\n",
      "43452 Training Loss: tensor(0.3244)\n",
      "43453 Training Loss: tensor(0.3253)\n",
      "43454 Training Loss: tensor(0.3235)\n",
      "43455 Training Loss: tensor(0.3245)\n",
      "43456 Training Loss: tensor(0.3247)\n",
      "43457 Training Loss: tensor(0.3248)\n",
      "43458 Training Loss: tensor(0.3244)\n",
      "43459 Training Loss: tensor(0.3246)\n",
      "43460 Training Loss: tensor(0.3240)\n",
      "43461 Training Loss: tensor(0.3240)\n",
      "43462 Training Loss: tensor(0.3255)\n",
      "43463 Training Loss: tensor(0.3241)\n",
      "43464 Training Loss: tensor(0.3251)\n",
      "43465 Training Loss: tensor(0.3259)\n",
      "43466 Training Loss: tensor(0.3248)\n",
      "43467 Training Loss: tensor(0.3260)\n",
      "43468 Training Loss: tensor(0.3247)\n",
      "43469 Training Loss: tensor(0.3249)\n",
      "43470 Training Loss: tensor(0.3240)\n",
      "43471 Training Loss: tensor(0.3246)\n",
      "43472 Training Loss: tensor(0.3248)\n",
      "43473 Training Loss: tensor(0.3242)\n",
      "43474 Training Loss: tensor(0.3248)\n",
      "43475 Training Loss: tensor(0.3242)\n",
      "43476 Training Loss: tensor(0.3252)\n",
      "43477 Training Loss: tensor(0.3242)\n",
      "43478 Training Loss: tensor(0.3254)\n",
      "43479 Training Loss: tensor(0.3239)\n",
      "43480 Training Loss: tensor(0.3242)\n",
      "43481 Training Loss: tensor(0.3236)\n",
      "43482 Training Loss: tensor(0.3247)\n",
      "43483 Training Loss: tensor(0.3249)\n",
      "43484 Training Loss: tensor(0.3250)\n",
      "43485 Training Loss: tensor(0.3259)\n",
      "43486 Training Loss: tensor(0.3243)\n",
      "43487 Training Loss: tensor(0.3243)\n",
      "43488 Training Loss: tensor(0.3238)\n",
      "43489 Training Loss: tensor(0.3247)\n",
      "43490 Training Loss: tensor(0.3250)\n",
      "43491 Training Loss: tensor(0.3245)\n",
      "43492 Training Loss: tensor(0.3243)\n",
      "43493 Training Loss: tensor(0.3245)\n",
      "43494 Training Loss: tensor(0.3245)\n",
      "43495 Training Loss: tensor(0.3239)\n",
      "43496 Training Loss: tensor(0.3248)\n",
      "43497 Training Loss: tensor(0.3251)\n",
      "43498 Training Loss: tensor(0.3248)\n",
      "43499 Training Loss: tensor(0.3248)\n",
      "43500 Training Loss: tensor(0.3242)\n",
      "43501 Training Loss: tensor(0.3248)\n",
      "43502 Training Loss: tensor(0.3248)\n",
      "43503 Training Loss: tensor(0.3244)\n",
      "43504 Training Loss: tensor(0.3242)\n",
      "43505 Training Loss: tensor(0.3249)\n",
      "43506 Training Loss: tensor(0.3248)\n",
      "43507 Training Loss: tensor(0.3254)\n",
      "43508 Training Loss: tensor(0.3237)\n",
      "43509 Training Loss: tensor(0.3251)\n",
      "43510 Training Loss: tensor(0.3248)\n",
      "43511 Training Loss: tensor(0.3248)\n",
      "43512 Training Loss: tensor(0.3246)\n",
      "43513 Training Loss: tensor(0.3240)\n",
      "43514 Training Loss: tensor(0.3246)\n",
      "43515 Training Loss: tensor(0.3252)\n",
      "43516 Training Loss: tensor(0.3255)\n",
      "43517 Training Loss: tensor(0.3240)\n",
      "43518 Training Loss: tensor(0.3242)\n",
      "43519 Training Loss: tensor(0.3249)\n",
      "43520 Training Loss: tensor(0.3248)\n",
      "43521 Training Loss: tensor(0.3245)\n",
      "43522 Training Loss: tensor(0.3245)\n",
      "43523 Training Loss: tensor(0.3254)\n",
      "43524 Training Loss: tensor(0.3241)\n",
      "43525 Training Loss: tensor(0.3242)\n",
      "43526 Training Loss: tensor(0.3243)\n",
      "43527 Training Loss: tensor(0.3246)\n",
      "43528 Training Loss: tensor(0.3249)\n",
      "43529 Training Loss: tensor(0.3251)\n",
      "43530 Training Loss: tensor(0.3241)\n",
      "43531 Training Loss: tensor(0.3248)\n",
      "43532 Training Loss: tensor(0.3252)\n",
      "43533 Training Loss: tensor(0.3240)\n",
      "43534 Training Loss: tensor(0.3248)\n",
      "43535 Training Loss: tensor(0.3241)\n",
      "43536 Training Loss: tensor(0.3239)\n",
      "43537 Training Loss: tensor(0.3243)\n",
      "43538 Training Loss: tensor(0.3241)\n",
      "43539 Training Loss: tensor(0.3248)\n",
      "43540 Training Loss: tensor(0.3250)\n",
      "43541 Training Loss: tensor(0.3246)\n",
      "43542 Training Loss: tensor(0.3243)\n",
      "43543 Training Loss: tensor(0.3262)\n",
      "43544 Training Loss: tensor(0.3243)\n",
      "43545 Training Loss: tensor(0.3245)\n",
      "43546 Training Loss: tensor(0.3239)\n",
      "43547 Training Loss: tensor(0.3262)\n",
      "43548 Training Loss: tensor(0.3239)\n",
      "43549 Training Loss: tensor(0.3244)\n",
      "43550 Training Loss: tensor(0.3250)\n",
      "43551 Training Loss: tensor(0.3243)\n",
      "43552 Training Loss: tensor(0.3242)\n",
      "43553 Training Loss: tensor(0.3248)\n",
      "43554 Training Loss: tensor(0.3247)\n",
      "43555 Training Loss: tensor(0.3244)\n",
      "43556 Training Loss: tensor(0.3252)\n",
      "43557 Training Loss: tensor(0.3245)\n",
      "43558 Training Loss: tensor(0.3245)\n",
      "43559 Training Loss: tensor(0.3239)\n",
      "43560 Training Loss: tensor(0.3257)\n",
      "43561 Training Loss: tensor(0.3241)\n",
      "43562 Training Loss: tensor(0.3239)\n",
      "43563 Training Loss: tensor(0.3248)\n",
      "43564 Training Loss: tensor(0.3245)\n",
      "43565 Training Loss: tensor(0.3249)\n",
      "43566 Training Loss: tensor(0.3239)\n",
      "43567 Training Loss: tensor(0.3250)\n",
      "43568 Training Loss: tensor(0.3250)\n",
      "43569 Training Loss: tensor(0.3246)\n",
      "43570 Training Loss: tensor(0.3249)\n",
      "43571 Training Loss: tensor(0.3246)\n",
      "43572 Training Loss: tensor(0.3249)\n",
      "43573 Training Loss: tensor(0.3244)\n",
      "43574 Training Loss: tensor(0.3248)\n",
      "43575 Training Loss: tensor(0.3241)\n",
      "43576 Training Loss: tensor(0.3244)\n",
      "43577 Training Loss: tensor(0.3244)\n",
      "43578 Training Loss: tensor(0.3250)\n",
      "43579 Training Loss: tensor(0.3240)\n",
      "43580 Training Loss: tensor(0.3242)\n",
      "43581 Training Loss: tensor(0.3243)\n",
      "43582 Training Loss: tensor(0.3244)\n",
      "43583 Training Loss: tensor(0.3250)\n",
      "43584 Training Loss: tensor(0.3253)\n",
      "43585 Training Loss: tensor(0.3249)\n",
      "43586 Training Loss: tensor(0.3241)\n",
      "43587 Training Loss: tensor(0.3250)\n",
      "43588 Training Loss: tensor(0.3249)\n",
      "43589 Training Loss: tensor(0.3239)\n",
      "43590 Training Loss: tensor(0.3246)\n",
      "43591 Training Loss: tensor(0.3258)\n",
      "43592 Training Loss: tensor(0.3249)\n",
      "43593 Training Loss: tensor(0.3243)\n",
      "43594 Training Loss: tensor(0.3241)\n",
      "43595 Training Loss: tensor(0.3248)\n",
      "43596 Training Loss: tensor(0.3237)\n",
      "43597 Training Loss: tensor(0.3242)\n",
      "43598 Training Loss: tensor(0.3240)\n",
      "43599 Training Loss: tensor(0.3242)\n",
      "43600 Training Loss: tensor(0.3245)\n",
      "43601 Training Loss: tensor(0.3246)\n",
      "43602 Training Loss: tensor(0.3245)\n",
      "43603 Training Loss: tensor(0.3250)\n",
      "43604 Training Loss: tensor(0.3247)\n",
      "43605 Training Loss: tensor(0.3247)\n",
      "43606 Training Loss: tensor(0.3238)\n",
      "43607 Training Loss: tensor(0.3251)\n",
      "43608 Training Loss: tensor(0.3241)\n",
      "43609 Training Loss: tensor(0.3245)\n",
      "43610 Training Loss: tensor(0.3242)\n",
      "43611 Training Loss: tensor(0.3241)\n",
      "43612 Training Loss: tensor(0.3242)\n",
      "43613 Training Loss: tensor(0.3247)\n",
      "43614 Training Loss: tensor(0.3241)\n",
      "43615 Training Loss: tensor(0.3251)\n",
      "43616 Training Loss: tensor(0.3241)\n",
      "43617 Training Loss: tensor(0.3242)\n",
      "43618 Training Loss: tensor(0.3262)\n",
      "43619 Training Loss: tensor(0.3253)\n",
      "43620 Training Loss: tensor(0.3247)\n",
      "43621 Training Loss: tensor(0.3252)\n",
      "43622 Training Loss: tensor(0.3246)\n",
      "43623 Training Loss: tensor(0.3249)\n",
      "43624 Training Loss: tensor(0.3240)\n",
      "43625 Training Loss: tensor(0.3250)\n",
      "43626 Training Loss: tensor(0.3246)\n",
      "43627 Training Loss: tensor(0.3243)\n",
      "43628 Training Loss: tensor(0.3244)\n",
      "43629 Training Loss: tensor(0.3243)\n",
      "43630 Training Loss: tensor(0.3246)\n",
      "43631 Training Loss: tensor(0.3242)\n",
      "43632 Training Loss: tensor(0.3250)\n",
      "43633 Training Loss: tensor(0.3251)\n",
      "43634 Training Loss: tensor(0.3247)\n",
      "43635 Training Loss: tensor(0.3247)\n",
      "43636 Training Loss: tensor(0.3248)\n",
      "43637 Training Loss: tensor(0.3244)\n",
      "43638 Training Loss: tensor(0.3249)\n",
      "43639 Training Loss: tensor(0.3245)\n",
      "43640 Training Loss: tensor(0.3243)\n",
      "43641 Training Loss: tensor(0.3246)\n",
      "43642 Training Loss: tensor(0.3244)\n",
      "43643 Training Loss: tensor(0.3239)\n",
      "43644 Training Loss: tensor(0.3242)\n",
      "43645 Training Loss: tensor(0.3242)\n",
      "43646 Training Loss: tensor(0.3245)\n",
      "43647 Training Loss: tensor(0.3239)\n",
      "43648 Training Loss: tensor(0.3242)\n",
      "43649 Training Loss: tensor(0.3241)\n",
      "43650 Training Loss: tensor(0.3244)\n",
      "43651 Training Loss: tensor(0.3240)\n",
      "43652 Training Loss: tensor(0.3240)\n",
      "43653 Training Loss: tensor(0.3247)\n",
      "43654 Training Loss: tensor(0.3240)\n",
      "43655 Training Loss: tensor(0.3245)\n",
      "43656 Training Loss: tensor(0.3239)\n",
      "43657 Training Loss: tensor(0.3236)\n",
      "43658 Training Loss: tensor(0.3259)\n",
      "43659 Training Loss: tensor(0.3245)\n",
      "43660 Training Loss: tensor(0.3246)\n",
      "43661 Training Loss: tensor(0.3242)\n",
      "43662 Training Loss: tensor(0.3242)\n",
      "43663 Training Loss: tensor(0.3244)\n",
      "43664 Training Loss: tensor(0.3245)\n",
      "43665 Training Loss: tensor(0.3251)\n",
      "43666 Training Loss: tensor(0.3237)\n",
      "43667 Training Loss: tensor(0.3242)\n",
      "43668 Training Loss: tensor(0.3240)\n",
      "43669 Training Loss: tensor(0.3243)\n",
      "43670 Training Loss: tensor(0.3242)\n",
      "43671 Training Loss: tensor(0.3240)\n",
      "43672 Training Loss: tensor(0.3242)\n",
      "43673 Training Loss: tensor(0.3249)\n",
      "43674 Training Loss: tensor(0.3241)\n",
      "43675 Training Loss: tensor(0.3254)\n",
      "43676 Training Loss: tensor(0.3244)\n",
      "43677 Training Loss: tensor(0.3251)\n",
      "43678 Training Loss: tensor(0.3256)\n",
      "43679 Training Loss: tensor(0.3262)\n",
      "43680 Training Loss: tensor(0.3244)\n",
      "43681 Training Loss: tensor(0.3248)\n",
      "43682 Training Loss: tensor(0.3246)\n",
      "43683 Training Loss: tensor(0.3246)\n",
      "43684 Training Loss: tensor(0.3244)\n",
      "43685 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43686 Training Loss: tensor(0.3249)\n",
      "43687 Training Loss: tensor(0.3254)\n",
      "43688 Training Loss: tensor(0.3244)\n",
      "43689 Training Loss: tensor(0.3243)\n",
      "43690 Training Loss: tensor(0.3244)\n",
      "43691 Training Loss: tensor(0.3243)\n",
      "43692 Training Loss: tensor(0.3251)\n",
      "43693 Training Loss: tensor(0.3245)\n",
      "43694 Training Loss: tensor(0.3251)\n",
      "43695 Training Loss: tensor(0.3244)\n",
      "43696 Training Loss: tensor(0.3263)\n",
      "43697 Training Loss: tensor(0.3249)\n",
      "43698 Training Loss: tensor(0.3246)\n",
      "43699 Training Loss: tensor(0.3251)\n",
      "43700 Training Loss: tensor(0.3248)\n",
      "43701 Training Loss: tensor(0.3244)\n",
      "43702 Training Loss: tensor(0.3243)\n",
      "43703 Training Loss: tensor(0.3249)\n",
      "43704 Training Loss: tensor(0.3248)\n",
      "43705 Training Loss: tensor(0.3244)\n",
      "43706 Training Loss: tensor(0.3251)\n",
      "43707 Training Loss: tensor(0.3248)\n",
      "43708 Training Loss: tensor(0.3250)\n",
      "43709 Training Loss: tensor(0.3242)\n",
      "43710 Training Loss: tensor(0.3242)\n",
      "43711 Training Loss: tensor(0.3243)\n",
      "43712 Training Loss: tensor(0.3263)\n",
      "43713 Training Loss: tensor(0.3251)\n",
      "43714 Training Loss: tensor(0.3245)\n",
      "43715 Training Loss: tensor(0.3248)\n",
      "43716 Training Loss: tensor(0.3246)\n",
      "43717 Training Loss: tensor(0.3241)\n",
      "43718 Training Loss: tensor(0.3252)\n",
      "43719 Training Loss: tensor(0.3253)\n",
      "43720 Training Loss: tensor(0.3239)\n",
      "43721 Training Loss: tensor(0.3251)\n",
      "43722 Training Loss: tensor(0.3246)\n",
      "43723 Training Loss: tensor(0.3265)\n",
      "43724 Training Loss: tensor(0.3246)\n",
      "43725 Training Loss: tensor(0.3245)\n",
      "43726 Training Loss: tensor(0.3243)\n",
      "43727 Training Loss: tensor(0.3248)\n",
      "43728 Training Loss: tensor(0.3246)\n",
      "43729 Training Loss: tensor(0.3242)\n",
      "43730 Training Loss: tensor(0.3243)\n",
      "43731 Training Loss: tensor(0.3248)\n",
      "43732 Training Loss: tensor(0.3249)\n",
      "43733 Training Loss: tensor(0.3240)\n",
      "43734 Training Loss: tensor(0.3237)\n",
      "43735 Training Loss: tensor(0.3244)\n",
      "43736 Training Loss: tensor(0.3244)\n",
      "43737 Training Loss: tensor(0.3240)\n",
      "43738 Training Loss: tensor(0.3242)\n",
      "43739 Training Loss: tensor(0.3251)\n",
      "43740 Training Loss: tensor(0.3247)\n",
      "43741 Training Loss: tensor(0.3243)\n",
      "43742 Training Loss: tensor(0.3258)\n",
      "43743 Training Loss: tensor(0.3239)\n",
      "43744 Training Loss: tensor(0.3237)\n",
      "43745 Training Loss: tensor(0.3237)\n",
      "43746 Training Loss: tensor(0.3243)\n",
      "43747 Training Loss: tensor(0.3243)\n",
      "43748 Training Loss: tensor(0.3254)\n",
      "43749 Training Loss: tensor(0.3245)\n",
      "43750 Training Loss: tensor(0.3239)\n",
      "43751 Training Loss: tensor(0.3244)\n",
      "43752 Training Loss: tensor(0.3260)\n",
      "43753 Training Loss: tensor(0.3238)\n",
      "43754 Training Loss: tensor(0.3241)\n",
      "43755 Training Loss: tensor(0.3246)\n",
      "43756 Training Loss: tensor(0.3244)\n",
      "43757 Training Loss: tensor(0.3250)\n",
      "43758 Training Loss: tensor(0.3238)\n",
      "43759 Training Loss: tensor(0.3244)\n",
      "43760 Training Loss: tensor(0.3237)\n",
      "43761 Training Loss: tensor(0.3236)\n",
      "43762 Training Loss: tensor(0.3250)\n",
      "43763 Training Loss: tensor(0.3247)\n",
      "43764 Training Loss: tensor(0.3240)\n",
      "43765 Training Loss: tensor(0.3249)\n",
      "43766 Training Loss: tensor(0.3245)\n",
      "43767 Training Loss: tensor(0.3245)\n",
      "43768 Training Loss: tensor(0.3245)\n",
      "43769 Training Loss: tensor(0.3238)\n",
      "43770 Training Loss: tensor(0.3238)\n",
      "43771 Training Loss: tensor(0.3237)\n",
      "43772 Training Loss: tensor(0.3242)\n",
      "43773 Training Loss: tensor(0.3253)\n",
      "43774 Training Loss: tensor(0.3242)\n",
      "43775 Training Loss: tensor(0.3250)\n",
      "43776 Training Loss: tensor(0.3253)\n",
      "43777 Training Loss: tensor(0.3238)\n",
      "43778 Training Loss: tensor(0.3246)\n",
      "43779 Training Loss: tensor(0.3243)\n",
      "43780 Training Loss: tensor(0.3243)\n",
      "43781 Training Loss: tensor(0.3252)\n",
      "43782 Training Loss: tensor(0.3248)\n",
      "43783 Training Loss: tensor(0.3244)\n",
      "43784 Training Loss: tensor(0.3241)\n",
      "43785 Training Loss: tensor(0.3245)\n",
      "43786 Training Loss: tensor(0.3246)\n",
      "43787 Training Loss: tensor(0.3236)\n",
      "43788 Training Loss: tensor(0.3240)\n",
      "43789 Training Loss: tensor(0.3240)\n",
      "43790 Training Loss: tensor(0.3247)\n",
      "43791 Training Loss: tensor(0.3251)\n",
      "43792 Training Loss: tensor(0.3257)\n",
      "43793 Training Loss: tensor(0.3244)\n",
      "43794 Training Loss: tensor(0.3240)\n",
      "43795 Training Loss: tensor(0.3242)\n",
      "43796 Training Loss: tensor(0.3250)\n",
      "43797 Training Loss: tensor(0.3254)\n",
      "43798 Training Loss: tensor(0.3248)\n",
      "43799 Training Loss: tensor(0.3252)\n",
      "43800 Training Loss: tensor(0.3248)\n",
      "43801 Training Loss: tensor(0.3249)\n",
      "43802 Training Loss: tensor(0.3236)\n",
      "43803 Training Loss: tensor(0.3243)\n",
      "43804 Training Loss: tensor(0.3240)\n",
      "43805 Training Loss: tensor(0.3241)\n",
      "43806 Training Loss: tensor(0.3254)\n",
      "43807 Training Loss: tensor(0.3261)\n",
      "43808 Training Loss: tensor(0.3244)\n",
      "43809 Training Loss: tensor(0.3245)\n",
      "43810 Training Loss: tensor(0.3247)\n",
      "43811 Training Loss: tensor(0.3242)\n",
      "43812 Training Loss: tensor(0.3239)\n",
      "43813 Training Loss: tensor(0.3241)\n",
      "43814 Training Loss: tensor(0.3242)\n",
      "43815 Training Loss: tensor(0.3245)\n",
      "43816 Training Loss: tensor(0.3247)\n",
      "43817 Training Loss: tensor(0.3255)\n",
      "43818 Training Loss: tensor(0.3243)\n",
      "43819 Training Loss: tensor(0.3241)\n",
      "43820 Training Loss: tensor(0.3242)\n",
      "43821 Training Loss: tensor(0.3245)\n",
      "43822 Training Loss: tensor(0.3251)\n",
      "43823 Training Loss: tensor(0.3243)\n",
      "43824 Training Loss: tensor(0.3248)\n",
      "43825 Training Loss: tensor(0.3244)\n",
      "43826 Training Loss: tensor(0.3247)\n",
      "43827 Training Loss: tensor(0.3248)\n",
      "43828 Training Loss: tensor(0.3256)\n",
      "43829 Training Loss: tensor(0.3245)\n",
      "43830 Training Loss: tensor(0.3246)\n",
      "43831 Training Loss: tensor(0.3251)\n",
      "43832 Training Loss: tensor(0.3254)\n",
      "43833 Training Loss: tensor(0.3243)\n",
      "43834 Training Loss: tensor(0.3239)\n",
      "43835 Training Loss: tensor(0.3249)\n",
      "43836 Training Loss: tensor(0.3253)\n",
      "43837 Training Loss: tensor(0.3251)\n",
      "43838 Training Loss: tensor(0.3253)\n",
      "43839 Training Loss: tensor(0.3244)\n",
      "43840 Training Loss: tensor(0.3241)\n",
      "43841 Training Loss: tensor(0.3252)\n",
      "43842 Training Loss: tensor(0.3244)\n",
      "43843 Training Loss: tensor(0.3246)\n",
      "43844 Training Loss: tensor(0.3246)\n",
      "43845 Training Loss: tensor(0.3245)\n",
      "43846 Training Loss: tensor(0.3247)\n",
      "43847 Training Loss: tensor(0.3243)\n",
      "43848 Training Loss: tensor(0.3246)\n",
      "43849 Training Loss: tensor(0.3240)\n",
      "43850 Training Loss: tensor(0.3250)\n",
      "43851 Training Loss: tensor(0.3239)\n",
      "43852 Training Loss: tensor(0.3246)\n",
      "43853 Training Loss: tensor(0.3239)\n",
      "43854 Training Loss: tensor(0.3241)\n",
      "43855 Training Loss: tensor(0.3243)\n",
      "43856 Training Loss: tensor(0.3241)\n",
      "43857 Training Loss: tensor(0.3245)\n",
      "43858 Training Loss: tensor(0.3238)\n",
      "43859 Training Loss: tensor(0.3239)\n",
      "43860 Training Loss: tensor(0.3246)\n",
      "43861 Training Loss: tensor(0.3240)\n",
      "43862 Training Loss: tensor(0.3236)\n",
      "43863 Training Loss: tensor(0.3244)\n",
      "43864 Training Loss: tensor(0.3236)\n",
      "43865 Training Loss: tensor(0.3248)\n",
      "43866 Training Loss: tensor(0.3247)\n",
      "43867 Training Loss: tensor(0.3241)\n",
      "43868 Training Loss: tensor(0.3241)\n",
      "43869 Training Loss: tensor(0.3238)\n",
      "43870 Training Loss: tensor(0.3241)\n",
      "43871 Training Loss: tensor(0.3245)\n",
      "43872 Training Loss: tensor(0.3240)\n",
      "43873 Training Loss: tensor(0.3243)\n",
      "43874 Training Loss: tensor(0.3247)\n",
      "43875 Training Loss: tensor(0.3243)\n",
      "43876 Training Loss: tensor(0.3249)\n",
      "43877 Training Loss: tensor(0.3234)\n",
      "43878 Training Loss: tensor(0.3247)\n",
      "43879 Training Loss: tensor(0.3235)\n",
      "43880 Training Loss: tensor(0.3242)\n",
      "43881 Training Loss: tensor(0.3235)\n",
      "43882 Training Loss: tensor(0.3246)\n",
      "43883 Training Loss: tensor(0.3258)\n",
      "43884 Training Loss: tensor(0.3246)\n",
      "43885 Training Loss: tensor(0.3242)\n",
      "43886 Training Loss: tensor(0.3248)\n",
      "43887 Training Loss: tensor(0.3247)\n",
      "43888 Training Loss: tensor(0.3253)\n",
      "43889 Training Loss: tensor(0.3243)\n",
      "43890 Training Loss: tensor(0.3239)\n",
      "43891 Training Loss: tensor(0.3244)\n",
      "43892 Training Loss: tensor(0.3245)\n",
      "43893 Training Loss: tensor(0.3244)\n",
      "43894 Training Loss: tensor(0.3253)\n",
      "43895 Training Loss: tensor(0.3249)\n",
      "43896 Training Loss: tensor(0.3247)\n",
      "43897 Training Loss: tensor(0.3240)\n",
      "43898 Training Loss: tensor(0.3242)\n",
      "43899 Training Loss: tensor(0.3244)\n",
      "43900 Training Loss: tensor(0.3239)\n",
      "43901 Training Loss: tensor(0.3243)\n",
      "43902 Training Loss: tensor(0.3242)\n",
      "43903 Training Loss: tensor(0.3242)\n",
      "43904 Training Loss: tensor(0.3241)\n",
      "43905 Training Loss: tensor(0.3240)\n",
      "43906 Training Loss: tensor(0.3254)\n",
      "43907 Training Loss: tensor(0.3264)\n",
      "43908 Training Loss: tensor(0.3255)\n",
      "43909 Training Loss: tensor(0.3248)\n",
      "43910 Training Loss: tensor(0.3253)\n",
      "43911 Training Loss: tensor(0.3242)\n",
      "43912 Training Loss: tensor(0.3252)\n",
      "43913 Training Loss: tensor(0.3253)\n",
      "43914 Training Loss: tensor(0.3263)\n",
      "43915 Training Loss: tensor(0.3258)\n",
      "43916 Training Loss: tensor(0.3252)\n",
      "43917 Training Loss: tensor(0.3246)\n",
      "43918 Training Loss: tensor(0.3241)\n",
      "43919 Training Loss: tensor(0.3249)\n",
      "43920 Training Loss: tensor(0.3247)\n",
      "43921 Training Loss: tensor(0.3243)\n",
      "43922 Training Loss: tensor(0.3247)\n",
      "43923 Training Loss: tensor(0.3244)\n",
      "43924 Training Loss: tensor(0.3243)\n",
      "43925 Training Loss: tensor(0.3247)\n",
      "43926 Training Loss: tensor(0.3246)\n",
      "43927 Training Loss: tensor(0.3241)\n",
      "43928 Training Loss: tensor(0.3247)\n",
      "43929 Training Loss: tensor(0.3244)\n",
      "43930 Training Loss: tensor(0.3247)\n",
      "43931 Training Loss: tensor(0.3254)\n",
      "43932 Training Loss: tensor(0.3245)\n",
      "43933 Training Loss: tensor(0.3240)\n",
      "43934 Training Loss: tensor(0.3241)\n",
      "43935 Training Loss: tensor(0.3244)\n",
      "43936 Training Loss: tensor(0.3243)\n",
      "43937 Training Loss: tensor(0.3246)\n",
      "43938 Training Loss: tensor(0.3251)\n",
      "43939 Training Loss: tensor(0.3242)\n",
      "43940 Training Loss: tensor(0.3243)\n",
      "43941 Training Loss: tensor(0.3252)\n",
      "43942 Training Loss: tensor(0.3243)\n",
      "43943 Training Loss: tensor(0.3244)\n",
      "43944 Training Loss: tensor(0.3245)\n",
      "43945 Training Loss: tensor(0.3256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43946 Training Loss: tensor(0.3239)\n",
      "43947 Training Loss: tensor(0.3241)\n",
      "43948 Training Loss: tensor(0.3251)\n",
      "43949 Training Loss: tensor(0.3246)\n",
      "43950 Training Loss: tensor(0.3248)\n",
      "43951 Training Loss: tensor(0.3240)\n",
      "43952 Training Loss: tensor(0.3249)\n",
      "43953 Training Loss: tensor(0.3246)\n",
      "43954 Training Loss: tensor(0.3242)\n",
      "43955 Training Loss: tensor(0.3256)\n",
      "43956 Training Loss: tensor(0.3248)\n",
      "43957 Training Loss: tensor(0.3253)\n",
      "43958 Training Loss: tensor(0.3245)\n",
      "43959 Training Loss: tensor(0.3244)\n",
      "43960 Training Loss: tensor(0.3245)\n",
      "43961 Training Loss: tensor(0.3247)\n",
      "43962 Training Loss: tensor(0.3256)\n",
      "43963 Training Loss: tensor(0.3239)\n",
      "43964 Training Loss: tensor(0.3241)\n",
      "43965 Training Loss: tensor(0.3244)\n",
      "43966 Training Loss: tensor(0.3243)\n",
      "43967 Training Loss: tensor(0.3246)\n",
      "43968 Training Loss: tensor(0.3246)\n",
      "43969 Training Loss: tensor(0.3243)\n",
      "43970 Training Loss: tensor(0.3246)\n",
      "43971 Training Loss: tensor(0.3251)\n",
      "43972 Training Loss: tensor(0.3246)\n",
      "43973 Training Loss: tensor(0.3251)\n",
      "43974 Training Loss: tensor(0.3240)\n",
      "43975 Training Loss: tensor(0.3240)\n",
      "43976 Training Loss: tensor(0.3244)\n",
      "43977 Training Loss: tensor(0.3245)\n",
      "43978 Training Loss: tensor(0.3247)\n",
      "43979 Training Loss: tensor(0.3242)\n",
      "43980 Training Loss: tensor(0.3239)\n",
      "43981 Training Loss: tensor(0.3246)\n",
      "43982 Training Loss: tensor(0.3238)\n",
      "43983 Training Loss: tensor(0.3247)\n",
      "43984 Training Loss: tensor(0.3243)\n",
      "43985 Training Loss: tensor(0.3242)\n",
      "43986 Training Loss: tensor(0.3253)\n",
      "43987 Training Loss: tensor(0.3247)\n",
      "43988 Training Loss: tensor(0.3247)\n",
      "43989 Training Loss: tensor(0.3251)\n",
      "43990 Training Loss: tensor(0.3245)\n",
      "43991 Training Loss: tensor(0.3243)\n",
      "43992 Training Loss: tensor(0.3244)\n",
      "43993 Training Loss: tensor(0.3249)\n",
      "43994 Training Loss: tensor(0.3250)\n",
      "43995 Training Loss: tensor(0.3242)\n",
      "43996 Training Loss: tensor(0.3245)\n",
      "43997 Training Loss: tensor(0.3240)\n",
      "43998 Training Loss: tensor(0.3245)\n",
      "43999 Training Loss: tensor(0.3247)\n",
      "44000 Training Loss: tensor(0.3243)\n",
      "44001 Training Loss: tensor(0.3239)\n",
      "44002 Training Loss: tensor(0.3242)\n",
      "44003 Training Loss: tensor(0.3242)\n",
      "44004 Training Loss: tensor(0.3258)\n",
      "44005 Training Loss: tensor(0.3259)\n",
      "44006 Training Loss: tensor(0.3247)\n",
      "44007 Training Loss: tensor(0.3243)\n",
      "44008 Training Loss: tensor(0.3242)\n",
      "44009 Training Loss: tensor(0.3243)\n",
      "44010 Training Loss: tensor(0.3246)\n",
      "44011 Training Loss: tensor(0.3250)\n",
      "44012 Training Loss: tensor(0.3247)\n",
      "44013 Training Loss: tensor(0.3244)\n",
      "44014 Training Loss: tensor(0.3236)\n",
      "44015 Training Loss: tensor(0.3256)\n",
      "44016 Training Loss: tensor(0.3244)\n",
      "44017 Training Loss: tensor(0.3242)\n",
      "44018 Training Loss: tensor(0.3261)\n",
      "44019 Training Loss: tensor(0.3247)\n",
      "44020 Training Loss: tensor(0.3244)\n",
      "44021 Training Loss: tensor(0.3243)\n",
      "44022 Training Loss: tensor(0.3239)\n",
      "44023 Training Loss: tensor(0.3238)\n",
      "44024 Training Loss: tensor(0.3253)\n",
      "44025 Training Loss: tensor(0.3239)\n",
      "44026 Training Loss: tensor(0.3239)\n",
      "44027 Training Loss: tensor(0.3249)\n",
      "44028 Training Loss: tensor(0.3247)\n",
      "44029 Training Loss: tensor(0.3243)\n",
      "44030 Training Loss: tensor(0.3241)\n",
      "44031 Training Loss: tensor(0.3244)\n",
      "44032 Training Loss: tensor(0.3244)\n",
      "44033 Training Loss: tensor(0.3243)\n",
      "44034 Training Loss: tensor(0.3244)\n",
      "44035 Training Loss: tensor(0.3239)\n",
      "44036 Training Loss: tensor(0.3242)\n",
      "44037 Training Loss: tensor(0.3254)\n",
      "44038 Training Loss: tensor(0.3240)\n",
      "44039 Training Loss: tensor(0.3238)\n",
      "44040 Training Loss: tensor(0.3240)\n",
      "44041 Training Loss: tensor(0.3237)\n",
      "44042 Training Loss: tensor(0.3247)\n",
      "44043 Training Loss: tensor(0.3244)\n",
      "44044 Training Loss: tensor(0.3237)\n",
      "44045 Training Loss: tensor(0.3237)\n",
      "44046 Training Loss: tensor(0.3246)\n",
      "44047 Training Loss: tensor(0.3257)\n",
      "44048 Training Loss: tensor(0.3247)\n",
      "44049 Training Loss: tensor(0.3241)\n",
      "44050 Training Loss: tensor(0.3239)\n",
      "44051 Training Loss: tensor(0.3247)\n",
      "44052 Training Loss: tensor(0.3240)\n",
      "44053 Training Loss: tensor(0.3246)\n",
      "44054 Training Loss: tensor(0.3248)\n",
      "44055 Training Loss: tensor(0.3240)\n",
      "44056 Training Loss: tensor(0.3239)\n",
      "44057 Training Loss: tensor(0.3243)\n",
      "44058 Training Loss: tensor(0.3238)\n",
      "44059 Training Loss: tensor(0.3245)\n",
      "44060 Training Loss: tensor(0.3252)\n",
      "44061 Training Loss: tensor(0.3241)\n",
      "44062 Training Loss: tensor(0.3237)\n",
      "44063 Training Loss: tensor(0.3254)\n",
      "44064 Training Loss: tensor(0.3239)\n",
      "44065 Training Loss: tensor(0.3238)\n",
      "44066 Training Loss: tensor(0.3236)\n",
      "44067 Training Loss: tensor(0.3239)\n",
      "44068 Training Loss: tensor(0.3240)\n",
      "44069 Training Loss: tensor(0.3247)\n",
      "44070 Training Loss: tensor(0.3255)\n",
      "44071 Training Loss: tensor(0.3236)\n",
      "44072 Training Loss: tensor(0.3249)\n",
      "44073 Training Loss: tensor(0.3236)\n",
      "44074 Training Loss: tensor(0.3243)\n",
      "44075 Training Loss: tensor(0.3247)\n",
      "44076 Training Loss: tensor(0.3236)\n",
      "44077 Training Loss: tensor(0.3244)\n",
      "44078 Training Loss: tensor(0.3244)\n",
      "44079 Training Loss: tensor(0.3247)\n",
      "44080 Training Loss: tensor(0.3240)\n",
      "44081 Training Loss: tensor(0.3247)\n",
      "44082 Training Loss: tensor(0.3248)\n",
      "44083 Training Loss: tensor(0.3247)\n",
      "44084 Training Loss: tensor(0.3247)\n",
      "44085 Training Loss: tensor(0.3244)\n",
      "44086 Training Loss: tensor(0.3256)\n",
      "44087 Training Loss: tensor(0.3245)\n",
      "44088 Training Loss: tensor(0.3242)\n",
      "44089 Training Loss: tensor(0.3240)\n",
      "44090 Training Loss: tensor(0.3239)\n",
      "44091 Training Loss: tensor(0.3245)\n",
      "44092 Training Loss: tensor(0.3237)\n",
      "44093 Training Loss: tensor(0.3238)\n",
      "44094 Training Loss: tensor(0.3244)\n",
      "44095 Training Loss: tensor(0.3243)\n",
      "44096 Training Loss: tensor(0.3244)\n",
      "44097 Training Loss: tensor(0.3254)\n",
      "44098 Training Loss: tensor(0.3248)\n",
      "44099 Training Loss: tensor(0.3246)\n",
      "44100 Training Loss: tensor(0.3238)\n",
      "44101 Training Loss: tensor(0.3237)\n",
      "44102 Training Loss: tensor(0.3244)\n",
      "44103 Training Loss: tensor(0.3255)\n",
      "44104 Training Loss: tensor(0.3240)\n",
      "44105 Training Loss: tensor(0.3246)\n",
      "44106 Training Loss: tensor(0.3239)\n",
      "44107 Training Loss: tensor(0.3241)\n",
      "44108 Training Loss: tensor(0.3246)\n",
      "44109 Training Loss: tensor(0.3243)\n",
      "44110 Training Loss: tensor(0.3241)\n",
      "44111 Training Loss: tensor(0.3242)\n",
      "44112 Training Loss: tensor(0.3242)\n",
      "44113 Training Loss: tensor(0.3247)\n",
      "44114 Training Loss: tensor(0.3241)\n",
      "44115 Training Loss: tensor(0.3249)\n",
      "44116 Training Loss: tensor(0.3240)\n",
      "44117 Training Loss: tensor(0.3240)\n",
      "44118 Training Loss: tensor(0.3244)\n",
      "44119 Training Loss: tensor(0.3237)\n",
      "44120 Training Loss: tensor(0.3240)\n",
      "44121 Training Loss: tensor(0.3257)\n",
      "44122 Training Loss: tensor(0.3250)\n",
      "44123 Training Loss: tensor(0.3242)\n",
      "44124 Training Loss: tensor(0.3246)\n",
      "44125 Training Loss: tensor(0.3239)\n",
      "44126 Training Loss: tensor(0.3247)\n",
      "44127 Training Loss: tensor(0.3241)\n",
      "44128 Training Loss: tensor(0.3240)\n",
      "44129 Training Loss: tensor(0.3246)\n",
      "44130 Training Loss: tensor(0.3244)\n",
      "44131 Training Loss: tensor(0.3251)\n",
      "44132 Training Loss: tensor(0.3251)\n",
      "44133 Training Loss: tensor(0.3246)\n",
      "44134 Training Loss: tensor(0.3251)\n",
      "44135 Training Loss: tensor(0.3234)\n",
      "44136 Training Loss: tensor(0.3246)\n",
      "44137 Training Loss: tensor(0.3244)\n",
      "44138 Training Loss: tensor(0.3247)\n",
      "44139 Training Loss: tensor(0.3244)\n",
      "44140 Training Loss: tensor(0.3243)\n",
      "44141 Training Loss: tensor(0.3246)\n",
      "44142 Training Loss: tensor(0.3252)\n",
      "44143 Training Loss: tensor(0.3245)\n",
      "44144 Training Loss: tensor(0.3240)\n",
      "44145 Training Loss: tensor(0.3244)\n",
      "44146 Training Loss: tensor(0.3258)\n",
      "44147 Training Loss: tensor(0.3250)\n",
      "44148 Training Loss: tensor(0.3246)\n",
      "44149 Training Loss: tensor(0.3244)\n",
      "44150 Training Loss: tensor(0.3245)\n",
      "44151 Training Loss: tensor(0.3247)\n",
      "44152 Training Loss: tensor(0.3245)\n",
      "44153 Training Loss: tensor(0.3247)\n",
      "44154 Training Loss: tensor(0.3250)\n",
      "44155 Training Loss: tensor(0.3248)\n",
      "44156 Training Loss: tensor(0.3247)\n",
      "44157 Training Loss: tensor(0.3242)\n",
      "44158 Training Loss: tensor(0.3245)\n",
      "44159 Training Loss: tensor(0.3245)\n",
      "44160 Training Loss: tensor(0.3249)\n",
      "44161 Training Loss: tensor(0.3242)\n",
      "44162 Training Loss: tensor(0.3246)\n",
      "44163 Training Loss: tensor(0.3246)\n",
      "44164 Training Loss: tensor(0.3239)\n",
      "44165 Training Loss: tensor(0.3244)\n",
      "44166 Training Loss: tensor(0.3241)\n",
      "44167 Training Loss: tensor(0.3242)\n",
      "44168 Training Loss: tensor(0.3257)\n",
      "44169 Training Loss: tensor(0.3239)\n",
      "44170 Training Loss: tensor(0.3239)\n",
      "44171 Training Loss: tensor(0.3249)\n",
      "44172 Training Loss: tensor(0.3241)\n",
      "44173 Training Loss: tensor(0.3249)\n",
      "44174 Training Loss: tensor(0.3250)\n",
      "44175 Training Loss: tensor(0.3243)\n",
      "44176 Training Loss: tensor(0.3248)\n",
      "44177 Training Loss: tensor(0.3241)\n",
      "44178 Training Loss: tensor(0.3248)\n",
      "44179 Training Loss: tensor(0.3249)\n",
      "44180 Training Loss: tensor(0.3243)\n",
      "44181 Training Loss: tensor(0.3245)\n",
      "44182 Training Loss: tensor(0.3241)\n",
      "44183 Training Loss: tensor(0.3239)\n",
      "44184 Training Loss: tensor(0.3250)\n",
      "44185 Training Loss: tensor(0.3243)\n",
      "44186 Training Loss: tensor(0.3248)\n",
      "44187 Training Loss: tensor(0.3242)\n",
      "44188 Training Loss: tensor(0.3238)\n",
      "44189 Training Loss: tensor(0.3248)\n",
      "44190 Training Loss: tensor(0.3249)\n",
      "44191 Training Loss: tensor(0.3240)\n",
      "44192 Training Loss: tensor(0.3242)\n",
      "44193 Training Loss: tensor(0.3244)\n",
      "44194 Training Loss: tensor(0.3253)\n",
      "44195 Training Loss: tensor(0.3254)\n",
      "44196 Training Loss: tensor(0.3244)\n",
      "44197 Training Loss: tensor(0.3240)\n",
      "44198 Training Loss: tensor(0.3248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44199 Training Loss: tensor(0.3240)\n",
      "44200 Training Loss: tensor(0.3245)\n",
      "44201 Training Loss: tensor(0.3243)\n",
      "44202 Training Loss: tensor(0.3251)\n",
      "44203 Training Loss: tensor(0.3258)\n",
      "44204 Training Loss: tensor(0.3236)\n",
      "44205 Training Loss: tensor(0.3242)\n",
      "44206 Training Loss: tensor(0.3249)\n",
      "44207 Training Loss: tensor(0.3256)\n",
      "44208 Training Loss: tensor(0.3242)\n",
      "44209 Training Loss: tensor(0.3240)\n",
      "44210 Training Loss: tensor(0.3245)\n",
      "44211 Training Loss: tensor(0.3240)\n",
      "44212 Training Loss: tensor(0.3243)\n",
      "44213 Training Loss: tensor(0.3243)\n",
      "44214 Training Loss: tensor(0.3253)\n",
      "44215 Training Loss: tensor(0.3241)\n",
      "44216 Training Loss: tensor(0.3241)\n",
      "44217 Training Loss: tensor(0.3242)\n",
      "44218 Training Loss: tensor(0.3237)\n",
      "44219 Training Loss: tensor(0.3242)\n",
      "44220 Training Loss: tensor(0.3245)\n",
      "44221 Training Loss: tensor(0.3247)\n",
      "44222 Training Loss: tensor(0.3236)\n",
      "44223 Training Loss: tensor(0.3246)\n",
      "44224 Training Loss: tensor(0.3242)\n",
      "44225 Training Loss: tensor(0.3243)\n",
      "44226 Training Loss: tensor(0.3237)\n",
      "44227 Training Loss: tensor(0.3244)\n",
      "44228 Training Loss: tensor(0.3245)\n",
      "44229 Training Loss: tensor(0.3248)\n",
      "44230 Training Loss: tensor(0.3250)\n",
      "44231 Training Loss: tensor(0.3237)\n",
      "44232 Training Loss: tensor(0.3242)\n",
      "44233 Training Loss: tensor(0.3248)\n",
      "44234 Training Loss: tensor(0.3245)\n",
      "44235 Training Loss: tensor(0.3247)\n",
      "44236 Training Loss: tensor(0.3255)\n",
      "44237 Training Loss: tensor(0.3239)\n",
      "44238 Training Loss: tensor(0.3249)\n",
      "44239 Training Loss: tensor(0.3260)\n",
      "44240 Training Loss: tensor(0.3237)\n",
      "44241 Training Loss: tensor(0.3247)\n",
      "44242 Training Loss: tensor(0.3252)\n",
      "44243 Training Loss: tensor(0.3241)\n",
      "44244 Training Loss: tensor(0.3250)\n",
      "44245 Training Loss: tensor(0.3243)\n",
      "44246 Training Loss: tensor(0.3254)\n",
      "44247 Training Loss: tensor(0.3244)\n",
      "44248 Training Loss: tensor(0.3242)\n",
      "44249 Training Loss: tensor(0.3249)\n",
      "44250 Training Loss: tensor(0.3242)\n",
      "44251 Training Loss: tensor(0.3258)\n",
      "44252 Training Loss: tensor(0.3246)\n",
      "44253 Training Loss: tensor(0.3257)\n",
      "44254 Training Loss: tensor(0.3252)\n",
      "44255 Training Loss: tensor(0.3240)\n",
      "44256 Training Loss: tensor(0.3253)\n",
      "44257 Training Loss: tensor(0.3249)\n",
      "44258 Training Loss: tensor(0.3247)\n",
      "44259 Training Loss: tensor(0.3245)\n",
      "44260 Training Loss: tensor(0.3251)\n",
      "44261 Training Loss: tensor(0.3243)\n",
      "44262 Training Loss: tensor(0.3243)\n",
      "44263 Training Loss: tensor(0.3245)\n",
      "44264 Training Loss: tensor(0.3239)\n",
      "44265 Training Loss: tensor(0.3239)\n",
      "44266 Training Loss: tensor(0.3242)\n",
      "44267 Training Loss: tensor(0.3239)\n",
      "44268 Training Loss: tensor(0.3252)\n",
      "44269 Training Loss: tensor(0.3256)\n",
      "44270 Training Loss: tensor(0.3238)\n",
      "44271 Training Loss: tensor(0.3241)\n",
      "44272 Training Loss: tensor(0.3244)\n",
      "44273 Training Loss: tensor(0.3246)\n",
      "44274 Training Loss: tensor(0.3246)\n",
      "44275 Training Loss: tensor(0.3248)\n",
      "44276 Training Loss: tensor(0.3239)\n",
      "44277 Training Loss: tensor(0.3247)\n",
      "44278 Training Loss: tensor(0.3240)\n",
      "44279 Training Loss: tensor(0.3244)\n",
      "44280 Training Loss: tensor(0.3242)\n",
      "44281 Training Loss: tensor(0.3238)\n",
      "44282 Training Loss: tensor(0.3248)\n",
      "44283 Training Loss: tensor(0.3240)\n",
      "44284 Training Loss: tensor(0.3242)\n",
      "44285 Training Loss: tensor(0.3250)\n",
      "44286 Training Loss: tensor(0.3252)\n",
      "44287 Training Loss: tensor(0.3246)\n",
      "44288 Training Loss: tensor(0.3238)\n",
      "44289 Training Loss: tensor(0.3245)\n",
      "44290 Training Loss: tensor(0.3239)\n",
      "44291 Training Loss: tensor(0.3245)\n",
      "44292 Training Loss: tensor(0.3246)\n",
      "44293 Training Loss: tensor(0.3247)\n",
      "44294 Training Loss: tensor(0.3242)\n",
      "44295 Training Loss: tensor(0.3246)\n",
      "44296 Training Loss: tensor(0.3241)\n",
      "44297 Training Loss: tensor(0.3233)\n",
      "44298 Training Loss: tensor(0.3239)\n",
      "44299 Training Loss: tensor(0.3256)\n",
      "44300 Training Loss: tensor(0.3250)\n",
      "44301 Training Loss: tensor(0.3242)\n",
      "44302 Training Loss: tensor(0.3241)\n",
      "44303 Training Loss: tensor(0.3242)\n",
      "44304 Training Loss: tensor(0.3250)\n",
      "44305 Training Loss: tensor(0.3242)\n",
      "44306 Training Loss: tensor(0.3240)\n",
      "44307 Training Loss: tensor(0.3242)\n",
      "44308 Training Loss: tensor(0.3237)\n",
      "44309 Training Loss: tensor(0.3250)\n",
      "44310 Training Loss: tensor(0.3253)\n",
      "44311 Training Loss: tensor(0.3240)\n",
      "44312 Training Loss: tensor(0.3253)\n",
      "44313 Training Loss: tensor(0.3251)\n",
      "44314 Training Loss: tensor(0.3241)\n",
      "44315 Training Loss: tensor(0.3265)\n",
      "44316 Training Loss: tensor(0.3251)\n",
      "44317 Training Loss: tensor(0.3249)\n",
      "44318 Training Loss: tensor(0.3255)\n",
      "44319 Training Loss: tensor(0.3256)\n",
      "44320 Training Loss: tensor(0.3263)\n",
      "44321 Training Loss: tensor(0.3241)\n",
      "44322 Training Loss: tensor(0.3239)\n",
      "44323 Training Loss: tensor(0.3266)\n",
      "44324 Training Loss: tensor(0.3243)\n",
      "44325 Training Loss: tensor(0.3242)\n",
      "44326 Training Loss: tensor(0.3243)\n",
      "44327 Training Loss: tensor(0.3245)\n",
      "44328 Training Loss: tensor(0.3249)\n",
      "44329 Training Loss: tensor(0.3244)\n",
      "44330 Training Loss: tensor(0.3259)\n",
      "44331 Training Loss: tensor(0.3253)\n",
      "44332 Training Loss: tensor(0.3240)\n",
      "44333 Training Loss: tensor(0.3243)\n",
      "44334 Training Loss: tensor(0.3245)\n",
      "44335 Training Loss: tensor(0.3249)\n",
      "44336 Training Loss: tensor(0.3245)\n",
      "44337 Training Loss: tensor(0.3253)\n",
      "44338 Training Loss: tensor(0.3242)\n",
      "44339 Training Loss: tensor(0.3241)\n",
      "44340 Training Loss: tensor(0.3250)\n",
      "44341 Training Loss: tensor(0.3239)\n",
      "44342 Training Loss: tensor(0.3245)\n",
      "44343 Training Loss: tensor(0.3247)\n",
      "44344 Training Loss: tensor(0.3245)\n",
      "44345 Training Loss: tensor(0.3241)\n",
      "44346 Training Loss: tensor(0.3252)\n",
      "44347 Training Loss: tensor(0.3243)\n",
      "44348 Training Loss: tensor(0.3247)\n",
      "44349 Training Loss: tensor(0.3247)\n",
      "44350 Training Loss: tensor(0.3242)\n",
      "44351 Training Loss: tensor(0.3244)\n",
      "44352 Training Loss: tensor(0.3241)\n",
      "44353 Training Loss: tensor(0.3239)\n",
      "44354 Training Loss: tensor(0.3253)\n",
      "44355 Training Loss: tensor(0.3238)\n",
      "44356 Training Loss: tensor(0.3241)\n",
      "44357 Training Loss: tensor(0.3239)\n",
      "44358 Training Loss: tensor(0.3237)\n",
      "44359 Training Loss: tensor(0.3244)\n",
      "44360 Training Loss: tensor(0.3237)\n",
      "44361 Training Loss: tensor(0.3255)\n",
      "44362 Training Loss: tensor(0.3260)\n",
      "44363 Training Loss: tensor(0.3251)\n",
      "44364 Training Loss: tensor(0.3253)\n",
      "44365 Training Loss: tensor(0.3240)\n",
      "44366 Training Loss: tensor(0.3247)\n",
      "44367 Training Loss: tensor(0.3241)\n",
      "44368 Training Loss: tensor(0.3252)\n",
      "44369 Training Loss: tensor(0.3239)\n",
      "44370 Training Loss: tensor(0.3242)\n",
      "44371 Training Loss: tensor(0.3242)\n",
      "44372 Training Loss: tensor(0.3246)\n",
      "44373 Training Loss: tensor(0.3248)\n",
      "44374 Training Loss: tensor(0.3249)\n",
      "44375 Training Loss: tensor(0.3246)\n",
      "44376 Training Loss: tensor(0.3244)\n",
      "44377 Training Loss: tensor(0.3247)\n",
      "44378 Training Loss: tensor(0.3241)\n",
      "44379 Training Loss: tensor(0.3239)\n",
      "44380 Training Loss: tensor(0.3238)\n",
      "44381 Training Loss: tensor(0.3239)\n",
      "44382 Training Loss: tensor(0.3243)\n",
      "44383 Training Loss: tensor(0.3240)\n",
      "44384 Training Loss: tensor(0.3242)\n",
      "44385 Training Loss: tensor(0.3241)\n",
      "44386 Training Loss: tensor(0.3243)\n",
      "44387 Training Loss: tensor(0.3241)\n",
      "44388 Training Loss: tensor(0.3245)\n",
      "44389 Training Loss: tensor(0.3250)\n",
      "44390 Training Loss: tensor(0.3239)\n",
      "44391 Training Loss: tensor(0.3255)\n",
      "44392 Training Loss: tensor(0.3256)\n",
      "44393 Training Loss: tensor(0.3245)\n",
      "44394 Training Loss: tensor(0.3248)\n",
      "44395 Training Loss: tensor(0.3245)\n",
      "44396 Training Loss: tensor(0.3247)\n",
      "44397 Training Loss: tensor(0.3248)\n",
      "44398 Training Loss: tensor(0.3241)\n",
      "44399 Training Loss: tensor(0.3246)\n",
      "44400 Training Loss: tensor(0.3244)\n",
      "44401 Training Loss: tensor(0.3243)\n",
      "44402 Training Loss: tensor(0.3243)\n",
      "44403 Training Loss: tensor(0.3239)\n",
      "44404 Training Loss: tensor(0.3247)\n",
      "44405 Training Loss: tensor(0.3250)\n",
      "44406 Training Loss: tensor(0.3242)\n",
      "44407 Training Loss: tensor(0.3248)\n",
      "44408 Training Loss: tensor(0.3249)\n",
      "44409 Training Loss: tensor(0.3247)\n",
      "44410 Training Loss: tensor(0.3246)\n",
      "44411 Training Loss: tensor(0.3245)\n",
      "44412 Training Loss: tensor(0.3245)\n",
      "44413 Training Loss: tensor(0.3246)\n",
      "44414 Training Loss: tensor(0.3241)\n",
      "44415 Training Loss: tensor(0.3236)\n",
      "44416 Training Loss: tensor(0.3245)\n",
      "44417 Training Loss: tensor(0.3246)\n",
      "44418 Training Loss: tensor(0.3250)\n",
      "44419 Training Loss: tensor(0.3244)\n",
      "44420 Training Loss: tensor(0.3266)\n",
      "44421 Training Loss: tensor(0.3246)\n",
      "44422 Training Loss: tensor(0.3251)\n",
      "44423 Training Loss: tensor(0.3256)\n",
      "44424 Training Loss: tensor(0.3248)\n",
      "44425 Training Loss: tensor(0.3243)\n",
      "44426 Training Loss: tensor(0.3245)\n",
      "44427 Training Loss: tensor(0.3256)\n",
      "44428 Training Loss: tensor(0.3246)\n",
      "44429 Training Loss: tensor(0.3247)\n",
      "44430 Training Loss: tensor(0.3242)\n",
      "44431 Training Loss: tensor(0.3243)\n",
      "44432 Training Loss: tensor(0.3257)\n",
      "44433 Training Loss: tensor(0.3247)\n",
      "44434 Training Loss: tensor(0.3246)\n",
      "44435 Training Loss: tensor(0.3245)\n",
      "44436 Training Loss: tensor(0.3243)\n",
      "44437 Training Loss: tensor(0.3247)\n",
      "44438 Training Loss: tensor(0.3254)\n",
      "44439 Training Loss: tensor(0.3247)\n",
      "44440 Training Loss: tensor(0.3238)\n",
      "44441 Training Loss: tensor(0.3246)\n",
      "44442 Training Loss: tensor(0.3238)\n",
      "44443 Training Loss: tensor(0.3251)\n",
      "44444 Training Loss: tensor(0.3245)\n",
      "44445 Training Loss: tensor(0.3248)\n",
      "44446 Training Loss: tensor(0.3239)\n",
      "44447 Training Loss: tensor(0.3248)\n",
      "44448 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44449 Training Loss: tensor(0.3242)\n",
      "44450 Training Loss: tensor(0.3244)\n",
      "44451 Training Loss: tensor(0.3240)\n",
      "44452 Training Loss: tensor(0.3246)\n",
      "44453 Training Loss: tensor(0.3243)\n",
      "44454 Training Loss: tensor(0.3242)\n",
      "44455 Training Loss: tensor(0.3251)\n",
      "44456 Training Loss: tensor(0.3238)\n",
      "44457 Training Loss: tensor(0.3238)\n",
      "44458 Training Loss: tensor(0.3243)\n",
      "44459 Training Loss: tensor(0.3253)\n",
      "44460 Training Loss: tensor(0.3241)\n",
      "44461 Training Loss: tensor(0.3243)\n",
      "44462 Training Loss: tensor(0.3249)\n",
      "44463 Training Loss: tensor(0.3243)\n",
      "44464 Training Loss: tensor(0.3256)\n",
      "44465 Training Loss: tensor(0.3253)\n",
      "44466 Training Loss: tensor(0.3238)\n",
      "44467 Training Loss: tensor(0.3243)\n",
      "44468 Training Loss: tensor(0.3251)\n",
      "44469 Training Loss: tensor(0.3244)\n",
      "44470 Training Loss: tensor(0.3239)\n",
      "44471 Training Loss: tensor(0.3241)\n",
      "44472 Training Loss: tensor(0.3251)\n",
      "44473 Training Loss: tensor(0.3245)\n",
      "44474 Training Loss: tensor(0.3249)\n",
      "44475 Training Loss: tensor(0.3238)\n",
      "44476 Training Loss: tensor(0.3248)\n",
      "44477 Training Loss: tensor(0.3246)\n",
      "44478 Training Loss: tensor(0.3237)\n",
      "44479 Training Loss: tensor(0.3254)\n",
      "44480 Training Loss: tensor(0.3247)\n",
      "44481 Training Loss: tensor(0.3246)\n",
      "44482 Training Loss: tensor(0.3250)\n",
      "44483 Training Loss: tensor(0.3247)\n",
      "44484 Training Loss: tensor(0.3237)\n",
      "44485 Training Loss: tensor(0.3262)\n",
      "44486 Training Loss: tensor(0.3247)\n",
      "44487 Training Loss: tensor(0.3240)\n",
      "44488 Training Loss: tensor(0.3249)\n",
      "44489 Training Loss: tensor(0.3245)\n",
      "44490 Training Loss: tensor(0.3245)\n",
      "44491 Training Loss: tensor(0.3237)\n",
      "44492 Training Loss: tensor(0.3241)\n",
      "44493 Training Loss: tensor(0.3242)\n",
      "44494 Training Loss: tensor(0.3236)\n",
      "44495 Training Loss: tensor(0.3248)\n",
      "44496 Training Loss: tensor(0.3247)\n",
      "44497 Training Loss: tensor(0.3240)\n",
      "44498 Training Loss: tensor(0.3244)\n",
      "44499 Training Loss: tensor(0.3244)\n",
      "44500 Training Loss: tensor(0.3240)\n",
      "44501 Training Loss: tensor(0.3244)\n",
      "44502 Training Loss: tensor(0.3253)\n",
      "44503 Training Loss: tensor(0.3240)\n",
      "44504 Training Loss: tensor(0.3243)\n",
      "44505 Training Loss: tensor(0.3242)\n",
      "44506 Training Loss: tensor(0.3236)\n",
      "44507 Training Loss: tensor(0.3251)\n",
      "44508 Training Loss: tensor(0.3251)\n",
      "44509 Training Loss: tensor(0.3242)\n",
      "44510 Training Loss: tensor(0.3243)\n",
      "44511 Training Loss: tensor(0.3239)\n",
      "44512 Training Loss: tensor(0.3243)\n",
      "44513 Training Loss: tensor(0.3248)\n",
      "44514 Training Loss: tensor(0.3238)\n",
      "44515 Training Loss: tensor(0.3252)\n",
      "44516 Training Loss: tensor(0.3243)\n",
      "44517 Training Loss: tensor(0.3242)\n",
      "44518 Training Loss: tensor(0.3240)\n",
      "44519 Training Loss: tensor(0.3244)\n",
      "44520 Training Loss: tensor(0.3238)\n",
      "44521 Training Loss: tensor(0.3240)\n",
      "44522 Training Loss: tensor(0.3236)\n",
      "44523 Training Loss: tensor(0.3252)\n",
      "44524 Training Loss: tensor(0.3243)\n",
      "44525 Training Loss: tensor(0.3247)\n",
      "44526 Training Loss: tensor(0.3243)\n",
      "44527 Training Loss: tensor(0.3253)\n",
      "44528 Training Loss: tensor(0.3245)\n",
      "44529 Training Loss: tensor(0.3248)\n",
      "44530 Training Loss: tensor(0.3245)\n",
      "44531 Training Loss: tensor(0.3241)\n",
      "44532 Training Loss: tensor(0.3239)\n",
      "44533 Training Loss: tensor(0.3245)\n",
      "44534 Training Loss: tensor(0.3245)\n",
      "44535 Training Loss: tensor(0.3246)\n",
      "44536 Training Loss: tensor(0.3242)\n",
      "44537 Training Loss: tensor(0.3244)\n",
      "44538 Training Loss: tensor(0.3248)\n",
      "44539 Training Loss: tensor(0.3243)\n",
      "44540 Training Loss: tensor(0.3250)\n",
      "44541 Training Loss: tensor(0.3244)\n",
      "44542 Training Loss: tensor(0.3244)\n",
      "44543 Training Loss: tensor(0.3242)\n",
      "44544 Training Loss: tensor(0.3244)\n",
      "44545 Training Loss: tensor(0.3240)\n",
      "44546 Training Loss: tensor(0.3242)\n",
      "44547 Training Loss: tensor(0.3244)\n",
      "44548 Training Loss: tensor(0.3243)\n",
      "44549 Training Loss: tensor(0.3249)\n",
      "44550 Training Loss: tensor(0.3239)\n",
      "44551 Training Loss: tensor(0.3252)\n",
      "44552 Training Loss: tensor(0.3246)\n",
      "44553 Training Loss: tensor(0.3244)\n",
      "44554 Training Loss: tensor(0.3254)\n",
      "44555 Training Loss: tensor(0.3236)\n",
      "44556 Training Loss: tensor(0.3254)\n",
      "44557 Training Loss: tensor(0.3251)\n",
      "44558 Training Loss: tensor(0.3239)\n",
      "44559 Training Loss: tensor(0.3242)\n",
      "44560 Training Loss: tensor(0.3262)\n",
      "44561 Training Loss: tensor(0.3237)\n",
      "44562 Training Loss: tensor(0.3237)\n",
      "44563 Training Loss: tensor(0.3241)\n",
      "44564 Training Loss: tensor(0.3236)\n",
      "44565 Training Loss: tensor(0.3247)\n",
      "44566 Training Loss: tensor(0.3244)\n",
      "44567 Training Loss: tensor(0.3240)\n",
      "44568 Training Loss: tensor(0.3243)\n",
      "44569 Training Loss: tensor(0.3236)\n",
      "44570 Training Loss: tensor(0.3246)\n",
      "44571 Training Loss: tensor(0.3243)\n",
      "44572 Training Loss: tensor(0.3247)\n",
      "44573 Training Loss: tensor(0.3253)\n",
      "44574 Training Loss: tensor(0.3251)\n",
      "44575 Training Loss: tensor(0.3240)\n",
      "44576 Training Loss: tensor(0.3242)\n",
      "44577 Training Loss: tensor(0.3237)\n",
      "44578 Training Loss: tensor(0.3241)\n",
      "44579 Training Loss: tensor(0.3247)\n",
      "44580 Training Loss: tensor(0.3245)\n",
      "44581 Training Loss: tensor(0.3251)\n",
      "44582 Training Loss: tensor(0.3240)\n",
      "44583 Training Loss: tensor(0.3246)\n",
      "44584 Training Loss: tensor(0.3246)\n",
      "44585 Training Loss: tensor(0.3249)\n",
      "44586 Training Loss: tensor(0.3242)\n",
      "44587 Training Loss: tensor(0.3244)\n",
      "44588 Training Loss: tensor(0.3247)\n",
      "44589 Training Loss: tensor(0.3246)\n",
      "44590 Training Loss: tensor(0.3244)\n",
      "44591 Training Loss: tensor(0.3246)\n",
      "44592 Training Loss: tensor(0.3251)\n",
      "44593 Training Loss: tensor(0.3245)\n",
      "44594 Training Loss: tensor(0.3247)\n",
      "44595 Training Loss: tensor(0.3241)\n",
      "44596 Training Loss: tensor(0.3240)\n",
      "44597 Training Loss: tensor(0.3242)\n",
      "44598 Training Loss: tensor(0.3241)\n",
      "44599 Training Loss: tensor(0.3257)\n",
      "44600 Training Loss: tensor(0.3238)\n",
      "44601 Training Loss: tensor(0.3244)\n",
      "44602 Training Loss: tensor(0.3242)\n",
      "44603 Training Loss: tensor(0.3240)\n",
      "44604 Training Loss: tensor(0.3241)\n",
      "44605 Training Loss: tensor(0.3244)\n",
      "44606 Training Loss: tensor(0.3248)\n",
      "44607 Training Loss: tensor(0.3254)\n",
      "44608 Training Loss: tensor(0.3251)\n",
      "44609 Training Loss: tensor(0.3240)\n",
      "44610 Training Loss: tensor(0.3251)\n",
      "44611 Training Loss: tensor(0.3238)\n",
      "44612 Training Loss: tensor(0.3247)\n",
      "44613 Training Loss: tensor(0.3259)\n",
      "44614 Training Loss: tensor(0.3254)\n",
      "44615 Training Loss: tensor(0.3248)\n",
      "44616 Training Loss: tensor(0.3238)\n",
      "44617 Training Loss: tensor(0.3242)\n",
      "44618 Training Loss: tensor(0.3245)\n",
      "44619 Training Loss: tensor(0.3244)\n",
      "44620 Training Loss: tensor(0.3240)\n",
      "44621 Training Loss: tensor(0.3252)\n",
      "44622 Training Loss: tensor(0.3248)\n",
      "44623 Training Loss: tensor(0.3242)\n",
      "44624 Training Loss: tensor(0.3241)\n",
      "44625 Training Loss: tensor(0.3248)\n",
      "44626 Training Loss: tensor(0.3249)\n",
      "44627 Training Loss: tensor(0.3244)\n",
      "44628 Training Loss: tensor(0.3249)\n",
      "44629 Training Loss: tensor(0.3255)\n",
      "44630 Training Loss: tensor(0.3241)\n",
      "44631 Training Loss: tensor(0.3244)\n",
      "44632 Training Loss: tensor(0.3239)\n",
      "44633 Training Loss: tensor(0.3253)\n",
      "44634 Training Loss: tensor(0.3251)\n",
      "44635 Training Loss: tensor(0.3243)\n",
      "44636 Training Loss: tensor(0.3244)\n",
      "44637 Training Loss: tensor(0.3246)\n",
      "44638 Training Loss: tensor(0.3238)\n",
      "44639 Training Loss: tensor(0.3249)\n",
      "44640 Training Loss: tensor(0.3258)\n",
      "44641 Training Loss: tensor(0.3258)\n",
      "44642 Training Loss: tensor(0.3262)\n",
      "44643 Training Loss: tensor(0.3240)\n",
      "44644 Training Loss: tensor(0.3247)\n",
      "44645 Training Loss: tensor(0.3242)\n",
      "44646 Training Loss: tensor(0.3250)\n",
      "44647 Training Loss: tensor(0.3253)\n",
      "44648 Training Loss: tensor(0.3245)\n",
      "44649 Training Loss: tensor(0.3248)\n",
      "44650 Training Loss: tensor(0.3247)\n",
      "44651 Training Loss: tensor(0.3251)\n",
      "44652 Training Loss: tensor(0.3256)\n",
      "44653 Training Loss: tensor(0.3243)\n",
      "44654 Training Loss: tensor(0.3249)\n",
      "44655 Training Loss: tensor(0.3252)\n",
      "44656 Training Loss: tensor(0.3246)\n",
      "44657 Training Loss: tensor(0.3255)\n",
      "44658 Training Loss: tensor(0.3250)\n",
      "44659 Training Loss: tensor(0.3244)\n",
      "44660 Training Loss: tensor(0.3253)\n",
      "44661 Training Loss: tensor(0.3244)\n",
      "44662 Training Loss: tensor(0.3241)\n",
      "44663 Training Loss: tensor(0.3241)\n",
      "44664 Training Loss: tensor(0.3248)\n",
      "44665 Training Loss: tensor(0.3238)\n",
      "44666 Training Loss: tensor(0.3246)\n",
      "44667 Training Loss: tensor(0.3254)\n",
      "44668 Training Loss: tensor(0.3241)\n",
      "44669 Training Loss: tensor(0.3242)\n",
      "44670 Training Loss: tensor(0.3241)\n",
      "44671 Training Loss: tensor(0.3246)\n",
      "44672 Training Loss: tensor(0.3244)\n",
      "44673 Training Loss: tensor(0.3249)\n",
      "44674 Training Loss: tensor(0.3244)\n",
      "44675 Training Loss: tensor(0.3241)\n",
      "44676 Training Loss: tensor(0.3239)\n",
      "44677 Training Loss: tensor(0.3242)\n",
      "44678 Training Loss: tensor(0.3244)\n",
      "44679 Training Loss: tensor(0.3241)\n",
      "44680 Training Loss: tensor(0.3245)\n",
      "44681 Training Loss: tensor(0.3239)\n",
      "44682 Training Loss: tensor(0.3249)\n",
      "44683 Training Loss: tensor(0.3246)\n",
      "44684 Training Loss: tensor(0.3255)\n",
      "44685 Training Loss: tensor(0.3242)\n",
      "44686 Training Loss: tensor(0.3235)\n",
      "44687 Training Loss: tensor(0.3249)\n",
      "44688 Training Loss: tensor(0.3238)\n",
      "44689 Training Loss: tensor(0.3243)\n",
      "44690 Training Loss: tensor(0.3246)\n",
      "44691 Training Loss: tensor(0.3246)\n",
      "44692 Training Loss: tensor(0.3246)\n",
      "44693 Training Loss: tensor(0.3249)\n",
      "44694 Training Loss: tensor(0.3242)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44695 Training Loss: tensor(0.3243)\n",
      "44696 Training Loss: tensor(0.3243)\n",
      "44697 Training Loss: tensor(0.3245)\n",
      "44698 Training Loss: tensor(0.3250)\n",
      "44699 Training Loss: tensor(0.3240)\n",
      "44700 Training Loss: tensor(0.3249)\n",
      "44701 Training Loss: tensor(0.3243)\n",
      "44702 Training Loss: tensor(0.3259)\n",
      "44703 Training Loss: tensor(0.3236)\n",
      "44704 Training Loss: tensor(0.3249)\n",
      "44705 Training Loss: tensor(0.3240)\n",
      "44706 Training Loss: tensor(0.3243)\n",
      "44707 Training Loss: tensor(0.3252)\n",
      "44708 Training Loss: tensor(0.3244)\n",
      "44709 Training Loss: tensor(0.3238)\n",
      "44710 Training Loss: tensor(0.3242)\n",
      "44711 Training Loss: tensor(0.3250)\n",
      "44712 Training Loss: tensor(0.3245)\n",
      "44713 Training Loss: tensor(0.3245)\n",
      "44714 Training Loss: tensor(0.3236)\n",
      "44715 Training Loss: tensor(0.3242)\n",
      "44716 Training Loss: tensor(0.3242)\n",
      "44717 Training Loss: tensor(0.3249)\n",
      "44718 Training Loss: tensor(0.3238)\n",
      "44719 Training Loss: tensor(0.3254)\n",
      "44720 Training Loss: tensor(0.3242)\n",
      "44721 Training Loss: tensor(0.3243)\n",
      "44722 Training Loss: tensor(0.3243)\n",
      "44723 Training Loss: tensor(0.3237)\n",
      "44724 Training Loss: tensor(0.3239)\n",
      "44725 Training Loss: tensor(0.3236)\n",
      "44726 Training Loss: tensor(0.3248)\n",
      "44727 Training Loss: tensor(0.3245)\n",
      "44728 Training Loss: tensor(0.3240)\n",
      "44729 Training Loss: tensor(0.3259)\n",
      "44730 Training Loss: tensor(0.3255)\n",
      "44731 Training Loss: tensor(0.3257)\n",
      "44732 Training Loss: tensor(0.3240)\n",
      "44733 Training Loss: tensor(0.3240)\n",
      "44734 Training Loss: tensor(0.3244)\n",
      "44735 Training Loss: tensor(0.3248)\n",
      "44736 Training Loss: tensor(0.3248)\n",
      "44737 Training Loss: tensor(0.3243)\n",
      "44738 Training Loss: tensor(0.3235)\n",
      "44739 Training Loss: tensor(0.3243)\n",
      "44740 Training Loss: tensor(0.3243)\n",
      "44741 Training Loss: tensor(0.3243)\n",
      "44742 Training Loss: tensor(0.3242)\n",
      "44743 Training Loss: tensor(0.3249)\n",
      "44744 Training Loss: tensor(0.3239)\n",
      "44745 Training Loss: tensor(0.3237)\n",
      "44746 Training Loss: tensor(0.3249)\n",
      "44747 Training Loss: tensor(0.3256)\n",
      "44748 Training Loss: tensor(0.3247)\n",
      "44749 Training Loss: tensor(0.3247)\n",
      "44750 Training Loss: tensor(0.3249)\n",
      "44751 Training Loss: tensor(0.3257)\n",
      "44752 Training Loss: tensor(0.3240)\n",
      "44753 Training Loss: tensor(0.3241)\n",
      "44754 Training Loss: tensor(0.3245)\n",
      "44755 Training Loss: tensor(0.3246)\n",
      "44756 Training Loss: tensor(0.3260)\n",
      "44757 Training Loss: tensor(0.3242)\n",
      "44758 Training Loss: tensor(0.3244)\n",
      "44759 Training Loss: tensor(0.3242)\n",
      "44760 Training Loss: tensor(0.3253)\n",
      "44761 Training Loss: tensor(0.3248)\n",
      "44762 Training Loss: tensor(0.3237)\n",
      "44763 Training Loss: tensor(0.3244)\n",
      "44764 Training Loss: tensor(0.3257)\n",
      "44765 Training Loss: tensor(0.3246)\n",
      "44766 Training Loss: tensor(0.3244)\n",
      "44767 Training Loss: tensor(0.3245)\n",
      "44768 Training Loss: tensor(0.3247)\n",
      "44769 Training Loss: tensor(0.3252)\n",
      "44770 Training Loss: tensor(0.3239)\n",
      "44771 Training Loss: tensor(0.3251)\n",
      "44772 Training Loss: tensor(0.3241)\n",
      "44773 Training Loss: tensor(0.3243)\n",
      "44774 Training Loss: tensor(0.3247)\n",
      "44775 Training Loss: tensor(0.3243)\n",
      "44776 Training Loss: tensor(0.3244)\n",
      "44777 Training Loss: tensor(0.3245)\n",
      "44778 Training Loss: tensor(0.3256)\n",
      "44779 Training Loss: tensor(0.3242)\n",
      "44780 Training Loss: tensor(0.3239)\n",
      "44781 Training Loss: tensor(0.3245)\n",
      "44782 Training Loss: tensor(0.3246)\n",
      "44783 Training Loss: tensor(0.3240)\n",
      "44784 Training Loss: tensor(0.3243)\n",
      "44785 Training Loss: tensor(0.3236)\n",
      "44786 Training Loss: tensor(0.3243)\n",
      "44787 Training Loss: tensor(0.3240)\n",
      "44788 Training Loss: tensor(0.3261)\n",
      "44789 Training Loss: tensor(0.3248)\n",
      "44790 Training Loss: tensor(0.3246)\n",
      "44791 Training Loss: tensor(0.3244)\n",
      "44792 Training Loss: tensor(0.3242)\n",
      "44793 Training Loss: tensor(0.3242)\n",
      "44794 Training Loss: tensor(0.3242)\n",
      "44795 Training Loss: tensor(0.3246)\n",
      "44796 Training Loss: tensor(0.3240)\n",
      "44797 Training Loss: tensor(0.3249)\n",
      "44798 Training Loss: tensor(0.3245)\n",
      "44799 Training Loss: tensor(0.3252)\n",
      "44800 Training Loss: tensor(0.3241)\n",
      "44801 Training Loss: tensor(0.3248)\n",
      "44802 Training Loss: tensor(0.3251)\n",
      "44803 Training Loss: tensor(0.3245)\n",
      "44804 Training Loss: tensor(0.3240)\n",
      "44805 Training Loss: tensor(0.3243)\n",
      "44806 Training Loss: tensor(0.3257)\n",
      "44807 Training Loss: tensor(0.3254)\n",
      "44808 Training Loss: tensor(0.3240)\n",
      "44809 Training Loss: tensor(0.3249)\n",
      "44810 Training Loss: tensor(0.3249)\n",
      "44811 Training Loss: tensor(0.3251)\n",
      "44812 Training Loss: tensor(0.3241)\n",
      "44813 Training Loss: tensor(0.3244)\n",
      "44814 Training Loss: tensor(0.3247)\n",
      "44815 Training Loss: tensor(0.3251)\n",
      "44816 Training Loss: tensor(0.3242)\n",
      "44817 Training Loss: tensor(0.3248)\n",
      "44818 Training Loss: tensor(0.3239)\n",
      "44819 Training Loss: tensor(0.3246)\n",
      "44820 Training Loss: tensor(0.3251)\n",
      "44821 Training Loss: tensor(0.3244)\n",
      "44822 Training Loss: tensor(0.3241)\n",
      "44823 Training Loss: tensor(0.3240)\n",
      "44824 Training Loss: tensor(0.3251)\n",
      "44825 Training Loss: tensor(0.3243)\n",
      "44826 Training Loss: tensor(0.3240)\n",
      "44827 Training Loss: tensor(0.3243)\n",
      "44828 Training Loss: tensor(0.3243)\n",
      "44829 Training Loss: tensor(0.3251)\n",
      "44830 Training Loss: tensor(0.3239)\n",
      "44831 Training Loss: tensor(0.3249)\n",
      "44832 Training Loss: tensor(0.3239)\n",
      "44833 Training Loss: tensor(0.3241)\n",
      "44834 Training Loss: tensor(0.3242)\n",
      "44835 Training Loss: tensor(0.3243)\n",
      "44836 Training Loss: tensor(0.3246)\n",
      "44837 Training Loss: tensor(0.3237)\n",
      "44838 Training Loss: tensor(0.3237)\n",
      "44839 Training Loss: tensor(0.3251)\n",
      "44840 Training Loss: tensor(0.3255)\n",
      "44841 Training Loss: tensor(0.3249)\n",
      "44842 Training Loss: tensor(0.3241)\n",
      "44843 Training Loss: tensor(0.3249)\n",
      "44844 Training Loss: tensor(0.3255)\n",
      "44845 Training Loss: tensor(0.3244)\n",
      "44846 Training Loss: tensor(0.3242)\n",
      "44847 Training Loss: tensor(0.3242)\n",
      "44848 Training Loss: tensor(0.3242)\n",
      "44849 Training Loss: tensor(0.3249)\n",
      "44850 Training Loss: tensor(0.3248)\n",
      "44851 Training Loss: tensor(0.3243)\n",
      "44852 Training Loss: tensor(0.3237)\n",
      "44853 Training Loss: tensor(0.3238)\n",
      "44854 Training Loss: tensor(0.3245)\n",
      "44855 Training Loss: tensor(0.3240)\n",
      "44856 Training Loss: tensor(0.3237)\n",
      "44857 Training Loss: tensor(0.3247)\n",
      "44858 Training Loss: tensor(0.3242)\n",
      "44859 Training Loss: tensor(0.3246)\n",
      "44860 Training Loss: tensor(0.3245)\n",
      "44861 Training Loss: tensor(0.3237)\n",
      "44862 Training Loss: tensor(0.3248)\n",
      "44863 Training Loss: tensor(0.3249)\n",
      "44864 Training Loss: tensor(0.3242)\n",
      "44865 Training Loss: tensor(0.3241)\n",
      "44866 Training Loss: tensor(0.3241)\n",
      "44867 Training Loss: tensor(0.3244)\n",
      "44868 Training Loss: tensor(0.3241)\n",
      "44869 Training Loss: tensor(0.3243)\n",
      "44870 Training Loss: tensor(0.3238)\n",
      "44871 Training Loss: tensor(0.3239)\n",
      "44872 Training Loss: tensor(0.3255)\n",
      "44873 Training Loss: tensor(0.3244)\n",
      "44874 Training Loss: tensor(0.3237)\n",
      "44875 Training Loss: tensor(0.3240)\n",
      "44876 Training Loss: tensor(0.3246)\n",
      "44877 Training Loss: tensor(0.3241)\n",
      "44878 Training Loss: tensor(0.3255)\n",
      "44879 Training Loss: tensor(0.3240)\n",
      "44880 Training Loss: tensor(0.3250)\n",
      "44881 Training Loss: tensor(0.3244)\n",
      "44882 Training Loss: tensor(0.3239)\n",
      "44883 Training Loss: tensor(0.3236)\n",
      "44884 Training Loss: tensor(0.3245)\n",
      "44885 Training Loss: tensor(0.3250)\n",
      "44886 Training Loss: tensor(0.3240)\n",
      "44887 Training Loss: tensor(0.3249)\n",
      "44888 Training Loss: tensor(0.3248)\n",
      "44889 Training Loss: tensor(0.3243)\n",
      "44890 Training Loss: tensor(0.3251)\n",
      "44891 Training Loss: tensor(0.3245)\n",
      "44892 Training Loss: tensor(0.3246)\n",
      "44893 Training Loss: tensor(0.3239)\n",
      "44894 Training Loss: tensor(0.3243)\n",
      "44895 Training Loss: tensor(0.3245)\n",
      "44896 Training Loss: tensor(0.3235)\n",
      "44897 Training Loss: tensor(0.3242)\n",
      "44898 Training Loss: tensor(0.3241)\n",
      "44899 Training Loss: tensor(0.3240)\n",
      "44900 Training Loss: tensor(0.3246)\n",
      "44901 Training Loss: tensor(0.3239)\n",
      "44902 Training Loss: tensor(0.3240)\n",
      "44903 Training Loss: tensor(0.3245)\n",
      "44904 Training Loss: tensor(0.3255)\n",
      "44905 Training Loss: tensor(0.3238)\n",
      "44906 Training Loss: tensor(0.3245)\n",
      "44907 Training Loss: tensor(0.3249)\n",
      "44908 Training Loss: tensor(0.3255)\n",
      "44909 Training Loss: tensor(0.3254)\n",
      "44910 Training Loss: tensor(0.3245)\n",
      "44911 Training Loss: tensor(0.3247)\n",
      "44912 Training Loss: tensor(0.3240)\n",
      "44913 Training Loss: tensor(0.3255)\n",
      "44914 Training Loss: tensor(0.3242)\n",
      "44915 Training Loss: tensor(0.3243)\n",
      "44916 Training Loss: tensor(0.3239)\n",
      "44917 Training Loss: tensor(0.3240)\n",
      "44918 Training Loss: tensor(0.3250)\n",
      "44919 Training Loss: tensor(0.3248)\n",
      "44920 Training Loss: tensor(0.3248)\n",
      "44921 Training Loss: tensor(0.3242)\n",
      "44922 Training Loss: tensor(0.3245)\n",
      "44923 Training Loss: tensor(0.3238)\n",
      "44924 Training Loss: tensor(0.3245)\n",
      "44925 Training Loss: tensor(0.3241)\n",
      "44926 Training Loss: tensor(0.3241)\n",
      "44927 Training Loss: tensor(0.3246)\n",
      "44928 Training Loss: tensor(0.3259)\n",
      "44929 Training Loss: tensor(0.3244)\n",
      "44930 Training Loss: tensor(0.3253)\n",
      "44931 Training Loss: tensor(0.3237)\n",
      "44932 Training Loss: tensor(0.3242)\n",
      "44933 Training Loss: tensor(0.3240)\n",
      "44934 Training Loss: tensor(0.3242)\n",
      "44935 Training Loss: tensor(0.3249)\n",
      "44936 Training Loss: tensor(0.3241)\n",
      "44937 Training Loss: tensor(0.3257)\n",
      "44938 Training Loss: tensor(0.3237)\n",
      "44939 Training Loss: tensor(0.3242)\n",
      "44940 Training Loss: tensor(0.3254)\n",
      "44941 Training Loss: tensor(0.3243)\n",
      "44942 Training Loss: tensor(0.3243)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44943 Training Loss: tensor(0.3245)\n",
      "44944 Training Loss: tensor(0.3245)\n",
      "44945 Training Loss: tensor(0.3244)\n",
      "44946 Training Loss: tensor(0.3247)\n",
      "44947 Training Loss: tensor(0.3237)\n",
      "44948 Training Loss: tensor(0.3235)\n",
      "44949 Training Loss: tensor(0.3239)\n",
      "44950 Training Loss: tensor(0.3241)\n",
      "44951 Training Loss: tensor(0.3247)\n",
      "44952 Training Loss: tensor(0.3248)\n",
      "44953 Training Loss: tensor(0.3242)\n",
      "44954 Training Loss: tensor(0.3245)\n",
      "44955 Training Loss: tensor(0.3240)\n",
      "44956 Training Loss: tensor(0.3245)\n",
      "44957 Training Loss: tensor(0.3248)\n",
      "44958 Training Loss: tensor(0.3242)\n",
      "44959 Training Loss: tensor(0.3244)\n",
      "44960 Training Loss: tensor(0.3241)\n",
      "44961 Training Loss: tensor(0.3246)\n",
      "44962 Training Loss: tensor(0.3242)\n",
      "44963 Training Loss: tensor(0.3241)\n",
      "44964 Training Loss: tensor(0.3247)\n",
      "44965 Training Loss: tensor(0.3238)\n",
      "44966 Training Loss: tensor(0.3240)\n",
      "44967 Training Loss: tensor(0.3244)\n",
      "44968 Training Loss: tensor(0.3248)\n",
      "44969 Training Loss: tensor(0.3239)\n",
      "44970 Training Loss: tensor(0.3240)\n",
      "44971 Training Loss: tensor(0.3238)\n",
      "44972 Training Loss: tensor(0.3241)\n",
      "44973 Training Loss: tensor(0.3246)\n",
      "44974 Training Loss: tensor(0.3246)\n",
      "44975 Training Loss: tensor(0.3243)\n",
      "44976 Training Loss: tensor(0.3241)\n",
      "44977 Training Loss: tensor(0.3244)\n",
      "44978 Training Loss: tensor(0.3258)\n",
      "44979 Training Loss: tensor(0.3242)\n",
      "44980 Training Loss: tensor(0.3236)\n",
      "44981 Training Loss: tensor(0.3259)\n",
      "44982 Training Loss: tensor(0.3238)\n",
      "44983 Training Loss: tensor(0.3243)\n",
      "44984 Training Loss: tensor(0.3242)\n",
      "44985 Training Loss: tensor(0.3244)\n",
      "44986 Training Loss: tensor(0.3241)\n",
      "44987 Training Loss: tensor(0.3244)\n",
      "44988 Training Loss: tensor(0.3238)\n",
      "44989 Training Loss: tensor(0.3247)\n",
      "44990 Training Loss: tensor(0.3239)\n",
      "44991 Training Loss: tensor(0.3245)\n",
      "44992 Training Loss: tensor(0.3247)\n",
      "44993 Training Loss: tensor(0.3249)\n",
      "44994 Training Loss: tensor(0.3246)\n",
      "44995 Training Loss: tensor(0.3244)\n",
      "44996 Training Loss: tensor(0.3238)\n",
      "44997 Training Loss: tensor(0.3243)\n",
      "44998 Training Loss: tensor(0.3248)\n",
      "44999 Training Loss: tensor(0.3245)\n",
      "45000 Training Loss: tensor(0.3253)\n",
      "45001 Training Loss: tensor(0.3243)\n",
      "45002 Training Loss: tensor(0.3238)\n",
      "45003 Training Loss: tensor(0.3251)\n",
      "45004 Training Loss: tensor(0.3248)\n",
      "45005 Training Loss: tensor(0.3248)\n",
      "45006 Training Loss: tensor(0.3248)\n",
      "45007 Training Loss: tensor(0.3247)\n",
      "45008 Training Loss: tensor(0.3241)\n",
      "45009 Training Loss: tensor(0.3255)\n",
      "45010 Training Loss: tensor(0.3242)\n",
      "45011 Training Loss: tensor(0.3245)\n",
      "45012 Training Loss: tensor(0.3245)\n",
      "45013 Training Loss: tensor(0.3244)\n",
      "45014 Training Loss: tensor(0.3253)\n",
      "45015 Training Loss: tensor(0.3237)\n",
      "45016 Training Loss: tensor(0.3247)\n",
      "45017 Training Loss: tensor(0.3254)\n",
      "45018 Training Loss: tensor(0.3240)\n",
      "45019 Training Loss: tensor(0.3243)\n",
      "45020 Training Loss: tensor(0.3244)\n",
      "45021 Training Loss: tensor(0.3241)\n",
      "45022 Training Loss: tensor(0.3241)\n",
      "45023 Training Loss: tensor(0.3246)\n",
      "45024 Training Loss: tensor(0.3246)\n",
      "45025 Training Loss: tensor(0.3245)\n",
      "45026 Training Loss: tensor(0.3244)\n",
      "45027 Training Loss: tensor(0.3257)\n",
      "45028 Training Loss: tensor(0.3252)\n",
      "45029 Training Loss: tensor(0.3251)\n",
      "45030 Training Loss: tensor(0.3253)\n",
      "45031 Training Loss: tensor(0.3246)\n",
      "45032 Training Loss: tensor(0.3249)\n",
      "45033 Training Loss: tensor(0.3253)\n",
      "45034 Training Loss: tensor(0.3247)\n",
      "45035 Training Loss: tensor(0.3250)\n",
      "45036 Training Loss: tensor(0.3252)\n",
      "45037 Training Loss: tensor(0.3241)\n",
      "45038 Training Loss: tensor(0.3237)\n",
      "45039 Training Loss: tensor(0.3239)\n",
      "45040 Training Loss: tensor(0.3252)\n",
      "45041 Training Loss: tensor(0.3241)\n",
      "45042 Training Loss: tensor(0.3237)\n",
      "45043 Training Loss: tensor(0.3240)\n",
      "45044 Training Loss: tensor(0.3243)\n",
      "45045 Training Loss: tensor(0.3240)\n",
      "45046 Training Loss: tensor(0.3246)\n",
      "45047 Training Loss: tensor(0.3245)\n",
      "45048 Training Loss: tensor(0.3241)\n",
      "45049 Training Loss: tensor(0.3243)\n",
      "45050 Training Loss: tensor(0.3243)\n",
      "45051 Training Loss: tensor(0.3250)\n",
      "45052 Training Loss: tensor(0.3267)\n",
      "45053 Training Loss: tensor(0.3248)\n",
      "45054 Training Loss: tensor(0.3245)\n",
      "45055 Training Loss: tensor(0.3249)\n",
      "45056 Training Loss: tensor(0.3248)\n",
      "45057 Training Loss: tensor(0.3251)\n",
      "45058 Training Loss: tensor(0.3240)\n",
      "45059 Training Loss: tensor(0.3249)\n",
      "45060 Training Loss: tensor(0.3253)\n",
      "45061 Training Loss: tensor(0.3237)\n",
      "45062 Training Loss: tensor(0.3256)\n",
      "45063 Training Loss: tensor(0.3246)\n",
      "45064 Training Loss: tensor(0.3238)\n",
      "45065 Training Loss: tensor(0.3243)\n",
      "45066 Training Loss: tensor(0.3252)\n",
      "45067 Training Loss: tensor(0.3251)\n",
      "45068 Training Loss: tensor(0.3234)\n",
      "45069 Training Loss: tensor(0.3231)\n",
      "45070 Training Loss: tensor(0.3253)\n",
      "45071 Training Loss: tensor(0.3244)\n",
      "45072 Training Loss: tensor(0.3239)\n",
      "45073 Training Loss: tensor(0.3249)\n",
      "45074 Training Loss: tensor(0.3238)\n",
      "45075 Training Loss: tensor(0.3253)\n",
      "45076 Training Loss: tensor(0.3252)\n",
      "45077 Training Loss: tensor(0.3241)\n",
      "45078 Training Loss: tensor(0.3244)\n",
      "45079 Training Loss: tensor(0.3248)\n",
      "45080 Training Loss: tensor(0.3246)\n",
      "45081 Training Loss: tensor(0.3251)\n",
      "45082 Training Loss: tensor(0.3240)\n",
      "45083 Training Loss: tensor(0.3242)\n",
      "45084 Training Loss: tensor(0.3240)\n",
      "45085 Training Loss: tensor(0.3250)\n",
      "45086 Training Loss: tensor(0.3248)\n",
      "45087 Training Loss: tensor(0.3243)\n",
      "45088 Training Loss: tensor(0.3243)\n",
      "45089 Training Loss: tensor(0.3240)\n",
      "45090 Training Loss: tensor(0.3234)\n",
      "45091 Training Loss: tensor(0.3241)\n",
      "45092 Training Loss: tensor(0.3242)\n",
      "45093 Training Loss: tensor(0.3237)\n",
      "45094 Training Loss: tensor(0.3238)\n",
      "45095 Training Loss: tensor(0.3239)\n",
      "45096 Training Loss: tensor(0.3237)\n",
      "45097 Training Loss: tensor(0.3248)\n",
      "45098 Training Loss: tensor(0.3240)\n",
      "45099 Training Loss: tensor(0.3262)\n",
      "45100 Training Loss: tensor(0.3238)\n",
      "45101 Training Loss: tensor(0.3247)\n",
      "45102 Training Loss: tensor(0.3237)\n",
      "45103 Training Loss: tensor(0.3246)\n",
      "45104 Training Loss: tensor(0.3238)\n",
      "45105 Training Loss: tensor(0.3243)\n",
      "45106 Training Loss: tensor(0.3248)\n",
      "45107 Training Loss: tensor(0.3243)\n",
      "45108 Training Loss: tensor(0.3253)\n",
      "45109 Training Loss: tensor(0.3247)\n",
      "45110 Training Loss: tensor(0.3243)\n",
      "45111 Training Loss: tensor(0.3260)\n",
      "45112 Training Loss: tensor(0.3245)\n",
      "45113 Training Loss: tensor(0.3251)\n",
      "45114 Training Loss: tensor(0.3248)\n",
      "45115 Training Loss: tensor(0.3238)\n",
      "45116 Training Loss: tensor(0.3248)\n",
      "45117 Training Loss: tensor(0.3249)\n",
      "45118 Training Loss: tensor(0.3247)\n",
      "45119 Training Loss: tensor(0.3240)\n",
      "45120 Training Loss: tensor(0.3246)\n",
      "45121 Training Loss: tensor(0.3248)\n",
      "45122 Training Loss: tensor(0.3238)\n",
      "45123 Training Loss: tensor(0.3241)\n",
      "45124 Training Loss: tensor(0.3243)\n",
      "45125 Training Loss: tensor(0.3236)\n",
      "45126 Training Loss: tensor(0.3244)\n",
      "45127 Training Loss: tensor(0.3252)\n",
      "45128 Training Loss: tensor(0.3238)\n",
      "45129 Training Loss: tensor(0.3243)\n",
      "45130 Training Loss: tensor(0.3245)\n",
      "45131 Training Loss: tensor(0.3237)\n",
      "45132 Training Loss: tensor(0.3247)\n",
      "45133 Training Loss: tensor(0.3239)\n",
      "45134 Training Loss: tensor(0.3239)\n",
      "45135 Training Loss: tensor(0.3236)\n",
      "45136 Training Loss: tensor(0.3248)\n",
      "45137 Training Loss: tensor(0.3251)\n",
      "45138 Training Loss: tensor(0.3252)\n",
      "45139 Training Loss: tensor(0.3239)\n",
      "45140 Training Loss: tensor(0.3245)\n",
      "45141 Training Loss: tensor(0.3243)\n",
      "45142 Training Loss: tensor(0.3248)\n",
      "45143 Training Loss: tensor(0.3240)\n",
      "45144 Training Loss: tensor(0.3246)\n",
      "45145 Training Loss: tensor(0.3242)\n",
      "45146 Training Loss: tensor(0.3242)\n",
      "45147 Training Loss: tensor(0.3254)\n",
      "45148 Training Loss: tensor(0.3245)\n",
      "45149 Training Loss: tensor(0.3254)\n",
      "45150 Training Loss: tensor(0.3247)\n",
      "45151 Training Loss: tensor(0.3249)\n",
      "45152 Training Loss: tensor(0.3248)\n",
      "45153 Training Loss: tensor(0.3243)\n",
      "45154 Training Loss: tensor(0.3248)\n",
      "45155 Training Loss: tensor(0.3244)\n",
      "45156 Training Loss: tensor(0.3246)\n",
      "45157 Training Loss: tensor(0.3255)\n",
      "45158 Training Loss: tensor(0.3248)\n",
      "45159 Training Loss: tensor(0.3248)\n",
      "45160 Training Loss: tensor(0.3249)\n",
      "45161 Training Loss: tensor(0.3245)\n",
      "45162 Training Loss: tensor(0.3247)\n",
      "45163 Training Loss: tensor(0.3239)\n",
      "45164 Training Loss: tensor(0.3246)\n",
      "45165 Training Loss: tensor(0.3251)\n",
      "45166 Training Loss: tensor(0.3246)\n",
      "45167 Training Loss: tensor(0.3242)\n",
      "45168 Training Loss: tensor(0.3256)\n",
      "45169 Training Loss: tensor(0.3243)\n",
      "45170 Training Loss: tensor(0.3249)\n",
      "45171 Training Loss: tensor(0.3238)\n",
      "45172 Training Loss: tensor(0.3242)\n",
      "45173 Training Loss: tensor(0.3238)\n",
      "45174 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45175 Training Loss: tensor(0.3240)\n",
      "45176 Training Loss: tensor(0.3245)\n",
      "45177 Training Loss: tensor(0.3242)\n",
      "45178 Training Loss: tensor(0.3247)\n",
      "45179 Training Loss: tensor(0.3249)\n",
      "45180 Training Loss: tensor(0.3244)\n",
      "45181 Training Loss: tensor(0.3242)\n",
      "45182 Training Loss: tensor(0.3242)\n",
      "45183 Training Loss: tensor(0.3257)\n",
      "45184 Training Loss: tensor(0.3248)\n",
      "45185 Training Loss: tensor(0.3240)\n",
      "45186 Training Loss: tensor(0.3247)\n",
      "45187 Training Loss: tensor(0.3247)\n",
      "45188 Training Loss: tensor(0.3244)\n",
      "45189 Training Loss: tensor(0.3244)\n",
      "45190 Training Loss: tensor(0.3244)\n",
      "45191 Training Loss: tensor(0.3239)\n",
      "45192 Training Loss: tensor(0.3242)\n",
      "45193 Training Loss: tensor(0.3240)\n",
      "45194 Training Loss: tensor(0.3239)\n",
      "45195 Training Loss: tensor(0.3253)\n",
      "45196 Training Loss: tensor(0.3246)\n",
      "45197 Training Loss: tensor(0.3250)\n",
      "45198 Training Loss: tensor(0.3245)\n",
      "45199 Training Loss: tensor(0.3240)\n",
      "45200 Training Loss: tensor(0.3251)\n",
      "45201 Training Loss: tensor(0.3252)\n",
      "45202 Training Loss: tensor(0.3246)\n",
      "45203 Training Loss: tensor(0.3248)\n",
      "45204 Training Loss: tensor(0.3243)\n",
      "45205 Training Loss: tensor(0.3245)\n",
      "45206 Training Loss: tensor(0.3248)\n",
      "45207 Training Loss: tensor(0.3239)\n",
      "45208 Training Loss: tensor(0.3240)\n",
      "45209 Training Loss: tensor(0.3243)\n",
      "45210 Training Loss: tensor(0.3248)\n",
      "45211 Training Loss: tensor(0.3241)\n",
      "45212 Training Loss: tensor(0.3246)\n",
      "45213 Training Loss: tensor(0.3240)\n",
      "45214 Training Loss: tensor(0.3249)\n",
      "45215 Training Loss: tensor(0.3250)\n",
      "45216 Training Loss: tensor(0.3244)\n",
      "45217 Training Loss: tensor(0.3236)\n",
      "45218 Training Loss: tensor(0.3239)\n",
      "45219 Training Loss: tensor(0.3250)\n",
      "45220 Training Loss: tensor(0.3243)\n",
      "45221 Training Loss: tensor(0.3236)\n",
      "45222 Training Loss: tensor(0.3241)\n",
      "45223 Training Loss: tensor(0.3241)\n",
      "45224 Training Loss: tensor(0.3248)\n",
      "45225 Training Loss: tensor(0.3238)\n",
      "45226 Training Loss: tensor(0.3245)\n",
      "45227 Training Loss: tensor(0.3241)\n",
      "45228 Training Loss: tensor(0.3241)\n",
      "45229 Training Loss: tensor(0.3257)\n",
      "45230 Training Loss: tensor(0.3236)\n",
      "45231 Training Loss: tensor(0.3242)\n",
      "45232 Training Loss: tensor(0.3251)\n",
      "45233 Training Loss: tensor(0.3242)\n",
      "45234 Training Loss: tensor(0.3241)\n",
      "45235 Training Loss: tensor(0.3239)\n",
      "45236 Training Loss: tensor(0.3257)\n",
      "45237 Training Loss: tensor(0.3253)\n",
      "45238 Training Loss: tensor(0.3245)\n",
      "45239 Training Loss: tensor(0.3243)\n",
      "45240 Training Loss: tensor(0.3244)\n",
      "45241 Training Loss: tensor(0.3239)\n",
      "45242 Training Loss: tensor(0.3236)\n",
      "45243 Training Loss: tensor(0.3240)\n",
      "45244 Training Loss: tensor(0.3240)\n",
      "45245 Training Loss: tensor(0.3238)\n",
      "45246 Training Loss: tensor(0.3249)\n",
      "45247 Training Loss: tensor(0.3239)\n",
      "45248 Training Loss: tensor(0.3249)\n",
      "45249 Training Loss: tensor(0.3239)\n",
      "45250 Training Loss: tensor(0.3244)\n",
      "45251 Training Loss: tensor(0.3242)\n",
      "45252 Training Loss: tensor(0.3242)\n",
      "45253 Training Loss: tensor(0.3236)\n",
      "45254 Training Loss: tensor(0.3237)\n",
      "45255 Training Loss: tensor(0.3243)\n",
      "45256 Training Loss: tensor(0.3254)\n",
      "45257 Training Loss: tensor(0.3251)\n",
      "45258 Training Loss: tensor(0.3237)\n",
      "45259 Training Loss: tensor(0.3235)\n",
      "45260 Training Loss: tensor(0.3242)\n",
      "45261 Training Loss: tensor(0.3244)\n",
      "45262 Training Loss: tensor(0.3249)\n",
      "45263 Training Loss: tensor(0.3236)\n",
      "45264 Training Loss: tensor(0.3243)\n",
      "45265 Training Loss: tensor(0.3251)\n",
      "45266 Training Loss: tensor(0.3247)\n",
      "45267 Training Loss: tensor(0.3243)\n",
      "45268 Training Loss: tensor(0.3245)\n",
      "45269 Training Loss: tensor(0.3249)\n",
      "45270 Training Loss: tensor(0.3242)\n",
      "45271 Training Loss: tensor(0.3250)\n",
      "45272 Training Loss: tensor(0.3240)\n",
      "45273 Training Loss: tensor(0.3239)\n",
      "45274 Training Loss: tensor(0.3256)\n",
      "45275 Training Loss: tensor(0.3247)\n",
      "45276 Training Loss: tensor(0.3247)\n",
      "45277 Training Loss: tensor(0.3246)\n",
      "45278 Training Loss: tensor(0.3249)\n",
      "45279 Training Loss: tensor(0.3246)\n",
      "45280 Training Loss: tensor(0.3245)\n",
      "45281 Training Loss: tensor(0.3243)\n",
      "45282 Training Loss: tensor(0.3244)\n",
      "45283 Training Loss: tensor(0.3249)\n",
      "45284 Training Loss: tensor(0.3245)\n",
      "45285 Training Loss: tensor(0.3241)\n",
      "45286 Training Loss: tensor(0.3243)\n",
      "45287 Training Loss: tensor(0.3241)\n",
      "45288 Training Loss: tensor(0.3251)\n",
      "45289 Training Loss: tensor(0.3240)\n",
      "45290 Training Loss: tensor(0.3250)\n",
      "45291 Training Loss: tensor(0.3238)\n",
      "45292 Training Loss: tensor(0.3244)\n",
      "45293 Training Loss: tensor(0.3242)\n",
      "45294 Training Loss: tensor(0.3242)\n",
      "45295 Training Loss: tensor(0.3252)\n",
      "45296 Training Loss: tensor(0.3240)\n",
      "45297 Training Loss: tensor(0.3239)\n",
      "45298 Training Loss: tensor(0.3240)\n",
      "45299 Training Loss: tensor(0.3236)\n",
      "45300 Training Loss: tensor(0.3249)\n",
      "45301 Training Loss: tensor(0.3242)\n",
      "45302 Training Loss: tensor(0.3239)\n",
      "45303 Training Loss: tensor(0.3243)\n",
      "45304 Training Loss: tensor(0.3246)\n",
      "45305 Training Loss: tensor(0.3242)\n",
      "45306 Training Loss: tensor(0.3245)\n",
      "45307 Training Loss: tensor(0.3241)\n",
      "45308 Training Loss: tensor(0.3250)\n",
      "45309 Training Loss: tensor(0.3243)\n",
      "45310 Training Loss: tensor(0.3239)\n",
      "45311 Training Loss: tensor(0.3250)\n",
      "45312 Training Loss: tensor(0.3243)\n",
      "45313 Training Loss: tensor(0.3248)\n",
      "45314 Training Loss: tensor(0.3241)\n",
      "45315 Training Loss: tensor(0.3248)\n",
      "45316 Training Loss: tensor(0.3249)\n",
      "45317 Training Loss: tensor(0.3243)\n",
      "45318 Training Loss: tensor(0.3241)\n",
      "45319 Training Loss: tensor(0.3251)\n",
      "45320 Training Loss: tensor(0.3242)\n",
      "45321 Training Loss: tensor(0.3241)\n",
      "45322 Training Loss: tensor(0.3250)\n",
      "45323 Training Loss: tensor(0.3253)\n",
      "45324 Training Loss: tensor(0.3236)\n",
      "45325 Training Loss: tensor(0.3243)\n",
      "45326 Training Loss: tensor(0.3248)\n",
      "45327 Training Loss: tensor(0.3244)\n",
      "45328 Training Loss: tensor(0.3247)\n",
      "45329 Training Loss: tensor(0.3243)\n",
      "45330 Training Loss: tensor(0.3242)\n",
      "45331 Training Loss: tensor(0.3242)\n",
      "45332 Training Loss: tensor(0.3245)\n",
      "45333 Training Loss: tensor(0.3242)\n",
      "45334 Training Loss: tensor(0.3249)\n",
      "45335 Training Loss: tensor(0.3236)\n",
      "45336 Training Loss: tensor(0.3249)\n",
      "45337 Training Loss: tensor(0.3248)\n",
      "45338 Training Loss: tensor(0.3238)\n",
      "45339 Training Loss: tensor(0.3245)\n",
      "45340 Training Loss: tensor(0.3239)\n",
      "45341 Training Loss: tensor(0.3252)\n",
      "45342 Training Loss: tensor(0.3241)\n",
      "45343 Training Loss: tensor(0.3238)\n",
      "45344 Training Loss: tensor(0.3251)\n",
      "45345 Training Loss: tensor(0.3239)\n",
      "45346 Training Loss: tensor(0.3250)\n",
      "45347 Training Loss: tensor(0.3250)\n",
      "45348 Training Loss: tensor(0.3241)\n",
      "45349 Training Loss: tensor(0.3248)\n",
      "45350 Training Loss: tensor(0.3236)\n",
      "45351 Training Loss: tensor(0.3234)\n",
      "45352 Training Loss: tensor(0.3245)\n",
      "45353 Training Loss: tensor(0.3254)\n",
      "45354 Training Loss: tensor(0.3240)\n",
      "45355 Training Loss: tensor(0.3244)\n",
      "45356 Training Loss: tensor(0.3245)\n",
      "45357 Training Loss: tensor(0.3249)\n",
      "45358 Training Loss: tensor(0.3243)\n",
      "45359 Training Loss: tensor(0.3243)\n",
      "45360 Training Loss: tensor(0.3237)\n",
      "45361 Training Loss: tensor(0.3241)\n",
      "45362 Training Loss: tensor(0.3238)\n",
      "45363 Training Loss: tensor(0.3241)\n",
      "45364 Training Loss: tensor(0.3243)\n",
      "45365 Training Loss: tensor(0.3240)\n",
      "45366 Training Loss: tensor(0.3250)\n",
      "45367 Training Loss: tensor(0.3242)\n",
      "45368 Training Loss: tensor(0.3243)\n",
      "45369 Training Loss: tensor(0.3248)\n",
      "45370 Training Loss: tensor(0.3242)\n",
      "45371 Training Loss: tensor(0.3254)\n",
      "45372 Training Loss: tensor(0.3243)\n",
      "45373 Training Loss: tensor(0.3254)\n",
      "45374 Training Loss: tensor(0.3243)\n",
      "45375 Training Loss: tensor(0.3252)\n",
      "45376 Training Loss: tensor(0.3251)\n",
      "45377 Training Loss: tensor(0.3246)\n",
      "45378 Training Loss: tensor(0.3240)\n",
      "45379 Training Loss: tensor(0.3240)\n",
      "45380 Training Loss: tensor(0.3250)\n",
      "45381 Training Loss: tensor(0.3239)\n",
      "45382 Training Loss: tensor(0.3253)\n",
      "45383 Training Loss: tensor(0.3246)\n",
      "45384 Training Loss: tensor(0.3237)\n",
      "45385 Training Loss: tensor(0.3245)\n",
      "45386 Training Loss: tensor(0.3246)\n",
      "45387 Training Loss: tensor(0.3240)\n",
      "45388 Training Loss: tensor(0.3242)\n",
      "45389 Training Loss: tensor(0.3245)\n",
      "45390 Training Loss: tensor(0.3242)\n",
      "45391 Training Loss: tensor(0.3236)\n",
      "45392 Training Loss: tensor(0.3240)\n",
      "45393 Training Loss: tensor(0.3249)\n",
      "45394 Training Loss: tensor(0.3240)\n",
      "45395 Training Loss: tensor(0.3253)\n",
      "45396 Training Loss: tensor(0.3259)\n",
      "45397 Training Loss: tensor(0.3241)\n",
      "45398 Training Loss: tensor(0.3244)\n",
      "45399 Training Loss: tensor(0.3248)\n",
      "45400 Training Loss: tensor(0.3247)\n",
      "45401 Training Loss: tensor(0.3245)\n",
      "45402 Training Loss: tensor(0.3254)\n",
      "45403 Training Loss: tensor(0.3238)\n",
      "45404 Training Loss: tensor(0.3246)\n",
      "45405 Training Loss: tensor(0.3252)\n",
      "45406 Training Loss: tensor(0.3243)\n",
      "45407 Training Loss: tensor(0.3249)\n",
      "45408 Training Loss: tensor(0.3247)\n",
      "45409 Training Loss: tensor(0.3243)\n",
      "45410 Training Loss: tensor(0.3238)\n",
      "45411 Training Loss: tensor(0.3243)\n",
      "45412 Training Loss: tensor(0.3253)\n",
      "45413 Training Loss: tensor(0.3259)\n",
      "45414 Training Loss: tensor(0.3241)\n",
      "45415 Training Loss: tensor(0.3237)\n",
      "45416 Training Loss: tensor(0.3256)\n",
      "45417 Training Loss: tensor(0.3248)\n",
      "45418 Training Loss: tensor(0.3243)\n",
      "45419 Training Loss: tensor(0.3251)\n",
      "45420 Training Loss: tensor(0.3248)\n",
      "45421 Training Loss: tensor(0.3249)\n",
      "45422 Training Loss: tensor(0.3248)\n",
      "45423 Training Loss: tensor(0.3244)\n",
      "45424 Training Loss: tensor(0.3243)\n",
      "45425 Training Loss: tensor(0.3246)\n",
      "45426 Training Loss: tensor(0.3246)\n",
      "45427 Training Loss: tensor(0.3245)\n",
      "45428 Training Loss: tensor(0.3242)\n",
      "45429 Training Loss: tensor(0.3245)\n",
      "45430 Training Loss: tensor(0.3249)\n",
      "45431 Training Loss: tensor(0.3240)\n",
      "45432 Training Loss: tensor(0.3248)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45433 Training Loss: tensor(0.3248)\n",
      "45434 Training Loss: tensor(0.3255)\n",
      "45435 Training Loss: tensor(0.3243)\n",
      "45436 Training Loss: tensor(0.3252)\n",
      "45437 Training Loss: tensor(0.3253)\n",
      "45438 Training Loss: tensor(0.3247)\n",
      "45439 Training Loss: tensor(0.3244)\n",
      "45440 Training Loss: tensor(0.3247)\n",
      "45441 Training Loss: tensor(0.3250)\n",
      "45442 Training Loss: tensor(0.3245)\n",
      "45443 Training Loss: tensor(0.3245)\n",
      "45444 Training Loss: tensor(0.3243)\n",
      "45445 Training Loss: tensor(0.3260)\n",
      "45446 Training Loss: tensor(0.3244)\n",
      "45447 Training Loss: tensor(0.3243)\n",
      "45448 Training Loss: tensor(0.3238)\n",
      "45449 Training Loss: tensor(0.3243)\n",
      "45450 Training Loss: tensor(0.3250)\n",
      "45451 Training Loss: tensor(0.3247)\n",
      "45452 Training Loss: tensor(0.3253)\n",
      "45453 Training Loss: tensor(0.3243)\n",
      "45454 Training Loss: tensor(0.3247)\n",
      "45455 Training Loss: tensor(0.3245)\n",
      "45456 Training Loss: tensor(0.3239)\n",
      "45457 Training Loss: tensor(0.3246)\n",
      "45458 Training Loss: tensor(0.3248)\n",
      "45459 Training Loss: tensor(0.3257)\n",
      "45460 Training Loss: tensor(0.3246)\n",
      "45461 Training Loss: tensor(0.3247)\n",
      "45462 Training Loss: tensor(0.3238)\n",
      "45463 Training Loss: tensor(0.3249)\n",
      "45464 Training Loss: tensor(0.3243)\n",
      "45465 Training Loss: tensor(0.3237)\n",
      "45466 Training Loss: tensor(0.3247)\n",
      "45467 Training Loss: tensor(0.3236)\n",
      "45468 Training Loss: tensor(0.3248)\n",
      "45469 Training Loss: tensor(0.3242)\n",
      "45470 Training Loss: tensor(0.3253)\n",
      "45471 Training Loss: tensor(0.3244)\n",
      "45472 Training Loss: tensor(0.3243)\n",
      "45473 Training Loss: tensor(0.3247)\n",
      "45474 Training Loss: tensor(0.3270)\n",
      "45475 Training Loss: tensor(0.3247)\n",
      "45476 Training Loss: tensor(0.3241)\n",
      "45477 Training Loss: tensor(0.3262)\n",
      "45478 Training Loss: tensor(0.3252)\n",
      "45479 Training Loss: tensor(0.3250)\n",
      "45480 Training Loss: tensor(0.3243)\n",
      "45481 Training Loss: tensor(0.3247)\n",
      "45482 Training Loss: tensor(0.3249)\n",
      "45483 Training Loss: tensor(0.3248)\n",
      "45484 Training Loss: tensor(0.3242)\n",
      "45485 Training Loss: tensor(0.3251)\n",
      "45486 Training Loss: tensor(0.3246)\n",
      "45487 Training Loss: tensor(0.3246)\n",
      "45488 Training Loss: tensor(0.3244)\n",
      "45489 Training Loss: tensor(0.3247)\n",
      "45490 Training Loss: tensor(0.3241)\n",
      "45491 Training Loss: tensor(0.3240)\n",
      "45492 Training Loss: tensor(0.3239)\n",
      "45493 Training Loss: tensor(0.3248)\n",
      "45494 Training Loss: tensor(0.3257)\n",
      "45495 Training Loss: tensor(0.3242)\n",
      "45496 Training Loss: tensor(0.3242)\n",
      "45497 Training Loss: tensor(0.3241)\n",
      "45498 Training Loss: tensor(0.3253)\n",
      "45499 Training Loss: tensor(0.3246)\n",
      "45500 Training Loss: tensor(0.3234)\n",
      "45501 Training Loss: tensor(0.3251)\n",
      "45502 Training Loss: tensor(0.3236)\n",
      "45503 Training Loss: tensor(0.3253)\n",
      "45504 Training Loss: tensor(0.3248)\n",
      "45505 Training Loss: tensor(0.3247)\n",
      "45506 Training Loss: tensor(0.3253)\n",
      "45507 Training Loss: tensor(0.3241)\n",
      "45508 Training Loss: tensor(0.3249)\n",
      "45509 Training Loss: tensor(0.3245)\n",
      "45510 Training Loss: tensor(0.3250)\n",
      "45511 Training Loss: tensor(0.3247)\n",
      "45512 Training Loss: tensor(0.3245)\n",
      "45513 Training Loss: tensor(0.3239)\n",
      "45514 Training Loss: tensor(0.3249)\n",
      "45515 Training Loss: tensor(0.3239)\n",
      "45516 Training Loss: tensor(0.3256)\n",
      "45517 Training Loss: tensor(0.3242)\n",
      "45518 Training Loss: tensor(0.3247)\n",
      "45519 Training Loss: tensor(0.3240)\n",
      "45520 Training Loss: tensor(0.3250)\n",
      "45521 Training Loss: tensor(0.3240)\n",
      "45522 Training Loss: tensor(0.3240)\n",
      "45523 Training Loss: tensor(0.3242)\n",
      "45524 Training Loss: tensor(0.3239)\n",
      "45525 Training Loss: tensor(0.3245)\n",
      "45526 Training Loss: tensor(0.3242)\n",
      "45527 Training Loss: tensor(0.3240)\n",
      "45528 Training Loss: tensor(0.3239)\n",
      "45529 Training Loss: tensor(0.3252)\n",
      "45530 Training Loss: tensor(0.3250)\n",
      "45531 Training Loss: tensor(0.3241)\n",
      "45532 Training Loss: tensor(0.3236)\n",
      "45533 Training Loss: tensor(0.3253)\n",
      "45534 Training Loss: tensor(0.3255)\n",
      "45535 Training Loss: tensor(0.3251)\n",
      "45536 Training Loss: tensor(0.3240)\n",
      "45537 Training Loss: tensor(0.3244)\n",
      "45538 Training Loss: tensor(0.3250)\n",
      "45539 Training Loss: tensor(0.3246)\n",
      "45540 Training Loss: tensor(0.3242)\n",
      "45541 Training Loss: tensor(0.3240)\n",
      "45542 Training Loss: tensor(0.3239)\n",
      "45543 Training Loss: tensor(0.3249)\n",
      "45544 Training Loss: tensor(0.3255)\n",
      "45545 Training Loss: tensor(0.3236)\n",
      "45546 Training Loss: tensor(0.3243)\n",
      "45547 Training Loss: tensor(0.3246)\n",
      "45548 Training Loss: tensor(0.3248)\n",
      "45549 Training Loss: tensor(0.3251)\n",
      "45550 Training Loss: tensor(0.3247)\n",
      "45551 Training Loss: tensor(0.3249)\n",
      "45552 Training Loss: tensor(0.3251)\n",
      "45553 Training Loss: tensor(0.3254)\n",
      "45554 Training Loss: tensor(0.3249)\n",
      "45555 Training Loss: tensor(0.3242)\n",
      "45556 Training Loss: tensor(0.3241)\n",
      "45557 Training Loss: tensor(0.3249)\n",
      "45558 Training Loss: tensor(0.3249)\n",
      "45559 Training Loss: tensor(0.3246)\n",
      "45560 Training Loss: tensor(0.3241)\n",
      "45561 Training Loss: tensor(0.3242)\n",
      "45562 Training Loss: tensor(0.3243)\n",
      "45563 Training Loss: tensor(0.3246)\n",
      "45564 Training Loss: tensor(0.3260)\n",
      "45565 Training Loss: tensor(0.3251)\n",
      "45566 Training Loss: tensor(0.3244)\n",
      "45567 Training Loss: tensor(0.3238)\n",
      "45568 Training Loss: tensor(0.3239)\n",
      "45569 Training Loss: tensor(0.3248)\n",
      "45570 Training Loss: tensor(0.3236)\n",
      "45571 Training Loss: tensor(0.3252)\n",
      "45572 Training Loss: tensor(0.3238)\n",
      "45573 Training Loss: tensor(0.3236)\n",
      "45574 Training Loss: tensor(0.3250)\n",
      "45575 Training Loss: tensor(0.3243)\n",
      "45576 Training Loss: tensor(0.3250)\n",
      "45577 Training Loss: tensor(0.3244)\n",
      "45578 Training Loss: tensor(0.3238)\n",
      "45579 Training Loss: tensor(0.3250)\n",
      "45580 Training Loss: tensor(0.3242)\n",
      "45581 Training Loss: tensor(0.3240)\n",
      "45582 Training Loss: tensor(0.3240)\n",
      "45583 Training Loss: tensor(0.3254)\n",
      "45584 Training Loss: tensor(0.3240)\n",
      "45585 Training Loss: tensor(0.3238)\n",
      "45586 Training Loss: tensor(0.3259)\n",
      "45587 Training Loss: tensor(0.3246)\n",
      "45588 Training Loss: tensor(0.3245)\n",
      "45589 Training Loss: tensor(0.3244)\n",
      "45590 Training Loss: tensor(0.3241)\n",
      "45591 Training Loss: tensor(0.3248)\n",
      "45592 Training Loss: tensor(0.3247)\n",
      "45593 Training Loss: tensor(0.3239)\n",
      "45594 Training Loss: tensor(0.3240)\n",
      "45595 Training Loss: tensor(0.3243)\n",
      "45596 Training Loss: tensor(0.3244)\n",
      "45597 Training Loss: tensor(0.3242)\n",
      "45598 Training Loss: tensor(0.3247)\n",
      "45599 Training Loss: tensor(0.3238)\n",
      "45600 Training Loss: tensor(0.3242)\n",
      "45601 Training Loss: tensor(0.3239)\n",
      "45602 Training Loss: tensor(0.3246)\n",
      "45603 Training Loss: tensor(0.3252)\n",
      "45604 Training Loss: tensor(0.3245)\n",
      "45605 Training Loss: tensor(0.3244)\n",
      "45606 Training Loss: tensor(0.3235)\n",
      "45607 Training Loss: tensor(0.3239)\n",
      "45608 Training Loss: tensor(0.3239)\n",
      "45609 Training Loss: tensor(0.3248)\n",
      "45610 Training Loss: tensor(0.3249)\n",
      "45611 Training Loss: tensor(0.3246)\n",
      "45612 Training Loss: tensor(0.3248)\n",
      "45613 Training Loss: tensor(0.3243)\n",
      "45614 Training Loss: tensor(0.3240)\n",
      "45615 Training Loss: tensor(0.3252)\n",
      "45616 Training Loss: tensor(0.3244)\n",
      "45617 Training Loss: tensor(0.3236)\n",
      "45618 Training Loss: tensor(0.3243)\n",
      "45619 Training Loss: tensor(0.3247)\n",
      "45620 Training Loss: tensor(0.3252)\n",
      "45621 Training Loss: tensor(0.3234)\n",
      "45622 Training Loss: tensor(0.3247)\n",
      "45623 Training Loss: tensor(0.3240)\n",
      "45624 Training Loss: tensor(0.3243)\n",
      "45625 Training Loss: tensor(0.3239)\n",
      "45626 Training Loss: tensor(0.3240)\n",
      "45627 Training Loss: tensor(0.3255)\n",
      "45628 Training Loss: tensor(0.3255)\n",
      "45629 Training Loss: tensor(0.3245)\n",
      "45630 Training Loss: tensor(0.3243)\n",
      "45631 Training Loss: tensor(0.3239)\n",
      "45632 Training Loss: tensor(0.3239)\n",
      "45633 Training Loss: tensor(0.3251)\n",
      "45634 Training Loss: tensor(0.3248)\n",
      "45635 Training Loss: tensor(0.3243)\n",
      "45636 Training Loss: tensor(0.3240)\n",
      "45637 Training Loss: tensor(0.3246)\n",
      "45638 Training Loss: tensor(0.3254)\n",
      "45639 Training Loss: tensor(0.3251)\n",
      "45640 Training Loss: tensor(0.3243)\n",
      "45641 Training Loss: tensor(0.3242)\n",
      "45642 Training Loss: tensor(0.3250)\n",
      "45643 Training Loss: tensor(0.3242)\n",
      "45644 Training Loss: tensor(0.3237)\n",
      "45645 Training Loss: tensor(0.3249)\n",
      "45646 Training Loss: tensor(0.3238)\n",
      "45647 Training Loss: tensor(0.3242)\n",
      "45648 Training Loss: tensor(0.3241)\n",
      "45649 Training Loss: tensor(0.3239)\n",
      "45650 Training Loss: tensor(0.3247)\n",
      "45651 Training Loss: tensor(0.3245)\n",
      "45652 Training Loss: tensor(0.3246)\n",
      "45653 Training Loss: tensor(0.3237)\n",
      "45654 Training Loss: tensor(0.3252)\n",
      "45655 Training Loss: tensor(0.3239)\n",
      "45656 Training Loss: tensor(0.3241)\n",
      "45657 Training Loss: tensor(0.3244)\n",
      "45658 Training Loss: tensor(0.3237)\n",
      "45659 Training Loss: tensor(0.3240)\n",
      "45660 Training Loss: tensor(0.3243)\n",
      "45661 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45662 Training Loss: tensor(0.3245)\n",
      "45663 Training Loss: tensor(0.3243)\n",
      "45664 Training Loss: tensor(0.3246)\n",
      "45665 Training Loss: tensor(0.3242)\n",
      "45666 Training Loss: tensor(0.3242)\n",
      "45667 Training Loss: tensor(0.3255)\n",
      "45668 Training Loss: tensor(0.3243)\n",
      "45669 Training Loss: tensor(0.3240)\n",
      "45670 Training Loss: tensor(0.3236)\n",
      "45671 Training Loss: tensor(0.3251)\n",
      "45672 Training Loss: tensor(0.3244)\n",
      "45673 Training Loss: tensor(0.3237)\n",
      "45674 Training Loss: tensor(0.3242)\n",
      "45675 Training Loss: tensor(0.3249)\n",
      "45676 Training Loss: tensor(0.3239)\n",
      "45677 Training Loss: tensor(0.3243)\n",
      "45678 Training Loss: tensor(0.3245)\n",
      "45679 Training Loss: tensor(0.3239)\n",
      "45680 Training Loss: tensor(0.3247)\n",
      "45681 Training Loss: tensor(0.3248)\n",
      "45682 Training Loss: tensor(0.3241)\n",
      "45683 Training Loss: tensor(0.3239)\n",
      "45684 Training Loss: tensor(0.3237)\n",
      "45685 Training Loss: tensor(0.3239)\n",
      "45686 Training Loss: tensor(0.3243)\n",
      "45687 Training Loss: tensor(0.3242)\n",
      "45688 Training Loss: tensor(0.3241)\n",
      "45689 Training Loss: tensor(0.3243)\n",
      "45690 Training Loss: tensor(0.3246)\n",
      "45691 Training Loss: tensor(0.3241)\n",
      "45692 Training Loss: tensor(0.3242)\n",
      "45693 Training Loss: tensor(0.3245)\n",
      "45694 Training Loss: tensor(0.3246)\n",
      "45695 Training Loss: tensor(0.3237)\n",
      "45696 Training Loss: tensor(0.3254)\n",
      "45697 Training Loss: tensor(0.3241)\n",
      "45698 Training Loss: tensor(0.3246)\n",
      "45699 Training Loss: tensor(0.3241)\n",
      "45700 Training Loss: tensor(0.3244)\n",
      "45701 Training Loss: tensor(0.3239)\n",
      "45702 Training Loss: tensor(0.3245)\n",
      "45703 Training Loss: tensor(0.3245)\n",
      "45704 Training Loss: tensor(0.3254)\n",
      "45705 Training Loss: tensor(0.3243)\n",
      "45706 Training Loss: tensor(0.3239)\n",
      "45707 Training Loss: tensor(0.3246)\n",
      "45708 Training Loss: tensor(0.3253)\n",
      "45709 Training Loss: tensor(0.3253)\n",
      "45710 Training Loss: tensor(0.3241)\n",
      "45711 Training Loss: tensor(0.3244)\n",
      "45712 Training Loss: tensor(0.3244)\n",
      "45713 Training Loss: tensor(0.3242)\n",
      "45714 Training Loss: tensor(0.3241)\n",
      "45715 Training Loss: tensor(0.3241)\n",
      "45716 Training Loss: tensor(0.3248)\n",
      "45717 Training Loss: tensor(0.3245)\n",
      "45718 Training Loss: tensor(0.3246)\n",
      "45719 Training Loss: tensor(0.3240)\n",
      "45720 Training Loss: tensor(0.3247)\n",
      "45721 Training Loss: tensor(0.3253)\n",
      "45722 Training Loss: tensor(0.3245)\n",
      "45723 Training Loss: tensor(0.3240)\n",
      "45724 Training Loss: tensor(0.3247)\n",
      "45725 Training Loss: tensor(0.3249)\n",
      "45726 Training Loss: tensor(0.3254)\n",
      "45727 Training Loss: tensor(0.3248)\n",
      "45728 Training Loss: tensor(0.3252)\n",
      "45729 Training Loss: tensor(0.3245)\n",
      "45730 Training Loss: tensor(0.3256)\n",
      "45731 Training Loss: tensor(0.3243)\n",
      "45732 Training Loss: tensor(0.3239)\n",
      "45733 Training Loss: tensor(0.3246)\n",
      "45734 Training Loss: tensor(0.3243)\n",
      "45735 Training Loss: tensor(0.3242)\n",
      "45736 Training Loss: tensor(0.3248)\n",
      "45737 Training Loss: tensor(0.3245)\n",
      "45738 Training Loss: tensor(0.3246)\n",
      "45739 Training Loss: tensor(0.3241)\n",
      "45740 Training Loss: tensor(0.3246)\n",
      "45741 Training Loss: tensor(0.3247)\n",
      "45742 Training Loss: tensor(0.3238)\n",
      "45743 Training Loss: tensor(0.3243)\n",
      "45744 Training Loss: tensor(0.3246)\n",
      "45745 Training Loss: tensor(0.3241)\n",
      "45746 Training Loss: tensor(0.3250)\n",
      "45747 Training Loss: tensor(0.3250)\n",
      "45748 Training Loss: tensor(0.3241)\n",
      "45749 Training Loss: tensor(0.3242)\n",
      "45750 Training Loss: tensor(0.3240)\n",
      "45751 Training Loss: tensor(0.3242)\n",
      "45752 Training Loss: tensor(0.3251)\n",
      "45753 Training Loss: tensor(0.3248)\n",
      "45754 Training Loss: tensor(0.3252)\n",
      "45755 Training Loss: tensor(0.3244)\n",
      "45756 Training Loss: tensor(0.3237)\n",
      "45757 Training Loss: tensor(0.3241)\n",
      "45758 Training Loss: tensor(0.3236)\n",
      "45759 Training Loss: tensor(0.3251)\n",
      "45760 Training Loss: tensor(0.3246)\n",
      "45761 Training Loss: tensor(0.3245)\n",
      "45762 Training Loss: tensor(0.3244)\n",
      "45763 Training Loss: tensor(0.3245)\n",
      "45764 Training Loss: tensor(0.3250)\n",
      "45765 Training Loss: tensor(0.3245)\n",
      "45766 Training Loss: tensor(0.3248)\n",
      "45767 Training Loss: tensor(0.3248)\n",
      "45768 Training Loss: tensor(0.3241)\n",
      "45769 Training Loss: tensor(0.3237)\n",
      "45770 Training Loss: tensor(0.3248)\n",
      "45771 Training Loss: tensor(0.3247)\n",
      "45772 Training Loss: tensor(0.3254)\n",
      "45773 Training Loss: tensor(0.3248)\n",
      "45774 Training Loss: tensor(0.3251)\n",
      "45775 Training Loss: tensor(0.3246)\n",
      "45776 Training Loss: tensor(0.3243)\n",
      "45777 Training Loss: tensor(0.3246)\n",
      "45778 Training Loss: tensor(0.3253)\n",
      "45779 Training Loss: tensor(0.3240)\n",
      "45780 Training Loss: tensor(0.3245)\n",
      "45781 Training Loss: tensor(0.3243)\n",
      "45782 Training Loss: tensor(0.3248)\n",
      "45783 Training Loss: tensor(0.3250)\n",
      "45784 Training Loss: tensor(0.3244)\n",
      "45785 Training Loss: tensor(0.3243)\n",
      "45786 Training Loss: tensor(0.3245)\n",
      "45787 Training Loss: tensor(0.3240)\n",
      "45788 Training Loss: tensor(0.3239)\n",
      "45789 Training Loss: tensor(0.3242)\n",
      "45790 Training Loss: tensor(0.3240)\n",
      "45791 Training Loss: tensor(0.3247)\n",
      "45792 Training Loss: tensor(0.3238)\n",
      "45793 Training Loss: tensor(0.3243)\n",
      "45794 Training Loss: tensor(0.3245)\n",
      "45795 Training Loss: tensor(0.3240)\n",
      "45796 Training Loss: tensor(0.3244)\n",
      "45797 Training Loss: tensor(0.3240)\n",
      "45798 Training Loss: tensor(0.3249)\n",
      "45799 Training Loss: tensor(0.3241)\n",
      "45800 Training Loss: tensor(0.3247)\n",
      "45801 Training Loss: tensor(0.3237)\n",
      "45802 Training Loss: tensor(0.3240)\n",
      "45803 Training Loss: tensor(0.3235)\n",
      "45804 Training Loss: tensor(0.3246)\n",
      "45805 Training Loss: tensor(0.3237)\n",
      "45806 Training Loss: tensor(0.3239)\n",
      "45807 Training Loss: tensor(0.3244)\n",
      "45808 Training Loss: tensor(0.3244)\n",
      "45809 Training Loss: tensor(0.3245)\n",
      "45810 Training Loss: tensor(0.3242)\n",
      "45811 Training Loss: tensor(0.3246)\n",
      "45812 Training Loss: tensor(0.3248)\n",
      "45813 Training Loss: tensor(0.3248)\n",
      "45814 Training Loss: tensor(0.3244)\n",
      "45815 Training Loss: tensor(0.3240)\n",
      "45816 Training Loss: tensor(0.3236)\n",
      "45817 Training Loss: tensor(0.3245)\n",
      "45818 Training Loss: tensor(0.3249)\n",
      "45819 Training Loss: tensor(0.3246)\n",
      "45820 Training Loss: tensor(0.3246)\n",
      "45821 Training Loss: tensor(0.3233)\n",
      "45822 Training Loss: tensor(0.3244)\n",
      "45823 Training Loss: tensor(0.3246)\n",
      "45824 Training Loss: tensor(0.3250)\n",
      "45825 Training Loss: tensor(0.3249)\n",
      "45826 Training Loss: tensor(0.3247)\n",
      "45827 Training Loss: tensor(0.3251)\n",
      "45828 Training Loss: tensor(0.3237)\n",
      "45829 Training Loss: tensor(0.3244)\n",
      "45830 Training Loss: tensor(0.3248)\n",
      "45831 Training Loss: tensor(0.3239)\n",
      "45832 Training Loss: tensor(0.3241)\n",
      "45833 Training Loss: tensor(0.3246)\n",
      "45834 Training Loss: tensor(0.3236)\n",
      "45835 Training Loss: tensor(0.3254)\n",
      "45836 Training Loss: tensor(0.3245)\n",
      "45837 Training Loss: tensor(0.3249)\n",
      "45838 Training Loss: tensor(0.3245)\n",
      "45839 Training Loss: tensor(0.3235)\n",
      "45840 Training Loss: tensor(0.3242)\n",
      "45841 Training Loss: tensor(0.3241)\n",
      "45842 Training Loss: tensor(0.3242)\n",
      "45843 Training Loss: tensor(0.3249)\n",
      "45844 Training Loss: tensor(0.3243)\n",
      "45845 Training Loss: tensor(0.3238)\n",
      "45846 Training Loss: tensor(0.3244)\n",
      "45847 Training Loss: tensor(0.3243)\n",
      "45848 Training Loss: tensor(0.3240)\n",
      "45849 Training Loss: tensor(0.3239)\n",
      "45850 Training Loss: tensor(0.3245)\n",
      "45851 Training Loss: tensor(0.3242)\n",
      "45852 Training Loss: tensor(0.3245)\n",
      "45853 Training Loss: tensor(0.3248)\n",
      "45854 Training Loss: tensor(0.3243)\n",
      "45855 Training Loss: tensor(0.3248)\n",
      "45856 Training Loss: tensor(0.3241)\n",
      "45857 Training Loss: tensor(0.3242)\n",
      "45858 Training Loss: tensor(0.3245)\n",
      "45859 Training Loss: tensor(0.3246)\n",
      "45860 Training Loss: tensor(0.3241)\n",
      "45861 Training Loss: tensor(0.3242)\n",
      "45862 Training Loss: tensor(0.3246)\n",
      "45863 Training Loss: tensor(0.3238)\n",
      "45864 Training Loss: tensor(0.3259)\n",
      "45865 Training Loss: tensor(0.3240)\n",
      "45866 Training Loss: tensor(0.3238)\n",
      "45867 Training Loss: tensor(0.3241)\n",
      "45868 Training Loss: tensor(0.3238)\n",
      "45869 Training Loss: tensor(0.3239)\n",
      "45870 Training Loss: tensor(0.3248)\n",
      "45871 Training Loss: tensor(0.3248)\n",
      "45872 Training Loss: tensor(0.3238)\n",
      "45873 Training Loss: tensor(0.3244)\n",
      "45874 Training Loss: tensor(0.3240)\n",
      "45875 Training Loss: tensor(0.3239)\n",
      "45876 Training Loss: tensor(0.3251)\n",
      "45877 Training Loss: tensor(0.3245)\n",
      "45878 Training Loss: tensor(0.3246)\n",
      "45879 Training Loss: tensor(0.3247)\n",
      "45880 Training Loss: tensor(0.3248)\n",
      "45881 Training Loss: tensor(0.3248)\n",
      "45882 Training Loss: tensor(0.3246)\n",
      "45883 Training Loss: tensor(0.3248)\n",
      "45884 Training Loss: tensor(0.3242)\n",
      "45885 Training Loss: tensor(0.3238)\n",
      "45886 Training Loss: tensor(0.3243)\n",
      "45887 Training Loss: tensor(0.3239)\n",
      "45888 Training Loss: tensor(0.3244)\n",
      "45889 Training Loss: tensor(0.3243)\n",
      "45890 Training Loss: tensor(0.3245)\n",
      "45891 Training Loss: tensor(0.3246)\n",
      "45892 Training Loss: tensor(0.3244)\n",
      "45893 Training Loss: tensor(0.3237)\n",
      "45894 Training Loss: tensor(0.3240)\n",
      "45895 Training Loss: tensor(0.3237)\n",
      "45896 Training Loss: tensor(0.3241)\n",
      "45897 Training Loss: tensor(0.3241)\n",
      "45898 Training Loss: tensor(0.3237)\n",
      "45899 Training Loss: tensor(0.3257)\n",
      "45900 Training Loss: tensor(0.3248)\n",
      "45901 Training Loss: tensor(0.3234)\n",
      "45902 Training Loss: tensor(0.3242)\n",
      "45903 Training Loss: tensor(0.3258)\n",
      "45904 Training Loss: tensor(0.3255)\n",
      "45905 Training Loss: tensor(0.3247)\n",
      "45906 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45907 Training Loss: tensor(0.3247)\n",
      "45908 Training Loss: tensor(0.3245)\n",
      "45909 Training Loss: tensor(0.3251)\n",
      "45910 Training Loss: tensor(0.3245)\n",
      "45911 Training Loss: tensor(0.3239)\n",
      "45912 Training Loss: tensor(0.3249)\n",
      "45913 Training Loss: tensor(0.3240)\n",
      "45914 Training Loss: tensor(0.3253)\n",
      "45915 Training Loss: tensor(0.3248)\n",
      "45916 Training Loss: tensor(0.3247)\n",
      "45917 Training Loss: tensor(0.3243)\n",
      "45918 Training Loss: tensor(0.3238)\n",
      "45919 Training Loss: tensor(0.3240)\n",
      "45920 Training Loss: tensor(0.3243)\n",
      "45921 Training Loss: tensor(0.3245)\n",
      "45922 Training Loss: tensor(0.3243)\n",
      "45923 Training Loss: tensor(0.3237)\n",
      "45924 Training Loss: tensor(0.3242)\n",
      "45925 Training Loss: tensor(0.3251)\n",
      "45926 Training Loss: tensor(0.3240)\n",
      "45927 Training Loss: tensor(0.3242)\n",
      "45928 Training Loss: tensor(0.3246)\n",
      "45929 Training Loss: tensor(0.3252)\n",
      "45930 Training Loss: tensor(0.3242)\n",
      "45931 Training Loss: tensor(0.3254)\n",
      "45932 Training Loss: tensor(0.3255)\n",
      "45933 Training Loss: tensor(0.3245)\n",
      "45934 Training Loss: tensor(0.3245)\n",
      "45935 Training Loss: tensor(0.3246)\n",
      "45936 Training Loss: tensor(0.3244)\n",
      "45937 Training Loss: tensor(0.3244)\n",
      "45938 Training Loss: tensor(0.3239)\n",
      "45939 Training Loss: tensor(0.3241)\n",
      "45940 Training Loss: tensor(0.3248)\n",
      "45941 Training Loss: tensor(0.3240)\n",
      "45942 Training Loss: tensor(0.3240)\n",
      "45943 Training Loss: tensor(0.3243)\n",
      "45944 Training Loss: tensor(0.3251)\n",
      "45945 Training Loss: tensor(0.3241)\n",
      "45946 Training Loss: tensor(0.3242)\n",
      "45947 Training Loss: tensor(0.3242)\n",
      "45948 Training Loss: tensor(0.3238)\n",
      "45949 Training Loss: tensor(0.3244)\n",
      "45950 Training Loss: tensor(0.3244)\n",
      "45951 Training Loss: tensor(0.3238)\n",
      "45952 Training Loss: tensor(0.3238)\n",
      "45953 Training Loss: tensor(0.3242)\n",
      "45954 Training Loss: tensor(0.3247)\n",
      "45955 Training Loss: tensor(0.3241)\n",
      "45956 Training Loss: tensor(0.3256)\n",
      "45957 Training Loss: tensor(0.3250)\n",
      "45958 Training Loss: tensor(0.3255)\n",
      "45959 Training Loss: tensor(0.3248)\n",
      "45960 Training Loss: tensor(0.3245)\n",
      "45961 Training Loss: tensor(0.3247)\n",
      "45962 Training Loss: tensor(0.3243)\n",
      "45963 Training Loss: tensor(0.3241)\n",
      "45964 Training Loss: tensor(0.3248)\n",
      "45965 Training Loss: tensor(0.3246)\n",
      "45966 Training Loss: tensor(0.3238)\n",
      "45967 Training Loss: tensor(0.3249)\n",
      "45968 Training Loss: tensor(0.3248)\n",
      "45969 Training Loss: tensor(0.3246)\n",
      "45970 Training Loss: tensor(0.3242)\n",
      "45971 Training Loss: tensor(0.3250)\n",
      "45972 Training Loss: tensor(0.3259)\n",
      "45973 Training Loss: tensor(0.3249)\n",
      "45974 Training Loss: tensor(0.3245)\n",
      "45975 Training Loss: tensor(0.3249)\n",
      "45976 Training Loss: tensor(0.3244)\n",
      "45977 Training Loss: tensor(0.3247)\n",
      "45978 Training Loss: tensor(0.3249)\n",
      "45979 Training Loss: tensor(0.3243)\n",
      "45980 Training Loss: tensor(0.3241)\n",
      "45981 Training Loss: tensor(0.3243)\n",
      "45982 Training Loss: tensor(0.3240)\n",
      "45983 Training Loss: tensor(0.3236)\n",
      "45984 Training Loss: tensor(0.3250)\n",
      "45985 Training Loss: tensor(0.3242)\n",
      "45986 Training Loss: tensor(0.3243)\n",
      "45987 Training Loss: tensor(0.3248)\n",
      "45988 Training Loss: tensor(0.3244)\n",
      "45989 Training Loss: tensor(0.3244)\n",
      "45990 Training Loss: tensor(0.3246)\n",
      "45991 Training Loss: tensor(0.3243)\n",
      "45992 Training Loss: tensor(0.3248)\n",
      "45993 Training Loss: tensor(0.3241)\n",
      "45994 Training Loss: tensor(0.3244)\n",
      "45995 Training Loss: tensor(0.3243)\n",
      "45996 Training Loss: tensor(0.3247)\n",
      "45997 Training Loss: tensor(0.3259)\n",
      "45998 Training Loss: tensor(0.3239)\n",
      "45999 Training Loss: tensor(0.3245)\n",
      "46000 Training Loss: tensor(0.3241)\n",
      "46001 Training Loss: tensor(0.3242)\n",
      "46002 Training Loss: tensor(0.3257)\n",
      "46003 Training Loss: tensor(0.3247)\n",
      "46004 Training Loss: tensor(0.3251)\n",
      "46005 Training Loss: tensor(0.3246)\n",
      "46006 Training Loss: tensor(0.3245)\n",
      "46007 Training Loss: tensor(0.3239)\n",
      "46008 Training Loss: tensor(0.3245)\n",
      "46009 Training Loss: tensor(0.3248)\n",
      "46010 Training Loss: tensor(0.3243)\n",
      "46011 Training Loss: tensor(0.3244)\n",
      "46012 Training Loss: tensor(0.3237)\n",
      "46013 Training Loss: tensor(0.3242)\n",
      "46014 Training Loss: tensor(0.3242)\n",
      "46015 Training Loss: tensor(0.3244)\n",
      "46016 Training Loss: tensor(0.3255)\n",
      "46017 Training Loss: tensor(0.3238)\n",
      "46018 Training Loss: tensor(0.3239)\n",
      "46019 Training Loss: tensor(0.3243)\n",
      "46020 Training Loss: tensor(0.3245)\n",
      "46021 Training Loss: tensor(0.3247)\n",
      "46022 Training Loss: tensor(0.3248)\n",
      "46023 Training Loss: tensor(0.3244)\n",
      "46024 Training Loss: tensor(0.3244)\n",
      "46025 Training Loss: tensor(0.3239)\n",
      "46026 Training Loss: tensor(0.3245)\n",
      "46027 Training Loss: tensor(0.3244)\n",
      "46028 Training Loss: tensor(0.3240)\n",
      "46029 Training Loss: tensor(0.3244)\n",
      "46030 Training Loss: tensor(0.3247)\n",
      "46031 Training Loss: tensor(0.3258)\n",
      "46032 Training Loss: tensor(0.3241)\n",
      "46033 Training Loss: tensor(0.3242)\n",
      "46034 Training Loss: tensor(0.3244)\n",
      "46035 Training Loss: tensor(0.3241)\n",
      "46036 Training Loss: tensor(0.3238)\n",
      "46037 Training Loss: tensor(0.3246)\n",
      "46038 Training Loss: tensor(0.3241)\n",
      "46039 Training Loss: tensor(0.3245)\n",
      "46040 Training Loss: tensor(0.3248)\n",
      "46041 Training Loss: tensor(0.3238)\n",
      "46042 Training Loss: tensor(0.3241)\n",
      "46043 Training Loss: tensor(0.3240)\n",
      "46044 Training Loss: tensor(0.3239)\n",
      "46045 Training Loss: tensor(0.3250)\n",
      "46046 Training Loss: tensor(0.3239)\n",
      "46047 Training Loss: tensor(0.3249)\n",
      "46048 Training Loss: tensor(0.3242)\n",
      "46049 Training Loss: tensor(0.3246)\n",
      "46050 Training Loss: tensor(0.3251)\n",
      "46051 Training Loss: tensor(0.3243)\n",
      "46052 Training Loss: tensor(0.3248)\n",
      "46053 Training Loss: tensor(0.3251)\n",
      "46054 Training Loss: tensor(0.3239)\n",
      "46055 Training Loss: tensor(0.3255)\n",
      "46056 Training Loss: tensor(0.3240)\n",
      "46057 Training Loss: tensor(0.3243)\n",
      "46058 Training Loss: tensor(0.3253)\n",
      "46059 Training Loss: tensor(0.3244)\n",
      "46060 Training Loss: tensor(0.3247)\n",
      "46061 Training Loss: tensor(0.3245)\n",
      "46062 Training Loss: tensor(0.3242)\n",
      "46063 Training Loss: tensor(0.3245)\n",
      "46064 Training Loss: tensor(0.3245)\n",
      "46065 Training Loss: tensor(0.3246)\n",
      "46066 Training Loss: tensor(0.3241)\n",
      "46067 Training Loss: tensor(0.3243)\n",
      "46068 Training Loss: tensor(0.3246)\n",
      "46069 Training Loss: tensor(0.3246)\n",
      "46070 Training Loss: tensor(0.3248)\n",
      "46071 Training Loss: tensor(0.3236)\n",
      "46072 Training Loss: tensor(0.3243)\n",
      "46073 Training Loss: tensor(0.3237)\n",
      "46074 Training Loss: tensor(0.3245)\n",
      "46075 Training Loss: tensor(0.3245)\n",
      "46076 Training Loss: tensor(0.3240)\n",
      "46077 Training Loss: tensor(0.3242)\n",
      "46078 Training Loss: tensor(0.3245)\n",
      "46079 Training Loss: tensor(0.3247)\n",
      "46080 Training Loss: tensor(0.3241)\n",
      "46081 Training Loss: tensor(0.3242)\n",
      "46082 Training Loss: tensor(0.3241)\n",
      "46083 Training Loss: tensor(0.3250)\n",
      "46084 Training Loss: tensor(0.3247)\n",
      "46085 Training Loss: tensor(0.3242)\n",
      "46086 Training Loss: tensor(0.3253)\n",
      "46087 Training Loss: tensor(0.3241)\n",
      "46088 Training Loss: tensor(0.3252)\n",
      "46089 Training Loss: tensor(0.3243)\n",
      "46090 Training Loss: tensor(0.3251)\n",
      "46091 Training Loss: tensor(0.3241)\n",
      "46092 Training Loss: tensor(0.3248)\n",
      "46093 Training Loss: tensor(0.3246)\n",
      "46094 Training Loss: tensor(0.3241)\n",
      "46095 Training Loss: tensor(0.3234)\n",
      "46096 Training Loss: tensor(0.3240)\n",
      "46097 Training Loss: tensor(0.3249)\n",
      "46098 Training Loss: tensor(0.3244)\n",
      "46099 Training Loss: tensor(0.3243)\n",
      "46100 Training Loss: tensor(0.3245)\n",
      "46101 Training Loss: tensor(0.3240)\n",
      "46102 Training Loss: tensor(0.3241)\n",
      "46103 Training Loss: tensor(0.3244)\n",
      "46104 Training Loss: tensor(0.3254)\n",
      "46105 Training Loss: tensor(0.3258)\n",
      "46106 Training Loss: tensor(0.3242)\n",
      "46107 Training Loss: tensor(0.3238)\n",
      "46108 Training Loss: tensor(0.3242)\n",
      "46109 Training Loss: tensor(0.3241)\n",
      "46110 Training Loss: tensor(0.3241)\n",
      "46111 Training Loss: tensor(0.3242)\n",
      "46112 Training Loss: tensor(0.3243)\n",
      "46113 Training Loss: tensor(0.3244)\n",
      "46114 Training Loss: tensor(0.3248)\n",
      "46115 Training Loss: tensor(0.3243)\n",
      "46116 Training Loss: tensor(0.3242)\n",
      "46117 Training Loss: tensor(0.3241)\n",
      "46118 Training Loss: tensor(0.3250)\n",
      "46119 Training Loss: tensor(0.3258)\n",
      "46120 Training Loss: tensor(0.3252)\n",
      "46121 Training Loss: tensor(0.3243)\n",
      "46122 Training Loss: tensor(0.3247)\n",
      "46123 Training Loss: tensor(0.3243)\n",
      "46124 Training Loss: tensor(0.3266)\n",
      "46125 Training Loss: tensor(0.3239)\n",
      "46126 Training Loss: tensor(0.3244)\n",
      "46127 Training Loss: tensor(0.3248)\n",
      "46128 Training Loss: tensor(0.3247)\n",
      "46129 Training Loss: tensor(0.3237)\n",
      "46130 Training Loss: tensor(0.3245)\n",
      "46131 Training Loss: tensor(0.3258)\n",
      "46132 Training Loss: tensor(0.3242)\n",
      "46133 Training Loss: tensor(0.3244)\n",
      "46134 Training Loss: tensor(0.3245)\n",
      "46135 Training Loss: tensor(0.3248)\n",
      "46136 Training Loss: tensor(0.3264)\n",
      "46137 Training Loss: tensor(0.3245)\n",
      "46138 Training Loss: tensor(0.3247)\n",
      "46139 Training Loss: tensor(0.3252)\n",
      "46140 Training Loss: tensor(0.3257)\n",
      "46141 Training Loss: tensor(0.3248)\n",
      "46142 Training Loss: tensor(0.3240)\n",
      "46143 Training Loss: tensor(0.3239)\n",
      "46144 Training Loss: tensor(0.3246)\n",
      "46145 Training Loss: tensor(0.3248)\n",
      "46146 Training Loss: tensor(0.3241)\n",
      "46147 Training Loss: tensor(0.3245)\n",
      "46148 Training Loss: tensor(0.3246)\n",
      "46149 Training Loss: tensor(0.3240)\n",
      "46150 Training Loss: tensor(0.3241)\n",
      "46151 Training Loss: tensor(0.3244)\n",
      "46152 Training Loss: tensor(0.3251)\n",
      "46153 Training Loss: tensor(0.3242)\n",
      "46154 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46155 Training Loss: tensor(0.3238)\n",
      "46156 Training Loss: tensor(0.3240)\n",
      "46157 Training Loss: tensor(0.3239)\n",
      "46158 Training Loss: tensor(0.3237)\n",
      "46159 Training Loss: tensor(0.3238)\n",
      "46160 Training Loss: tensor(0.3255)\n",
      "46161 Training Loss: tensor(0.3251)\n",
      "46162 Training Loss: tensor(0.3254)\n",
      "46163 Training Loss: tensor(0.3243)\n",
      "46164 Training Loss: tensor(0.3237)\n",
      "46165 Training Loss: tensor(0.3248)\n",
      "46166 Training Loss: tensor(0.3245)\n",
      "46167 Training Loss: tensor(0.3240)\n",
      "46168 Training Loss: tensor(0.3241)\n",
      "46169 Training Loss: tensor(0.3238)\n",
      "46170 Training Loss: tensor(0.3247)\n",
      "46171 Training Loss: tensor(0.3246)\n",
      "46172 Training Loss: tensor(0.3243)\n",
      "46173 Training Loss: tensor(0.3243)\n",
      "46174 Training Loss: tensor(0.3240)\n",
      "46175 Training Loss: tensor(0.3242)\n",
      "46176 Training Loss: tensor(0.3259)\n",
      "46177 Training Loss: tensor(0.3246)\n",
      "46178 Training Loss: tensor(0.3243)\n",
      "46179 Training Loss: tensor(0.3251)\n",
      "46180 Training Loss: tensor(0.3250)\n",
      "46181 Training Loss: tensor(0.3244)\n",
      "46182 Training Loss: tensor(0.3248)\n",
      "46183 Training Loss: tensor(0.3248)\n",
      "46184 Training Loss: tensor(0.3253)\n",
      "46185 Training Loss: tensor(0.3246)\n",
      "46186 Training Loss: tensor(0.3244)\n",
      "46187 Training Loss: tensor(0.3247)\n",
      "46188 Training Loss: tensor(0.3241)\n",
      "46189 Training Loss: tensor(0.3239)\n",
      "46190 Training Loss: tensor(0.3236)\n",
      "46191 Training Loss: tensor(0.3246)\n",
      "46192 Training Loss: tensor(0.3240)\n",
      "46193 Training Loss: tensor(0.3244)\n",
      "46194 Training Loss: tensor(0.3238)\n",
      "46195 Training Loss: tensor(0.3238)\n",
      "46196 Training Loss: tensor(0.3245)\n",
      "46197 Training Loss: tensor(0.3241)\n",
      "46198 Training Loss: tensor(0.3242)\n",
      "46199 Training Loss: tensor(0.3241)\n",
      "46200 Training Loss: tensor(0.3244)\n",
      "46201 Training Loss: tensor(0.3244)\n",
      "46202 Training Loss: tensor(0.3244)\n",
      "46203 Training Loss: tensor(0.3244)\n",
      "46204 Training Loss: tensor(0.3244)\n",
      "46205 Training Loss: tensor(0.3250)\n",
      "46206 Training Loss: tensor(0.3232)\n",
      "46207 Training Loss: tensor(0.3249)\n",
      "46208 Training Loss: tensor(0.3248)\n",
      "46209 Training Loss: tensor(0.3239)\n",
      "46210 Training Loss: tensor(0.3245)\n",
      "46211 Training Loss: tensor(0.3242)\n",
      "46212 Training Loss: tensor(0.3246)\n",
      "46213 Training Loss: tensor(0.3250)\n",
      "46214 Training Loss: tensor(0.3247)\n",
      "46215 Training Loss: tensor(0.3247)\n",
      "46216 Training Loss: tensor(0.3244)\n",
      "46217 Training Loss: tensor(0.3248)\n",
      "46218 Training Loss: tensor(0.3248)\n",
      "46219 Training Loss: tensor(0.3248)\n",
      "46220 Training Loss: tensor(0.3238)\n",
      "46221 Training Loss: tensor(0.3254)\n",
      "46222 Training Loss: tensor(0.3240)\n",
      "46223 Training Loss: tensor(0.3254)\n",
      "46224 Training Loss: tensor(0.3250)\n",
      "46225 Training Loss: tensor(0.3242)\n",
      "46226 Training Loss: tensor(0.3250)\n",
      "46227 Training Loss: tensor(0.3245)\n",
      "46228 Training Loss: tensor(0.3237)\n",
      "46229 Training Loss: tensor(0.3244)\n",
      "46230 Training Loss: tensor(0.3244)\n",
      "46231 Training Loss: tensor(0.3241)\n",
      "46232 Training Loss: tensor(0.3244)\n",
      "46233 Training Loss: tensor(0.3242)\n",
      "46234 Training Loss: tensor(0.3251)\n",
      "46235 Training Loss: tensor(0.3247)\n",
      "46236 Training Loss: tensor(0.3237)\n",
      "46237 Training Loss: tensor(0.3238)\n",
      "46238 Training Loss: tensor(0.3243)\n",
      "46239 Training Loss: tensor(0.3249)\n",
      "46240 Training Loss: tensor(0.3241)\n",
      "46241 Training Loss: tensor(0.3239)\n",
      "46242 Training Loss: tensor(0.3244)\n",
      "46243 Training Loss: tensor(0.3244)\n",
      "46244 Training Loss: tensor(0.3235)\n",
      "46245 Training Loss: tensor(0.3238)\n",
      "46246 Training Loss: tensor(0.3248)\n",
      "46247 Training Loss: tensor(0.3245)\n",
      "46248 Training Loss: tensor(0.3240)\n",
      "46249 Training Loss: tensor(0.3244)\n",
      "46250 Training Loss: tensor(0.3245)\n",
      "46251 Training Loss: tensor(0.3237)\n",
      "46252 Training Loss: tensor(0.3238)\n",
      "46253 Training Loss: tensor(0.3235)\n",
      "46254 Training Loss: tensor(0.3243)\n",
      "46255 Training Loss: tensor(0.3238)\n",
      "46256 Training Loss: tensor(0.3248)\n",
      "46257 Training Loss: tensor(0.3241)\n",
      "46258 Training Loss: tensor(0.3239)\n",
      "46259 Training Loss: tensor(0.3266)\n",
      "46260 Training Loss: tensor(0.3247)\n",
      "46261 Training Loss: tensor(0.3239)\n",
      "46262 Training Loss: tensor(0.3247)\n",
      "46263 Training Loss: tensor(0.3250)\n",
      "46264 Training Loss: tensor(0.3244)\n",
      "46265 Training Loss: tensor(0.3234)\n",
      "46266 Training Loss: tensor(0.3239)\n",
      "46267 Training Loss: tensor(0.3244)\n",
      "46268 Training Loss: tensor(0.3244)\n",
      "46269 Training Loss: tensor(0.3245)\n",
      "46270 Training Loss: tensor(0.3243)\n",
      "46271 Training Loss: tensor(0.3238)\n",
      "46272 Training Loss: tensor(0.3236)\n",
      "46273 Training Loss: tensor(0.3246)\n",
      "46274 Training Loss: tensor(0.3259)\n",
      "46275 Training Loss: tensor(0.3239)\n",
      "46276 Training Loss: tensor(0.3243)\n",
      "46277 Training Loss: tensor(0.3242)\n",
      "46278 Training Loss: tensor(0.3262)\n",
      "46279 Training Loss: tensor(0.3247)\n",
      "46280 Training Loss: tensor(0.3249)\n",
      "46281 Training Loss: tensor(0.3244)\n",
      "46282 Training Loss: tensor(0.3243)\n",
      "46283 Training Loss: tensor(0.3248)\n",
      "46284 Training Loss: tensor(0.3250)\n",
      "46285 Training Loss: tensor(0.3255)\n",
      "46286 Training Loss: tensor(0.3248)\n",
      "46287 Training Loss: tensor(0.3250)\n",
      "46288 Training Loss: tensor(0.3241)\n",
      "46289 Training Loss: tensor(0.3245)\n",
      "46290 Training Loss: tensor(0.3248)\n",
      "46291 Training Loss: tensor(0.3238)\n",
      "46292 Training Loss: tensor(0.3239)\n",
      "46293 Training Loss: tensor(0.3244)\n",
      "46294 Training Loss: tensor(0.3240)\n",
      "46295 Training Loss: tensor(0.3251)\n",
      "46296 Training Loss: tensor(0.3241)\n",
      "46297 Training Loss: tensor(0.3240)\n",
      "46298 Training Loss: tensor(0.3247)\n",
      "46299 Training Loss: tensor(0.3248)\n",
      "46300 Training Loss: tensor(0.3241)\n",
      "46301 Training Loss: tensor(0.3250)\n",
      "46302 Training Loss: tensor(0.3244)\n",
      "46303 Training Loss: tensor(0.3251)\n",
      "46304 Training Loss: tensor(0.3249)\n",
      "46305 Training Loss: tensor(0.3244)\n",
      "46306 Training Loss: tensor(0.3253)\n",
      "46307 Training Loss: tensor(0.3240)\n",
      "46308 Training Loss: tensor(0.3245)\n",
      "46309 Training Loss: tensor(0.3247)\n",
      "46310 Training Loss: tensor(0.3245)\n",
      "46311 Training Loss: tensor(0.3259)\n",
      "46312 Training Loss: tensor(0.3242)\n",
      "46313 Training Loss: tensor(0.3254)\n",
      "46314 Training Loss: tensor(0.3242)\n",
      "46315 Training Loss: tensor(0.3249)\n",
      "46316 Training Loss: tensor(0.3245)\n",
      "46317 Training Loss: tensor(0.3248)\n",
      "46318 Training Loss: tensor(0.3239)\n",
      "46319 Training Loss: tensor(0.3244)\n",
      "46320 Training Loss: tensor(0.3236)\n",
      "46321 Training Loss: tensor(0.3245)\n",
      "46322 Training Loss: tensor(0.3244)\n",
      "46323 Training Loss: tensor(0.3242)\n",
      "46324 Training Loss: tensor(0.3247)\n",
      "46325 Training Loss: tensor(0.3235)\n",
      "46326 Training Loss: tensor(0.3242)\n",
      "46327 Training Loss: tensor(0.3244)\n",
      "46328 Training Loss: tensor(0.3251)\n",
      "46329 Training Loss: tensor(0.3246)\n",
      "46330 Training Loss: tensor(0.3245)\n",
      "46331 Training Loss: tensor(0.3237)\n",
      "46332 Training Loss: tensor(0.3252)\n",
      "46333 Training Loss: tensor(0.3236)\n",
      "46334 Training Loss: tensor(0.3245)\n",
      "46335 Training Loss: tensor(0.3241)\n",
      "46336 Training Loss: tensor(0.3250)\n",
      "46337 Training Loss: tensor(0.3247)\n",
      "46338 Training Loss: tensor(0.3249)\n",
      "46339 Training Loss: tensor(0.3241)\n",
      "46340 Training Loss: tensor(0.3252)\n",
      "46341 Training Loss: tensor(0.3242)\n",
      "46342 Training Loss: tensor(0.3251)\n",
      "46343 Training Loss: tensor(0.3247)\n",
      "46344 Training Loss: tensor(0.3243)\n",
      "46345 Training Loss: tensor(0.3250)\n",
      "46346 Training Loss: tensor(0.3239)\n",
      "46347 Training Loss: tensor(0.3240)\n",
      "46348 Training Loss: tensor(0.3254)\n",
      "46349 Training Loss: tensor(0.3244)\n",
      "46350 Training Loss: tensor(0.3244)\n",
      "46351 Training Loss: tensor(0.3240)\n",
      "46352 Training Loss: tensor(0.3245)\n",
      "46353 Training Loss: tensor(0.3254)\n",
      "46354 Training Loss: tensor(0.3243)\n",
      "46355 Training Loss: tensor(0.3241)\n",
      "46356 Training Loss: tensor(0.3239)\n",
      "46357 Training Loss: tensor(0.3237)\n",
      "46358 Training Loss: tensor(0.3244)\n",
      "46359 Training Loss: tensor(0.3248)\n",
      "46360 Training Loss: tensor(0.3243)\n",
      "46361 Training Loss: tensor(0.3254)\n",
      "46362 Training Loss: tensor(0.3245)\n",
      "46363 Training Loss: tensor(0.3254)\n",
      "46364 Training Loss: tensor(0.3242)\n",
      "46365 Training Loss: tensor(0.3244)\n",
      "46366 Training Loss: tensor(0.3243)\n",
      "46367 Training Loss: tensor(0.3242)\n",
      "46368 Training Loss: tensor(0.3243)\n",
      "46369 Training Loss: tensor(0.3247)\n",
      "46370 Training Loss: tensor(0.3242)\n",
      "46371 Training Loss: tensor(0.3249)\n",
      "46372 Training Loss: tensor(0.3252)\n",
      "46373 Training Loss: tensor(0.3241)\n",
      "46374 Training Loss: tensor(0.3248)\n",
      "46375 Training Loss: tensor(0.3252)\n",
      "46376 Training Loss: tensor(0.3254)\n",
      "46377 Training Loss: tensor(0.3250)\n",
      "46378 Training Loss: tensor(0.3241)\n",
      "46379 Training Loss: tensor(0.3246)\n",
      "46380 Training Loss: tensor(0.3240)\n",
      "46381 Training Loss: tensor(0.3236)\n",
      "46382 Training Loss: tensor(0.3245)\n",
      "46383 Training Loss: tensor(0.3245)\n",
      "46384 Training Loss: tensor(0.3236)\n",
      "46385 Training Loss: tensor(0.3237)\n",
      "46386 Training Loss: tensor(0.3249)\n",
      "46387 Training Loss: tensor(0.3244)\n",
      "46388 Training Loss: tensor(0.3239)\n",
      "46389 Training Loss: tensor(0.3251)\n",
      "46390 Training Loss: tensor(0.3239)\n",
      "46391 Training Loss: tensor(0.3241)\n",
      "46392 Training Loss: tensor(0.3256)\n",
      "46393 Training Loss: tensor(0.3252)\n",
      "46394 Training Loss: tensor(0.3247)\n",
      "46395 Training Loss: tensor(0.3253)\n",
      "46396 Training Loss: tensor(0.3235)\n",
      "46397 Training Loss: tensor(0.3251)\n",
      "46398 Training Loss: tensor(0.3235)\n",
      "46399 Training Loss: tensor(0.3240)\n",
      "46400 Training Loss: tensor(0.3238)\n",
      "46401 Training Loss: tensor(0.3237)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46402 Training Loss: tensor(0.3247)\n",
      "46403 Training Loss: tensor(0.3252)\n",
      "46404 Training Loss: tensor(0.3239)\n",
      "46405 Training Loss: tensor(0.3238)\n",
      "46406 Training Loss: tensor(0.3246)\n",
      "46407 Training Loss: tensor(0.3247)\n",
      "46408 Training Loss: tensor(0.3245)\n",
      "46409 Training Loss: tensor(0.3240)\n",
      "46410 Training Loss: tensor(0.3243)\n",
      "46411 Training Loss: tensor(0.3246)\n",
      "46412 Training Loss: tensor(0.3239)\n",
      "46413 Training Loss: tensor(0.3241)\n",
      "46414 Training Loss: tensor(0.3240)\n",
      "46415 Training Loss: tensor(0.3258)\n",
      "46416 Training Loss: tensor(0.3245)\n",
      "46417 Training Loss: tensor(0.3248)\n",
      "46418 Training Loss: tensor(0.3246)\n",
      "46419 Training Loss: tensor(0.3260)\n",
      "46420 Training Loss: tensor(0.3242)\n",
      "46421 Training Loss: tensor(0.3245)\n",
      "46422 Training Loss: tensor(0.3244)\n",
      "46423 Training Loss: tensor(0.3243)\n",
      "46424 Training Loss: tensor(0.3248)\n",
      "46425 Training Loss: tensor(0.3249)\n",
      "46426 Training Loss: tensor(0.3254)\n",
      "46427 Training Loss: tensor(0.3247)\n",
      "46428 Training Loss: tensor(0.3247)\n",
      "46429 Training Loss: tensor(0.3247)\n",
      "46430 Training Loss: tensor(0.3254)\n",
      "46431 Training Loss: tensor(0.3241)\n",
      "46432 Training Loss: tensor(0.3242)\n",
      "46433 Training Loss: tensor(0.3237)\n",
      "46434 Training Loss: tensor(0.3242)\n",
      "46435 Training Loss: tensor(0.3241)\n",
      "46436 Training Loss: tensor(0.3242)\n",
      "46437 Training Loss: tensor(0.3242)\n",
      "46438 Training Loss: tensor(0.3243)\n",
      "46439 Training Loss: tensor(0.3240)\n",
      "46440 Training Loss: tensor(0.3245)\n",
      "46441 Training Loss: tensor(0.3239)\n",
      "46442 Training Loss: tensor(0.3257)\n",
      "46443 Training Loss: tensor(0.3241)\n",
      "46444 Training Loss: tensor(0.3244)\n",
      "46445 Training Loss: tensor(0.3249)\n",
      "46446 Training Loss: tensor(0.3249)\n",
      "46447 Training Loss: tensor(0.3242)\n",
      "46448 Training Loss: tensor(0.3240)\n",
      "46449 Training Loss: tensor(0.3240)\n",
      "46450 Training Loss: tensor(0.3247)\n",
      "46451 Training Loss: tensor(0.3251)\n",
      "46452 Training Loss: tensor(0.3247)\n",
      "46453 Training Loss: tensor(0.3251)\n",
      "46454 Training Loss: tensor(0.3246)\n",
      "46455 Training Loss: tensor(0.3241)\n",
      "46456 Training Loss: tensor(0.3236)\n",
      "46457 Training Loss: tensor(0.3244)\n",
      "46458 Training Loss: tensor(0.3237)\n",
      "46459 Training Loss: tensor(0.3240)\n",
      "46460 Training Loss: tensor(0.3243)\n",
      "46461 Training Loss: tensor(0.3256)\n",
      "46462 Training Loss: tensor(0.3253)\n",
      "46463 Training Loss: tensor(0.3246)\n",
      "46464 Training Loss: tensor(0.3246)\n",
      "46465 Training Loss: tensor(0.3243)\n",
      "46466 Training Loss: tensor(0.3241)\n",
      "46467 Training Loss: tensor(0.3244)\n",
      "46468 Training Loss: tensor(0.3241)\n",
      "46469 Training Loss: tensor(0.3239)\n",
      "46470 Training Loss: tensor(0.3249)\n",
      "46471 Training Loss: tensor(0.3240)\n",
      "46472 Training Loss: tensor(0.3239)\n",
      "46473 Training Loss: tensor(0.3241)\n",
      "46474 Training Loss: tensor(0.3240)\n",
      "46475 Training Loss: tensor(0.3241)\n",
      "46476 Training Loss: tensor(0.3243)\n",
      "46477 Training Loss: tensor(0.3249)\n",
      "46478 Training Loss: tensor(0.3251)\n",
      "46479 Training Loss: tensor(0.3236)\n",
      "46480 Training Loss: tensor(0.3242)\n",
      "46481 Training Loss: tensor(0.3248)\n",
      "46482 Training Loss: tensor(0.3237)\n",
      "46483 Training Loss: tensor(0.3248)\n",
      "46484 Training Loss: tensor(0.3238)\n",
      "46485 Training Loss: tensor(0.3238)\n",
      "46486 Training Loss: tensor(0.3238)\n",
      "46487 Training Loss: tensor(0.3247)\n",
      "46488 Training Loss: tensor(0.3246)\n",
      "46489 Training Loss: tensor(0.3244)\n",
      "46490 Training Loss: tensor(0.3235)\n",
      "46491 Training Loss: tensor(0.3239)\n",
      "46492 Training Loss: tensor(0.3242)\n",
      "46493 Training Loss: tensor(0.3242)\n",
      "46494 Training Loss: tensor(0.3245)\n",
      "46495 Training Loss: tensor(0.3248)\n",
      "46496 Training Loss: tensor(0.3249)\n",
      "46497 Training Loss: tensor(0.3241)\n",
      "46498 Training Loss: tensor(0.3247)\n",
      "46499 Training Loss: tensor(0.3239)\n",
      "46500 Training Loss: tensor(0.3244)\n",
      "46501 Training Loss: tensor(0.3243)\n",
      "46502 Training Loss: tensor(0.3241)\n",
      "46503 Training Loss: tensor(0.3246)\n",
      "46504 Training Loss: tensor(0.3247)\n",
      "46505 Training Loss: tensor(0.3239)\n",
      "46506 Training Loss: tensor(0.3244)\n",
      "46507 Training Loss: tensor(0.3238)\n",
      "46508 Training Loss: tensor(0.3248)\n",
      "46509 Training Loss: tensor(0.3251)\n",
      "46510 Training Loss: tensor(0.3250)\n",
      "46511 Training Loss: tensor(0.3249)\n",
      "46512 Training Loss: tensor(0.3254)\n",
      "46513 Training Loss: tensor(0.3253)\n",
      "46514 Training Loss: tensor(0.3245)\n",
      "46515 Training Loss: tensor(0.3235)\n",
      "46516 Training Loss: tensor(0.3238)\n",
      "46517 Training Loss: tensor(0.3234)\n",
      "46518 Training Loss: tensor(0.3239)\n",
      "46519 Training Loss: tensor(0.3238)\n",
      "46520 Training Loss: tensor(0.3242)\n",
      "46521 Training Loss: tensor(0.3238)\n",
      "46522 Training Loss: tensor(0.3238)\n",
      "46523 Training Loss: tensor(0.3243)\n",
      "46524 Training Loss: tensor(0.3245)\n",
      "46525 Training Loss: tensor(0.3242)\n",
      "46526 Training Loss: tensor(0.3244)\n",
      "46527 Training Loss: tensor(0.3234)\n",
      "46528 Training Loss: tensor(0.3238)\n",
      "46529 Training Loss: tensor(0.3243)\n",
      "46530 Training Loss: tensor(0.3233)\n",
      "46531 Training Loss: tensor(0.3246)\n",
      "46532 Training Loss: tensor(0.3249)\n",
      "46533 Training Loss: tensor(0.3242)\n",
      "46534 Training Loss: tensor(0.3239)\n",
      "46535 Training Loss: tensor(0.3241)\n",
      "46536 Training Loss: tensor(0.3254)\n",
      "46537 Training Loss: tensor(0.3248)\n",
      "46538 Training Loss: tensor(0.3236)\n",
      "46539 Training Loss: tensor(0.3242)\n",
      "46540 Training Loss: tensor(0.3249)\n",
      "46541 Training Loss: tensor(0.3239)\n",
      "46542 Training Loss: tensor(0.3248)\n",
      "46543 Training Loss: tensor(0.3236)\n",
      "46544 Training Loss: tensor(0.3238)\n",
      "46545 Training Loss: tensor(0.3240)\n",
      "46546 Training Loss: tensor(0.3244)\n",
      "46547 Training Loss: tensor(0.3252)\n",
      "46548 Training Loss: tensor(0.3240)\n",
      "46549 Training Loss: tensor(0.3264)\n",
      "46550 Training Loss: tensor(0.3235)\n",
      "46551 Training Loss: tensor(0.3237)\n",
      "46552 Training Loss: tensor(0.3249)\n",
      "46553 Training Loss: tensor(0.3252)\n",
      "46554 Training Loss: tensor(0.3252)\n",
      "46555 Training Loss: tensor(0.3241)\n",
      "46556 Training Loss: tensor(0.3245)\n",
      "46557 Training Loss: tensor(0.3239)\n",
      "46558 Training Loss: tensor(0.3256)\n",
      "46559 Training Loss: tensor(0.3238)\n",
      "46560 Training Loss: tensor(0.3248)\n",
      "46561 Training Loss: tensor(0.3249)\n",
      "46562 Training Loss: tensor(0.3241)\n",
      "46563 Training Loss: tensor(0.3256)\n",
      "46564 Training Loss: tensor(0.3243)\n",
      "46565 Training Loss: tensor(0.3245)\n",
      "46566 Training Loss: tensor(0.3239)\n",
      "46567 Training Loss: tensor(0.3243)\n",
      "46568 Training Loss: tensor(0.3243)\n",
      "46569 Training Loss: tensor(0.3240)\n",
      "46570 Training Loss: tensor(0.3245)\n",
      "46571 Training Loss: tensor(0.3240)\n",
      "46572 Training Loss: tensor(0.3242)\n",
      "46573 Training Loss: tensor(0.3238)\n",
      "46574 Training Loss: tensor(0.3240)\n",
      "46575 Training Loss: tensor(0.3243)\n",
      "46576 Training Loss: tensor(0.3242)\n",
      "46577 Training Loss: tensor(0.3239)\n",
      "46578 Training Loss: tensor(0.3246)\n",
      "46579 Training Loss: tensor(0.3248)\n",
      "46580 Training Loss: tensor(0.3237)\n",
      "46581 Training Loss: tensor(0.3241)\n",
      "46582 Training Loss: tensor(0.3243)\n",
      "46583 Training Loss: tensor(0.3254)\n",
      "46584 Training Loss: tensor(0.3242)\n",
      "46585 Training Loss: tensor(0.3244)\n",
      "46586 Training Loss: tensor(0.3250)\n",
      "46587 Training Loss: tensor(0.3251)\n",
      "46588 Training Loss: tensor(0.3247)\n",
      "46589 Training Loss: tensor(0.3242)\n",
      "46590 Training Loss: tensor(0.3236)\n",
      "46591 Training Loss: tensor(0.3251)\n",
      "46592 Training Loss: tensor(0.3243)\n",
      "46593 Training Loss: tensor(0.3243)\n",
      "46594 Training Loss: tensor(0.3247)\n",
      "46595 Training Loss: tensor(0.3247)\n",
      "46596 Training Loss: tensor(0.3243)\n",
      "46597 Training Loss: tensor(0.3240)\n",
      "46598 Training Loss: tensor(0.3253)\n",
      "46599 Training Loss: tensor(0.3247)\n",
      "46600 Training Loss: tensor(0.3239)\n",
      "46601 Training Loss: tensor(0.3243)\n",
      "46602 Training Loss: tensor(0.3244)\n",
      "46603 Training Loss: tensor(0.3239)\n",
      "46604 Training Loss: tensor(0.3246)\n",
      "46605 Training Loss: tensor(0.3242)\n",
      "46606 Training Loss: tensor(0.3241)\n",
      "46607 Training Loss: tensor(0.3248)\n",
      "46608 Training Loss: tensor(0.3246)\n",
      "46609 Training Loss: tensor(0.3245)\n",
      "46610 Training Loss: tensor(0.3238)\n",
      "46611 Training Loss: tensor(0.3246)\n",
      "46612 Training Loss: tensor(0.3240)\n",
      "46613 Training Loss: tensor(0.3241)\n",
      "46614 Training Loss: tensor(0.3239)\n",
      "46615 Training Loss: tensor(0.3236)\n",
      "46616 Training Loss: tensor(0.3240)\n",
      "46617 Training Loss: tensor(0.3247)\n",
      "46618 Training Loss: tensor(0.3242)\n",
      "46619 Training Loss: tensor(0.3247)\n",
      "46620 Training Loss: tensor(0.3242)\n",
      "46621 Training Loss: tensor(0.3242)\n",
      "46622 Training Loss: tensor(0.3237)\n",
      "46623 Training Loss: tensor(0.3244)\n",
      "46624 Training Loss: tensor(0.3243)\n",
      "46625 Training Loss: tensor(0.3248)\n",
      "46626 Training Loss: tensor(0.3247)\n",
      "46627 Training Loss: tensor(0.3247)\n",
      "46628 Training Loss: tensor(0.3259)\n",
      "46629 Training Loss: tensor(0.3256)\n",
      "46630 Training Loss: tensor(0.3246)\n",
      "46631 Training Loss: tensor(0.3239)\n",
      "46632 Training Loss: tensor(0.3246)\n",
      "46633 Training Loss: tensor(0.3236)\n",
      "46634 Training Loss: tensor(0.3245)\n",
      "46635 Training Loss: tensor(0.3242)\n",
      "46636 Training Loss: tensor(0.3243)\n",
      "46637 Training Loss: tensor(0.3241)\n",
      "46638 Training Loss: tensor(0.3239)\n",
      "46639 Training Loss: tensor(0.3238)\n",
      "46640 Training Loss: tensor(0.3246)\n",
      "46641 Training Loss: tensor(0.3241)\n",
      "46642 Training Loss: tensor(0.3242)\n",
      "46643 Training Loss: tensor(0.3251)\n",
      "46644 Training Loss: tensor(0.3250)\n",
      "46645 Training Loss: tensor(0.3257)\n",
      "46646 Training Loss: tensor(0.3237)\n",
      "46647 Training Loss: tensor(0.3240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46648 Training Loss: tensor(0.3238)\n",
      "46649 Training Loss: tensor(0.3250)\n",
      "46650 Training Loss: tensor(0.3244)\n",
      "46651 Training Loss: tensor(0.3251)\n",
      "46652 Training Loss: tensor(0.3242)\n",
      "46653 Training Loss: tensor(0.3241)\n",
      "46654 Training Loss: tensor(0.3241)\n",
      "46655 Training Loss: tensor(0.3245)\n",
      "46656 Training Loss: tensor(0.3241)\n",
      "46657 Training Loss: tensor(0.3234)\n",
      "46658 Training Loss: tensor(0.3241)\n",
      "46659 Training Loss: tensor(0.3240)\n",
      "46660 Training Loss: tensor(0.3246)\n",
      "46661 Training Loss: tensor(0.3249)\n",
      "46662 Training Loss: tensor(0.3246)\n",
      "46663 Training Loss: tensor(0.3240)\n",
      "46664 Training Loss: tensor(0.3250)\n",
      "46665 Training Loss: tensor(0.3240)\n",
      "46666 Training Loss: tensor(0.3244)\n",
      "46667 Training Loss: tensor(0.3244)\n",
      "46668 Training Loss: tensor(0.3243)\n",
      "46669 Training Loss: tensor(0.3252)\n",
      "46670 Training Loss: tensor(0.3242)\n",
      "46671 Training Loss: tensor(0.3251)\n",
      "46672 Training Loss: tensor(0.3248)\n",
      "46673 Training Loss: tensor(0.3244)\n",
      "46674 Training Loss: tensor(0.3245)\n",
      "46675 Training Loss: tensor(0.3248)\n",
      "46676 Training Loss: tensor(0.3241)\n",
      "46677 Training Loss: tensor(0.3254)\n",
      "46678 Training Loss: tensor(0.3263)\n",
      "46679 Training Loss: tensor(0.3246)\n",
      "46680 Training Loss: tensor(0.3251)\n",
      "46681 Training Loss: tensor(0.3252)\n",
      "46682 Training Loss: tensor(0.3248)\n",
      "46683 Training Loss: tensor(0.3249)\n",
      "46684 Training Loss: tensor(0.3243)\n",
      "46685 Training Loss: tensor(0.3246)\n",
      "46686 Training Loss: tensor(0.3250)\n",
      "46687 Training Loss: tensor(0.3240)\n",
      "46688 Training Loss: tensor(0.3244)\n",
      "46689 Training Loss: tensor(0.3238)\n",
      "46690 Training Loss: tensor(0.3237)\n",
      "46691 Training Loss: tensor(0.3242)\n",
      "46692 Training Loss: tensor(0.3241)\n",
      "46693 Training Loss: tensor(0.3247)\n",
      "46694 Training Loss: tensor(0.3247)\n",
      "46695 Training Loss: tensor(0.3244)\n",
      "46696 Training Loss: tensor(0.3245)\n",
      "46697 Training Loss: tensor(0.3241)\n",
      "46698 Training Loss: tensor(0.3254)\n",
      "46699 Training Loss: tensor(0.3243)\n",
      "46700 Training Loss: tensor(0.3244)\n",
      "46701 Training Loss: tensor(0.3244)\n",
      "46702 Training Loss: tensor(0.3237)\n",
      "46703 Training Loss: tensor(0.3245)\n",
      "46704 Training Loss: tensor(0.3246)\n",
      "46705 Training Loss: tensor(0.3253)\n",
      "46706 Training Loss: tensor(0.3242)\n",
      "46707 Training Loss: tensor(0.3249)\n",
      "46708 Training Loss: tensor(0.3246)\n",
      "46709 Training Loss: tensor(0.3238)\n",
      "46710 Training Loss: tensor(0.3238)\n",
      "46711 Training Loss: tensor(0.3244)\n",
      "46712 Training Loss: tensor(0.3239)\n",
      "46713 Training Loss: tensor(0.3241)\n",
      "46714 Training Loss: tensor(0.3245)\n",
      "46715 Training Loss: tensor(0.3242)\n",
      "46716 Training Loss: tensor(0.3238)\n",
      "46717 Training Loss: tensor(0.3241)\n",
      "46718 Training Loss: tensor(0.3241)\n",
      "46719 Training Loss: tensor(0.3239)\n",
      "46720 Training Loss: tensor(0.3249)\n",
      "46721 Training Loss: tensor(0.3236)\n",
      "46722 Training Loss: tensor(0.3243)\n",
      "46723 Training Loss: tensor(0.3244)\n",
      "46724 Training Loss: tensor(0.3239)\n",
      "46725 Training Loss: tensor(0.3240)\n",
      "46726 Training Loss: tensor(0.3241)\n",
      "46727 Training Loss: tensor(0.3242)\n",
      "46728 Training Loss: tensor(0.3239)\n",
      "46729 Training Loss: tensor(0.3248)\n",
      "46730 Training Loss: tensor(0.3236)\n",
      "46731 Training Loss: tensor(0.3241)\n",
      "46732 Training Loss: tensor(0.3241)\n",
      "46733 Training Loss: tensor(0.3254)\n",
      "46734 Training Loss: tensor(0.3239)\n",
      "46735 Training Loss: tensor(0.3248)\n",
      "46736 Training Loss: tensor(0.3238)\n",
      "46737 Training Loss: tensor(0.3250)\n",
      "46738 Training Loss: tensor(0.3254)\n",
      "46739 Training Loss: tensor(0.3242)\n",
      "46740 Training Loss: tensor(0.3248)\n",
      "46741 Training Loss: tensor(0.3247)\n",
      "46742 Training Loss: tensor(0.3236)\n",
      "46743 Training Loss: tensor(0.3241)\n",
      "46744 Training Loss: tensor(0.3251)\n",
      "46745 Training Loss: tensor(0.3244)\n",
      "46746 Training Loss: tensor(0.3240)\n",
      "46747 Training Loss: tensor(0.3244)\n",
      "46748 Training Loss: tensor(0.3246)\n",
      "46749 Training Loss: tensor(0.3252)\n",
      "46750 Training Loss: tensor(0.3236)\n",
      "46751 Training Loss: tensor(0.3239)\n",
      "46752 Training Loss: tensor(0.3254)\n",
      "46753 Training Loss: tensor(0.3242)\n",
      "46754 Training Loss: tensor(0.3246)\n",
      "46755 Training Loss: tensor(0.3239)\n",
      "46756 Training Loss: tensor(0.3245)\n",
      "46757 Training Loss: tensor(0.3240)\n",
      "46758 Training Loss: tensor(0.3240)\n",
      "46759 Training Loss: tensor(0.3245)\n",
      "46760 Training Loss: tensor(0.3251)\n",
      "46761 Training Loss: tensor(0.3253)\n",
      "46762 Training Loss: tensor(0.3247)\n",
      "46763 Training Loss: tensor(0.3251)\n",
      "46764 Training Loss: tensor(0.3242)\n",
      "46765 Training Loss: tensor(0.3240)\n",
      "46766 Training Loss: tensor(0.3240)\n",
      "46767 Training Loss: tensor(0.3246)\n",
      "46768 Training Loss: tensor(0.3247)\n",
      "46769 Training Loss: tensor(0.3238)\n",
      "46770 Training Loss: tensor(0.3242)\n",
      "46771 Training Loss: tensor(0.3244)\n",
      "46772 Training Loss: tensor(0.3247)\n",
      "46773 Training Loss: tensor(0.3245)\n",
      "46774 Training Loss: tensor(0.3247)\n",
      "46775 Training Loss: tensor(0.3240)\n",
      "46776 Training Loss: tensor(0.3238)\n",
      "46777 Training Loss: tensor(0.3238)\n",
      "46778 Training Loss: tensor(0.3250)\n",
      "46779 Training Loss: tensor(0.3240)\n",
      "46780 Training Loss: tensor(0.3248)\n",
      "46781 Training Loss: tensor(0.3236)\n",
      "46782 Training Loss: tensor(0.3246)\n",
      "46783 Training Loss: tensor(0.3248)\n",
      "46784 Training Loss: tensor(0.3239)\n",
      "46785 Training Loss: tensor(0.3240)\n",
      "46786 Training Loss: tensor(0.3239)\n",
      "46787 Training Loss: tensor(0.3243)\n",
      "46788 Training Loss: tensor(0.3242)\n",
      "46789 Training Loss: tensor(0.3246)\n",
      "46790 Training Loss: tensor(0.3243)\n",
      "46791 Training Loss: tensor(0.3246)\n",
      "46792 Training Loss: tensor(0.3239)\n",
      "46793 Training Loss: tensor(0.3239)\n",
      "46794 Training Loss: tensor(0.3256)\n",
      "46795 Training Loss: tensor(0.3246)\n",
      "46796 Training Loss: tensor(0.3238)\n",
      "46797 Training Loss: tensor(0.3242)\n",
      "46798 Training Loss: tensor(0.3244)\n",
      "46799 Training Loss: tensor(0.3239)\n",
      "46800 Training Loss: tensor(0.3233)\n",
      "46801 Training Loss: tensor(0.3249)\n",
      "46802 Training Loss: tensor(0.3248)\n",
      "46803 Training Loss: tensor(0.3239)\n",
      "46804 Training Loss: tensor(0.3252)\n",
      "46805 Training Loss: tensor(0.3246)\n",
      "46806 Training Loss: tensor(0.3258)\n",
      "46807 Training Loss: tensor(0.3246)\n",
      "46808 Training Loss: tensor(0.3256)\n",
      "46809 Training Loss: tensor(0.3247)\n",
      "46810 Training Loss: tensor(0.3240)\n",
      "46811 Training Loss: tensor(0.3242)\n",
      "46812 Training Loss: tensor(0.3244)\n",
      "46813 Training Loss: tensor(0.3239)\n",
      "46814 Training Loss: tensor(0.3245)\n",
      "46815 Training Loss: tensor(0.3242)\n",
      "46816 Training Loss: tensor(0.3241)\n",
      "46817 Training Loss: tensor(0.3247)\n",
      "46818 Training Loss: tensor(0.3247)\n",
      "46819 Training Loss: tensor(0.3261)\n",
      "46820 Training Loss: tensor(0.3253)\n",
      "46821 Training Loss: tensor(0.3244)\n",
      "46822 Training Loss: tensor(0.3246)\n",
      "46823 Training Loss: tensor(0.3240)\n",
      "46824 Training Loss: tensor(0.3245)\n",
      "46825 Training Loss: tensor(0.3245)\n",
      "46826 Training Loss: tensor(0.3248)\n",
      "46827 Training Loss: tensor(0.3250)\n",
      "46828 Training Loss: tensor(0.3240)\n",
      "46829 Training Loss: tensor(0.3241)\n",
      "46830 Training Loss: tensor(0.3239)\n",
      "46831 Training Loss: tensor(0.3251)\n",
      "46832 Training Loss: tensor(0.3241)\n",
      "46833 Training Loss: tensor(0.3238)\n",
      "46834 Training Loss: tensor(0.3244)\n",
      "46835 Training Loss: tensor(0.3262)\n",
      "46836 Training Loss: tensor(0.3247)\n",
      "46837 Training Loss: tensor(0.3242)\n",
      "46838 Training Loss: tensor(0.3240)\n",
      "46839 Training Loss: tensor(0.3249)\n",
      "46840 Training Loss: tensor(0.3245)\n",
      "46841 Training Loss: tensor(0.3241)\n",
      "46842 Training Loss: tensor(0.3246)\n",
      "46843 Training Loss: tensor(0.3251)\n",
      "46844 Training Loss: tensor(0.3248)\n",
      "46845 Training Loss: tensor(0.3250)\n",
      "46846 Training Loss: tensor(0.3247)\n",
      "46847 Training Loss: tensor(0.3253)\n",
      "46848 Training Loss: tensor(0.3242)\n",
      "46849 Training Loss: tensor(0.3246)\n",
      "46850 Training Loss: tensor(0.3248)\n",
      "46851 Training Loss: tensor(0.3246)\n",
      "46852 Training Loss: tensor(0.3236)\n",
      "46853 Training Loss: tensor(0.3241)\n",
      "46854 Training Loss: tensor(0.3236)\n",
      "46855 Training Loss: tensor(0.3245)\n",
      "46856 Training Loss: tensor(0.3250)\n",
      "46857 Training Loss: tensor(0.3246)\n",
      "46858 Training Loss: tensor(0.3246)\n",
      "46859 Training Loss: tensor(0.3253)\n",
      "46860 Training Loss: tensor(0.3241)\n",
      "46861 Training Loss: tensor(0.3240)\n",
      "46862 Training Loss: tensor(0.3251)\n",
      "46863 Training Loss: tensor(0.3242)\n",
      "46864 Training Loss: tensor(0.3244)\n",
      "46865 Training Loss: tensor(0.3251)\n",
      "46866 Training Loss: tensor(0.3242)\n",
      "46867 Training Loss: tensor(0.3248)\n",
      "46868 Training Loss: tensor(0.3246)\n",
      "46869 Training Loss: tensor(0.3240)\n",
      "46870 Training Loss: tensor(0.3239)\n",
      "46871 Training Loss: tensor(0.3250)\n",
      "46872 Training Loss: tensor(0.3240)\n",
      "46873 Training Loss: tensor(0.3238)\n",
      "46874 Training Loss: tensor(0.3246)\n",
      "46875 Training Loss: tensor(0.3241)\n",
      "46876 Training Loss: tensor(0.3250)\n",
      "46877 Training Loss: tensor(0.3238)\n",
      "46878 Training Loss: tensor(0.3247)\n",
      "46879 Training Loss: tensor(0.3243)\n",
      "46880 Training Loss: tensor(0.3242)\n",
      "46881 Training Loss: tensor(0.3246)\n",
      "46882 Training Loss: tensor(0.3240)\n",
      "46883 Training Loss: tensor(0.3238)\n",
      "46884 Training Loss: tensor(0.3253)\n",
      "46885 Training Loss: tensor(0.3249)\n",
      "46886 Training Loss: tensor(0.3251)\n",
      "46887 Training Loss: tensor(0.3241)\n",
      "46888 Training Loss: tensor(0.3237)\n",
      "46889 Training Loss: tensor(0.3244)\n",
      "46890 Training Loss: tensor(0.3244)\n",
      "46891 Training Loss: tensor(0.3236)\n",
      "46892 Training Loss: tensor(0.3243)\n",
      "46893 Training Loss: tensor(0.3241)\n",
      "46894 Training Loss: tensor(0.3242)\n",
      "46895 Training Loss: tensor(0.3239)\n",
      "46896 Training Loss: tensor(0.3244)\n",
      "46897 Training Loss: tensor(0.3239)\n",
      "46898 Training Loss: tensor(0.3247)\n",
      "46899 Training Loss: tensor(0.3247)\n",
      "46900 Training Loss: tensor(0.3269)\n",
      "46901 Training Loss: tensor(0.3241)\n",
      "46902 Training Loss: tensor(0.3244)\n",
      "46903 Training Loss: tensor(0.3244)\n",
      "46904 Training Loss: tensor(0.3241)\n",
      "46905 Training Loss: tensor(0.3250)\n",
      "46906 Training Loss: tensor(0.3241)\n",
      "46907 Training Loss: tensor(0.3241)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46908 Training Loss: tensor(0.3253)\n",
      "46909 Training Loss: tensor(0.3258)\n",
      "46910 Training Loss: tensor(0.3245)\n",
      "46911 Training Loss: tensor(0.3235)\n",
      "46912 Training Loss: tensor(0.3238)\n",
      "46913 Training Loss: tensor(0.3246)\n",
      "46914 Training Loss: tensor(0.3244)\n",
      "46915 Training Loss: tensor(0.3253)\n",
      "46916 Training Loss: tensor(0.3236)\n",
      "46917 Training Loss: tensor(0.3245)\n",
      "46918 Training Loss: tensor(0.3253)\n",
      "46919 Training Loss: tensor(0.3243)\n",
      "46920 Training Loss: tensor(0.3240)\n",
      "46921 Training Loss: tensor(0.3246)\n",
      "46922 Training Loss: tensor(0.3237)\n",
      "46923 Training Loss: tensor(0.3238)\n",
      "46924 Training Loss: tensor(0.3238)\n",
      "46925 Training Loss: tensor(0.3240)\n",
      "46926 Training Loss: tensor(0.3244)\n",
      "46927 Training Loss: tensor(0.3238)\n",
      "46928 Training Loss: tensor(0.3240)\n",
      "46929 Training Loss: tensor(0.3245)\n",
      "46930 Training Loss: tensor(0.3245)\n",
      "46931 Training Loss: tensor(0.3239)\n",
      "46932 Training Loss: tensor(0.3248)\n",
      "46933 Training Loss: tensor(0.3244)\n",
      "46934 Training Loss: tensor(0.3247)\n",
      "46935 Training Loss: tensor(0.3247)\n",
      "46936 Training Loss: tensor(0.3250)\n",
      "46937 Training Loss: tensor(0.3243)\n",
      "46938 Training Loss: tensor(0.3240)\n",
      "46939 Training Loss: tensor(0.3241)\n",
      "46940 Training Loss: tensor(0.3247)\n",
      "46941 Training Loss: tensor(0.3244)\n",
      "46942 Training Loss: tensor(0.3245)\n",
      "46943 Training Loss: tensor(0.3254)\n",
      "46944 Training Loss: tensor(0.3239)\n",
      "46945 Training Loss: tensor(0.3240)\n",
      "46946 Training Loss: tensor(0.3252)\n",
      "46947 Training Loss: tensor(0.3241)\n",
      "46948 Training Loss: tensor(0.3240)\n",
      "46949 Training Loss: tensor(0.3242)\n",
      "46950 Training Loss: tensor(0.3255)\n",
      "46951 Training Loss: tensor(0.3251)\n",
      "46952 Training Loss: tensor(0.3242)\n",
      "46953 Training Loss: tensor(0.3239)\n",
      "46954 Training Loss: tensor(0.3253)\n",
      "46955 Training Loss: tensor(0.3249)\n",
      "46956 Training Loss: tensor(0.3241)\n",
      "46957 Training Loss: tensor(0.3245)\n",
      "46958 Training Loss: tensor(0.3245)\n",
      "46959 Training Loss: tensor(0.3254)\n",
      "46960 Training Loss: tensor(0.3250)\n",
      "46961 Training Loss: tensor(0.3244)\n",
      "46962 Training Loss: tensor(0.3250)\n",
      "46963 Training Loss: tensor(0.3241)\n",
      "46964 Training Loss: tensor(0.3251)\n",
      "46965 Training Loss: tensor(0.3250)\n",
      "46966 Training Loss: tensor(0.3253)\n",
      "46967 Training Loss: tensor(0.3241)\n",
      "46968 Training Loss: tensor(0.3245)\n",
      "46969 Training Loss: tensor(0.3245)\n",
      "46970 Training Loss: tensor(0.3246)\n",
      "46971 Training Loss: tensor(0.3238)\n",
      "46972 Training Loss: tensor(0.3241)\n",
      "46973 Training Loss: tensor(0.3240)\n",
      "46974 Training Loss: tensor(0.3239)\n",
      "46975 Training Loss: tensor(0.3245)\n",
      "46976 Training Loss: tensor(0.3249)\n",
      "46977 Training Loss: tensor(0.3244)\n",
      "46978 Training Loss: tensor(0.3252)\n",
      "46979 Training Loss: tensor(0.3239)\n",
      "46980 Training Loss: tensor(0.3242)\n",
      "46981 Training Loss: tensor(0.3243)\n",
      "46982 Training Loss: tensor(0.3235)\n",
      "46983 Training Loss: tensor(0.3235)\n",
      "46984 Training Loss: tensor(0.3245)\n",
      "46985 Training Loss: tensor(0.3244)\n",
      "46986 Training Loss: tensor(0.3239)\n",
      "46987 Training Loss: tensor(0.3245)\n",
      "46988 Training Loss: tensor(0.3242)\n",
      "46989 Training Loss: tensor(0.3243)\n",
      "46990 Training Loss: tensor(0.3247)\n",
      "46991 Training Loss: tensor(0.3246)\n",
      "46992 Training Loss: tensor(0.3240)\n",
      "46993 Training Loss: tensor(0.3234)\n",
      "46994 Training Loss: tensor(0.3253)\n",
      "46995 Training Loss: tensor(0.3248)\n",
      "46996 Training Loss: tensor(0.3249)\n",
      "46997 Training Loss: tensor(0.3242)\n",
      "46998 Training Loss: tensor(0.3260)\n",
      "46999 Training Loss: tensor(0.3249)\n",
      "47000 Training Loss: tensor(0.3238)\n",
      "47001 Training Loss: tensor(0.3244)\n",
      "47002 Training Loss: tensor(0.3247)\n",
      "47003 Training Loss: tensor(0.3239)\n",
      "47004 Training Loss: tensor(0.3245)\n",
      "47005 Training Loss: tensor(0.3253)\n",
      "47006 Training Loss: tensor(0.3240)\n",
      "47007 Training Loss: tensor(0.3248)\n",
      "47008 Training Loss: tensor(0.3243)\n",
      "47009 Training Loss: tensor(0.3243)\n",
      "47010 Training Loss: tensor(0.3244)\n",
      "47011 Training Loss: tensor(0.3246)\n",
      "47012 Training Loss: tensor(0.3239)\n",
      "47013 Training Loss: tensor(0.3243)\n",
      "47014 Training Loss: tensor(0.3242)\n",
      "47015 Training Loss: tensor(0.3238)\n",
      "47016 Training Loss: tensor(0.3237)\n",
      "47017 Training Loss: tensor(0.3243)\n",
      "47018 Training Loss: tensor(0.3247)\n",
      "47019 Training Loss: tensor(0.3248)\n",
      "47020 Training Loss: tensor(0.3250)\n",
      "47021 Training Loss: tensor(0.3239)\n",
      "47022 Training Loss: tensor(0.3249)\n",
      "47023 Training Loss: tensor(0.3245)\n",
      "47024 Training Loss: tensor(0.3246)\n",
      "47025 Training Loss: tensor(0.3249)\n",
      "47026 Training Loss: tensor(0.3234)\n",
      "47027 Training Loss: tensor(0.3239)\n",
      "47028 Training Loss: tensor(0.3247)\n",
      "47029 Training Loss: tensor(0.3247)\n",
      "47030 Training Loss: tensor(0.3241)\n",
      "47031 Training Loss: tensor(0.3236)\n",
      "47032 Training Loss: tensor(0.3239)\n",
      "47033 Training Loss: tensor(0.3256)\n",
      "47034 Training Loss: tensor(0.3245)\n",
      "47035 Training Loss: tensor(0.3240)\n",
      "47036 Training Loss: tensor(0.3241)\n",
      "47037 Training Loss: tensor(0.3253)\n",
      "47038 Training Loss: tensor(0.3244)\n",
      "47039 Training Loss: tensor(0.3239)\n",
      "47040 Training Loss: tensor(0.3243)\n",
      "47041 Training Loss: tensor(0.3256)\n",
      "47042 Training Loss: tensor(0.3245)\n",
      "47043 Training Loss: tensor(0.3239)\n",
      "47044 Training Loss: tensor(0.3246)\n",
      "47045 Training Loss: tensor(0.3250)\n",
      "47046 Training Loss: tensor(0.3247)\n",
      "47047 Training Loss: tensor(0.3245)\n",
      "47048 Training Loss: tensor(0.3240)\n",
      "47049 Training Loss: tensor(0.3244)\n",
      "47050 Training Loss: tensor(0.3242)\n",
      "47051 Training Loss: tensor(0.3251)\n",
      "47052 Training Loss: tensor(0.3252)\n",
      "47053 Training Loss: tensor(0.3244)\n",
      "47054 Training Loss: tensor(0.3247)\n",
      "47055 Training Loss: tensor(0.3238)\n",
      "47056 Training Loss: tensor(0.3244)\n",
      "47057 Training Loss: tensor(0.3246)\n",
      "47058 Training Loss: tensor(0.3243)\n",
      "47059 Training Loss: tensor(0.3255)\n",
      "47060 Training Loss: tensor(0.3238)\n",
      "47061 Training Loss: tensor(0.3239)\n",
      "47062 Training Loss: tensor(0.3238)\n",
      "47063 Training Loss: tensor(0.3253)\n",
      "47064 Training Loss: tensor(0.3242)\n",
      "47065 Training Loss: tensor(0.3243)\n",
      "47066 Training Loss: tensor(0.3244)\n",
      "47067 Training Loss: tensor(0.3244)\n",
      "47068 Training Loss: tensor(0.3236)\n",
      "47069 Training Loss: tensor(0.3239)\n",
      "47070 Training Loss: tensor(0.3248)\n",
      "47071 Training Loss: tensor(0.3235)\n",
      "47072 Training Loss: tensor(0.3247)\n",
      "47073 Training Loss: tensor(0.3242)\n",
      "47074 Training Loss: tensor(0.3241)\n",
      "47075 Training Loss: tensor(0.3251)\n",
      "47076 Training Loss: tensor(0.3239)\n",
      "47077 Training Loss: tensor(0.3243)\n",
      "47078 Training Loss: tensor(0.3242)\n",
      "47079 Training Loss: tensor(0.3239)\n",
      "47080 Training Loss: tensor(0.3243)\n",
      "47081 Training Loss: tensor(0.3239)\n",
      "47082 Training Loss: tensor(0.3245)\n",
      "47083 Training Loss: tensor(0.3244)\n",
      "47084 Training Loss: tensor(0.3243)\n",
      "47085 Training Loss: tensor(0.3243)\n",
      "47086 Training Loss: tensor(0.3237)\n",
      "47087 Training Loss: tensor(0.3245)\n",
      "47088 Training Loss: tensor(0.3244)\n",
      "47089 Training Loss: tensor(0.3240)\n",
      "47090 Training Loss: tensor(0.3254)\n",
      "47091 Training Loss: tensor(0.3264)\n",
      "47092 Training Loss: tensor(0.3247)\n",
      "47093 Training Loss: tensor(0.3251)\n",
      "47094 Training Loss: tensor(0.3244)\n",
      "47095 Training Loss: tensor(0.3240)\n",
      "47096 Training Loss: tensor(0.3240)\n",
      "47097 Training Loss: tensor(0.3246)\n",
      "47098 Training Loss: tensor(0.3243)\n",
      "47099 Training Loss: tensor(0.3245)\n",
      "47100 Training Loss: tensor(0.3243)\n",
      "47101 Training Loss: tensor(0.3251)\n",
      "47102 Training Loss: tensor(0.3238)\n",
      "47103 Training Loss: tensor(0.3249)\n",
      "47104 Training Loss: tensor(0.3243)\n",
      "47105 Training Loss: tensor(0.3247)\n",
      "47106 Training Loss: tensor(0.3244)\n",
      "47107 Training Loss: tensor(0.3239)\n",
      "47108 Training Loss: tensor(0.3240)\n",
      "47109 Training Loss: tensor(0.3248)\n",
      "47110 Training Loss: tensor(0.3244)\n",
      "47111 Training Loss: tensor(0.3246)\n",
      "47112 Training Loss: tensor(0.3244)\n",
      "47113 Training Loss: tensor(0.3241)\n",
      "47114 Training Loss: tensor(0.3249)\n",
      "47115 Training Loss: tensor(0.3245)\n",
      "47116 Training Loss: tensor(0.3242)\n",
      "47117 Training Loss: tensor(0.3249)\n",
      "47118 Training Loss: tensor(0.3237)\n",
      "47119 Training Loss: tensor(0.3243)\n",
      "47120 Training Loss: tensor(0.3243)\n",
      "47121 Training Loss: tensor(0.3242)\n",
      "47122 Training Loss: tensor(0.3245)\n",
      "47123 Training Loss: tensor(0.3238)\n",
      "47124 Training Loss: tensor(0.3240)\n",
      "47125 Training Loss: tensor(0.3245)\n",
      "47126 Training Loss: tensor(0.3239)\n",
      "47127 Training Loss: tensor(0.3242)\n",
      "47128 Training Loss: tensor(0.3243)\n",
      "47129 Training Loss: tensor(0.3252)\n",
      "47130 Training Loss: tensor(0.3249)\n",
      "47131 Training Loss: tensor(0.3245)\n",
      "47132 Training Loss: tensor(0.3250)\n",
      "47133 Training Loss: tensor(0.3249)\n",
      "47134 Training Loss: tensor(0.3246)\n",
      "47135 Training Loss: tensor(0.3252)\n",
      "47136 Training Loss: tensor(0.3249)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47137 Training Loss: tensor(0.3254)\n",
      "47138 Training Loss: tensor(0.3244)\n",
      "47139 Training Loss: tensor(0.3242)\n",
      "47140 Training Loss: tensor(0.3242)\n",
      "47141 Training Loss: tensor(0.3242)\n",
      "47142 Training Loss: tensor(0.3244)\n",
      "47143 Training Loss: tensor(0.3243)\n",
      "47144 Training Loss: tensor(0.3244)\n",
      "47145 Training Loss: tensor(0.3249)\n",
      "47146 Training Loss: tensor(0.3245)\n",
      "47147 Training Loss: tensor(0.3251)\n",
      "47148 Training Loss: tensor(0.3249)\n",
      "47149 Training Loss: tensor(0.3249)\n",
      "47150 Training Loss: tensor(0.3256)\n",
      "47151 Training Loss: tensor(0.3245)\n",
      "47152 Training Loss: tensor(0.3243)\n",
      "47153 Training Loss: tensor(0.3255)\n",
      "47154 Training Loss: tensor(0.3253)\n",
      "47155 Training Loss: tensor(0.3244)\n",
      "47156 Training Loss: tensor(0.3251)\n",
      "47157 Training Loss: tensor(0.3266)\n",
      "47158 Training Loss: tensor(0.3247)\n",
      "47159 Training Loss: tensor(0.3247)\n",
      "47160 Training Loss: tensor(0.3248)\n",
      "47161 Training Loss: tensor(0.3240)\n",
      "47162 Training Loss: tensor(0.3254)\n",
      "47163 Training Loss: tensor(0.3248)\n",
      "47164 Training Loss: tensor(0.3249)\n",
      "47165 Training Loss: tensor(0.3257)\n",
      "47166 Training Loss: tensor(0.3247)\n",
      "47167 Training Loss: tensor(0.3252)\n",
      "47168 Training Loss: tensor(0.3243)\n",
      "47169 Training Loss: tensor(0.3246)\n",
      "47170 Training Loss: tensor(0.3244)\n",
      "47171 Training Loss: tensor(0.3243)\n",
      "47172 Training Loss: tensor(0.3247)\n",
      "47173 Training Loss: tensor(0.3239)\n",
      "47174 Training Loss: tensor(0.3244)\n",
      "47175 Training Loss: tensor(0.3250)\n",
      "47176 Training Loss: tensor(0.3253)\n",
      "47177 Training Loss: tensor(0.3242)\n",
      "47178 Training Loss: tensor(0.3248)\n",
      "47179 Training Loss: tensor(0.3242)\n",
      "47180 Training Loss: tensor(0.3243)\n",
      "47181 Training Loss: tensor(0.3237)\n",
      "47182 Training Loss: tensor(0.3254)\n",
      "47183 Training Loss: tensor(0.3242)\n",
      "47184 Training Loss: tensor(0.3241)\n",
      "47185 Training Loss: tensor(0.3236)\n",
      "47186 Training Loss: tensor(0.3238)\n",
      "47187 Training Loss: tensor(0.3243)\n",
      "47188 Training Loss: tensor(0.3241)\n",
      "47189 Training Loss: tensor(0.3250)\n",
      "47190 Training Loss: tensor(0.3242)\n",
      "47191 Training Loss: tensor(0.3242)\n",
      "47192 Training Loss: tensor(0.3250)\n",
      "47193 Training Loss: tensor(0.3244)\n",
      "47194 Training Loss: tensor(0.3255)\n",
      "47195 Training Loss: tensor(0.3244)\n",
      "47196 Training Loss: tensor(0.3247)\n",
      "47197 Training Loss: tensor(0.3236)\n",
      "47198 Training Loss: tensor(0.3238)\n",
      "47199 Training Loss: tensor(0.3237)\n",
      "47200 Training Loss: tensor(0.3244)\n",
      "47201 Training Loss: tensor(0.3248)\n",
      "47202 Training Loss: tensor(0.3242)\n",
      "47203 Training Loss: tensor(0.3242)\n",
      "47204 Training Loss: tensor(0.3240)\n",
      "47205 Training Loss: tensor(0.3241)\n",
      "47206 Training Loss: tensor(0.3239)\n",
      "47207 Training Loss: tensor(0.3251)\n",
      "47208 Training Loss: tensor(0.3238)\n",
      "47209 Training Loss: tensor(0.3247)\n",
      "47210 Training Loss: tensor(0.3241)\n",
      "47211 Training Loss: tensor(0.3245)\n",
      "47212 Training Loss: tensor(0.3239)\n",
      "47213 Training Loss: tensor(0.3243)\n",
      "47214 Training Loss: tensor(0.3241)\n",
      "47215 Training Loss: tensor(0.3241)\n",
      "47216 Training Loss: tensor(0.3242)\n",
      "47217 Training Loss: tensor(0.3248)\n",
      "47218 Training Loss: tensor(0.3246)\n",
      "47219 Training Loss: tensor(0.3242)\n",
      "47220 Training Loss: tensor(0.3242)\n",
      "47221 Training Loss: tensor(0.3242)\n",
      "47222 Training Loss: tensor(0.3254)\n",
      "47223 Training Loss: tensor(0.3242)\n",
      "47224 Training Loss: tensor(0.3239)\n",
      "47225 Training Loss: tensor(0.3235)\n",
      "47226 Training Loss: tensor(0.3259)\n",
      "47227 Training Loss: tensor(0.3236)\n",
      "47228 Training Loss: tensor(0.3237)\n",
      "47229 Training Loss: tensor(0.3252)\n",
      "47230 Training Loss: tensor(0.3238)\n",
      "47231 Training Loss: tensor(0.3244)\n",
      "47232 Training Loss: tensor(0.3241)\n",
      "47233 Training Loss: tensor(0.3245)\n",
      "47234 Training Loss: tensor(0.3253)\n",
      "47235 Training Loss: tensor(0.3252)\n",
      "47236 Training Loss: tensor(0.3245)\n",
      "47237 Training Loss: tensor(0.3248)\n",
      "47238 Training Loss: tensor(0.3240)\n",
      "47239 Training Loss: tensor(0.3241)\n",
      "47240 Training Loss: tensor(0.3237)\n",
      "47241 Training Loss: tensor(0.3260)\n",
      "47242 Training Loss: tensor(0.3239)\n",
      "47243 Training Loss: tensor(0.3259)\n",
      "47244 Training Loss: tensor(0.3259)\n",
      "47245 Training Loss: tensor(0.3237)\n",
      "47246 Training Loss: tensor(0.3253)\n",
      "47247 Training Loss: tensor(0.3252)\n",
      "47248 Training Loss: tensor(0.3250)\n",
      "47249 Training Loss: tensor(0.3238)\n",
      "47250 Training Loss: tensor(0.3239)\n",
      "47251 Training Loss: tensor(0.3247)\n",
      "47252 Training Loss: tensor(0.3238)\n",
      "47253 Training Loss: tensor(0.3242)\n",
      "47254 Training Loss: tensor(0.3245)\n",
      "47255 Training Loss: tensor(0.3245)\n",
      "47256 Training Loss: tensor(0.3242)\n",
      "47257 Training Loss: tensor(0.3248)\n",
      "47258 Training Loss: tensor(0.3250)\n",
      "47259 Training Loss: tensor(0.3237)\n",
      "47260 Training Loss: tensor(0.3239)\n",
      "47261 Training Loss: tensor(0.3249)\n",
      "47262 Training Loss: tensor(0.3247)\n",
      "47263 Training Loss: tensor(0.3239)\n",
      "47264 Training Loss: tensor(0.3244)\n",
      "47265 Training Loss: tensor(0.3252)\n",
      "47266 Training Loss: tensor(0.3243)\n",
      "47267 Training Loss: tensor(0.3249)\n",
      "47268 Training Loss: tensor(0.3249)\n",
      "47269 Training Loss: tensor(0.3246)\n",
      "47270 Training Loss: tensor(0.3236)\n",
      "47271 Training Loss: tensor(0.3244)\n",
      "47272 Training Loss: tensor(0.3250)\n",
      "47273 Training Loss: tensor(0.3244)\n",
      "47274 Training Loss: tensor(0.3241)\n",
      "47275 Training Loss: tensor(0.3246)\n",
      "47276 Training Loss: tensor(0.3244)\n",
      "47277 Training Loss: tensor(0.3244)\n",
      "47278 Training Loss: tensor(0.3244)\n",
      "47279 Training Loss: tensor(0.3244)\n",
      "47280 Training Loss: tensor(0.3239)\n",
      "47281 Training Loss: tensor(0.3239)\n",
      "47282 Training Loss: tensor(0.3240)\n",
      "47283 Training Loss: tensor(0.3241)\n",
      "47284 Training Loss: tensor(0.3246)\n",
      "47285 Training Loss: tensor(0.3239)\n",
      "47286 Training Loss: tensor(0.3238)\n",
      "47287 Training Loss: tensor(0.3248)\n",
      "47288 Training Loss: tensor(0.3249)\n",
      "47289 Training Loss: tensor(0.3245)\n",
      "47290 Training Loss: tensor(0.3239)\n",
      "47291 Training Loss: tensor(0.3243)\n",
      "47292 Training Loss: tensor(0.3238)\n",
      "47293 Training Loss: tensor(0.3237)\n",
      "47294 Training Loss: tensor(0.3244)\n",
      "47295 Training Loss: tensor(0.3256)\n",
      "47296 Training Loss: tensor(0.3242)\n",
      "47297 Training Loss: tensor(0.3243)\n",
      "47298 Training Loss: tensor(0.3238)\n",
      "47299 Training Loss: tensor(0.3234)\n",
      "47300 Training Loss: tensor(0.3248)\n",
      "47301 Training Loss: tensor(0.3247)\n",
      "47302 Training Loss: tensor(0.3257)\n",
      "47303 Training Loss: tensor(0.3250)\n",
      "47304 Training Loss: tensor(0.3241)\n",
      "47305 Training Loss: tensor(0.3235)\n",
      "47306 Training Loss: tensor(0.3247)\n",
      "47307 Training Loss: tensor(0.3244)\n",
      "47308 Training Loss: tensor(0.3241)\n",
      "47309 Training Loss: tensor(0.3242)\n",
      "47310 Training Loss: tensor(0.3243)\n",
      "47311 Training Loss: tensor(0.3240)\n",
      "47312 Training Loss: tensor(0.3237)\n",
      "47313 Training Loss: tensor(0.3248)\n",
      "47314 Training Loss: tensor(0.3245)\n",
      "47315 Training Loss: tensor(0.3245)\n",
      "47316 Training Loss: tensor(0.3246)\n",
      "47317 Training Loss: tensor(0.3256)\n",
      "47318 Training Loss: tensor(0.3250)\n",
      "47319 Training Loss: tensor(0.3253)\n",
      "47320 Training Loss: tensor(0.3247)\n",
      "47321 Training Loss: tensor(0.3239)\n",
      "47322 Training Loss: tensor(0.3238)\n",
      "47323 Training Loss: tensor(0.3246)\n",
      "47324 Training Loss: tensor(0.3241)\n",
      "47325 Training Loss: tensor(0.3244)\n",
      "47326 Training Loss: tensor(0.3235)\n",
      "47327 Training Loss: tensor(0.3240)\n",
      "47328 Training Loss: tensor(0.3256)\n",
      "47329 Training Loss: tensor(0.3234)\n",
      "47330 Training Loss: tensor(0.3235)\n",
      "47331 Training Loss: tensor(0.3239)\n",
      "47332 Training Loss: tensor(0.3253)\n",
      "47333 Training Loss: tensor(0.3243)\n",
      "47334 Training Loss: tensor(0.3247)\n",
      "47335 Training Loss: tensor(0.3241)\n",
      "47336 Training Loss: tensor(0.3248)\n",
      "47337 Training Loss: tensor(0.3250)\n",
      "47338 Training Loss: tensor(0.3245)\n",
      "47339 Training Loss: tensor(0.3250)\n",
      "47340 Training Loss: tensor(0.3248)\n",
      "47341 Training Loss: tensor(0.3249)\n",
      "47342 Training Loss: tensor(0.3251)\n",
      "47343 Training Loss: tensor(0.3242)\n",
      "47344 Training Loss: tensor(0.3239)\n",
      "47345 Training Loss: tensor(0.3245)\n",
      "47346 Training Loss: tensor(0.3238)\n",
      "47347 Training Loss: tensor(0.3241)\n",
      "47348 Training Loss: tensor(0.3251)\n",
      "47349 Training Loss: tensor(0.3250)\n",
      "47350 Training Loss: tensor(0.3240)\n",
      "47351 Training Loss: tensor(0.3243)\n",
      "47352 Training Loss: tensor(0.3235)\n",
      "47353 Training Loss: tensor(0.3244)\n",
      "47354 Training Loss: tensor(0.3259)\n",
      "47355 Training Loss: tensor(0.3248)\n",
      "47356 Training Loss: tensor(0.3242)\n",
      "47357 Training Loss: tensor(0.3244)\n",
      "47358 Training Loss: tensor(0.3245)\n",
      "47359 Training Loss: tensor(0.3250)\n",
      "47360 Training Loss: tensor(0.3243)\n",
      "47361 Training Loss: tensor(0.3242)\n",
      "47362 Training Loss: tensor(0.3244)\n",
      "47363 Training Loss: tensor(0.3253)\n",
      "47364 Training Loss: tensor(0.3241)\n",
      "47365 Training Loss: tensor(0.3244)\n",
      "47366 Training Loss: tensor(0.3241)\n",
      "47367 Training Loss: tensor(0.3244)\n",
      "47368 Training Loss: tensor(0.3237)\n",
      "47369 Training Loss: tensor(0.3244)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47370 Training Loss: tensor(0.3248)\n",
      "47371 Training Loss: tensor(0.3242)\n",
      "47372 Training Loss: tensor(0.3237)\n",
      "47373 Training Loss: tensor(0.3249)\n",
      "47374 Training Loss: tensor(0.3244)\n",
      "47375 Training Loss: tensor(0.3242)\n",
      "47376 Training Loss: tensor(0.3253)\n",
      "47377 Training Loss: tensor(0.3240)\n",
      "47378 Training Loss: tensor(0.3241)\n",
      "47379 Training Loss: tensor(0.3245)\n",
      "47380 Training Loss: tensor(0.3249)\n",
      "47381 Training Loss: tensor(0.3245)\n",
      "47382 Training Loss: tensor(0.3246)\n",
      "47383 Training Loss: tensor(0.3247)\n",
      "47384 Training Loss: tensor(0.3248)\n",
      "47385 Training Loss: tensor(0.3239)\n",
      "47386 Training Loss: tensor(0.3247)\n",
      "47387 Training Loss: tensor(0.3254)\n",
      "47388 Training Loss: tensor(0.3241)\n",
      "47389 Training Loss: tensor(0.3241)\n",
      "47390 Training Loss: tensor(0.3248)\n",
      "47391 Training Loss: tensor(0.3237)\n",
      "47392 Training Loss: tensor(0.3245)\n",
      "47393 Training Loss: tensor(0.3246)\n",
      "47394 Training Loss: tensor(0.3241)\n",
      "47395 Training Loss: tensor(0.3243)\n",
      "47396 Training Loss: tensor(0.3247)\n",
      "47397 Training Loss: tensor(0.3244)\n",
      "47398 Training Loss: tensor(0.3242)\n",
      "47399 Training Loss: tensor(0.3261)\n",
      "47400 Training Loss: tensor(0.3250)\n",
      "47401 Training Loss: tensor(0.3250)\n",
      "47402 Training Loss: tensor(0.3244)\n",
      "47403 Training Loss: tensor(0.3245)\n",
      "47404 Training Loss: tensor(0.3250)\n",
      "47405 Training Loss: tensor(0.3247)\n",
      "47406 Training Loss: tensor(0.3237)\n",
      "47407 Training Loss: tensor(0.3243)\n",
      "47408 Training Loss: tensor(0.3235)\n",
      "47409 Training Loss: tensor(0.3251)\n",
      "47410 Training Loss: tensor(0.3243)\n",
      "47411 Training Loss: tensor(0.3240)\n",
      "47412 Training Loss: tensor(0.3244)\n",
      "47413 Training Loss: tensor(0.3251)\n",
      "47414 Training Loss: tensor(0.3266)\n",
      "47415 Training Loss: tensor(0.3246)\n",
      "47416 Training Loss: tensor(0.3238)\n",
      "47417 Training Loss: tensor(0.3242)\n",
      "47418 Training Loss: tensor(0.3260)\n",
      "47419 Training Loss: tensor(0.3241)\n",
      "47420 Training Loss: tensor(0.3239)\n",
      "47421 Training Loss: tensor(0.3252)\n",
      "47422 Training Loss: tensor(0.3252)\n",
      "47423 Training Loss: tensor(0.3243)\n",
      "47424 Training Loss: tensor(0.3241)\n",
      "47425 Training Loss: tensor(0.3246)\n",
      "47426 Training Loss: tensor(0.3247)\n",
      "47427 Training Loss: tensor(0.3247)\n",
      "47428 Training Loss: tensor(0.3247)\n",
      "47429 Training Loss: tensor(0.3246)\n",
      "47430 Training Loss: tensor(0.3246)\n",
      "47431 Training Loss: tensor(0.3240)\n",
      "47432 Training Loss: tensor(0.3248)\n",
      "47433 Training Loss: tensor(0.3247)\n",
      "47434 Training Loss: tensor(0.3236)\n",
      "47435 Training Loss: tensor(0.3236)\n",
      "47436 Training Loss: tensor(0.3247)\n",
      "47437 Training Loss: tensor(0.3242)\n",
      "47438 Training Loss: tensor(0.3246)\n",
      "47439 Training Loss: tensor(0.3243)\n",
      "47440 Training Loss: tensor(0.3245)\n",
      "47441 Training Loss: tensor(0.3245)\n",
      "47442 Training Loss: tensor(0.3243)\n",
      "47443 Training Loss: tensor(0.3239)\n",
      "47444 Training Loss: tensor(0.3239)\n",
      "47445 Training Loss: tensor(0.3254)\n",
      "47446 Training Loss: tensor(0.3240)\n",
      "47447 Training Loss: tensor(0.3241)\n",
      "47448 Training Loss: tensor(0.3237)\n",
      "47449 Training Loss: tensor(0.3247)\n",
      "47450 Training Loss: tensor(0.3251)\n",
      "47451 Training Loss: tensor(0.3248)\n",
      "47452 Training Loss: tensor(0.3244)\n",
      "47453 Training Loss: tensor(0.3242)\n",
      "47454 Training Loss: tensor(0.3244)\n",
      "47455 Training Loss: tensor(0.3246)\n",
      "47456 Training Loss: tensor(0.3254)\n",
      "47457 Training Loss: tensor(0.3248)\n",
      "47458 Training Loss: tensor(0.3247)\n",
      "47459 Training Loss: tensor(0.3244)\n",
      "47460 Training Loss: tensor(0.3240)\n",
      "47461 Training Loss: tensor(0.3245)\n",
      "47462 Training Loss: tensor(0.3240)\n",
      "47463 Training Loss: tensor(0.3244)\n",
      "47464 Training Loss: tensor(0.3249)\n",
      "47465 Training Loss: tensor(0.3240)\n",
      "47466 Training Loss: tensor(0.3253)\n",
      "47467 Training Loss: tensor(0.3240)\n",
      "47468 Training Loss: tensor(0.3242)\n",
      "47469 Training Loss: tensor(0.3247)\n",
      "47470 Training Loss: tensor(0.3242)\n",
      "47471 Training Loss: tensor(0.3240)\n",
      "47472 Training Loss: tensor(0.3241)\n",
      "47473 Training Loss: tensor(0.3241)\n",
      "47474 Training Loss: tensor(0.3243)\n",
      "47475 Training Loss: tensor(0.3250)\n",
      "47476 Training Loss: tensor(0.3241)\n",
      "47477 Training Loss: tensor(0.3238)\n",
      "47478 Training Loss: tensor(0.3237)\n",
      "47479 Training Loss: tensor(0.3242)\n",
      "47480 Training Loss: tensor(0.3251)\n",
      "47481 Training Loss: tensor(0.3235)\n",
      "47482 Training Loss: tensor(0.3261)\n",
      "47483 Training Loss: tensor(0.3237)\n",
      "47484 Training Loss: tensor(0.3242)\n",
      "47485 Training Loss: tensor(0.3248)\n",
      "47486 Training Loss: tensor(0.3244)\n",
      "47487 Training Loss: tensor(0.3240)\n",
      "47488 Training Loss: tensor(0.3237)\n",
      "47489 Training Loss: tensor(0.3238)\n",
      "47490 Training Loss: tensor(0.3243)\n",
      "47491 Training Loss: tensor(0.3238)\n",
      "47492 Training Loss: tensor(0.3244)\n",
      "47493 Training Loss: tensor(0.3248)\n",
      "47494 Training Loss: tensor(0.3254)\n",
      "47495 Training Loss: tensor(0.3253)\n",
      "47496 Training Loss: tensor(0.3251)\n",
      "47497 Training Loss: tensor(0.3237)\n",
      "47498 Training Loss: tensor(0.3247)\n",
      "47499 Training Loss: tensor(0.3256)\n",
      "47500 Training Loss: tensor(0.3244)\n",
      "47501 Training Loss: tensor(0.3241)\n",
      "47502 Training Loss: tensor(0.3257)\n",
      "47503 Training Loss: tensor(0.3249)\n",
      "47504 Training Loss: tensor(0.3247)\n",
      "47505 Training Loss: tensor(0.3251)\n",
      "47506 Training Loss: tensor(0.3250)\n",
      "47507 Training Loss: tensor(0.3245)\n",
      "47508 Training Loss: tensor(0.3250)\n",
      "47509 Training Loss: tensor(0.3247)\n",
      "47510 Training Loss: tensor(0.3252)\n",
      "47511 Training Loss: tensor(0.3247)\n",
      "47512 Training Loss: tensor(0.3240)\n",
      "47513 Training Loss: tensor(0.3245)\n",
      "47514 Training Loss: tensor(0.3245)\n",
      "47515 Training Loss: tensor(0.3244)\n",
      "47516 Training Loss: tensor(0.3242)\n",
      "47517 Training Loss: tensor(0.3240)\n",
      "47518 Training Loss: tensor(0.3235)\n",
      "47519 Training Loss: tensor(0.3259)\n",
      "47520 Training Loss: tensor(0.3237)\n",
      "47521 Training Loss: tensor(0.3241)\n",
      "47522 Training Loss: tensor(0.3245)\n",
      "47523 Training Loss: tensor(0.3242)\n",
      "47524 Training Loss: tensor(0.3235)\n",
      "47525 Training Loss: tensor(0.3241)\n",
      "47526 Training Loss: tensor(0.3251)\n",
      "47527 Training Loss: tensor(0.3236)\n",
      "47528 Training Loss: tensor(0.3245)\n",
      "47529 Training Loss: tensor(0.3234)\n",
      "47530 Training Loss: tensor(0.3240)\n",
      "47531 Training Loss: tensor(0.3254)\n",
      "47532 Training Loss: tensor(0.3242)\n",
      "47533 Training Loss: tensor(0.3245)\n",
      "47534 Training Loss: tensor(0.3250)\n",
      "47535 Training Loss: tensor(0.3232)\n",
      "47536 Training Loss: tensor(0.3246)\n",
      "47537 Training Loss: tensor(0.3244)\n",
      "47538 Training Loss: tensor(0.3236)\n",
      "47539 Training Loss: tensor(0.3240)\n",
      "47540 Training Loss: tensor(0.3241)\n",
      "47541 Training Loss: tensor(0.3236)\n",
      "47542 Training Loss: tensor(0.3241)\n",
      "47543 Training Loss: tensor(0.3238)\n",
      "47544 Training Loss: tensor(0.3243)\n",
      "47545 Training Loss: tensor(0.3251)\n",
      "47546 Training Loss: tensor(0.3245)\n",
      "47547 Training Loss: tensor(0.3246)\n",
      "47548 Training Loss: tensor(0.3243)\n",
      "47549 Training Loss: tensor(0.3236)\n",
      "47550 Training Loss: tensor(0.3259)\n",
      "47551 Training Loss: tensor(0.3237)\n",
      "47552 Training Loss: tensor(0.3245)\n",
      "47553 Training Loss: tensor(0.3242)\n",
      "47554 Training Loss: tensor(0.3246)\n",
      "47555 Training Loss: tensor(0.3239)\n",
      "47556 Training Loss: tensor(0.3236)\n",
      "47557 Training Loss: tensor(0.3248)\n",
      "47558 Training Loss: tensor(0.3244)\n",
      "47559 Training Loss: tensor(0.3242)\n",
      "47560 Training Loss: tensor(0.3253)\n",
      "47561 Training Loss: tensor(0.3245)\n",
      "47562 Training Loss: tensor(0.3246)\n",
      "47563 Training Loss: tensor(0.3252)\n",
      "47564 Training Loss: tensor(0.3238)\n",
      "47565 Training Loss: tensor(0.3240)\n",
      "47566 Training Loss: tensor(0.3242)\n",
      "47567 Training Loss: tensor(0.3236)\n",
      "47568 Training Loss: tensor(0.3248)\n",
      "47569 Training Loss: tensor(0.3238)\n",
      "47570 Training Loss: tensor(0.3247)\n",
      "47571 Training Loss: tensor(0.3244)\n",
      "47572 Training Loss: tensor(0.3249)\n",
      "47573 Training Loss: tensor(0.3241)\n",
      "47574 Training Loss: tensor(0.3246)\n",
      "47575 Training Loss: tensor(0.3249)\n",
      "47576 Training Loss: tensor(0.3248)\n",
      "47577 Training Loss: tensor(0.3246)\n",
      "47578 Training Loss: tensor(0.3259)\n",
      "47579 Training Loss: tensor(0.3243)\n",
      "47580 Training Loss: tensor(0.3241)\n",
      "47581 Training Loss: tensor(0.3255)\n",
      "47582 Training Loss: tensor(0.3255)\n",
      "47583 Training Loss: tensor(0.3241)\n",
      "47584 Training Loss: tensor(0.3240)\n",
      "47585 Training Loss: tensor(0.3244)\n",
      "47586 Training Loss: tensor(0.3248)\n",
      "47587 Training Loss: tensor(0.3248)\n",
      "47588 Training Loss: tensor(0.3241)\n",
      "47589 Training Loss: tensor(0.3238)\n",
      "47590 Training Loss: tensor(0.3237)\n",
      "47591 Training Loss: tensor(0.3247)\n",
      "47592 Training Loss: tensor(0.3253)\n",
      "47593 Training Loss: tensor(0.3242)\n",
      "47594 Training Loss: tensor(0.3241)\n",
      "47595 Training Loss: tensor(0.3244)\n",
      "47596 Training Loss: tensor(0.3239)\n",
      "47597 Training Loss: tensor(0.3245)\n",
      "47598 Training Loss: tensor(0.3241)\n",
      "47599 Training Loss: tensor(0.3244)\n",
      "47600 Training Loss: tensor(0.3252)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47601 Training Loss: tensor(0.3249)\n",
      "47602 Training Loss: tensor(0.3250)\n",
      "47603 Training Loss: tensor(0.3247)\n",
      "47604 Training Loss: tensor(0.3245)\n",
      "47605 Training Loss: tensor(0.3246)\n",
      "47606 Training Loss: tensor(0.3242)\n",
      "47607 Training Loss: tensor(0.3248)\n",
      "47608 Training Loss: tensor(0.3254)\n",
      "47609 Training Loss: tensor(0.3250)\n",
      "47610 Training Loss: tensor(0.3238)\n",
      "47611 Training Loss: tensor(0.3241)\n",
      "47612 Training Loss: tensor(0.3241)\n",
      "47613 Training Loss: tensor(0.3244)\n",
      "47614 Training Loss: tensor(0.3248)\n",
      "47615 Training Loss: tensor(0.3250)\n",
      "47616 Training Loss: tensor(0.3237)\n",
      "47617 Training Loss: tensor(0.3246)\n",
      "47618 Training Loss: tensor(0.3236)\n",
      "47619 Training Loss: tensor(0.3239)\n",
      "47620 Training Loss: tensor(0.3237)\n",
      "47621 Training Loss: tensor(0.3240)\n",
      "47622 Training Loss: tensor(0.3239)\n",
      "47623 Training Loss: tensor(0.3253)\n",
      "47624 Training Loss: tensor(0.3241)\n",
      "47625 Training Loss: tensor(0.3241)\n",
      "47626 Training Loss: tensor(0.3243)\n",
      "47627 Training Loss: tensor(0.3241)\n",
      "47628 Training Loss: tensor(0.3238)\n",
      "47629 Training Loss: tensor(0.3238)\n",
      "47630 Training Loss: tensor(0.3235)\n",
      "47631 Training Loss: tensor(0.3238)\n",
      "47632 Training Loss: tensor(0.3238)\n",
      "47633 Training Loss: tensor(0.3238)\n",
      "47634 Training Loss: tensor(0.3238)\n",
      "47635 Training Loss: tensor(0.3238)\n",
      "47636 Training Loss: tensor(0.3244)\n",
      "47637 Training Loss: tensor(0.3242)\n",
      "47638 Training Loss: tensor(0.3248)\n",
      "47639 Training Loss: tensor(0.3251)\n",
      "47640 Training Loss: tensor(0.3248)\n",
      "47641 Training Loss: tensor(0.3239)\n",
      "47642 Training Loss: tensor(0.3246)\n",
      "47643 Training Loss: tensor(0.3246)\n",
      "47644 Training Loss: tensor(0.3244)\n",
      "47645 Training Loss: tensor(0.3241)\n",
      "47646 Training Loss: tensor(0.3250)\n",
      "47647 Training Loss: tensor(0.3252)\n",
      "47648 Training Loss: tensor(0.3250)\n",
      "47649 Training Loss: tensor(0.3244)\n",
      "47650 Training Loss: tensor(0.3254)\n",
      "47651 Training Loss: tensor(0.3246)\n",
      "47652 Training Loss: tensor(0.3242)\n",
      "47653 Training Loss: tensor(0.3240)\n",
      "47654 Training Loss: tensor(0.3248)\n",
      "47655 Training Loss: tensor(0.3238)\n",
      "47656 Training Loss: tensor(0.3242)\n",
      "47657 Training Loss: tensor(0.3239)\n",
      "47658 Training Loss: tensor(0.3247)\n",
      "47659 Training Loss: tensor(0.3240)\n",
      "47660 Training Loss: tensor(0.3237)\n",
      "47661 Training Loss: tensor(0.3240)\n",
      "47662 Training Loss: tensor(0.3246)\n",
      "47663 Training Loss: tensor(0.3250)\n",
      "47664 Training Loss: tensor(0.3246)\n",
      "47665 Training Loss: tensor(0.3241)\n",
      "47666 Training Loss: tensor(0.3259)\n",
      "47667 Training Loss: tensor(0.3239)\n",
      "47668 Training Loss: tensor(0.3244)\n",
      "47669 Training Loss: tensor(0.3242)\n",
      "47670 Training Loss: tensor(0.3242)\n",
      "47671 Training Loss: tensor(0.3242)\n",
      "47672 Training Loss: tensor(0.3249)\n",
      "47673 Training Loss: tensor(0.3246)\n",
      "47674 Training Loss: tensor(0.3240)\n",
      "47675 Training Loss: tensor(0.3243)\n",
      "47676 Training Loss: tensor(0.3240)\n",
      "47677 Training Loss: tensor(0.3237)\n",
      "47678 Training Loss: tensor(0.3233)\n",
      "47679 Training Loss: tensor(0.3247)\n",
      "47680 Training Loss: tensor(0.3244)\n",
      "47681 Training Loss: tensor(0.3244)\n",
      "47682 Training Loss: tensor(0.3248)\n",
      "47683 Training Loss: tensor(0.3246)\n",
      "47684 Training Loss: tensor(0.3243)\n",
      "47685 Training Loss: tensor(0.3241)\n",
      "47686 Training Loss: tensor(0.3259)\n",
      "47687 Training Loss: tensor(0.3244)\n",
      "47688 Training Loss: tensor(0.3247)\n",
      "47689 Training Loss: tensor(0.3249)\n",
      "47690 Training Loss: tensor(0.3249)\n",
      "47691 Training Loss: tensor(0.3246)\n",
      "47692 Training Loss: tensor(0.3234)\n",
      "47693 Training Loss: tensor(0.3247)\n",
      "47694 Training Loss: tensor(0.3235)\n",
      "47695 Training Loss: tensor(0.3242)\n",
      "47696 Training Loss: tensor(0.3237)\n",
      "47697 Training Loss: tensor(0.3240)\n",
      "47698 Training Loss: tensor(0.3240)\n",
      "47699 Training Loss: tensor(0.3246)\n",
      "47700 Training Loss: tensor(0.3245)\n",
      "47701 Training Loss: tensor(0.3258)\n",
      "47702 Training Loss: tensor(0.3240)\n",
      "47703 Training Loss: tensor(0.3249)\n",
      "47704 Training Loss: tensor(0.3252)\n",
      "47705 Training Loss: tensor(0.3239)\n",
      "47706 Training Loss: tensor(0.3252)\n",
      "47707 Training Loss: tensor(0.3241)\n",
      "47708 Training Loss: tensor(0.3244)\n",
      "47709 Training Loss: tensor(0.3241)\n",
      "47710 Training Loss: tensor(0.3240)\n",
      "47711 Training Loss: tensor(0.3244)\n",
      "47712 Training Loss: tensor(0.3253)\n",
      "47713 Training Loss: tensor(0.3244)\n",
      "47714 Training Loss: tensor(0.3245)\n",
      "47715 Training Loss: tensor(0.3239)\n",
      "47716 Training Loss: tensor(0.3253)\n",
      "47717 Training Loss: tensor(0.3244)\n",
      "47718 Training Loss: tensor(0.3244)\n",
      "47719 Training Loss: tensor(0.3252)\n",
      "47720 Training Loss: tensor(0.3245)\n",
      "47721 Training Loss: tensor(0.3247)\n",
      "47722 Training Loss: tensor(0.3240)\n",
      "47723 Training Loss: tensor(0.3239)\n",
      "47724 Training Loss: tensor(0.3243)\n",
      "47725 Training Loss: tensor(0.3240)\n",
      "47726 Training Loss: tensor(0.3244)\n",
      "47727 Training Loss: tensor(0.3240)\n",
      "47728 Training Loss: tensor(0.3242)\n",
      "47729 Training Loss: tensor(0.3244)\n",
      "47730 Training Loss: tensor(0.3249)\n",
      "47731 Training Loss: tensor(0.3240)\n",
      "47732 Training Loss: tensor(0.3243)\n",
      "47733 Training Loss: tensor(0.3240)\n",
      "47734 Training Loss: tensor(0.3245)\n",
      "47735 Training Loss: tensor(0.3244)\n",
      "47736 Training Loss: tensor(0.3243)\n",
      "47737 Training Loss: tensor(0.3252)\n",
      "47738 Training Loss: tensor(0.3238)\n",
      "47739 Training Loss: tensor(0.3242)\n",
      "47740 Training Loss: tensor(0.3246)\n",
      "47741 Training Loss: tensor(0.3245)\n",
      "47742 Training Loss: tensor(0.3241)\n",
      "47743 Training Loss: tensor(0.3244)\n",
      "47744 Training Loss: tensor(0.3239)\n",
      "47745 Training Loss: tensor(0.3242)\n",
      "47746 Training Loss: tensor(0.3254)\n",
      "47747 Training Loss: tensor(0.3248)\n",
      "47748 Training Loss: tensor(0.3248)\n",
      "47749 Training Loss: tensor(0.3248)\n",
      "47750 Training Loss: tensor(0.3239)\n",
      "47751 Training Loss: tensor(0.3246)\n",
      "47752 Training Loss: tensor(0.3240)\n",
      "47753 Training Loss: tensor(0.3247)\n",
      "47754 Training Loss: tensor(0.3236)\n",
      "47755 Training Loss: tensor(0.3246)\n",
      "47756 Training Loss: tensor(0.3241)\n",
      "47757 Training Loss: tensor(0.3243)\n",
      "47758 Training Loss: tensor(0.3238)\n",
      "47759 Training Loss: tensor(0.3241)\n",
      "47760 Training Loss: tensor(0.3244)\n",
      "47761 Training Loss: tensor(0.3242)\n",
      "47762 Training Loss: tensor(0.3254)\n",
      "47763 Training Loss: tensor(0.3250)\n",
      "47764 Training Loss: tensor(0.3241)\n",
      "47765 Training Loss: tensor(0.3236)\n",
      "47766 Training Loss: tensor(0.3241)\n",
      "47767 Training Loss: tensor(0.3241)\n",
      "47768 Training Loss: tensor(0.3255)\n",
      "47769 Training Loss: tensor(0.3246)\n",
      "47770 Training Loss: tensor(0.3244)\n",
      "47771 Training Loss: tensor(0.3254)\n",
      "47772 Training Loss: tensor(0.3246)\n",
      "47773 Training Loss: tensor(0.3237)\n",
      "47774 Training Loss: tensor(0.3246)\n",
      "47775 Training Loss: tensor(0.3240)\n",
      "47776 Training Loss: tensor(0.3240)\n",
      "47777 Training Loss: tensor(0.3251)\n",
      "47778 Training Loss: tensor(0.3248)\n",
      "47779 Training Loss: tensor(0.3244)\n",
      "47780 Training Loss: tensor(0.3246)\n",
      "47781 Training Loss: tensor(0.3243)\n",
      "47782 Training Loss: tensor(0.3247)\n",
      "47783 Training Loss: tensor(0.3240)\n",
      "47784 Training Loss: tensor(0.3245)\n",
      "47785 Training Loss: tensor(0.3241)\n",
      "47786 Training Loss: tensor(0.3238)\n",
      "47787 Training Loss: tensor(0.3240)\n",
      "47788 Training Loss: tensor(0.3252)\n",
      "47789 Training Loss: tensor(0.3243)\n",
      "47790 Training Loss: tensor(0.3243)\n",
      "47791 Training Loss: tensor(0.3249)\n",
      "47792 Training Loss: tensor(0.3259)\n",
      "47793 Training Loss: tensor(0.3249)\n",
      "47794 Training Loss: tensor(0.3252)\n",
      "47795 Training Loss: tensor(0.3240)\n",
      "47796 Training Loss: tensor(0.3239)\n",
      "47797 Training Loss: tensor(0.3248)\n",
      "47798 Training Loss: tensor(0.3256)\n",
      "47799 Training Loss: tensor(0.3245)\n",
      "47800 Training Loss: tensor(0.3254)\n",
      "47801 Training Loss: tensor(0.3242)\n",
      "47802 Training Loss: tensor(0.3245)\n",
      "47803 Training Loss: tensor(0.3253)\n",
      "47804 Training Loss: tensor(0.3240)\n",
      "47805 Training Loss: tensor(0.3246)\n",
      "47806 Training Loss: tensor(0.3242)\n",
      "47807 Training Loss: tensor(0.3242)\n",
      "47808 Training Loss: tensor(0.3245)\n",
      "47809 Training Loss: tensor(0.3244)\n",
      "47810 Training Loss: tensor(0.3261)\n",
      "47811 Training Loss: tensor(0.3243)\n",
      "47812 Training Loss: tensor(0.3241)\n",
      "47813 Training Loss: tensor(0.3250)\n",
      "47814 Training Loss: tensor(0.3253)\n",
      "47815 Training Loss: tensor(0.3241)\n",
      "47816 Training Loss: tensor(0.3243)\n",
      "47817 Training Loss: tensor(0.3241)\n",
      "47818 Training Loss: tensor(0.3242)\n",
      "47819 Training Loss: tensor(0.3251)\n",
      "47820 Training Loss: tensor(0.3237)\n",
      "47821 Training Loss: tensor(0.3242)\n",
      "47822 Training Loss: tensor(0.3242)\n",
      "47823 Training Loss: tensor(0.3245)\n",
      "47824 Training Loss: tensor(0.3246)\n",
      "47825 Training Loss: tensor(0.3237)\n",
      "47826 Training Loss: tensor(0.3241)\n",
      "47827 Training Loss: tensor(0.3246)\n",
      "47828 Training Loss: tensor(0.3252)\n",
      "47829 Training Loss: tensor(0.3260)\n",
      "47830 Training Loss: tensor(0.3244)\n",
      "47831 Training Loss: tensor(0.3244)\n",
      "47832 Training Loss: tensor(0.3244)\n",
      "47833 Training Loss: tensor(0.3246)\n",
      "47834 Training Loss: tensor(0.3247)\n",
      "47835 Training Loss: tensor(0.3253)\n",
      "47836 Training Loss: tensor(0.3244)\n",
      "47837 Training Loss: tensor(0.3243)\n",
      "47838 Training Loss: tensor(0.3246)\n",
      "47839 Training Loss: tensor(0.3241)\n",
      "47840 Training Loss: tensor(0.3243)\n",
      "47841 Training Loss: tensor(0.3243)\n",
      "47842 Training Loss: tensor(0.3249)\n",
      "47843 Training Loss: tensor(0.3242)\n",
      "47844 Training Loss: tensor(0.3248)\n",
      "47845 Training Loss: tensor(0.3240)\n",
      "47846 Training Loss: tensor(0.3245)\n",
      "47847 Training Loss: tensor(0.3244)\n",
      "47848 Training Loss: tensor(0.3241)\n",
      "47849 Training Loss: tensor(0.3243)\n",
      "47850 Training Loss: tensor(0.3247)\n",
      "47851 Training Loss: tensor(0.3245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47852 Training Loss: tensor(0.3249)\n",
      "47853 Training Loss: tensor(0.3240)\n",
      "47854 Training Loss: tensor(0.3240)\n",
      "47855 Training Loss: tensor(0.3247)\n",
      "47856 Training Loss: tensor(0.3248)\n",
      "47857 Training Loss: tensor(0.3241)\n",
      "47858 Training Loss: tensor(0.3236)\n",
      "47859 Training Loss: tensor(0.3245)\n",
      "47860 Training Loss: tensor(0.3244)\n",
      "47861 Training Loss: tensor(0.3239)\n",
      "47862 Training Loss: tensor(0.3241)\n",
      "47863 Training Loss: tensor(0.3258)\n",
      "47864 Training Loss: tensor(0.3240)\n",
      "47865 Training Loss: tensor(0.3245)\n",
      "47866 Training Loss: tensor(0.3248)\n",
      "47867 Training Loss: tensor(0.3242)\n",
      "47868 Training Loss: tensor(0.3252)\n",
      "47869 Training Loss: tensor(0.3247)\n",
      "47870 Training Loss: tensor(0.3243)\n",
      "47871 Training Loss: tensor(0.3237)\n",
      "47872 Training Loss: tensor(0.3241)\n",
      "47873 Training Loss: tensor(0.3242)\n",
      "47874 Training Loss: tensor(0.3239)\n",
      "47875 Training Loss: tensor(0.3261)\n",
      "47876 Training Loss: tensor(0.3242)\n",
      "47877 Training Loss: tensor(0.3240)\n",
      "47878 Training Loss: tensor(0.3241)\n",
      "47879 Training Loss: tensor(0.3241)\n",
      "47880 Training Loss: tensor(0.3241)\n",
      "47881 Training Loss: tensor(0.3246)\n",
      "47882 Training Loss: tensor(0.3241)\n",
      "47883 Training Loss: tensor(0.3237)\n",
      "47884 Training Loss: tensor(0.3238)\n",
      "47885 Training Loss: tensor(0.3246)\n",
      "47886 Training Loss: tensor(0.3248)\n",
      "47887 Training Loss: tensor(0.3244)\n",
      "47888 Training Loss: tensor(0.3246)\n",
      "47889 Training Loss: tensor(0.3241)\n",
      "47890 Training Loss: tensor(0.3237)\n",
      "47891 Training Loss: tensor(0.3242)\n",
      "47892 Training Loss: tensor(0.3244)\n",
      "47893 Training Loss: tensor(0.3245)\n",
      "47894 Training Loss: tensor(0.3241)\n",
      "47895 Training Loss: tensor(0.3237)\n",
      "47896 Training Loss: tensor(0.3252)\n",
      "47897 Training Loss: tensor(0.3246)\n",
      "47898 Training Loss: tensor(0.3245)\n",
      "47899 Training Loss: tensor(0.3245)\n",
      "47900 Training Loss: tensor(0.3251)\n",
      "47901 Training Loss: tensor(0.3242)\n",
      "47902 Training Loss: tensor(0.3236)\n",
      "47903 Training Loss: tensor(0.3240)\n",
      "47904 Training Loss: tensor(0.3250)\n",
      "47905 Training Loss: tensor(0.3243)\n",
      "47906 Training Loss: tensor(0.3239)\n",
      "47907 Training Loss: tensor(0.3238)\n",
      "47908 Training Loss: tensor(0.3241)\n",
      "47909 Training Loss: tensor(0.3237)\n",
      "47910 Training Loss: tensor(0.3237)\n",
      "47911 Training Loss: tensor(0.3240)\n",
      "47912 Training Loss: tensor(0.3253)\n",
      "47913 Training Loss: tensor(0.3236)\n",
      "47914 Training Loss: tensor(0.3240)\n",
      "47915 Training Loss: tensor(0.3240)\n",
      "47916 Training Loss: tensor(0.3238)\n",
      "47917 Training Loss: tensor(0.3250)\n",
      "47918 Training Loss: tensor(0.3243)\n",
      "47919 Training Loss: tensor(0.3244)\n",
      "47920 Training Loss: tensor(0.3240)\n",
      "47921 Training Loss: tensor(0.3242)\n",
      "47922 Training Loss: tensor(0.3243)\n",
      "47923 Training Loss: tensor(0.3246)\n",
      "47924 Training Loss: tensor(0.3238)\n",
      "47925 Training Loss: tensor(0.3244)\n",
      "47926 Training Loss: tensor(0.3240)\n",
      "47927 Training Loss: tensor(0.3243)\n",
      "47928 Training Loss: tensor(0.3241)\n",
      "47929 Training Loss: tensor(0.3237)\n",
      "47930 Training Loss: tensor(0.3240)\n",
      "47931 Training Loss: tensor(0.3239)\n",
      "47932 Training Loss: tensor(0.3241)\n",
      "47933 Training Loss: tensor(0.3241)\n",
      "47934 Training Loss: tensor(0.3242)\n",
      "47935 Training Loss: tensor(0.3251)\n",
      "47936 Training Loss: tensor(0.3243)\n",
      "47937 Training Loss: tensor(0.3242)\n",
      "47938 Training Loss: tensor(0.3238)\n",
      "47939 Training Loss: tensor(0.3237)\n",
      "47940 Training Loss: tensor(0.3243)\n",
      "47941 Training Loss: tensor(0.3243)\n",
      "47942 Training Loss: tensor(0.3258)\n",
      "47943 Training Loss: tensor(0.3241)\n",
      "47944 Training Loss: tensor(0.3242)\n",
      "47945 Training Loss: tensor(0.3239)\n",
      "47946 Training Loss: tensor(0.3242)\n",
      "47947 Training Loss: tensor(0.3243)\n",
      "47948 Training Loss: tensor(0.3263)\n",
      "47949 Training Loss: tensor(0.3241)\n",
      "47950 Training Loss: tensor(0.3252)\n",
      "47951 Training Loss: tensor(0.3238)\n",
      "47952 Training Loss: tensor(0.3246)\n",
      "47953 Training Loss: tensor(0.3241)\n",
      "47954 Training Loss: tensor(0.3246)\n",
      "47955 Training Loss: tensor(0.3245)\n",
      "47956 Training Loss: tensor(0.3237)\n",
      "47957 Training Loss: tensor(0.3255)\n",
      "47958 Training Loss: tensor(0.3251)\n",
      "47959 Training Loss: tensor(0.3242)\n",
      "47960 Training Loss: tensor(0.3244)\n",
      "47961 Training Loss: tensor(0.3242)\n",
      "47962 Training Loss: tensor(0.3247)\n",
      "47963 Training Loss: tensor(0.3254)\n",
      "47964 Training Loss: tensor(0.3247)\n",
      "47965 Training Loss: tensor(0.3248)\n",
      "47966 Training Loss: tensor(0.3245)\n",
      "47967 Training Loss: tensor(0.3244)\n",
      "47968 Training Loss: tensor(0.3237)\n",
      "47969 Training Loss: tensor(0.3244)\n",
      "47970 Training Loss: tensor(0.3242)\n",
      "47971 Training Loss: tensor(0.3254)\n",
      "47972 Training Loss: tensor(0.3238)\n",
      "47973 Training Loss: tensor(0.3238)\n",
      "47974 Training Loss: tensor(0.3248)\n",
      "47975 Training Loss: tensor(0.3244)\n",
      "47976 Training Loss: tensor(0.3246)\n",
      "47977 Training Loss: tensor(0.3243)\n",
      "47978 Training Loss: tensor(0.3250)\n",
      "47979 Training Loss: tensor(0.3254)\n",
      "47980 Training Loss: tensor(0.3238)\n",
      "47981 Training Loss: tensor(0.3240)\n",
      "47982 Training Loss: tensor(0.3260)\n",
      "47983 Training Loss: tensor(0.3240)\n",
      "47984 Training Loss: tensor(0.3243)\n",
      "47985 Training Loss: tensor(0.3243)\n",
      "47986 Training Loss: tensor(0.3241)\n",
      "47987 Training Loss: tensor(0.3240)\n",
      "47988 Training Loss: tensor(0.3253)\n",
      "47989 Training Loss: tensor(0.3252)\n",
      "47990 Training Loss: tensor(0.3246)\n",
      "47991 Training Loss: tensor(0.3246)\n",
      "47992 Training Loss: tensor(0.3250)\n",
      "47993 Training Loss: tensor(0.3260)\n",
      "47994 Training Loss: tensor(0.3237)\n",
      "47995 Training Loss: tensor(0.3247)\n",
      "47996 Training Loss: tensor(0.3242)\n",
      "47997 Training Loss: tensor(0.3246)\n",
      "47998 Training Loss: tensor(0.3246)\n",
      "47999 Training Loss: tensor(0.3237)\n",
      "48000 Training Loss: tensor(0.3240)\n",
      "48001 Training Loss: tensor(0.3250)\n",
      "48002 Training Loss: tensor(0.3241)\n",
      "48003 Training Loss: tensor(0.3243)\n",
      "48004 Training Loss: tensor(0.3247)\n",
      "48005 Training Loss: tensor(0.3256)\n",
      "48006 Training Loss: tensor(0.3242)\n",
      "48007 Training Loss: tensor(0.3236)\n",
      "48008 Training Loss: tensor(0.3244)\n",
      "48009 Training Loss: tensor(0.3246)\n",
      "48010 Training Loss: tensor(0.3249)\n",
      "48011 Training Loss: tensor(0.3242)\n",
      "48012 Training Loss: tensor(0.3242)\n",
      "48013 Training Loss: tensor(0.3246)\n",
      "48014 Training Loss: tensor(0.3242)\n",
      "48015 Training Loss: tensor(0.3246)\n",
      "48016 Training Loss: tensor(0.3238)\n",
      "48017 Training Loss: tensor(0.3243)\n",
      "48018 Training Loss: tensor(0.3246)\n",
      "48019 Training Loss: tensor(0.3249)\n",
      "48020 Training Loss: tensor(0.3243)\n",
      "48021 Training Loss: tensor(0.3239)\n",
      "48022 Training Loss: tensor(0.3243)\n",
      "48023 Training Loss: tensor(0.3238)\n",
      "48024 Training Loss: tensor(0.3240)\n",
      "48025 Training Loss: tensor(0.3257)\n",
      "48026 Training Loss: tensor(0.3245)\n",
      "48027 Training Loss: tensor(0.3237)\n",
      "48028 Training Loss: tensor(0.3254)\n",
      "48029 Training Loss: tensor(0.3240)\n",
      "48030 Training Loss: tensor(0.3243)\n",
      "48031 Training Loss: tensor(0.3245)\n",
      "48032 Training Loss: tensor(0.3241)\n",
      "48033 Training Loss: tensor(0.3244)\n",
      "48034 Training Loss: tensor(0.3249)\n",
      "48035 Training Loss: tensor(0.3242)\n",
      "48036 Training Loss: tensor(0.3244)\n",
      "48037 Training Loss: tensor(0.3236)\n",
      "48038 Training Loss: tensor(0.3245)\n",
      "48039 Training Loss: tensor(0.3246)\n",
      "48040 Training Loss: tensor(0.3251)\n",
      "48041 Training Loss: tensor(0.3237)\n",
      "48042 Training Loss: tensor(0.3243)\n",
      "48043 Training Loss: tensor(0.3252)\n",
      "48044 Training Loss: tensor(0.3240)\n",
      "48045 Training Loss: tensor(0.3243)\n",
      "48046 Training Loss: tensor(0.3233)\n",
      "48047 Training Loss: tensor(0.3241)\n",
      "48048 Training Loss: tensor(0.3247)\n",
      "48049 Training Loss: tensor(0.3239)\n",
      "48050 Training Loss: tensor(0.3236)\n",
      "48051 Training Loss: tensor(0.3242)\n",
      "48052 Training Loss: tensor(0.3235)\n",
      "48053 Training Loss: tensor(0.3240)\n",
      "48054 Training Loss: tensor(0.3245)\n",
      "48055 Training Loss: tensor(0.3243)\n",
      "48056 Training Loss: tensor(0.3242)\n",
      "48057 Training Loss: tensor(0.3245)\n",
      "48058 Training Loss: tensor(0.3240)\n",
      "48059 Training Loss: tensor(0.3244)\n",
      "48060 Training Loss: tensor(0.3244)\n",
      "48061 Training Loss: tensor(0.3249)\n",
      "48062 Training Loss: tensor(0.3240)\n",
      "48063 Training Loss: tensor(0.3243)\n",
      "48064 Training Loss: tensor(0.3243)\n",
      "48065 Training Loss: tensor(0.3240)\n",
      "48066 Training Loss: tensor(0.3240)\n",
      "48067 Training Loss: tensor(0.3249)\n",
      "48068 Training Loss: tensor(0.3237)\n",
      "48069 Training Loss: tensor(0.3245)\n",
      "48070 Training Loss: tensor(0.3249)\n",
      "48071 Training Loss: tensor(0.3242)\n",
      "48072 Training Loss: tensor(0.3243)\n",
      "48073 Training Loss: tensor(0.3249)\n",
      "48074 Training Loss: tensor(0.3249)\n",
      "48075 Training Loss: tensor(0.3241)\n",
      "48076 Training Loss: tensor(0.3240)\n",
      "48077 Training Loss: tensor(0.3239)\n",
      "48078 Training Loss: tensor(0.3249)\n",
      "48079 Training Loss: tensor(0.3240)\n",
      "48080 Training Loss: tensor(0.3251)\n",
      "48081 Training Loss: tensor(0.3244)\n",
      "48082 Training Loss: tensor(0.3245)\n",
      "48083 Training Loss: tensor(0.3238)\n",
      "48084 Training Loss: tensor(0.3243)\n",
      "48085 Training Loss: tensor(0.3244)\n",
      "48086 Training Loss: tensor(0.3245)\n",
      "48087 Training Loss: tensor(0.3249)\n",
      "48088 Training Loss: tensor(0.3240)\n",
      "48089 Training Loss: tensor(0.3238)\n",
      "48090 Training Loss: tensor(0.3247)\n",
      "48091 Training Loss: tensor(0.3239)\n",
      "48092 Training Loss: tensor(0.3239)\n",
      "48093 Training Loss: tensor(0.3241)\n",
      "48094 Training Loss: tensor(0.3257)\n",
      "48095 Training Loss: tensor(0.3238)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48096 Training Loss: tensor(0.3243)\n",
      "48097 Training Loss: tensor(0.3240)\n",
      "48098 Training Loss: tensor(0.3236)\n",
      "48099 Training Loss: tensor(0.3237)\n",
      "48100 Training Loss: tensor(0.3233)\n",
      "48101 Training Loss: tensor(0.3245)\n",
      "48102 Training Loss: tensor(0.3238)\n",
      "48103 Training Loss: tensor(0.3243)\n",
      "48104 Training Loss: tensor(0.3241)\n",
      "48105 Training Loss: tensor(0.3234)\n",
      "48106 Training Loss: tensor(0.3242)\n",
      "48107 Training Loss: tensor(0.3241)\n",
      "48108 Training Loss: tensor(0.3251)\n",
      "48109 Training Loss: tensor(0.3246)\n",
      "48110 Training Loss: tensor(0.3264)\n",
      "48111 Training Loss: tensor(0.3240)\n",
      "48112 Training Loss: tensor(0.3233)\n",
      "48113 Training Loss: tensor(0.3240)\n",
      "48114 Training Loss: tensor(0.3248)\n",
      "48115 Training Loss: tensor(0.3242)\n",
      "48116 Training Loss: tensor(0.3237)\n",
      "48117 Training Loss: tensor(0.3243)\n",
      "48118 Training Loss: tensor(0.3240)\n",
      "48119 Training Loss: tensor(0.3246)\n",
      "48120 Training Loss: tensor(0.3241)\n",
      "48121 Training Loss: tensor(0.3235)\n",
      "48122 Training Loss: tensor(0.3238)\n",
      "48123 Training Loss: tensor(0.3242)\n",
      "48124 Training Loss: tensor(0.3246)\n",
      "48125 Training Loss: tensor(0.3238)\n",
      "48126 Training Loss: tensor(0.3241)\n",
      "48127 Training Loss: tensor(0.3246)\n",
      "48128 Training Loss: tensor(0.3244)\n",
      "48129 Training Loss: tensor(0.3255)\n",
      "48130 Training Loss: tensor(0.3244)\n",
      "48131 Training Loss: tensor(0.3249)\n",
      "48132 Training Loss: tensor(0.3260)\n",
      "48133 Training Loss: tensor(0.3247)\n",
      "48134 Training Loss: tensor(0.3243)\n",
      "48135 Training Loss: tensor(0.3244)\n",
      "48136 Training Loss: tensor(0.3238)\n",
      "48137 Training Loss: tensor(0.3253)\n",
      "48138 Training Loss: tensor(0.3247)\n",
      "48139 Training Loss: tensor(0.3246)\n",
      "48140 Training Loss: tensor(0.3249)\n",
      "48141 Training Loss: tensor(0.3242)\n",
      "48142 Training Loss: tensor(0.3253)\n",
      "48143 Training Loss: tensor(0.3246)\n",
      "48144 Training Loss: tensor(0.3241)\n",
      "48145 Training Loss: tensor(0.3246)\n",
      "48146 Training Loss: tensor(0.3241)\n",
      "48147 Training Loss: tensor(0.3235)\n",
      "48148 Training Loss: tensor(0.3241)\n",
      "48149 Training Loss: tensor(0.3248)\n",
      "48150 Training Loss: tensor(0.3249)\n",
      "48151 Training Loss: tensor(0.3240)\n",
      "48152 Training Loss: tensor(0.3246)\n",
      "48153 Training Loss: tensor(0.3235)\n",
      "48154 Training Loss: tensor(0.3246)\n",
      "48155 Training Loss: tensor(0.3244)\n",
      "48156 Training Loss: tensor(0.3247)\n",
      "48157 Training Loss: tensor(0.3240)\n",
      "48158 Training Loss: tensor(0.3250)\n",
      "48159 Training Loss: tensor(0.3263)\n",
      "48160 Training Loss: tensor(0.3245)\n",
      "48161 Training Loss: tensor(0.3237)\n",
      "48162 Training Loss: tensor(0.3248)\n",
      "48163 Training Loss: tensor(0.3237)\n",
      "48164 Training Loss: tensor(0.3241)\n",
      "48165 Training Loss: tensor(0.3244)\n",
      "48166 Training Loss: tensor(0.3246)\n",
      "48167 Training Loss: tensor(0.3251)\n",
      "48168 Training Loss: tensor(0.3247)\n",
      "48169 Training Loss: tensor(0.3240)\n",
      "48170 Training Loss: tensor(0.3244)\n",
      "48171 Training Loss: tensor(0.3240)\n",
      "48172 Training Loss: tensor(0.3253)\n",
      "48173 Training Loss: tensor(0.3245)\n",
      "48174 Training Loss: tensor(0.3245)\n",
      "48175 Training Loss: tensor(0.3241)\n",
      "48176 Training Loss: tensor(0.3242)\n",
      "48177 Training Loss: tensor(0.3246)\n",
      "48178 Training Loss: tensor(0.3238)\n",
      "48179 Training Loss: tensor(0.3243)\n",
      "48180 Training Loss: tensor(0.3250)\n",
      "48181 Training Loss: tensor(0.3242)\n",
      "48182 Training Loss: tensor(0.3241)\n",
      "48183 Training Loss: tensor(0.3244)\n",
      "48184 Training Loss: tensor(0.3247)\n",
      "48185 Training Loss: tensor(0.3244)\n",
      "48186 Training Loss: tensor(0.3246)\n",
      "48187 Training Loss: tensor(0.3241)\n",
      "48188 Training Loss: tensor(0.3239)\n",
      "48189 Training Loss: tensor(0.3244)\n",
      "48190 Training Loss: tensor(0.3237)\n",
      "48191 Training Loss: tensor(0.3240)\n",
      "48192 Training Loss: tensor(0.3234)\n",
      "48193 Training Loss: tensor(0.3243)\n",
      "48194 Training Loss: tensor(0.3240)\n",
      "48195 Training Loss: tensor(0.3247)\n",
      "48196 Training Loss: tensor(0.3242)\n",
      "48197 Training Loss: tensor(0.3247)\n",
      "48198 Training Loss: tensor(0.3241)\n",
      "48199 Training Loss: tensor(0.3249)\n",
      "48200 Training Loss: tensor(0.3247)\n",
      "48201 Training Loss: tensor(0.3241)\n",
      "48202 Training Loss: tensor(0.3239)\n",
      "48203 Training Loss: tensor(0.3243)\n",
      "48204 Training Loss: tensor(0.3263)\n",
      "48205 Training Loss: tensor(0.3241)\n",
      "48206 Training Loss: tensor(0.3245)\n",
      "48207 Training Loss: tensor(0.3259)\n",
      "48208 Training Loss: tensor(0.3245)\n",
      "48209 Training Loss: tensor(0.3242)\n",
      "48210 Training Loss: tensor(0.3242)\n",
      "48211 Training Loss: tensor(0.3255)\n",
      "48212 Training Loss: tensor(0.3251)\n",
      "48213 Training Loss: tensor(0.3245)\n",
      "48214 Training Loss: tensor(0.3238)\n",
      "48215 Training Loss: tensor(0.3256)\n",
      "48216 Training Loss: tensor(0.3260)\n",
      "48217 Training Loss: tensor(0.3248)\n",
      "48218 Training Loss: tensor(0.3243)\n",
      "48219 Training Loss: tensor(0.3238)\n",
      "48220 Training Loss: tensor(0.3245)\n",
      "48221 Training Loss: tensor(0.3240)\n",
      "48222 Training Loss: tensor(0.3243)\n",
      "48223 Training Loss: tensor(0.3245)\n",
      "48224 Training Loss: tensor(0.3240)\n",
      "48225 Training Loss: tensor(0.3240)\n",
      "48226 Training Loss: tensor(0.3233)\n",
      "48227 Training Loss: tensor(0.3249)\n",
      "48228 Training Loss: tensor(0.3267)\n",
      "48229 Training Loss: tensor(0.3240)\n",
      "48230 Training Loss: tensor(0.3236)\n",
      "48231 Training Loss: tensor(0.3236)\n",
      "48232 Training Loss: tensor(0.3239)\n",
      "48233 Training Loss: tensor(0.3244)\n",
      "48234 Training Loss: tensor(0.3247)\n",
      "48235 Training Loss: tensor(0.3242)\n",
      "48236 Training Loss: tensor(0.3235)\n",
      "48237 Training Loss: tensor(0.3240)\n",
      "48238 Training Loss: tensor(0.3238)\n",
      "48239 Training Loss: tensor(0.3236)\n",
      "48240 Training Loss: tensor(0.3240)\n",
      "48241 Training Loss: tensor(0.3244)\n",
      "48242 Training Loss: tensor(0.3242)\n",
      "48243 Training Loss: tensor(0.3246)\n",
      "48244 Training Loss: tensor(0.3259)\n",
      "48245 Training Loss: tensor(0.3248)\n",
      "48246 Training Loss: tensor(0.3236)\n",
      "48247 Training Loss: tensor(0.3247)\n",
      "48248 Training Loss: tensor(0.3240)\n",
      "48249 Training Loss: tensor(0.3242)\n",
      "48250 Training Loss: tensor(0.3246)\n",
      "48251 Training Loss: tensor(0.3252)\n",
      "48252 Training Loss: tensor(0.3249)\n",
      "48253 Training Loss: tensor(0.3241)\n",
      "48254 Training Loss: tensor(0.3244)\n",
      "48255 Training Loss: tensor(0.3236)\n",
      "48256 Training Loss: tensor(0.3236)\n",
      "48257 Training Loss: tensor(0.3235)\n",
      "48258 Training Loss: tensor(0.3241)\n",
      "48259 Training Loss: tensor(0.3246)\n",
      "48260 Training Loss: tensor(0.3246)\n",
      "48261 Training Loss: tensor(0.3242)\n",
      "48262 Training Loss: tensor(0.3244)\n",
      "48263 Training Loss: tensor(0.3249)\n",
      "48264 Training Loss: tensor(0.3244)\n",
      "48265 Training Loss: tensor(0.3243)\n",
      "48266 Training Loss: tensor(0.3248)\n",
      "48267 Training Loss: tensor(0.3238)\n",
      "48268 Training Loss: tensor(0.3244)\n",
      "48269 Training Loss: tensor(0.3240)\n",
      "48270 Training Loss: tensor(0.3234)\n",
      "48271 Training Loss: tensor(0.3246)\n",
      "48272 Training Loss: tensor(0.3250)\n",
      "48273 Training Loss: tensor(0.3236)\n",
      "48274 Training Loss: tensor(0.3255)\n",
      "48275 Training Loss: tensor(0.3252)\n",
      "48276 Training Loss: tensor(0.3241)\n",
      "48277 Training Loss: tensor(0.3243)\n",
      "48278 Training Loss: tensor(0.3242)\n",
      "48279 Training Loss: tensor(0.3248)\n",
      "48280 Training Loss: tensor(0.3240)\n",
      "48281 Training Loss: tensor(0.3236)\n",
      "48282 Training Loss: tensor(0.3239)\n",
      "48283 Training Loss: tensor(0.3242)\n",
      "48284 Training Loss: tensor(0.3247)\n",
      "48285 Training Loss: tensor(0.3255)\n",
      "48286 Training Loss: tensor(0.3239)\n",
      "48287 Training Loss: tensor(0.3248)\n",
      "48288 Training Loss: tensor(0.3250)\n",
      "48289 Training Loss: tensor(0.3244)\n",
      "48290 Training Loss: tensor(0.3247)\n",
      "48291 Training Loss: tensor(0.3239)\n",
      "48292 Training Loss: tensor(0.3243)\n",
      "48293 Training Loss: tensor(0.3250)\n",
      "48294 Training Loss: tensor(0.3242)\n",
      "48295 Training Loss: tensor(0.3241)\n",
      "48296 Training Loss: tensor(0.3245)\n",
      "48297 Training Loss: tensor(0.3240)\n",
      "48298 Training Loss: tensor(0.3242)\n",
      "48299 Training Loss: tensor(0.3247)\n",
      "48300 Training Loss: tensor(0.3245)\n",
      "48301 Training Loss: tensor(0.3240)\n",
      "48302 Training Loss: tensor(0.3247)\n",
      "48303 Training Loss: tensor(0.3249)\n",
      "48304 Training Loss: tensor(0.3240)\n",
      "48305 Training Loss: tensor(0.3237)\n",
      "48306 Training Loss: tensor(0.3239)\n",
      "48307 Training Loss: tensor(0.3249)\n",
      "48308 Training Loss: tensor(0.3249)\n",
      "48309 Training Loss: tensor(0.3244)\n",
      "48310 Training Loss: tensor(0.3249)\n",
      "48311 Training Loss: tensor(0.3257)\n",
      "48312 Training Loss: tensor(0.3243)\n",
      "48313 Training Loss: tensor(0.3243)\n",
      "48314 Training Loss: tensor(0.3252)\n",
      "48315 Training Loss: tensor(0.3246)\n",
      "48316 Training Loss: tensor(0.3250)\n",
      "48317 Training Loss: tensor(0.3244)\n",
      "48318 Training Loss: tensor(0.3249)\n",
      "48319 Training Loss: tensor(0.3248)\n",
      "48320 Training Loss: tensor(0.3254)\n",
      "48321 Training Loss: tensor(0.3246)\n",
      "48322 Training Loss: tensor(0.3240)\n",
      "48323 Training Loss: tensor(0.3244)\n",
      "48324 Training Loss: tensor(0.3240)\n",
      "48325 Training Loss: tensor(0.3247)\n",
      "48326 Training Loss: tensor(0.3243)\n",
      "48327 Training Loss: tensor(0.3247)\n",
      "48328 Training Loss: tensor(0.3246)\n",
      "48329 Training Loss: tensor(0.3248)\n",
      "48330 Training Loss: tensor(0.3240)\n",
      "48331 Training Loss: tensor(0.3242)\n",
      "48332 Training Loss: tensor(0.3249)\n",
      "48333 Training Loss: tensor(0.3244)\n",
      "48334 Training Loss: tensor(0.3245)\n",
      "48335 Training Loss: tensor(0.3238)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48336 Training Loss: tensor(0.3242)\n",
      "48337 Training Loss: tensor(0.3241)\n",
      "48338 Training Loss: tensor(0.3244)\n",
      "48339 Training Loss: tensor(0.3242)\n",
      "48340 Training Loss: tensor(0.3242)\n",
      "48341 Training Loss: tensor(0.3243)\n",
      "48342 Training Loss: tensor(0.3240)\n",
      "48343 Training Loss: tensor(0.3245)\n",
      "48344 Training Loss: tensor(0.3249)\n",
      "48345 Training Loss: tensor(0.3240)\n",
      "48346 Training Loss: tensor(0.3242)\n",
      "48347 Training Loss: tensor(0.3249)\n",
      "48348 Training Loss: tensor(0.3245)\n",
      "48349 Training Loss: tensor(0.3247)\n",
      "48350 Training Loss: tensor(0.3238)\n",
      "48351 Training Loss: tensor(0.3249)\n",
      "48352 Training Loss: tensor(0.3244)\n",
      "48353 Training Loss: tensor(0.3242)\n",
      "48354 Training Loss: tensor(0.3244)\n",
      "48355 Training Loss: tensor(0.3248)\n",
      "48356 Training Loss: tensor(0.3243)\n",
      "48357 Training Loss: tensor(0.3236)\n",
      "48358 Training Loss: tensor(0.3240)\n",
      "48359 Training Loss: tensor(0.3239)\n",
      "48360 Training Loss: tensor(0.3235)\n",
      "48361 Training Loss: tensor(0.3242)\n",
      "48362 Training Loss: tensor(0.3242)\n",
      "48363 Training Loss: tensor(0.3243)\n",
      "48364 Training Loss: tensor(0.3243)\n",
      "48365 Training Loss: tensor(0.3247)\n",
      "48366 Training Loss: tensor(0.3244)\n",
      "48367 Training Loss: tensor(0.3241)\n",
      "48368 Training Loss: tensor(0.3259)\n",
      "48369 Training Loss: tensor(0.3252)\n",
      "48370 Training Loss: tensor(0.3261)\n",
      "48371 Training Loss: tensor(0.3235)\n",
      "48372 Training Loss: tensor(0.3242)\n",
      "48373 Training Loss: tensor(0.3250)\n",
      "48374 Training Loss: tensor(0.3239)\n",
      "48375 Training Loss: tensor(0.3242)\n",
      "48376 Training Loss: tensor(0.3238)\n",
      "48377 Training Loss: tensor(0.3241)\n",
      "48378 Training Loss: tensor(0.3238)\n",
      "48379 Training Loss: tensor(0.3251)\n",
      "48380 Training Loss: tensor(0.3238)\n",
      "48381 Training Loss: tensor(0.3238)\n",
      "48382 Training Loss: tensor(0.3247)\n",
      "48383 Training Loss: tensor(0.3238)\n",
      "48384 Training Loss: tensor(0.3240)\n",
      "48385 Training Loss: tensor(0.3250)\n",
      "48386 Training Loss: tensor(0.3244)\n",
      "48387 Training Loss: tensor(0.3247)\n",
      "48388 Training Loss: tensor(0.3243)\n",
      "48389 Training Loss: tensor(0.3257)\n",
      "48390 Training Loss: tensor(0.3237)\n",
      "48391 Training Loss: tensor(0.3243)\n",
      "48392 Training Loss: tensor(0.3241)\n",
      "48393 Training Loss: tensor(0.3242)\n",
      "48394 Training Loss: tensor(0.3243)\n",
      "48395 Training Loss: tensor(0.3245)\n",
      "48396 Training Loss: tensor(0.3241)\n",
      "48397 Training Loss: tensor(0.3251)\n",
      "48398 Training Loss: tensor(0.3247)\n",
      "48399 Training Loss: tensor(0.3236)\n",
      "48400 Training Loss: tensor(0.3240)\n",
      "48401 Training Loss: tensor(0.3249)\n",
      "48402 Training Loss: tensor(0.3239)\n",
      "48403 Training Loss: tensor(0.3260)\n",
      "48404 Training Loss: tensor(0.3246)\n",
      "48405 Training Loss: tensor(0.3245)\n",
      "48406 Training Loss: tensor(0.3254)\n",
      "48407 Training Loss: tensor(0.3245)\n",
      "48408 Training Loss: tensor(0.3252)\n",
      "48409 Training Loss: tensor(0.3248)\n",
      "48410 Training Loss: tensor(0.3249)\n",
      "48411 Training Loss: tensor(0.3248)\n",
      "48412 Training Loss: tensor(0.3248)\n",
      "48413 Training Loss: tensor(0.3245)\n",
      "48414 Training Loss: tensor(0.3254)\n",
      "48415 Training Loss: tensor(0.3251)\n",
      "48416 Training Loss: tensor(0.3243)\n",
      "48417 Training Loss: tensor(0.3245)\n",
      "48418 Training Loss: tensor(0.3266)\n",
      "48419 Training Loss: tensor(0.3243)\n",
      "48420 Training Loss: tensor(0.3249)\n",
      "48421 Training Loss: tensor(0.3252)\n",
      "48422 Training Loss: tensor(0.3245)\n",
      "48423 Training Loss: tensor(0.3242)\n",
      "48424 Training Loss: tensor(0.3250)\n",
      "48425 Training Loss: tensor(0.3259)\n",
      "48426 Training Loss: tensor(0.3243)\n",
      "48427 Training Loss: tensor(0.3244)\n",
      "48428 Training Loss: tensor(0.3240)\n",
      "48429 Training Loss: tensor(0.3254)\n",
      "48430 Training Loss: tensor(0.3242)\n",
      "48431 Training Loss: tensor(0.3245)\n",
      "48432 Training Loss: tensor(0.3244)\n",
      "48433 Training Loss: tensor(0.3247)\n",
      "48434 Training Loss: tensor(0.3245)\n",
      "48435 Training Loss: tensor(0.3242)\n",
      "48436 Training Loss: tensor(0.3255)\n",
      "48437 Training Loss: tensor(0.3249)\n",
      "48438 Training Loss: tensor(0.3241)\n",
      "48439 Training Loss: tensor(0.3251)\n",
      "48440 Training Loss: tensor(0.3243)\n",
      "48441 Training Loss: tensor(0.3247)\n",
      "48442 Training Loss: tensor(0.3246)\n",
      "48443 Training Loss: tensor(0.3247)\n",
      "48444 Training Loss: tensor(0.3247)\n",
      "48445 Training Loss: tensor(0.3250)\n",
      "48446 Training Loss: tensor(0.3242)\n",
      "48447 Training Loss: tensor(0.3240)\n",
      "48448 Training Loss: tensor(0.3240)\n",
      "48449 Training Loss: tensor(0.3249)\n",
      "48450 Training Loss: tensor(0.3251)\n",
      "48451 Training Loss: tensor(0.3238)\n",
      "48452 Training Loss: tensor(0.3248)\n",
      "48453 Training Loss: tensor(0.3247)\n",
      "48454 Training Loss: tensor(0.3244)\n",
      "48455 Training Loss: tensor(0.3242)\n",
      "48456 Training Loss: tensor(0.3244)\n",
      "48457 Training Loss: tensor(0.3243)\n",
      "48458 Training Loss: tensor(0.3239)\n",
      "48459 Training Loss: tensor(0.3236)\n",
      "48460 Training Loss: tensor(0.3239)\n",
      "48461 Training Loss: tensor(0.3243)\n",
      "48462 Training Loss: tensor(0.3247)\n",
      "48463 Training Loss: tensor(0.3243)\n",
      "48464 Training Loss: tensor(0.3237)\n",
      "48465 Training Loss: tensor(0.3249)\n",
      "48466 Training Loss: tensor(0.3246)\n",
      "48467 Training Loss: tensor(0.3242)\n",
      "48468 Training Loss: tensor(0.3242)\n",
      "48469 Training Loss: tensor(0.3246)\n",
      "48470 Training Loss: tensor(0.3246)\n",
      "48471 Training Loss: tensor(0.3250)\n",
      "48472 Training Loss: tensor(0.3239)\n",
      "48473 Training Loss: tensor(0.3238)\n",
      "48474 Training Loss: tensor(0.3235)\n",
      "48475 Training Loss: tensor(0.3246)\n",
      "48476 Training Loss: tensor(0.3243)\n",
      "48477 Training Loss: tensor(0.3247)\n",
      "48478 Training Loss: tensor(0.3239)\n",
      "48479 Training Loss: tensor(0.3252)\n",
      "48480 Training Loss: tensor(0.3238)\n",
      "48481 Training Loss: tensor(0.3246)\n",
      "48482 Training Loss: tensor(0.3241)\n",
      "48483 Training Loss: tensor(0.3240)\n",
      "48484 Training Loss: tensor(0.3248)\n",
      "48485 Training Loss: tensor(0.3249)\n",
      "48486 Training Loss: tensor(0.3236)\n",
      "48487 Training Loss: tensor(0.3247)\n",
      "48488 Training Loss: tensor(0.3244)\n",
      "48489 Training Loss: tensor(0.3243)\n",
      "48490 Training Loss: tensor(0.3240)\n",
      "48491 Training Loss: tensor(0.3239)\n",
      "48492 Training Loss: tensor(0.3245)\n",
      "48493 Training Loss: tensor(0.3250)\n",
      "48494 Training Loss: tensor(0.3245)\n",
      "48495 Training Loss: tensor(0.3249)\n",
      "48496 Training Loss: tensor(0.3240)\n",
      "48497 Training Loss: tensor(0.3247)\n",
      "48498 Training Loss: tensor(0.3236)\n",
      "48499 Training Loss: tensor(0.3239)\n",
      "48500 Training Loss: tensor(0.3251)\n",
      "48501 Training Loss: tensor(0.3247)\n",
      "48502 Training Loss: tensor(0.3242)\n",
      "48503 Training Loss: tensor(0.3235)\n",
      "48504 Training Loss: tensor(0.3242)\n",
      "48505 Training Loss: tensor(0.3250)\n",
      "48506 Training Loss: tensor(0.3246)\n",
      "48507 Training Loss: tensor(0.3253)\n",
      "48508 Training Loss: tensor(0.3248)\n",
      "48509 Training Loss: tensor(0.3243)\n",
      "48510 Training Loss: tensor(0.3245)\n",
      "48511 Training Loss: tensor(0.3245)\n",
      "48512 Training Loss: tensor(0.3238)\n",
      "48513 Training Loss: tensor(0.3241)\n",
      "48514 Training Loss: tensor(0.3245)\n",
      "48515 Training Loss: tensor(0.3244)\n",
      "48516 Training Loss: tensor(0.3257)\n",
      "48517 Training Loss: tensor(0.3238)\n",
      "48518 Training Loss: tensor(0.3249)\n",
      "48519 Training Loss: tensor(0.3241)\n",
      "48520 Training Loss: tensor(0.3247)\n",
      "48521 Training Loss: tensor(0.3252)\n",
      "48522 Training Loss: tensor(0.3243)\n",
      "48523 Training Loss: tensor(0.3246)\n",
      "48524 Training Loss: tensor(0.3245)\n",
      "48525 Training Loss: tensor(0.3239)\n",
      "48526 Training Loss: tensor(0.3245)\n",
      "48527 Training Loss: tensor(0.3240)\n",
      "48528 Training Loss: tensor(0.3247)\n",
      "48529 Training Loss: tensor(0.3247)\n",
      "48530 Training Loss: tensor(0.3250)\n",
      "48531 Training Loss: tensor(0.3239)\n",
      "48532 Training Loss: tensor(0.3267)\n",
      "48533 Training Loss: tensor(0.3247)\n",
      "48534 Training Loss: tensor(0.3240)\n",
      "48535 Training Loss: tensor(0.3243)\n",
      "48536 Training Loss: tensor(0.3239)\n",
      "48537 Training Loss: tensor(0.3244)\n",
      "48538 Training Loss: tensor(0.3249)\n",
      "48539 Training Loss: tensor(0.3251)\n",
      "48540 Training Loss: tensor(0.3243)\n",
      "48541 Training Loss: tensor(0.3237)\n",
      "48542 Training Loss: tensor(0.3251)\n",
      "48543 Training Loss: tensor(0.3240)\n",
      "48544 Training Loss: tensor(0.3249)\n",
      "48545 Training Loss: tensor(0.3255)\n",
      "48546 Training Loss: tensor(0.3243)\n",
      "48547 Training Loss: tensor(0.3255)\n",
      "48548 Training Loss: tensor(0.3238)\n",
      "48549 Training Loss: tensor(0.3236)\n",
      "48550 Training Loss: tensor(0.3242)\n",
      "48551 Training Loss: tensor(0.3249)\n",
      "48552 Training Loss: tensor(0.3247)\n",
      "48553 Training Loss: tensor(0.3244)\n",
      "48554 Training Loss: tensor(0.3250)\n",
      "48555 Training Loss: tensor(0.3240)\n",
      "48556 Training Loss: tensor(0.3238)\n",
      "48557 Training Loss: tensor(0.3251)\n",
      "48558 Training Loss: tensor(0.3240)\n",
      "48559 Training Loss: tensor(0.3245)\n",
      "48560 Training Loss: tensor(0.3239)\n",
      "48561 Training Loss: tensor(0.3239)\n",
      "48562 Training Loss: tensor(0.3241)\n",
      "48563 Training Loss: tensor(0.3238)\n",
      "48564 Training Loss: tensor(0.3244)\n",
      "48565 Training Loss: tensor(0.3251)\n",
      "48566 Training Loss: tensor(0.3245)\n",
      "48567 Training Loss: tensor(0.3248)\n",
      "48568 Training Loss: tensor(0.3246)\n",
      "48569 Training Loss: tensor(0.3246)\n",
      "48570 Training Loss: tensor(0.3246)\n",
      "48571 Training Loss: tensor(0.3241)\n",
      "48572 Training Loss: tensor(0.3239)\n",
      "48573 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48574 Training Loss: tensor(0.3243)\n",
      "48575 Training Loss: tensor(0.3236)\n",
      "48576 Training Loss: tensor(0.3249)\n",
      "48577 Training Loss: tensor(0.3235)\n",
      "48578 Training Loss: tensor(0.3243)\n",
      "48579 Training Loss: tensor(0.3251)\n",
      "48580 Training Loss: tensor(0.3248)\n",
      "48581 Training Loss: tensor(0.3236)\n",
      "48582 Training Loss: tensor(0.3250)\n",
      "48583 Training Loss: tensor(0.3238)\n",
      "48584 Training Loss: tensor(0.3241)\n",
      "48585 Training Loss: tensor(0.3251)\n",
      "48586 Training Loss: tensor(0.3246)\n",
      "48587 Training Loss: tensor(0.3247)\n",
      "48588 Training Loss: tensor(0.3250)\n",
      "48589 Training Loss: tensor(0.3243)\n",
      "48590 Training Loss: tensor(0.3245)\n",
      "48591 Training Loss: tensor(0.3243)\n",
      "48592 Training Loss: tensor(0.3237)\n",
      "48593 Training Loss: tensor(0.3252)\n",
      "48594 Training Loss: tensor(0.3246)\n",
      "48595 Training Loss: tensor(0.3241)\n",
      "48596 Training Loss: tensor(0.3253)\n",
      "48597 Training Loss: tensor(0.3249)\n",
      "48598 Training Loss: tensor(0.3245)\n",
      "48599 Training Loss: tensor(0.3237)\n",
      "48600 Training Loss: tensor(0.3257)\n",
      "48601 Training Loss: tensor(0.3250)\n",
      "48602 Training Loss: tensor(0.3245)\n",
      "48603 Training Loss: tensor(0.3244)\n",
      "48604 Training Loss: tensor(0.3248)\n",
      "48605 Training Loss: tensor(0.3243)\n",
      "48606 Training Loss: tensor(0.3241)\n",
      "48607 Training Loss: tensor(0.3244)\n",
      "48608 Training Loss: tensor(0.3248)\n",
      "48609 Training Loss: tensor(0.3252)\n",
      "48610 Training Loss: tensor(0.3248)\n",
      "48611 Training Loss: tensor(0.3241)\n",
      "48612 Training Loss: tensor(0.3242)\n",
      "48613 Training Loss: tensor(0.3249)\n",
      "48614 Training Loss: tensor(0.3244)\n",
      "48615 Training Loss: tensor(0.3245)\n",
      "48616 Training Loss: tensor(0.3242)\n",
      "48617 Training Loss: tensor(0.3245)\n",
      "48618 Training Loss: tensor(0.3238)\n",
      "48619 Training Loss: tensor(0.3248)\n",
      "48620 Training Loss: tensor(0.3236)\n",
      "48621 Training Loss: tensor(0.3260)\n",
      "48622 Training Loss: tensor(0.3244)\n",
      "48623 Training Loss: tensor(0.3257)\n",
      "48624 Training Loss: tensor(0.3244)\n",
      "48625 Training Loss: tensor(0.3245)\n",
      "48626 Training Loss: tensor(0.3242)\n",
      "48627 Training Loss: tensor(0.3238)\n",
      "48628 Training Loss: tensor(0.3245)\n",
      "48629 Training Loss: tensor(0.3249)\n",
      "48630 Training Loss: tensor(0.3245)\n",
      "48631 Training Loss: tensor(0.3241)\n",
      "48632 Training Loss: tensor(0.3243)\n",
      "48633 Training Loss: tensor(0.3240)\n",
      "48634 Training Loss: tensor(0.3243)\n",
      "48635 Training Loss: tensor(0.3245)\n",
      "48636 Training Loss: tensor(0.3244)\n",
      "48637 Training Loss: tensor(0.3235)\n",
      "48638 Training Loss: tensor(0.3251)\n",
      "48639 Training Loss: tensor(0.3259)\n",
      "48640 Training Loss: tensor(0.3237)\n",
      "48641 Training Loss: tensor(0.3240)\n",
      "48642 Training Loss: tensor(0.3254)\n",
      "48643 Training Loss: tensor(0.3243)\n",
      "48644 Training Loss: tensor(0.3238)\n",
      "48645 Training Loss: tensor(0.3257)\n",
      "48646 Training Loss: tensor(0.3242)\n",
      "48647 Training Loss: tensor(0.3237)\n",
      "48648 Training Loss: tensor(0.3245)\n",
      "48649 Training Loss: tensor(0.3242)\n",
      "48650 Training Loss: tensor(0.3249)\n",
      "48651 Training Loss: tensor(0.3242)\n",
      "48652 Training Loss: tensor(0.3240)\n",
      "48653 Training Loss: tensor(0.3241)\n",
      "48654 Training Loss: tensor(0.3242)\n",
      "48655 Training Loss: tensor(0.3240)\n",
      "48656 Training Loss: tensor(0.3243)\n",
      "48657 Training Loss: tensor(0.3243)\n",
      "48658 Training Loss: tensor(0.3240)\n",
      "48659 Training Loss: tensor(0.3241)\n",
      "48660 Training Loss: tensor(0.3242)\n",
      "48661 Training Loss: tensor(0.3253)\n",
      "48662 Training Loss: tensor(0.3238)\n",
      "48663 Training Loss: tensor(0.3239)\n",
      "48664 Training Loss: tensor(0.3244)\n",
      "48665 Training Loss: tensor(0.3239)\n",
      "48666 Training Loss: tensor(0.3245)\n",
      "48667 Training Loss: tensor(0.3243)\n",
      "48668 Training Loss: tensor(0.3251)\n",
      "48669 Training Loss: tensor(0.3246)\n",
      "48670 Training Loss: tensor(0.3244)\n",
      "48671 Training Loss: tensor(0.3246)\n",
      "48672 Training Loss: tensor(0.3239)\n",
      "48673 Training Loss: tensor(0.3238)\n",
      "48674 Training Loss: tensor(0.3251)\n",
      "48675 Training Loss: tensor(0.3244)\n",
      "48676 Training Loss: tensor(0.3239)\n",
      "48677 Training Loss: tensor(0.3247)\n",
      "48678 Training Loss: tensor(0.3244)\n",
      "48679 Training Loss: tensor(0.3239)\n",
      "48680 Training Loss: tensor(0.3253)\n",
      "48681 Training Loss: tensor(0.3249)\n",
      "48682 Training Loss: tensor(0.3247)\n",
      "48683 Training Loss: tensor(0.3250)\n",
      "48684 Training Loss: tensor(0.3242)\n",
      "48685 Training Loss: tensor(0.3251)\n",
      "48686 Training Loss: tensor(0.3246)\n",
      "48687 Training Loss: tensor(0.3242)\n",
      "48688 Training Loss: tensor(0.3244)\n",
      "48689 Training Loss: tensor(0.3242)\n",
      "48690 Training Loss: tensor(0.3251)\n",
      "48691 Training Loss: tensor(0.3252)\n",
      "48692 Training Loss: tensor(0.3249)\n",
      "48693 Training Loss: tensor(0.3256)\n",
      "48694 Training Loss: tensor(0.3252)\n",
      "48695 Training Loss: tensor(0.3242)\n",
      "48696 Training Loss: tensor(0.3244)\n",
      "48697 Training Loss: tensor(0.3238)\n",
      "48698 Training Loss: tensor(0.3253)\n",
      "48699 Training Loss: tensor(0.3240)\n",
      "48700 Training Loss: tensor(0.3242)\n",
      "48701 Training Loss: tensor(0.3241)\n",
      "48702 Training Loss: tensor(0.3242)\n",
      "48703 Training Loss: tensor(0.3251)\n",
      "48704 Training Loss: tensor(0.3236)\n",
      "48705 Training Loss: tensor(0.3238)\n",
      "48706 Training Loss: tensor(0.3246)\n",
      "48707 Training Loss: tensor(0.3240)\n",
      "48708 Training Loss: tensor(0.3242)\n",
      "48709 Training Loss: tensor(0.3241)\n",
      "48710 Training Loss: tensor(0.3253)\n",
      "48711 Training Loss: tensor(0.3240)\n",
      "48712 Training Loss: tensor(0.3247)\n",
      "48713 Training Loss: tensor(0.3246)\n",
      "48714 Training Loss: tensor(0.3236)\n",
      "48715 Training Loss: tensor(0.3241)\n",
      "48716 Training Loss: tensor(0.3247)\n",
      "48717 Training Loss: tensor(0.3244)\n",
      "48718 Training Loss: tensor(0.3238)\n",
      "48719 Training Loss: tensor(0.3244)\n",
      "48720 Training Loss: tensor(0.3242)\n",
      "48721 Training Loss: tensor(0.3251)\n",
      "48722 Training Loss: tensor(0.3253)\n",
      "48723 Training Loss: tensor(0.3257)\n",
      "48724 Training Loss: tensor(0.3260)\n",
      "48725 Training Loss: tensor(0.3247)\n",
      "48726 Training Loss: tensor(0.3249)\n",
      "48727 Training Loss: tensor(0.3249)\n",
      "48728 Training Loss: tensor(0.3242)\n",
      "48729 Training Loss: tensor(0.3242)\n",
      "48730 Training Loss: tensor(0.3244)\n",
      "48731 Training Loss: tensor(0.3242)\n",
      "48732 Training Loss: tensor(0.3239)\n",
      "48733 Training Loss: tensor(0.3239)\n",
      "48734 Training Loss: tensor(0.3239)\n",
      "48735 Training Loss: tensor(0.3245)\n",
      "48736 Training Loss: tensor(0.3242)\n",
      "48737 Training Loss: tensor(0.3243)\n",
      "48738 Training Loss: tensor(0.3241)\n",
      "48739 Training Loss: tensor(0.3241)\n",
      "48740 Training Loss: tensor(0.3248)\n",
      "48741 Training Loss: tensor(0.3242)\n",
      "48742 Training Loss: tensor(0.3241)\n",
      "48743 Training Loss: tensor(0.3243)\n",
      "48744 Training Loss: tensor(0.3257)\n",
      "48745 Training Loss: tensor(0.3245)\n",
      "48746 Training Loss: tensor(0.3245)\n",
      "48747 Training Loss: tensor(0.3238)\n",
      "48748 Training Loss: tensor(0.3241)\n",
      "48749 Training Loss: tensor(0.3240)\n",
      "48750 Training Loss: tensor(0.3247)\n",
      "48751 Training Loss: tensor(0.3248)\n",
      "48752 Training Loss: tensor(0.3250)\n",
      "48753 Training Loss: tensor(0.3239)\n",
      "48754 Training Loss: tensor(0.3240)\n",
      "48755 Training Loss: tensor(0.3239)\n",
      "48756 Training Loss: tensor(0.3249)\n",
      "48757 Training Loss: tensor(0.3238)\n",
      "48758 Training Loss: tensor(0.3249)\n",
      "48759 Training Loss: tensor(0.3239)\n",
      "48760 Training Loss: tensor(0.3243)\n",
      "48761 Training Loss: tensor(0.3243)\n",
      "48762 Training Loss: tensor(0.3252)\n",
      "48763 Training Loss: tensor(0.3240)\n",
      "48764 Training Loss: tensor(0.3246)\n",
      "48765 Training Loss: tensor(0.3245)\n",
      "48766 Training Loss: tensor(0.3242)\n",
      "48767 Training Loss: tensor(0.3238)\n",
      "48768 Training Loss: tensor(0.3240)\n",
      "48769 Training Loss: tensor(0.3239)\n",
      "48770 Training Loss: tensor(0.3246)\n",
      "48771 Training Loss: tensor(0.3248)\n",
      "48772 Training Loss: tensor(0.3240)\n",
      "48773 Training Loss: tensor(0.3241)\n",
      "48774 Training Loss: tensor(0.3242)\n",
      "48775 Training Loss: tensor(0.3246)\n",
      "48776 Training Loss: tensor(0.3240)\n",
      "48777 Training Loss: tensor(0.3242)\n",
      "48778 Training Loss: tensor(0.3245)\n",
      "48779 Training Loss: tensor(0.3244)\n",
      "48780 Training Loss: tensor(0.3247)\n",
      "48781 Training Loss: tensor(0.3242)\n",
      "48782 Training Loss: tensor(0.3253)\n",
      "48783 Training Loss: tensor(0.3238)\n",
      "48784 Training Loss: tensor(0.3247)\n",
      "48785 Training Loss: tensor(0.3251)\n",
      "48786 Training Loss: tensor(0.3241)\n",
      "48787 Training Loss: tensor(0.3247)\n",
      "48788 Training Loss: tensor(0.3248)\n",
      "48789 Training Loss: tensor(0.3238)\n",
      "48790 Training Loss: tensor(0.3243)\n",
      "48791 Training Loss: tensor(0.3244)\n",
      "48792 Training Loss: tensor(0.3249)\n",
      "48793 Training Loss: tensor(0.3248)\n",
      "48794 Training Loss: tensor(0.3241)\n",
      "48795 Training Loss: tensor(0.3244)\n",
      "48796 Training Loss: tensor(0.3248)\n",
      "48797 Training Loss: tensor(0.3244)\n",
      "48798 Training Loss: tensor(0.3243)\n",
      "48799 Training Loss: tensor(0.3256)\n",
      "48800 Training Loss: tensor(0.3237)\n",
      "48801 Training Loss: tensor(0.3239)\n",
      "48802 Training Loss: tensor(0.3250)\n",
      "48803 Training Loss: tensor(0.3245)\n",
      "48804 Training Loss: tensor(0.3238)\n",
      "48805 Training Loss: tensor(0.3238)\n",
      "48806 Training Loss: tensor(0.3253)\n",
      "48807 Training Loss: tensor(0.3241)\n",
      "48808 Training Loss: tensor(0.3245)\n",
      "48809 Training Loss: tensor(0.3238)\n",
      "48810 Training Loss: tensor(0.3251)\n",
      "48811 Training Loss: tensor(0.3243)\n",
      "48812 Training Loss: tensor(0.3246)\n",
      "48813 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48814 Training Loss: tensor(0.3249)\n",
      "48815 Training Loss: tensor(0.3240)\n",
      "48816 Training Loss: tensor(0.3241)\n",
      "48817 Training Loss: tensor(0.3240)\n",
      "48818 Training Loss: tensor(0.3248)\n",
      "48819 Training Loss: tensor(0.3241)\n",
      "48820 Training Loss: tensor(0.3244)\n",
      "48821 Training Loss: tensor(0.3237)\n",
      "48822 Training Loss: tensor(0.3242)\n",
      "48823 Training Loss: tensor(0.3251)\n",
      "48824 Training Loss: tensor(0.3251)\n",
      "48825 Training Loss: tensor(0.3242)\n",
      "48826 Training Loss: tensor(0.3238)\n",
      "48827 Training Loss: tensor(0.3250)\n",
      "48828 Training Loss: tensor(0.3248)\n",
      "48829 Training Loss: tensor(0.3244)\n",
      "48830 Training Loss: tensor(0.3241)\n",
      "48831 Training Loss: tensor(0.3241)\n",
      "48832 Training Loss: tensor(0.3244)\n",
      "48833 Training Loss: tensor(0.3249)\n",
      "48834 Training Loss: tensor(0.3248)\n",
      "48835 Training Loss: tensor(0.3244)\n",
      "48836 Training Loss: tensor(0.3252)\n",
      "48837 Training Loss: tensor(0.3244)\n",
      "48838 Training Loss: tensor(0.3249)\n",
      "48839 Training Loss: tensor(0.3237)\n",
      "48840 Training Loss: tensor(0.3244)\n",
      "48841 Training Loss: tensor(0.3239)\n",
      "48842 Training Loss: tensor(0.3239)\n",
      "48843 Training Loss: tensor(0.3240)\n",
      "48844 Training Loss: tensor(0.3249)\n",
      "48845 Training Loss: tensor(0.3248)\n",
      "48846 Training Loss: tensor(0.3250)\n",
      "48847 Training Loss: tensor(0.3257)\n",
      "48848 Training Loss: tensor(0.3243)\n",
      "48849 Training Loss: tensor(0.3241)\n",
      "48850 Training Loss: tensor(0.3259)\n",
      "48851 Training Loss: tensor(0.3242)\n",
      "48852 Training Loss: tensor(0.3242)\n",
      "48853 Training Loss: tensor(0.3248)\n",
      "48854 Training Loss: tensor(0.3241)\n",
      "48855 Training Loss: tensor(0.3246)\n",
      "48856 Training Loss: tensor(0.3251)\n",
      "48857 Training Loss: tensor(0.3240)\n",
      "48858 Training Loss: tensor(0.3254)\n",
      "48859 Training Loss: tensor(0.3239)\n",
      "48860 Training Loss: tensor(0.3240)\n",
      "48861 Training Loss: tensor(0.3246)\n",
      "48862 Training Loss: tensor(0.3244)\n",
      "48863 Training Loss: tensor(0.3256)\n",
      "48864 Training Loss: tensor(0.3252)\n",
      "48865 Training Loss: tensor(0.3248)\n",
      "48866 Training Loss: tensor(0.3246)\n",
      "48867 Training Loss: tensor(0.3248)\n",
      "48868 Training Loss: tensor(0.3240)\n",
      "48869 Training Loss: tensor(0.3242)\n",
      "48870 Training Loss: tensor(0.3240)\n",
      "48871 Training Loss: tensor(0.3242)\n",
      "48872 Training Loss: tensor(0.3249)\n",
      "48873 Training Loss: tensor(0.3249)\n",
      "48874 Training Loss: tensor(0.3246)\n",
      "48875 Training Loss: tensor(0.3246)\n",
      "48876 Training Loss: tensor(0.3254)\n",
      "48877 Training Loss: tensor(0.3242)\n",
      "48878 Training Loss: tensor(0.3245)\n",
      "48879 Training Loss: tensor(0.3254)\n",
      "48880 Training Loss: tensor(0.3240)\n",
      "48881 Training Loss: tensor(0.3242)\n",
      "48882 Training Loss: tensor(0.3246)\n",
      "48883 Training Loss: tensor(0.3243)\n",
      "48884 Training Loss: tensor(0.3242)\n",
      "48885 Training Loss: tensor(0.3238)\n",
      "48886 Training Loss: tensor(0.3239)\n",
      "48887 Training Loss: tensor(0.3238)\n",
      "48888 Training Loss: tensor(0.3241)\n",
      "48889 Training Loss: tensor(0.3246)\n",
      "48890 Training Loss: tensor(0.3241)\n",
      "48891 Training Loss: tensor(0.3247)\n",
      "48892 Training Loss: tensor(0.3247)\n",
      "48893 Training Loss: tensor(0.3238)\n",
      "48894 Training Loss: tensor(0.3245)\n",
      "48895 Training Loss: tensor(0.3244)\n",
      "48896 Training Loss: tensor(0.3242)\n",
      "48897 Training Loss: tensor(0.3239)\n",
      "48898 Training Loss: tensor(0.3249)\n",
      "48899 Training Loss: tensor(0.3247)\n",
      "48900 Training Loss: tensor(0.3245)\n",
      "48901 Training Loss: tensor(0.3246)\n",
      "48902 Training Loss: tensor(0.3241)\n",
      "48903 Training Loss: tensor(0.3242)\n",
      "48904 Training Loss: tensor(0.3242)\n",
      "48905 Training Loss: tensor(0.3238)\n",
      "48906 Training Loss: tensor(0.3242)\n",
      "48907 Training Loss: tensor(0.3249)\n",
      "48908 Training Loss: tensor(0.3249)\n",
      "48909 Training Loss: tensor(0.3244)\n",
      "48910 Training Loss: tensor(0.3248)\n",
      "48911 Training Loss: tensor(0.3242)\n",
      "48912 Training Loss: tensor(0.3248)\n",
      "48913 Training Loss: tensor(0.3241)\n",
      "48914 Training Loss: tensor(0.3241)\n",
      "48915 Training Loss: tensor(0.3237)\n",
      "48916 Training Loss: tensor(0.3239)\n",
      "48917 Training Loss: tensor(0.3251)\n",
      "48918 Training Loss: tensor(0.3250)\n",
      "48919 Training Loss: tensor(0.3244)\n",
      "48920 Training Loss: tensor(0.3251)\n",
      "48921 Training Loss: tensor(0.3249)\n",
      "48922 Training Loss: tensor(0.3246)\n",
      "48923 Training Loss: tensor(0.3254)\n",
      "48924 Training Loss: tensor(0.3238)\n",
      "48925 Training Loss: tensor(0.3240)\n",
      "48926 Training Loss: tensor(0.3241)\n",
      "48927 Training Loss: tensor(0.3251)\n",
      "48928 Training Loss: tensor(0.3245)\n",
      "48929 Training Loss: tensor(0.3253)\n",
      "48930 Training Loss: tensor(0.3243)\n",
      "48931 Training Loss: tensor(0.3242)\n",
      "48932 Training Loss: tensor(0.3236)\n",
      "48933 Training Loss: tensor(0.3245)\n",
      "48934 Training Loss: tensor(0.3245)\n",
      "48935 Training Loss: tensor(0.3244)\n",
      "48936 Training Loss: tensor(0.3251)\n",
      "48937 Training Loss: tensor(0.3252)\n",
      "48938 Training Loss: tensor(0.3246)\n",
      "48939 Training Loss: tensor(0.3254)\n",
      "48940 Training Loss: tensor(0.3244)\n",
      "48941 Training Loss: tensor(0.3244)\n",
      "48942 Training Loss: tensor(0.3240)\n",
      "48943 Training Loss: tensor(0.3243)\n",
      "48944 Training Loss: tensor(0.3241)\n",
      "48945 Training Loss: tensor(0.3246)\n",
      "48946 Training Loss: tensor(0.3241)\n",
      "48947 Training Loss: tensor(0.3251)\n",
      "48948 Training Loss: tensor(0.3242)\n",
      "48949 Training Loss: tensor(0.3243)\n",
      "48950 Training Loss: tensor(0.3248)\n",
      "48951 Training Loss: tensor(0.3247)\n",
      "48952 Training Loss: tensor(0.3245)\n",
      "48953 Training Loss: tensor(0.3239)\n",
      "48954 Training Loss: tensor(0.3237)\n",
      "48955 Training Loss: tensor(0.3243)\n",
      "48956 Training Loss: tensor(0.3249)\n",
      "48957 Training Loss: tensor(0.3247)\n",
      "48958 Training Loss: tensor(0.3247)\n",
      "48959 Training Loss: tensor(0.3251)\n",
      "48960 Training Loss: tensor(0.3242)\n",
      "48961 Training Loss: tensor(0.3243)\n",
      "48962 Training Loss: tensor(0.3243)\n",
      "48963 Training Loss: tensor(0.3238)\n",
      "48964 Training Loss: tensor(0.3241)\n",
      "48965 Training Loss: tensor(0.3240)\n",
      "48966 Training Loss: tensor(0.3241)\n",
      "48967 Training Loss: tensor(0.3246)\n",
      "48968 Training Loss: tensor(0.3241)\n",
      "48969 Training Loss: tensor(0.3240)\n",
      "48970 Training Loss: tensor(0.3238)\n",
      "48971 Training Loss: tensor(0.3245)\n",
      "48972 Training Loss: tensor(0.3242)\n",
      "48973 Training Loss: tensor(0.3240)\n",
      "48974 Training Loss: tensor(0.3239)\n",
      "48975 Training Loss: tensor(0.3238)\n",
      "48976 Training Loss: tensor(0.3248)\n",
      "48977 Training Loss: tensor(0.3243)\n",
      "48978 Training Loss: tensor(0.3240)\n",
      "48979 Training Loss: tensor(0.3237)\n",
      "48980 Training Loss: tensor(0.3250)\n",
      "48981 Training Loss: tensor(0.3246)\n",
      "48982 Training Loss: tensor(0.3251)\n",
      "48983 Training Loss: tensor(0.3238)\n",
      "48984 Training Loss: tensor(0.3247)\n",
      "48985 Training Loss: tensor(0.3245)\n",
      "48986 Training Loss: tensor(0.3243)\n",
      "48987 Training Loss: tensor(0.3237)\n",
      "48988 Training Loss: tensor(0.3239)\n",
      "48989 Training Loss: tensor(0.3245)\n",
      "48990 Training Loss: tensor(0.3246)\n",
      "48991 Training Loss: tensor(0.3238)\n",
      "48992 Training Loss: tensor(0.3243)\n",
      "48993 Training Loss: tensor(0.3245)\n",
      "48994 Training Loss: tensor(0.3245)\n",
      "48995 Training Loss: tensor(0.3249)\n",
      "48996 Training Loss: tensor(0.3246)\n",
      "48997 Training Loss: tensor(0.3239)\n",
      "48998 Training Loss: tensor(0.3250)\n",
      "48999 Training Loss: tensor(0.3243)\n",
      "49000 Training Loss: tensor(0.3248)\n",
      "49001 Training Loss: tensor(0.3244)\n",
      "49002 Training Loss: tensor(0.3249)\n",
      "49003 Training Loss: tensor(0.3240)\n",
      "49004 Training Loss: tensor(0.3246)\n",
      "49005 Training Loss: tensor(0.3254)\n",
      "49006 Training Loss: tensor(0.3239)\n",
      "49007 Training Loss: tensor(0.3242)\n",
      "49008 Training Loss: tensor(0.3248)\n",
      "49009 Training Loss: tensor(0.3243)\n",
      "49010 Training Loss: tensor(0.3242)\n",
      "49011 Training Loss: tensor(0.3255)\n",
      "49012 Training Loss: tensor(0.3241)\n",
      "49013 Training Loss: tensor(0.3252)\n",
      "49014 Training Loss: tensor(0.3250)\n",
      "49015 Training Loss: tensor(0.3262)\n",
      "49016 Training Loss: tensor(0.3246)\n",
      "49017 Training Loss: tensor(0.3245)\n",
      "49018 Training Loss: tensor(0.3244)\n",
      "49019 Training Loss: tensor(0.3238)\n",
      "49020 Training Loss: tensor(0.3235)\n",
      "49021 Training Loss: tensor(0.3245)\n",
      "49022 Training Loss: tensor(0.3241)\n",
      "49023 Training Loss: tensor(0.3241)\n",
      "49024 Training Loss: tensor(0.3242)\n",
      "49025 Training Loss: tensor(0.3247)\n",
      "49026 Training Loss: tensor(0.3240)\n",
      "49027 Training Loss: tensor(0.3247)\n",
      "49028 Training Loss: tensor(0.3237)\n",
      "49029 Training Loss: tensor(0.3242)\n",
      "49030 Training Loss: tensor(0.3244)\n",
      "49031 Training Loss: tensor(0.3247)\n",
      "49032 Training Loss: tensor(0.3243)\n",
      "49033 Training Loss: tensor(0.3238)\n",
      "49034 Training Loss: tensor(0.3241)\n",
      "49035 Training Loss: tensor(0.3243)\n",
      "49036 Training Loss: tensor(0.3242)\n",
      "49037 Training Loss: tensor(0.3244)\n",
      "49038 Training Loss: tensor(0.3242)\n",
      "49039 Training Loss: tensor(0.3243)\n",
      "49040 Training Loss: tensor(0.3243)\n",
      "49041 Training Loss: tensor(0.3243)\n",
      "49042 Training Loss: tensor(0.3242)\n",
      "49043 Training Loss: tensor(0.3243)\n",
      "49044 Training Loss: tensor(0.3241)\n",
      "49045 Training Loss: tensor(0.3236)\n",
      "49046 Training Loss: tensor(0.3240)\n",
      "49047 Training Loss: tensor(0.3251)\n",
      "49048 Training Loss: tensor(0.3242)\n",
      "49049 Training Loss: tensor(0.3238)\n",
      "49050 Training Loss: tensor(0.3241)\n",
      "49051 Training Loss: tensor(0.3262)\n",
      "49052 Training Loss: tensor(0.3233)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49053 Training Loss: tensor(0.3245)\n",
      "49054 Training Loss: tensor(0.3241)\n",
      "49055 Training Loss: tensor(0.3238)\n",
      "49056 Training Loss: tensor(0.3240)\n",
      "49057 Training Loss: tensor(0.3245)\n",
      "49058 Training Loss: tensor(0.3248)\n",
      "49059 Training Loss: tensor(0.3239)\n",
      "49060 Training Loss: tensor(0.3236)\n",
      "49061 Training Loss: tensor(0.3243)\n",
      "49062 Training Loss: tensor(0.3238)\n",
      "49063 Training Loss: tensor(0.3246)\n",
      "49064 Training Loss: tensor(0.3233)\n",
      "49065 Training Loss: tensor(0.3252)\n",
      "49066 Training Loss: tensor(0.3236)\n",
      "49067 Training Loss: tensor(0.3252)\n",
      "49068 Training Loss: tensor(0.3241)\n",
      "49069 Training Loss: tensor(0.3238)\n",
      "49070 Training Loss: tensor(0.3244)\n",
      "49071 Training Loss: tensor(0.3250)\n",
      "49072 Training Loss: tensor(0.3242)\n",
      "49073 Training Loss: tensor(0.3247)\n",
      "49074 Training Loss: tensor(0.3238)\n",
      "49075 Training Loss: tensor(0.3234)\n",
      "49076 Training Loss: tensor(0.3247)\n",
      "49077 Training Loss: tensor(0.3243)\n",
      "49078 Training Loss: tensor(0.3244)\n",
      "49079 Training Loss: tensor(0.3237)\n",
      "49080 Training Loss: tensor(0.3246)\n",
      "49081 Training Loss: tensor(0.3239)\n",
      "49082 Training Loss: tensor(0.3246)\n",
      "49083 Training Loss: tensor(0.3246)\n",
      "49084 Training Loss: tensor(0.3234)\n",
      "49085 Training Loss: tensor(0.3245)\n",
      "49086 Training Loss: tensor(0.3241)\n",
      "49087 Training Loss: tensor(0.3238)\n",
      "49088 Training Loss: tensor(0.3244)\n",
      "49089 Training Loss: tensor(0.3245)\n",
      "49090 Training Loss: tensor(0.3236)\n",
      "49091 Training Loss: tensor(0.3248)\n",
      "49092 Training Loss: tensor(0.3237)\n",
      "49093 Training Loss: tensor(0.3239)\n",
      "49094 Training Loss: tensor(0.3250)\n",
      "49095 Training Loss: tensor(0.3240)\n",
      "49096 Training Loss: tensor(0.3260)\n",
      "49097 Training Loss: tensor(0.3238)\n",
      "49098 Training Loss: tensor(0.3239)\n",
      "49099 Training Loss: tensor(0.3251)\n",
      "49100 Training Loss: tensor(0.3252)\n",
      "49101 Training Loss: tensor(0.3248)\n",
      "49102 Training Loss: tensor(0.3238)\n",
      "49103 Training Loss: tensor(0.3237)\n",
      "49104 Training Loss: tensor(0.3247)\n",
      "49105 Training Loss: tensor(0.3240)\n",
      "49106 Training Loss: tensor(0.3244)\n",
      "49107 Training Loss: tensor(0.3238)\n",
      "49108 Training Loss: tensor(0.3248)\n",
      "49109 Training Loss: tensor(0.3248)\n",
      "49110 Training Loss: tensor(0.3244)\n",
      "49111 Training Loss: tensor(0.3241)\n",
      "49112 Training Loss: tensor(0.3246)\n",
      "49113 Training Loss: tensor(0.3260)\n",
      "49114 Training Loss: tensor(0.3250)\n",
      "49115 Training Loss: tensor(0.3243)\n",
      "49116 Training Loss: tensor(0.3244)\n",
      "49117 Training Loss: tensor(0.3235)\n",
      "49118 Training Loss: tensor(0.3253)\n",
      "49119 Training Loss: tensor(0.3253)\n",
      "49120 Training Loss: tensor(0.3249)\n",
      "49121 Training Loss: tensor(0.3245)\n",
      "49122 Training Loss: tensor(0.3241)\n",
      "49123 Training Loss: tensor(0.3239)\n",
      "49124 Training Loss: tensor(0.3242)\n",
      "49125 Training Loss: tensor(0.3241)\n",
      "49126 Training Loss: tensor(0.3243)\n",
      "49127 Training Loss: tensor(0.3240)\n",
      "49128 Training Loss: tensor(0.3248)\n",
      "49129 Training Loss: tensor(0.3241)\n",
      "49130 Training Loss: tensor(0.3242)\n",
      "49131 Training Loss: tensor(0.3239)\n",
      "49132 Training Loss: tensor(0.3247)\n",
      "49133 Training Loss: tensor(0.3248)\n",
      "49134 Training Loss: tensor(0.3241)\n",
      "49135 Training Loss: tensor(0.3241)\n",
      "49136 Training Loss: tensor(0.3237)\n",
      "49137 Training Loss: tensor(0.3242)\n",
      "49138 Training Loss: tensor(0.3243)\n",
      "49139 Training Loss: tensor(0.3247)\n",
      "49140 Training Loss: tensor(0.3241)\n",
      "49141 Training Loss: tensor(0.3245)\n",
      "49142 Training Loss: tensor(0.3237)\n",
      "49143 Training Loss: tensor(0.3242)\n",
      "49144 Training Loss: tensor(0.3245)\n",
      "49145 Training Loss: tensor(0.3258)\n",
      "49146 Training Loss: tensor(0.3248)\n",
      "49147 Training Loss: tensor(0.3241)\n",
      "49148 Training Loss: tensor(0.3241)\n",
      "49149 Training Loss: tensor(0.3248)\n",
      "49150 Training Loss: tensor(0.3242)\n",
      "49151 Training Loss: tensor(0.3247)\n",
      "49152 Training Loss: tensor(0.3243)\n",
      "49153 Training Loss: tensor(0.3243)\n",
      "49154 Training Loss: tensor(0.3243)\n",
      "49155 Training Loss: tensor(0.3253)\n",
      "49156 Training Loss: tensor(0.3247)\n",
      "49157 Training Loss: tensor(0.3245)\n",
      "49158 Training Loss: tensor(0.3246)\n",
      "49159 Training Loss: tensor(0.3238)\n",
      "49160 Training Loss: tensor(0.3239)\n",
      "49161 Training Loss: tensor(0.3242)\n",
      "49162 Training Loss: tensor(0.3237)\n",
      "49163 Training Loss: tensor(0.3253)\n",
      "49164 Training Loss: tensor(0.3242)\n",
      "49165 Training Loss: tensor(0.3240)\n",
      "49166 Training Loss: tensor(0.3244)\n",
      "49167 Training Loss: tensor(0.3240)\n",
      "49168 Training Loss: tensor(0.3238)\n",
      "49169 Training Loss: tensor(0.3245)\n",
      "49170 Training Loss: tensor(0.3243)\n",
      "49171 Training Loss: tensor(0.3242)\n",
      "49172 Training Loss: tensor(0.3240)\n",
      "49173 Training Loss: tensor(0.3241)\n",
      "49174 Training Loss: tensor(0.3249)\n",
      "49175 Training Loss: tensor(0.3232)\n",
      "49176 Training Loss: tensor(0.3257)\n",
      "49177 Training Loss: tensor(0.3243)\n",
      "49178 Training Loss: tensor(0.3244)\n",
      "49179 Training Loss: tensor(0.3247)\n",
      "49180 Training Loss: tensor(0.3243)\n",
      "49181 Training Loss: tensor(0.3252)\n",
      "49182 Training Loss: tensor(0.3248)\n",
      "49183 Training Loss: tensor(0.3244)\n",
      "49184 Training Loss: tensor(0.3241)\n",
      "49185 Training Loss: tensor(0.3246)\n",
      "49186 Training Loss: tensor(0.3241)\n",
      "49187 Training Loss: tensor(0.3244)\n",
      "49188 Training Loss: tensor(0.3242)\n",
      "49189 Training Loss: tensor(0.3239)\n",
      "49190 Training Loss: tensor(0.3239)\n",
      "49191 Training Loss: tensor(0.3241)\n",
      "49192 Training Loss: tensor(0.3245)\n",
      "49193 Training Loss: tensor(0.3246)\n",
      "49194 Training Loss: tensor(0.3257)\n",
      "49195 Training Loss: tensor(0.3241)\n",
      "49196 Training Loss: tensor(0.3245)\n",
      "49197 Training Loss: tensor(0.3255)\n",
      "49198 Training Loss: tensor(0.3238)\n",
      "49199 Training Loss: tensor(0.3236)\n",
      "49200 Training Loss: tensor(0.3239)\n",
      "49201 Training Loss: tensor(0.3247)\n",
      "49202 Training Loss: tensor(0.3246)\n",
      "49203 Training Loss: tensor(0.3238)\n",
      "49204 Training Loss: tensor(0.3244)\n",
      "49205 Training Loss: tensor(0.3255)\n",
      "49206 Training Loss: tensor(0.3237)\n",
      "49207 Training Loss: tensor(0.3248)\n",
      "49208 Training Loss: tensor(0.3244)\n",
      "49209 Training Loss: tensor(0.3243)\n",
      "49210 Training Loss: tensor(0.3239)\n",
      "49211 Training Loss: tensor(0.3240)\n",
      "49212 Training Loss: tensor(0.3248)\n",
      "49213 Training Loss: tensor(0.3240)\n",
      "49214 Training Loss: tensor(0.3251)\n",
      "49215 Training Loss: tensor(0.3243)\n",
      "49216 Training Loss: tensor(0.3247)\n",
      "49217 Training Loss: tensor(0.3241)\n",
      "49218 Training Loss: tensor(0.3242)\n",
      "49219 Training Loss: tensor(0.3241)\n",
      "49220 Training Loss: tensor(0.3245)\n",
      "49221 Training Loss: tensor(0.3241)\n",
      "49222 Training Loss: tensor(0.3250)\n",
      "49223 Training Loss: tensor(0.3251)\n",
      "49224 Training Loss: tensor(0.3238)\n",
      "49225 Training Loss: tensor(0.3253)\n",
      "49226 Training Loss: tensor(0.3239)\n",
      "49227 Training Loss: tensor(0.3243)\n",
      "49228 Training Loss: tensor(0.3241)\n",
      "49229 Training Loss: tensor(0.3243)\n",
      "49230 Training Loss: tensor(0.3254)\n",
      "49231 Training Loss: tensor(0.3244)\n",
      "49232 Training Loss: tensor(0.3239)\n",
      "49233 Training Loss: tensor(0.3252)\n",
      "49234 Training Loss: tensor(0.3250)\n",
      "49235 Training Loss: tensor(0.3249)\n",
      "49236 Training Loss: tensor(0.3253)\n",
      "49237 Training Loss: tensor(0.3236)\n",
      "49238 Training Loss: tensor(0.3251)\n",
      "49239 Training Loss: tensor(0.3241)\n",
      "49240 Training Loss: tensor(0.3245)\n",
      "49241 Training Loss: tensor(0.3252)\n",
      "49242 Training Loss: tensor(0.3240)\n",
      "49243 Training Loss: tensor(0.3240)\n",
      "49244 Training Loss: tensor(0.3239)\n",
      "49245 Training Loss: tensor(0.3241)\n",
      "49246 Training Loss: tensor(0.3245)\n",
      "49247 Training Loss: tensor(0.3238)\n",
      "49248 Training Loss: tensor(0.3245)\n",
      "49249 Training Loss: tensor(0.3240)\n",
      "49250 Training Loss: tensor(0.3245)\n",
      "49251 Training Loss: tensor(0.3243)\n",
      "49252 Training Loss: tensor(0.3236)\n",
      "49253 Training Loss: tensor(0.3245)\n",
      "49254 Training Loss: tensor(0.3234)\n",
      "49255 Training Loss: tensor(0.3246)\n",
      "49256 Training Loss: tensor(0.3246)\n",
      "49257 Training Loss: tensor(0.3242)\n",
      "49258 Training Loss: tensor(0.3246)\n",
      "49259 Training Loss: tensor(0.3248)\n",
      "49260 Training Loss: tensor(0.3246)\n",
      "49261 Training Loss: tensor(0.3242)\n",
      "49262 Training Loss: tensor(0.3242)\n",
      "49263 Training Loss: tensor(0.3238)\n",
      "49264 Training Loss: tensor(0.3241)\n",
      "49265 Training Loss: tensor(0.3266)\n",
      "49266 Training Loss: tensor(0.3259)\n",
      "49267 Training Loss: tensor(0.3251)\n",
      "49268 Training Loss: tensor(0.3241)\n",
      "49269 Training Loss: tensor(0.3251)\n",
      "49270 Training Loss: tensor(0.3245)\n",
      "49271 Training Loss: tensor(0.3243)\n",
      "49272 Training Loss: tensor(0.3244)\n",
      "49273 Training Loss: tensor(0.3236)\n",
      "49274 Training Loss: tensor(0.3241)\n",
      "49275 Training Loss: tensor(0.3245)\n",
      "49276 Training Loss: tensor(0.3249)\n",
      "49277 Training Loss: tensor(0.3247)\n",
      "49278 Training Loss: tensor(0.3241)\n",
      "49279 Training Loss: tensor(0.3237)\n",
      "49280 Training Loss: tensor(0.3241)\n",
      "49281 Training Loss: tensor(0.3242)\n",
      "49282 Training Loss: tensor(0.3250)\n",
      "49283 Training Loss: tensor(0.3248)\n",
      "49284 Training Loss: tensor(0.3240)\n",
      "49285 Training Loss: tensor(0.3252)\n",
      "49286 Training Loss: tensor(0.3242)\n",
      "49287 Training Loss: tensor(0.3249)\n",
      "49288 Training Loss: tensor(0.3244)\n",
      "49289 Training Loss: tensor(0.3241)\n",
      "49290 Training Loss: tensor(0.3237)\n",
      "49291 Training Loss: tensor(0.3236)\n",
      "49292 Training Loss: tensor(0.3245)\n",
      "49293 Training Loss: tensor(0.3247)\n",
      "49294 Training Loss: tensor(0.3247)\n",
      "49295 Training Loss: tensor(0.3247)\n",
      "49296 Training Loss: tensor(0.3247)\n",
      "49297 Training Loss: tensor(0.3244)\n",
      "49298 Training Loss: tensor(0.3247)\n",
      "49299 Training Loss: tensor(0.3249)\n",
      "49300 Training Loss: tensor(0.3246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49301 Training Loss: tensor(0.3247)\n",
      "49302 Training Loss: tensor(0.3260)\n",
      "49303 Training Loss: tensor(0.3244)\n",
      "49304 Training Loss: tensor(0.3243)\n",
      "49305 Training Loss: tensor(0.3240)\n",
      "49306 Training Loss: tensor(0.3246)\n",
      "49307 Training Loss: tensor(0.3246)\n",
      "49308 Training Loss: tensor(0.3250)\n",
      "49309 Training Loss: tensor(0.3252)\n",
      "49310 Training Loss: tensor(0.3238)\n",
      "49311 Training Loss: tensor(0.3242)\n",
      "49312 Training Loss: tensor(0.3240)\n",
      "49313 Training Loss: tensor(0.3243)\n",
      "49314 Training Loss: tensor(0.3237)\n",
      "49315 Training Loss: tensor(0.3246)\n",
      "49316 Training Loss: tensor(0.3240)\n",
      "49317 Training Loss: tensor(0.3242)\n",
      "49318 Training Loss: tensor(0.3242)\n",
      "49319 Training Loss: tensor(0.3244)\n",
      "49320 Training Loss: tensor(0.3240)\n",
      "49321 Training Loss: tensor(0.3243)\n",
      "49322 Training Loss: tensor(0.3248)\n",
      "49323 Training Loss: tensor(0.3242)\n",
      "49324 Training Loss: tensor(0.3250)\n",
      "49325 Training Loss: tensor(0.3240)\n",
      "49326 Training Loss: tensor(0.3246)\n",
      "49327 Training Loss: tensor(0.3248)\n",
      "49328 Training Loss: tensor(0.3243)\n",
      "49329 Training Loss: tensor(0.3237)\n",
      "49330 Training Loss: tensor(0.3239)\n",
      "49331 Training Loss: tensor(0.3246)\n",
      "49332 Training Loss: tensor(0.3245)\n",
      "49333 Training Loss: tensor(0.3243)\n",
      "49334 Training Loss: tensor(0.3237)\n",
      "49335 Training Loss: tensor(0.3248)\n",
      "49336 Training Loss: tensor(0.3243)\n",
      "49337 Training Loss: tensor(0.3244)\n",
      "49338 Training Loss: tensor(0.3252)\n",
      "49339 Training Loss: tensor(0.3241)\n",
      "49340 Training Loss: tensor(0.3242)\n",
      "49341 Training Loss: tensor(0.3243)\n",
      "49342 Training Loss: tensor(0.3240)\n",
      "49343 Training Loss: tensor(0.3246)\n",
      "49344 Training Loss: tensor(0.3248)\n",
      "49345 Training Loss: tensor(0.3244)\n",
      "49346 Training Loss: tensor(0.3249)\n",
      "49347 Training Loss: tensor(0.3243)\n",
      "49348 Training Loss: tensor(0.3242)\n",
      "49349 Training Loss: tensor(0.3246)\n",
      "49350 Training Loss: tensor(0.3252)\n",
      "49351 Training Loss: tensor(0.3247)\n",
      "49352 Training Loss: tensor(0.3246)\n",
      "49353 Training Loss: tensor(0.3242)\n",
      "49354 Training Loss: tensor(0.3246)\n",
      "49355 Training Loss: tensor(0.3245)\n",
      "49356 Training Loss: tensor(0.3248)\n",
      "49357 Training Loss: tensor(0.3236)\n",
      "49358 Training Loss: tensor(0.3241)\n",
      "49359 Training Loss: tensor(0.3239)\n",
      "49360 Training Loss: tensor(0.3240)\n",
      "49361 Training Loss: tensor(0.3250)\n",
      "49362 Training Loss: tensor(0.3241)\n",
      "49363 Training Loss: tensor(0.3250)\n",
      "49364 Training Loss: tensor(0.3237)\n",
      "49365 Training Loss: tensor(0.3243)\n",
      "49366 Training Loss: tensor(0.3247)\n",
      "49367 Training Loss: tensor(0.3248)\n",
      "49368 Training Loss: tensor(0.3251)\n",
      "49369 Training Loss: tensor(0.3244)\n",
      "49370 Training Loss: tensor(0.3248)\n",
      "49371 Training Loss: tensor(0.3242)\n",
      "49372 Training Loss: tensor(0.3247)\n",
      "49373 Training Loss: tensor(0.3244)\n",
      "49374 Training Loss: tensor(0.3242)\n",
      "49375 Training Loss: tensor(0.3245)\n",
      "49376 Training Loss: tensor(0.3249)\n",
      "49377 Training Loss: tensor(0.3251)\n",
      "49378 Training Loss: tensor(0.3245)\n",
      "49379 Training Loss: tensor(0.3246)\n",
      "49380 Training Loss: tensor(0.3244)\n",
      "49381 Training Loss: tensor(0.3250)\n",
      "49382 Training Loss: tensor(0.3250)\n",
      "49383 Training Loss: tensor(0.3240)\n",
      "49384 Training Loss: tensor(0.3243)\n",
      "49385 Training Loss: tensor(0.3246)\n",
      "49386 Training Loss: tensor(0.3239)\n",
      "49387 Training Loss: tensor(0.3242)\n",
      "49388 Training Loss: tensor(0.3242)\n",
      "49389 Training Loss: tensor(0.3242)\n",
      "49390 Training Loss: tensor(0.3244)\n",
      "49391 Training Loss: tensor(0.3241)\n",
      "49392 Training Loss: tensor(0.3239)\n",
      "49393 Training Loss: tensor(0.3256)\n",
      "49394 Training Loss: tensor(0.3239)\n",
      "49395 Training Loss: tensor(0.3242)\n",
      "49396 Training Loss: tensor(0.3245)\n",
      "49397 Training Loss: tensor(0.3240)\n",
      "49398 Training Loss: tensor(0.3248)\n",
      "49399 Training Loss: tensor(0.3239)\n",
      "49400 Training Loss: tensor(0.3242)\n",
      "49401 Training Loss: tensor(0.3235)\n",
      "49402 Training Loss: tensor(0.3248)\n",
      "49403 Training Loss: tensor(0.3257)\n",
      "49404 Training Loss: tensor(0.3245)\n",
      "49405 Training Loss: tensor(0.3248)\n",
      "49406 Training Loss: tensor(0.3235)\n",
      "49407 Training Loss: tensor(0.3239)\n",
      "49408 Training Loss: tensor(0.3243)\n",
      "49409 Training Loss: tensor(0.3248)\n",
      "49410 Training Loss: tensor(0.3241)\n",
      "49411 Training Loss: tensor(0.3246)\n",
      "49412 Training Loss: tensor(0.3243)\n",
      "49413 Training Loss: tensor(0.3253)\n",
      "49414 Training Loss: tensor(0.3242)\n",
      "49415 Training Loss: tensor(0.3249)\n",
      "49416 Training Loss: tensor(0.3252)\n",
      "49417 Training Loss: tensor(0.3255)\n",
      "49418 Training Loss: tensor(0.3242)\n",
      "49419 Training Loss: tensor(0.3249)\n",
      "49420 Training Loss: tensor(0.3238)\n",
      "49421 Training Loss: tensor(0.3241)\n",
      "49422 Training Loss: tensor(0.3244)\n",
      "49423 Training Loss: tensor(0.3253)\n",
      "49424 Training Loss: tensor(0.3241)\n",
      "49425 Training Loss: tensor(0.3245)\n",
      "49426 Training Loss: tensor(0.3253)\n",
      "49427 Training Loss: tensor(0.3253)\n",
      "49428 Training Loss: tensor(0.3242)\n",
      "49429 Training Loss: tensor(0.3239)\n",
      "49430 Training Loss: tensor(0.3257)\n",
      "49431 Training Loss: tensor(0.3251)\n",
      "49432 Training Loss: tensor(0.3249)\n",
      "49433 Training Loss: tensor(0.3244)\n",
      "49434 Training Loss: tensor(0.3240)\n",
      "49435 Training Loss: tensor(0.3238)\n",
      "49436 Training Loss: tensor(0.3248)\n",
      "49437 Training Loss: tensor(0.3244)\n",
      "49438 Training Loss: tensor(0.3241)\n",
      "49439 Training Loss: tensor(0.3240)\n",
      "49440 Training Loss: tensor(0.3245)\n",
      "49441 Training Loss: tensor(0.3248)\n",
      "49442 Training Loss: tensor(0.3242)\n",
      "49443 Training Loss: tensor(0.3242)\n",
      "49444 Training Loss: tensor(0.3256)\n",
      "49445 Training Loss: tensor(0.3249)\n",
      "49446 Training Loss: tensor(0.3243)\n",
      "49447 Training Loss: tensor(0.3252)\n",
      "49448 Training Loss: tensor(0.3244)\n",
      "49449 Training Loss: tensor(0.3238)\n",
      "49450 Training Loss: tensor(0.3241)\n",
      "49451 Training Loss: tensor(0.3242)\n",
      "49452 Training Loss: tensor(0.3240)\n",
      "49453 Training Loss: tensor(0.3253)\n",
      "49454 Training Loss: tensor(0.3240)\n",
      "49455 Training Loss: tensor(0.3245)\n",
      "49456 Training Loss: tensor(0.3238)\n",
      "49457 Training Loss: tensor(0.3246)\n",
      "49458 Training Loss: tensor(0.3242)\n",
      "49459 Training Loss: tensor(0.3242)\n",
      "49460 Training Loss: tensor(0.3247)\n",
      "49461 Training Loss: tensor(0.3239)\n",
      "49462 Training Loss: tensor(0.3244)\n",
      "49463 Training Loss: tensor(0.3249)\n",
      "49464 Training Loss: tensor(0.3240)\n",
      "49465 Training Loss: tensor(0.3241)\n",
      "49466 Training Loss: tensor(0.3239)\n",
      "49467 Training Loss: tensor(0.3239)\n",
      "49468 Training Loss: tensor(0.3247)\n",
      "49469 Training Loss: tensor(0.3244)\n",
      "49470 Training Loss: tensor(0.3247)\n",
      "49471 Training Loss: tensor(0.3244)\n",
      "49472 Training Loss: tensor(0.3243)\n",
      "49473 Training Loss: tensor(0.3243)\n",
      "49474 Training Loss: tensor(0.3241)\n",
      "49475 Training Loss: tensor(0.3247)\n",
      "49476 Training Loss: tensor(0.3250)\n",
      "49477 Training Loss: tensor(0.3241)\n",
      "49478 Training Loss: tensor(0.3244)\n",
      "49479 Training Loss: tensor(0.3257)\n",
      "49480 Training Loss: tensor(0.3240)\n",
      "49481 Training Loss: tensor(0.3235)\n",
      "49482 Training Loss: tensor(0.3235)\n",
      "49483 Training Loss: tensor(0.3245)\n",
      "49484 Training Loss: tensor(0.3240)\n",
      "49485 Training Loss: tensor(0.3242)\n",
      "49486 Training Loss: tensor(0.3253)\n",
      "49487 Training Loss: tensor(0.3242)\n",
      "49488 Training Loss: tensor(0.3248)\n",
      "49489 Training Loss: tensor(0.3259)\n",
      "49490 Training Loss: tensor(0.3241)\n",
      "49491 Training Loss: tensor(0.3251)\n",
      "49492 Training Loss: tensor(0.3241)\n",
      "49493 Training Loss: tensor(0.3250)\n",
      "49494 Training Loss: tensor(0.3242)\n",
      "49495 Training Loss: tensor(0.3236)\n",
      "49496 Training Loss: tensor(0.3246)\n",
      "49497 Training Loss: tensor(0.3241)\n",
      "49498 Training Loss: tensor(0.3242)\n",
      "49499 Training Loss: tensor(0.3236)\n",
      "49500 Training Loss: tensor(0.3249)\n",
      "49501 Training Loss: tensor(0.3236)\n",
      "49502 Training Loss: tensor(0.3243)\n",
      "49503 Training Loss: tensor(0.3239)\n",
      "49504 Training Loss: tensor(0.3250)\n",
      "49505 Training Loss: tensor(0.3250)\n",
      "49506 Training Loss: tensor(0.3249)\n",
      "49507 Training Loss: tensor(0.3244)\n",
      "49508 Training Loss: tensor(0.3246)\n",
      "49509 Training Loss: tensor(0.3247)\n",
      "49510 Training Loss: tensor(0.3240)\n",
      "49511 Training Loss: tensor(0.3241)\n",
      "49512 Training Loss: tensor(0.3243)\n",
      "49513 Training Loss: tensor(0.3237)\n",
      "49514 Training Loss: tensor(0.3239)\n",
      "49515 Training Loss: tensor(0.3238)\n",
      "49516 Training Loss: tensor(0.3244)\n",
      "49517 Training Loss: tensor(0.3242)\n",
      "49518 Training Loss: tensor(0.3243)\n",
      "49519 Training Loss: tensor(0.3248)\n",
      "49520 Training Loss: tensor(0.3247)\n",
      "49521 Training Loss: tensor(0.3246)\n",
      "49522 Training Loss: tensor(0.3240)\n",
      "49523 Training Loss: tensor(0.3247)\n",
      "49524 Training Loss: tensor(0.3252)\n",
      "49525 Training Loss: tensor(0.3240)\n",
      "49526 Training Loss: tensor(0.3250)\n",
      "49527 Training Loss: tensor(0.3237)\n",
      "49528 Training Loss: tensor(0.3242)\n",
      "49529 Training Loss: tensor(0.3239)\n",
      "49530 Training Loss: tensor(0.3247)\n",
      "49531 Training Loss: tensor(0.3240)\n",
      "49532 Training Loss: tensor(0.3242)\n",
      "49533 Training Loss: tensor(0.3236)\n",
      "49534 Training Loss: tensor(0.3241)\n",
      "49535 Training Loss: tensor(0.3240)\n",
      "49536 Training Loss: tensor(0.3240)\n",
      "49537 Training Loss: tensor(0.3234)\n",
      "49538 Training Loss: tensor(0.3245)\n",
      "49539 Training Loss: tensor(0.3253)\n",
      "49540 Training Loss: tensor(0.3245)\n",
      "49541 Training Loss: tensor(0.3252)\n",
      "49542 Training Loss: tensor(0.3245)\n",
      "49543 Training Loss: tensor(0.3245)\n",
      "49544 Training Loss: tensor(0.3243)\n",
      "49545 Training Loss: tensor(0.3241)\n",
      "49546 Training Loss: tensor(0.3241)\n",
      "49547 Training Loss: tensor(0.3239)\n",
      "49548 Training Loss: tensor(0.3241)\n",
      "49549 Training Loss: tensor(0.3243)\n",
      "49550 Training Loss: tensor(0.3241)\n",
      "49551 Training Loss: tensor(0.3239)\n",
      "49552 Training Loss: tensor(0.3242)\n",
      "49553 Training Loss: tensor(0.3241)\n",
      "49554 Training Loss: tensor(0.3238)\n",
      "49555 Training Loss: tensor(0.3238)\n",
      "49556 Training Loss: tensor(0.3243)\n",
      "49557 Training Loss: tensor(0.3241)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49558 Training Loss: tensor(0.3244)\n",
      "49559 Training Loss: tensor(0.3242)\n",
      "49560 Training Loss: tensor(0.3247)\n",
      "49561 Training Loss: tensor(0.3241)\n",
      "49562 Training Loss: tensor(0.3237)\n",
      "49563 Training Loss: tensor(0.3243)\n",
      "49564 Training Loss: tensor(0.3238)\n",
      "49565 Training Loss: tensor(0.3247)\n",
      "49566 Training Loss: tensor(0.3246)\n",
      "49567 Training Loss: tensor(0.3242)\n",
      "49568 Training Loss: tensor(0.3240)\n",
      "49569 Training Loss: tensor(0.3243)\n",
      "49570 Training Loss: tensor(0.3241)\n",
      "49571 Training Loss: tensor(0.3254)\n",
      "49572 Training Loss: tensor(0.3238)\n",
      "49573 Training Loss: tensor(0.3244)\n",
      "49574 Training Loss: tensor(0.3248)\n",
      "49575 Training Loss: tensor(0.3259)\n",
      "49576 Training Loss: tensor(0.3249)\n",
      "49577 Training Loss: tensor(0.3240)\n",
      "49578 Training Loss: tensor(0.3241)\n",
      "49579 Training Loss: tensor(0.3243)\n",
      "49580 Training Loss: tensor(0.3237)\n",
      "49581 Training Loss: tensor(0.3238)\n",
      "49582 Training Loss: tensor(0.3246)\n",
      "49583 Training Loss: tensor(0.3247)\n",
      "49584 Training Loss: tensor(0.3257)\n",
      "49585 Training Loss: tensor(0.3243)\n",
      "49586 Training Loss: tensor(0.3239)\n",
      "49587 Training Loss: tensor(0.3243)\n",
      "49588 Training Loss: tensor(0.3238)\n",
      "49589 Training Loss: tensor(0.3237)\n",
      "49590 Training Loss: tensor(0.3244)\n",
      "49591 Training Loss: tensor(0.3240)\n",
      "49592 Training Loss: tensor(0.3241)\n",
      "49593 Training Loss: tensor(0.3244)\n",
      "49594 Training Loss: tensor(0.3236)\n",
      "49595 Training Loss: tensor(0.3246)\n",
      "49596 Training Loss: tensor(0.3254)\n",
      "49597 Training Loss: tensor(0.3242)\n",
      "49598 Training Loss: tensor(0.3246)\n",
      "49599 Training Loss: tensor(0.3236)\n",
      "49600 Training Loss: tensor(0.3240)\n",
      "49601 Training Loss: tensor(0.3251)\n",
      "49602 Training Loss: tensor(0.3246)\n",
      "49603 Training Loss: tensor(0.3245)\n",
      "49604 Training Loss: tensor(0.3251)\n",
      "49605 Training Loss: tensor(0.3244)\n",
      "49606 Training Loss: tensor(0.3250)\n",
      "49607 Training Loss: tensor(0.3233)\n",
      "49608 Training Loss: tensor(0.3247)\n",
      "49609 Training Loss: tensor(0.3236)\n",
      "49610 Training Loss: tensor(0.3242)\n",
      "49611 Training Loss: tensor(0.3242)\n",
      "49612 Training Loss: tensor(0.3245)\n",
      "49613 Training Loss: tensor(0.3244)\n",
      "49614 Training Loss: tensor(0.3240)\n",
      "49615 Training Loss: tensor(0.3243)\n",
      "49616 Training Loss: tensor(0.3250)\n",
      "49617 Training Loss: tensor(0.3236)\n",
      "49618 Training Loss: tensor(0.3260)\n",
      "49619 Training Loss: tensor(0.3251)\n",
      "49620 Training Loss: tensor(0.3246)\n",
      "49621 Training Loss: tensor(0.3255)\n",
      "49622 Training Loss: tensor(0.3247)\n",
      "49623 Training Loss: tensor(0.3236)\n",
      "49624 Training Loss: tensor(0.3240)\n",
      "49625 Training Loss: tensor(0.3247)\n",
      "49626 Training Loss: tensor(0.3246)\n",
      "49627 Training Loss: tensor(0.3246)\n",
      "49628 Training Loss: tensor(0.3251)\n",
      "49629 Training Loss: tensor(0.3242)\n",
      "49630 Training Loss: tensor(0.3246)\n",
      "49631 Training Loss: tensor(0.3242)\n",
      "49632 Training Loss: tensor(0.3239)\n",
      "49633 Training Loss: tensor(0.3242)\n",
      "49634 Training Loss: tensor(0.3254)\n",
      "49635 Training Loss: tensor(0.3242)\n",
      "49636 Training Loss: tensor(0.3253)\n",
      "49637 Training Loss: tensor(0.3240)\n",
      "49638 Training Loss: tensor(0.3239)\n",
      "49639 Training Loss: tensor(0.3238)\n",
      "49640 Training Loss: tensor(0.3243)\n",
      "49641 Training Loss: tensor(0.3247)\n",
      "49642 Training Loss: tensor(0.3247)\n",
      "49643 Training Loss: tensor(0.3242)\n",
      "49644 Training Loss: tensor(0.3244)\n",
      "49645 Training Loss: tensor(0.3247)\n",
      "49646 Training Loss: tensor(0.3239)\n",
      "49647 Training Loss: tensor(0.3234)\n",
      "49648 Training Loss: tensor(0.3244)\n",
      "49649 Training Loss: tensor(0.3240)\n",
      "49650 Training Loss: tensor(0.3237)\n",
      "49651 Training Loss: tensor(0.3243)\n",
      "49652 Training Loss: tensor(0.3248)\n",
      "49653 Training Loss: tensor(0.3237)\n",
      "49654 Training Loss: tensor(0.3244)\n",
      "49655 Training Loss: tensor(0.3241)\n",
      "49656 Training Loss: tensor(0.3243)\n",
      "49657 Training Loss: tensor(0.3235)\n",
      "49658 Training Loss: tensor(0.3238)\n",
      "49659 Training Loss: tensor(0.3254)\n",
      "49660 Training Loss: tensor(0.3240)\n",
      "49661 Training Loss: tensor(0.3244)\n",
      "49662 Training Loss: tensor(0.3241)\n",
      "49663 Training Loss: tensor(0.3249)\n",
      "49664 Training Loss: tensor(0.3241)\n",
      "49665 Training Loss: tensor(0.3262)\n",
      "49666 Training Loss: tensor(0.3258)\n",
      "49667 Training Loss: tensor(0.3242)\n",
      "49668 Training Loss: tensor(0.3239)\n",
      "49669 Training Loss: tensor(0.3250)\n",
      "49670 Training Loss: tensor(0.3249)\n",
      "49671 Training Loss: tensor(0.3254)\n",
      "49672 Training Loss: tensor(0.3244)\n",
      "49673 Training Loss: tensor(0.3244)\n",
      "49674 Training Loss: tensor(0.3241)\n",
      "49675 Training Loss: tensor(0.3255)\n",
      "49676 Training Loss: tensor(0.3250)\n",
      "49677 Training Loss: tensor(0.3249)\n",
      "49678 Training Loss: tensor(0.3242)\n",
      "49679 Training Loss: tensor(0.3245)\n",
      "49680 Training Loss: tensor(0.3242)\n",
      "49681 Training Loss: tensor(0.3242)\n",
      "49682 Training Loss: tensor(0.3242)\n",
      "49683 Training Loss: tensor(0.3238)\n",
      "49684 Training Loss: tensor(0.3250)\n",
      "49685 Training Loss: tensor(0.3246)\n",
      "49686 Training Loss: tensor(0.3242)\n",
      "49687 Training Loss: tensor(0.3247)\n",
      "49688 Training Loss: tensor(0.3237)\n",
      "49689 Training Loss: tensor(0.3242)\n",
      "49690 Training Loss: tensor(0.3249)\n",
      "49691 Training Loss: tensor(0.3245)\n",
      "49692 Training Loss: tensor(0.3243)\n",
      "49693 Training Loss: tensor(0.3238)\n",
      "49694 Training Loss: tensor(0.3245)\n",
      "49695 Training Loss: tensor(0.3247)\n",
      "49696 Training Loss: tensor(0.3252)\n",
      "49697 Training Loss: tensor(0.3236)\n",
      "49698 Training Loss: tensor(0.3257)\n",
      "49699 Training Loss: tensor(0.3247)\n",
      "49700 Training Loss: tensor(0.3237)\n",
      "49701 Training Loss: tensor(0.3251)\n",
      "49702 Training Loss: tensor(0.3238)\n",
      "49703 Training Loss: tensor(0.3247)\n",
      "49704 Training Loss: tensor(0.3245)\n",
      "49705 Training Loss: tensor(0.3244)\n",
      "49706 Training Loss: tensor(0.3238)\n",
      "49707 Training Loss: tensor(0.3250)\n",
      "49708 Training Loss: tensor(0.3247)\n",
      "49709 Training Loss: tensor(0.3241)\n",
      "49710 Training Loss: tensor(0.3245)\n",
      "49711 Training Loss: tensor(0.3243)\n",
      "49712 Training Loss: tensor(0.3254)\n",
      "49713 Training Loss: tensor(0.3239)\n",
      "49714 Training Loss: tensor(0.3240)\n",
      "49715 Training Loss: tensor(0.3239)\n",
      "49716 Training Loss: tensor(0.3245)\n",
      "49717 Training Loss: tensor(0.3247)\n",
      "49718 Training Loss: tensor(0.3244)\n",
      "49719 Training Loss: tensor(0.3243)\n",
      "49720 Training Loss: tensor(0.3242)\n",
      "49721 Training Loss: tensor(0.3245)\n",
      "49722 Training Loss: tensor(0.3249)\n",
      "49723 Training Loss: tensor(0.3244)\n",
      "49724 Training Loss: tensor(0.3246)\n",
      "49725 Training Loss: tensor(0.3242)\n",
      "49726 Training Loss: tensor(0.3242)\n",
      "49727 Training Loss: tensor(0.3243)\n",
      "49728 Training Loss: tensor(0.3246)\n",
      "49729 Training Loss: tensor(0.3254)\n",
      "49730 Training Loss: tensor(0.3241)\n",
      "49731 Training Loss: tensor(0.3237)\n",
      "49732 Training Loss: tensor(0.3246)\n",
      "49733 Training Loss: tensor(0.3244)\n",
      "49734 Training Loss: tensor(0.3253)\n",
      "49735 Training Loss: tensor(0.3246)\n",
      "49736 Training Loss: tensor(0.3240)\n",
      "49737 Training Loss: tensor(0.3249)\n",
      "49738 Training Loss: tensor(0.3242)\n",
      "49739 Training Loss: tensor(0.3246)\n",
      "49740 Training Loss: tensor(0.3248)\n",
      "49741 Training Loss: tensor(0.3243)\n",
      "49742 Training Loss: tensor(0.3245)\n",
      "49743 Training Loss: tensor(0.3246)\n",
      "49744 Training Loss: tensor(0.3240)\n",
      "49745 Training Loss: tensor(0.3243)\n",
      "49746 Training Loss: tensor(0.3242)\n",
      "49747 Training Loss: tensor(0.3250)\n",
      "49748 Training Loss: tensor(0.3251)\n",
      "49749 Training Loss: tensor(0.3244)\n",
      "49750 Training Loss: tensor(0.3243)\n",
      "49751 Training Loss: tensor(0.3248)\n",
      "49752 Training Loss: tensor(0.3249)\n",
      "49753 Training Loss: tensor(0.3239)\n",
      "49754 Training Loss: tensor(0.3242)\n",
      "49755 Training Loss: tensor(0.3236)\n",
      "49756 Training Loss: tensor(0.3246)\n",
      "49757 Training Loss: tensor(0.3240)\n",
      "49758 Training Loss: tensor(0.3237)\n",
      "49759 Training Loss: tensor(0.3237)\n",
      "49760 Training Loss: tensor(0.3241)\n",
      "49761 Training Loss: tensor(0.3243)\n",
      "49762 Training Loss: tensor(0.3242)\n",
      "49763 Training Loss: tensor(0.3243)\n",
      "49764 Training Loss: tensor(0.3244)\n",
      "49765 Training Loss: tensor(0.3237)\n",
      "49766 Training Loss: tensor(0.3249)\n",
      "49767 Training Loss: tensor(0.3240)\n",
      "49768 Training Loss: tensor(0.3236)\n",
      "49769 Training Loss: tensor(0.3236)\n",
      "49770 Training Loss: tensor(0.3240)\n",
      "49771 Training Loss: tensor(0.3245)\n",
      "49772 Training Loss: tensor(0.3247)\n",
      "49773 Training Loss: tensor(0.3241)\n",
      "49774 Training Loss: tensor(0.3241)\n",
      "49775 Training Loss: tensor(0.3245)\n",
      "49776 Training Loss: tensor(0.3240)\n",
      "49777 Training Loss: tensor(0.3256)\n",
      "49778 Training Loss: tensor(0.3244)\n",
      "49779 Training Loss: tensor(0.3252)\n",
      "49780 Training Loss: tensor(0.3239)\n",
      "49781 Training Loss: tensor(0.3240)\n",
      "49782 Training Loss: tensor(0.3246)\n",
      "49783 Training Loss: tensor(0.3241)\n",
      "49784 Training Loss: tensor(0.3239)\n",
      "49785 Training Loss: tensor(0.3239)\n",
      "49786 Training Loss: tensor(0.3243)\n",
      "49787 Training Loss: tensor(0.3237)\n",
      "49788 Training Loss: tensor(0.3249)\n",
      "49789 Training Loss: tensor(0.3241)\n",
      "49790 Training Loss: tensor(0.3241)\n",
      "49791 Training Loss: tensor(0.3252)\n",
      "49792 Training Loss: tensor(0.3252)\n",
      "49793 Training Loss: tensor(0.3245)\n",
      "49794 Training Loss: tensor(0.3241)\n",
      "49795 Training Loss: tensor(0.3243)\n",
      "49796 Training Loss: tensor(0.3258)\n",
      "49797 Training Loss: tensor(0.3252)\n",
      "49798 Training Loss: tensor(0.3240)\n",
      "49799 Training Loss: tensor(0.3247)\n",
      "49800 Training Loss: tensor(0.3243)\n",
      "49801 Training Loss: tensor(0.3238)\n",
      "49802 Training Loss: tensor(0.3243)\n",
      "49803 Training Loss: tensor(0.3244)\n",
      "49804 Training Loss: tensor(0.3242)\n",
      "49805 Training Loss: tensor(0.3244)\n",
      "49806 Training Loss: tensor(0.3236)\n",
      "49807 Training Loss: tensor(0.3238)\n",
      "49808 Training Loss: tensor(0.3245)\n",
      "49809 Training Loss: tensor(0.3246)\n",
      "49810 Training Loss: tensor(0.3248)\n",
      "49811 Training Loss: tensor(0.3247)\n",
      "49812 Training Loss: tensor(0.3243)\n",
      "49813 Training Loss: tensor(0.3253)\n",
      "49814 Training Loss: tensor(0.3245)\n",
      "49815 Training Loss: tensor(0.3251)\n",
      "49816 Training Loss: tensor(0.3239)\n",
      "49817 Training Loss: tensor(0.3259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49818 Training Loss: tensor(0.3243)\n",
      "49819 Training Loss: tensor(0.3243)\n",
      "49820 Training Loss: tensor(0.3234)\n",
      "49821 Training Loss: tensor(0.3248)\n",
      "49822 Training Loss: tensor(0.3240)\n",
      "49823 Training Loss: tensor(0.3238)\n",
      "49824 Training Loss: tensor(0.3243)\n",
      "49825 Training Loss: tensor(0.3236)\n",
      "49826 Training Loss: tensor(0.3241)\n",
      "49827 Training Loss: tensor(0.3236)\n",
      "49828 Training Loss: tensor(0.3248)\n",
      "49829 Training Loss: tensor(0.3240)\n",
      "49830 Training Loss: tensor(0.3243)\n",
      "49831 Training Loss: tensor(0.3250)\n",
      "49832 Training Loss: tensor(0.3247)\n",
      "49833 Training Loss: tensor(0.3240)\n",
      "49834 Training Loss: tensor(0.3244)\n",
      "49835 Training Loss: tensor(0.3243)\n",
      "49836 Training Loss: tensor(0.3239)\n",
      "49837 Training Loss: tensor(0.3239)\n",
      "49838 Training Loss: tensor(0.3241)\n",
      "49839 Training Loss: tensor(0.3233)\n",
      "49840 Training Loss: tensor(0.3254)\n",
      "49841 Training Loss: tensor(0.3241)\n",
      "49842 Training Loss: tensor(0.3244)\n",
      "49843 Training Loss: tensor(0.3237)\n",
      "49844 Training Loss: tensor(0.3240)\n",
      "49845 Training Loss: tensor(0.3238)\n",
      "49846 Training Loss: tensor(0.3245)\n",
      "49847 Training Loss: tensor(0.3242)\n",
      "49848 Training Loss: tensor(0.3250)\n",
      "49849 Training Loss: tensor(0.3238)\n",
      "49850 Training Loss: tensor(0.3237)\n",
      "49851 Training Loss: tensor(0.3241)\n",
      "49852 Training Loss: tensor(0.3245)\n",
      "49853 Training Loss: tensor(0.3236)\n",
      "49854 Training Loss: tensor(0.3240)\n",
      "49855 Training Loss: tensor(0.3251)\n",
      "49856 Training Loss: tensor(0.3255)\n",
      "49857 Training Loss: tensor(0.3242)\n",
      "49858 Training Loss: tensor(0.3253)\n",
      "49859 Training Loss: tensor(0.3240)\n",
      "49860 Training Loss: tensor(0.3241)\n",
      "49861 Training Loss: tensor(0.3245)\n",
      "49862 Training Loss: tensor(0.3246)\n",
      "49863 Training Loss: tensor(0.3240)\n",
      "49864 Training Loss: tensor(0.3242)\n",
      "49865 Training Loss: tensor(0.3241)\n",
      "49866 Training Loss: tensor(0.3243)\n",
      "49867 Training Loss: tensor(0.3237)\n",
      "49868 Training Loss: tensor(0.3240)\n",
      "49869 Training Loss: tensor(0.3242)\n",
      "49870 Training Loss: tensor(0.3249)\n",
      "49871 Training Loss: tensor(0.3247)\n",
      "49872 Training Loss: tensor(0.3238)\n",
      "49873 Training Loss: tensor(0.3248)\n",
      "49874 Training Loss: tensor(0.3250)\n",
      "49875 Training Loss: tensor(0.3240)\n",
      "49876 Training Loss: tensor(0.3244)\n",
      "49877 Training Loss: tensor(0.3246)\n",
      "49878 Training Loss: tensor(0.3243)\n",
      "49879 Training Loss: tensor(0.3244)\n",
      "49880 Training Loss: tensor(0.3245)\n",
      "49881 Training Loss: tensor(0.3239)\n",
      "49882 Training Loss: tensor(0.3248)\n",
      "49883 Training Loss: tensor(0.3243)\n",
      "49884 Training Loss: tensor(0.3246)\n",
      "49885 Training Loss: tensor(0.3242)\n",
      "49886 Training Loss: tensor(0.3248)\n",
      "49887 Training Loss: tensor(0.3238)\n",
      "49888 Training Loss: tensor(0.3257)\n",
      "49889 Training Loss: tensor(0.3247)\n",
      "49890 Training Loss: tensor(0.3241)\n",
      "49891 Training Loss: tensor(0.3238)\n",
      "49892 Training Loss: tensor(0.3241)\n",
      "49893 Training Loss: tensor(0.3247)\n",
      "49894 Training Loss: tensor(0.3242)\n",
      "49895 Training Loss: tensor(0.3246)\n",
      "49896 Training Loss: tensor(0.3243)\n",
      "49897 Training Loss: tensor(0.3244)\n",
      "49898 Training Loss: tensor(0.3250)\n",
      "49899 Training Loss: tensor(0.3248)\n",
      "49900 Training Loss: tensor(0.3239)\n",
      "49901 Training Loss: tensor(0.3236)\n",
      "49902 Training Loss: tensor(0.3237)\n",
      "49903 Training Loss: tensor(0.3239)\n",
      "49904 Training Loss: tensor(0.3246)\n",
      "49905 Training Loss: tensor(0.3241)\n",
      "49906 Training Loss: tensor(0.3247)\n",
      "49907 Training Loss: tensor(0.3240)\n",
      "49908 Training Loss: tensor(0.3250)\n",
      "49909 Training Loss: tensor(0.3243)\n",
      "49910 Training Loss: tensor(0.3244)\n",
      "49911 Training Loss: tensor(0.3249)\n",
      "49912 Training Loss: tensor(0.3243)\n",
      "49913 Training Loss: tensor(0.3237)\n",
      "49914 Training Loss: tensor(0.3242)\n",
      "49915 Training Loss: tensor(0.3241)\n",
      "49916 Training Loss: tensor(0.3251)\n",
      "49917 Training Loss: tensor(0.3241)\n",
      "49918 Training Loss: tensor(0.3254)\n",
      "49919 Training Loss: tensor(0.3240)\n",
      "49920 Training Loss: tensor(0.3244)\n",
      "49921 Training Loss: tensor(0.3241)\n",
      "49922 Training Loss: tensor(0.3244)\n",
      "49923 Training Loss: tensor(0.3240)\n",
      "49924 Training Loss: tensor(0.3239)\n",
      "49925 Training Loss: tensor(0.3253)\n",
      "49926 Training Loss: tensor(0.3245)\n",
      "49927 Training Loss: tensor(0.3243)\n",
      "49928 Training Loss: tensor(0.3248)\n",
      "49929 Training Loss: tensor(0.3236)\n",
      "49930 Training Loss: tensor(0.3241)\n",
      "49931 Training Loss: tensor(0.3242)\n",
      "49932 Training Loss: tensor(0.3238)\n",
      "49933 Training Loss: tensor(0.3242)\n",
      "49934 Training Loss: tensor(0.3246)\n",
      "49935 Training Loss: tensor(0.3241)\n",
      "49936 Training Loss: tensor(0.3235)\n",
      "49937 Training Loss: tensor(0.3238)\n",
      "49938 Training Loss: tensor(0.3244)\n",
      "49939 Training Loss: tensor(0.3242)\n",
      "49940 Training Loss: tensor(0.3247)\n",
      "49941 Training Loss: tensor(0.3259)\n",
      "49942 Training Loss: tensor(0.3237)\n",
      "49943 Training Loss: tensor(0.3237)\n",
      "49944 Training Loss: tensor(0.3245)\n",
      "49945 Training Loss: tensor(0.3243)\n",
      "49946 Training Loss: tensor(0.3244)\n",
      "49947 Training Loss: tensor(0.3238)\n",
      "49948 Training Loss: tensor(0.3239)\n",
      "49949 Training Loss: tensor(0.3254)\n",
      "49950 Training Loss: tensor(0.3242)\n",
      "49951 Training Loss: tensor(0.3258)\n",
      "49952 Training Loss: tensor(0.3240)\n",
      "49953 Training Loss: tensor(0.3240)\n",
      "49954 Training Loss: tensor(0.3243)\n",
      "49955 Training Loss: tensor(0.3253)\n",
      "49956 Training Loss: tensor(0.3250)\n",
      "49957 Training Loss: tensor(0.3241)\n",
      "49958 Training Loss: tensor(0.3241)\n",
      "49959 Training Loss: tensor(0.3252)\n",
      "49960 Training Loss: tensor(0.3243)\n",
      "49961 Training Loss: tensor(0.3239)\n",
      "49962 Training Loss: tensor(0.3243)\n",
      "49963 Training Loss: tensor(0.3250)\n",
      "49964 Training Loss: tensor(0.3243)\n",
      "49965 Training Loss: tensor(0.3253)\n",
      "49966 Training Loss: tensor(0.3242)\n",
      "49967 Training Loss: tensor(0.3238)\n",
      "49968 Training Loss: tensor(0.3243)\n",
      "49969 Training Loss: tensor(0.3250)\n",
      "49970 Training Loss: tensor(0.3245)\n",
      "49971 Training Loss: tensor(0.3245)\n",
      "49972 Training Loss: tensor(0.3246)\n",
      "49973 Training Loss: tensor(0.3239)\n",
      "49974 Training Loss: tensor(0.3242)\n",
      "49975 Training Loss: tensor(0.3244)\n",
      "49976 Training Loss: tensor(0.3241)\n",
      "49977 Training Loss: tensor(0.3241)\n",
      "49978 Training Loss: tensor(0.3249)\n",
      "49979 Training Loss: tensor(0.3248)\n",
      "49980 Training Loss: tensor(0.3240)\n",
      "49981 Training Loss: tensor(0.3244)\n",
      "49982 Training Loss: tensor(0.3239)\n",
      "49983 Training Loss: tensor(0.3241)\n",
      "49984 Training Loss: tensor(0.3249)\n",
      "49985 Training Loss: tensor(0.3237)\n",
      "49986 Training Loss: tensor(0.3242)\n",
      "49987 Training Loss: tensor(0.3240)\n",
      "49988 Training Loss: tensor(0.3237)\n",
      "49989 Training Loss: tensor(0.3249)\n",
      "49990 Training Loss: tensor(0.3243)\n",
      "49991 Training Loss: tensor(0.3242)\n",
      "49992 Training Loss: tensor(0.3240)\n",
      "49993 Training Loss: tensor(0.3242)\n",
      "49994 Training Loss: tensor(0.3241)\n",
      "49995 Training Loss: tensor(0.3243)\n",
      "49996 Training Loss: tensor(0.3241)\n",
      "49997 Training Loss: tensor(0.3269)\n",
      "49998 Training Loss: tensor(0.3247)\n",
      "49999 Training Loss: tensor(0.3243)\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of iterations\n",
    "num_epochs = 50000\n",
    "#prev_val_loss = 9999999.0 \n",
    "\n",
    "# Iterating through epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Resetting gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss calculation based on boundary conditions\n",
    "    input_x_bc = Variable(x_bc.float(), requires_grad=False).to(device)\n",
    "    #input_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "    input_t_bc = Variable(t_bc.float(), requires_grad=False).to(device)\n",
    "    #input_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    \n",
    "    target_u_bc = Variable(u_bc.float(), requires_grad=False).to(device)\n",
    "    #target_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "    target_u_initial1 = Variable(u_initial1.float(), requires_grad=False).to(device)\n",
    "    target_u_initial2 = Variable(u_initial2.float(), requires_grad=False).to(device)\n",
    "\n",
    "    # Getting network output for boundary condition and initial conditions\n",
    "    output_bc = net(input_x_bc, input_t_bc)\n",
    "    output_initial1 = net(input_t_bc, target_u_initial1)\n",
    "    output_initial2 = net(input_t_bc, target_u_initial2)\n",
    "    \n",
    "    mse_u = mse_cost_function(output_bc, target_u_bc)\n",
    "    mse_initial1 = mse_cost_function(u_initial1, target_u_initial1)\n",
    "    mse_initial2 = mse_cost_function(u_initial2, target_u_initial2)\n",
    "    # Loss calculation based on partial differential equation (PDE) \n",
    "    collocation_x = np.random.uniform(low= -2.0, high=2.0, size=(500, 1)) \n",
    "    collocation_t = np.random.uniform(low=0.0, high=1.0, size=(500, 1))\n",
    "    all_zeros_target = np.zeros((500, 1))\n",
    "    \n",
    "    input_x_collocation = Variable(torch.from_numpy(collocation_x).float(), requires_grad=True).to(device)\n",
    "    input_t_collocation = Variable(torch.from_numpy(collocation_t).float(), requires_grad=True).to(device)\n",
    "    target_all_zeros = Variable(torch.from_numpy(all_zeros_target).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    # Getting network output for PDE\n",
    "    output_f = pde_loss(input_x_collocation, input_t_collocation, net)\n",
    "    mse_f = mse_cost_function(output_f, target_all_zeros)\n",
    "    \n",
    "    # Combining the loss functions\n",
    "    total_loss = mse_u + mse_f + mse_initial1 + mse_initial2\n",
    "    \n",
    "    # Backward propagation for computing gradients\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Optimizer step to update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch, \"Training Loss:\", total_loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a3815",
   "metadata": {},
   "source": [
    "### 3.6.2 Visualizing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "087f2b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the results\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Creating meshgrid for visualization\n",
    "x_visual = np.arange(-2.5 , 2.5, 0.002)\n",
    "t_visual = np.arange(0, 2, 0.002)\n",
    "ms_x_visual, ms_t_visual = np.meshgrid(x_visual, t_visual)\n",
    "## np.ravel returns a flattened and contiguous 1-D array containing the elements of the input.\n",
    "## A copy is created only if necessary.\n",
    "x_visual = np.ravel(ms_x_visual).reshape(-1, 1)\n",
    "t_visual = np.ravel(ms_t_visual).reshape(-1, 1)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "pt_x_visual = Variable(torch.from_numpy(x_visual).float(), requires_grad=True).to(device)\n",
    "pt_t_visual = Variable(torch.from_numpy(t_visual).float(), requires_grad=True).to(device)\n",
    "\n",
    "# Obtaining network predictions\n",
    "## In the function net(pt_x_visualization, pt_t_visualization), the training process updates the parameters represented by theta.\n",
    "pt_u_visual = net(pt_x_visual, pt_t_visual)\n",
    "u_visual = pt_u_visual.data.cpu().numpy()\n",
    "ms_u_visual = u_visual.reshape(ms_x_visual.shape)\n",
    "\n",
    "# Plotting the surface\n",
    "surf = ax.plot_surface(ms_x_visual, ms_t_visual, ms_u_visual, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.0003f'))\n",
    "ax.set_xlabel(\"x_axis\")\n",
    "ax.set_ylabel(\"t_axis\")\n",
    "\n",
    "\n",
    "# Adding colorbar for reference\n",
    "fig.colorbar(surf, shrink=1, aspect=5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130a201",
   "metadata": {},
   "source": [
    "### 3.6.3 Comparing Training set with Actual Solution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fc60368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Parameter setting\n",
    "terminal_time = 1  # terminal time\n",
    "num_spatial_values = 80  # spatial values for FEM\n",
    "num_time_values = 1600  # time values/hops; (time_values ~ spatial_values^2)\n",
    "s_value = 0.25  # s_value := k * ( (dt) / (dx)^2 )\n",
    "\n",
    "# Uniform mesh\n",
    "x_values = np.linspace(-2, 2, num_spatial_values)\n",
    "\n",
    "# Time discretization\n",
    "time_values = np.linspace(0, terminal_time, num_time_values)\n",
    "\n",
    "# Definition of the solution u(x, t) to u_t = k * u_xx\n",
    "u_values = np.zeros((num_spatial_values, num_time_values + 1))  # array to store values of the solution\n",
    "\n",
    "# Finite Difference Scheme:\n",
    "\n",
    "u_values[:, 0] = np.sin(np.pi * x_values)  # x_values * (x_values - 1)  # initial condition\n",
    "\n",
    "for m in range(num_time_values):\n",
    "    for j in range(1, num_spatial_values - 1):\n",
    "        if j == 1 or j == num_spatial_values - 1:\n",
    "            u_values[j, m] = 0  ## Boundary condition\n",
    "        else:\n",
    "            u_values[j, m + 1] = u_values[j, m] + s_value * (\n",
    "                    u_values[j + 1, m] - 2 * u_values[j, m] + u_values[j - 1, m]) ##Inner domain values\n",
    "\n",
    "# Plot 3D surface\n",
    "T, X = np.meshgrid(time_values, x_values)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim3d(left=-2, right=2)\n",
    "ax.set_ylim3d(bottom=0, top=1)\n",
    "ax.set_zlim3d(bottom=-1, top=1)\n",
    "surf = ax.plot_surface(X, T, u_values[:, :1600], rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83d30e",
   "metadata": {},
   "source": [
    "### 3.6.4 Result\n",
    "For the training set case with 50000 iteration, we easily note that the graph shape looks more similar to the actual one, than the case with 20000 iteration. The PINN (Physics-Informed Neural Network) method has the potential to improve the error bound as much as possible by increasing the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477507c8",
   "metadata": {},
   "source": [
    "# 4. Reference\n",
    "\n",
    "Files of tutorial :\n",
    "\n",
    "[1] https://github.com/nanditadoloi/PINN/blob/main/solve_PDE_NN.ipynb\n",
    "\n",
    "[2] https://services.math.duke.edu/education/baltimore/sample.html\n",
    "\n",
    "[3] 'TwoDNSCH.py' file by Kevin\n",
    "\n",
    "Paper :\n",
    "\n",
    "[1] 'Proposal for quantitative benchmark\n",
    "computations of bubble dynamics' by S. Hysing, S. Turek, D. Kuzmin, N. Parolini, E. Burman, S. Ganesank, and L. Tobiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bd43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
